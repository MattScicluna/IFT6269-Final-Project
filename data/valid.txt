   := \underbrace{\L(\xt,\y^*) -\L^*}_{:=  w_t^{(x)}} + \underbrace{\L^* - \L(\x^*,\yt)}_{:=w_t^{(y)}}.
\end{equation}
%
%
%
%
%
%
%
%
Notice that $ w_t^{(x)}$ and $w_t^{(y)}$ are non-negative, and that $w_t \leq h_t$ since:
\[
  \L(\xt,\ytm) - \L(\xtm,\yt) \geq \L(\xt,\y^*) - \L(\x^*,\yt).
\]
In general, $w_t$ can be zero even if we have not reached a solution.
For example, with $\L(\x,\y) = \x \cdot \y$ and $\X = \Y = [-1,1]$, then $\x^* = \y^* = \0$, implying $w_t = 0$ for any $(\x^{(t)},\y^{(t)})$. 
But for a uniformly strongly convex-concave~$\L$, this cannot happen and we can prove that~$w_t$ has the following nice property (akin to $\|\x - \x^*\| \leq \sqrt{\nicefrac{2}{\mu} (f(\x) - f(\x^*))}$ for a $\mu$-strongly convex function~$f$; see Proposition~\ref{prop:relation_primal} in Appendix~\ref{sub:relation_between_the_primal_errors}):
\begin{equation} \label{eq:relate_h_w}
  h_t \leq \sqrt{2} P_\L \sqrt{w_t} \, , \vspace{-1mm} 
\end{equation}
where 
%
%
%
%
%
\begin{equation}
  \!\!\! P_\L \leq \sqrt{2} \underset{\z \in \M}{\sup} \left\{\frac{\|\nabla_x \L(\z)\|_{\X^*}}{\sqrt{\mu_\X}} ,\frac{\|\nabla_y \L(\z)\|_{\Y^*}}{\sqrt{\mu_\Y}}   \right\} .
\end{equation}
%
\paragraph{Pyramidal width and distance to the border.} %
\label{par:pyramidal_width}
We now provide a theorem that establishes convergence in two situations: \eqref{enu:situation1} when the SP belongs to the interior of $\M$; \eqref{enu:situation2} when the set is a polytope, i.e. when there exist two finite sets such that $\X = \conv(\mathcal A)$ and $\Y = \conv(\mathcal B)$).
Our convergence result holds when (roughly) the strong convex-concavity of~$\L$ is big enough in comparison to the cross Lipschitz constants $L_{XY}$, $L_{YX}$ of $\nabla \L$ (defined in~\eqref{eq:cross-lip} below) multiplied by geometric ``condition numbers'' of each set. The condition number of $\X$ (and similarly for $\Y$) is defined as the ratio of its \emph{diameter} $D_\X : = \sup_{\x, \x' \in \X} \|\x -\x'\|$ over the following appropriate notions of ``width'':
\begin{align} 
\hspace{-7mm}\text{\small border distance: }\, \delta_\X &:= \min_{\s \in \partial  \X} \|\x^* -\s\|  \!\!\! &&\text{for \eqref{enu:situation1},} \hspace{-2mm} \\
\hspace{-7mm}\text{\small pyramidal width: }\, \delta_{\mathcal A} &:=\PWidth(\mathcal A)  &&\text{for \eqref{enu:situation2}.} & \label{eq:PwidthL} \hspace{-2mm}
\end{align}
The pyramidal width~\eqref{eq:PwidthL} is formally defined in~Eq.~9 of~\citet{lacoste2015global} and in  Appendix~\ref{sub:affine_invariant_measures_of_strong_convexity}.
Given the above constants, we can state below a non-affine invariant version of our convergence theorem (for simplicity). The affine invariant versions of this theorem are given in Thm.~\ref{thm:conv_affine_invariant} and~\ref{thm:conv_sublin} in Appendix~\ref{subsec:proof_main_thm} (with proofs).

\begin{theorem}\label{thm:conv}
  Let $\L$ be a convex-concave function and $\M$ a convex and compact set. 
  Assume that the gradient of  $\L$ is $L$-Lipschitz continuous, that $\L$ is $(\mu_\X,\mu_\Y)$-strongly convex-concave, and that we are in one of the two following situations:
  \leqnomode %
  \begin{align*}
  &\parbox{7cm}{The SP belongs to the interior of $\M$. In this case, set $\gap = \gap^{\FW}$ (as in L\ref{algLine:gapFW} of Alg.~\ref{alg:AFW}), $\delta_\mu:= \sqrt{\min(\mu_\X\delta_\X^2,\mu_\Y\delta_\Y^2)}$ and $a :=  1 $. ``Algorithm'' then refers to SP-FW.
    }
    \label{enu:situation1} 
    \tag{I}
      \\[2mm]
    &\parbox{7cm}{The sets $\X$ and $\Y$ are polytopes. In this case, set $\gap = \gap^{\PW}$ (as in L\ref{algLine:gapPFW} of Alg.~\ref{alg:AFW}), $\delta_\mu := \sqrt{\min(\mu_\X \delta_{\mathcal A}^2, \mu_\Y \delta_{\mathcal B}^2)}$ and $a:=  \frac{1}{2}$. ``Algorithm'' then refers to SP-AFW. Here $\delta_\mu$ needs to use the Euclidean norm for its defining constants.
    }
    \label{enu:situation2} 
    \tag{P}
  \end{align*}
  \reqnomode
  In both cases, if $\CondNumb:= a -  \tfrac{\sqrt{2}}{\delta_\mu}\max\left\{ \tfrac{D_\X L_{XY}}{\sqrt{\mu_\Y}}, \tfrac{D_\Y L_{YX}}{\sqrt{\mu_\X}}\right\}$ is positive, then the errors $h_t$~\eqref{def:subopt} of the iterates of the algorithm with step size
  $\gamma_t = \min\{\stepmax, \frac\CondNumb{2C}\gap\}$ decrease
  geometrically as 
  \begin{equation*} 
    h_t = O \left( (1- \rho)^{\frac{k(t)}{2}} \right) \: \text{ and }
 \: \min_{s\leq t} g_s^\FW = O \left( (1- \rho)^{\frac{k(t)}{2}} \right)
  \end{equation*} 
  where
  $\rho := \CondNumb^2\frac{\delta_\mu^2}{2C}$, $C := \tfrac{L D_\X^2+ L D_\Y^2}{2}$ and $k(t)$ is the number of non-drop step after $t$ steps (see L\ref{algLine:drop_step} in Alg.~\ref{alg:AFW}). In case~\eqref{enu:situation1} we have $k(t)=t$ and in case~\eqref{enu:situation2} we have $k(t)\geq t/3$.
  For both algorithms, if $\delta_\mu >2\max\left\{ \tfrac{D_\X L_{XY}}{\mu_\X}, \tfrac{D_\Y L_{YX}}{\mu_\Y}\right\}$, we also obtain a sublinear rate with the universal choice $\gamma_t =\min\{\stepmax, \frac{2}{2+k(t)} \}$. This yields the rates:
  \vspace{-3mm}
  \begin{equation}
         \min_{s\leq t} h_s \leq  \min_{s\leq t} g_s^\FW = O\left( \frac{1}{t} \right).
    \end{equation}
\end{theorem}

Clearly, the sublinear rate seems less interesting than the linear one but has the added convenience that the step size can be set without knowledge of various constants that characterize $\L$.  Moreover, it provides a partial answer to the conjecture from~\citet{hammond1984solving}.

%
%
%



\paragraph{Proof sketch.} %
\label{par:proof_sketch}

%

%
Strong convexity is an essential assumption in our proof; it allows us to relate~$w_t$ to how close we are to the optimum. 
Actually, by $\mu_\Y$-strong concavity of $\L(\x^*,\cdot)$, we have
\begingroup
\setlength{\thinmuskip}{0.5mu}
\setlength{\medmuskip}{0mu}
\setlength{\thickmuskip}{0.5mu}
\begin{equation}\label{eq:close_y_w}
   \| \yt - \y^*\| \leq \sqrt{\frac{2}{\mu_\Y} \left( \L^*  - \L(\x^*,\yt)\right)} = \sqrt{\frac{2}{\mu_\Y} w_t^{(y)}}.
 \end{equation}
\endgroup
Now, recall that we assumed that $\nabla \L$ is Lipschitz continuous. In the following, we will call $L$ the {\em Lipschitz continuity constant} of $\nabla \L$ and $L_{XY}$ and $L_{YX}$ its (cross) {\em  partial Lipschitz constants}. For all $\x, \, \x' \in \X, \;\y, \, \y'  \in \Y$, these constants satisfy
\begin{equation}
\begin{aligned}\label{eq:cross-lip}
   \!\! \|\nabla_x\L(\x,\y) - \nabla_x\L(\x,\y')\|_{\X^*} \leq L_{XY} \|\y- \y'\|_\Y,\\
   \!\! \|\nabla_y\L(\x,\y) - \nabla_y\L(\x',\y)\|_{\Y^*} \leq L_{YX} \|\x- \x'\|_\X.
\end{aligned}
\end{equation}
Note that $L_{XY},L_{YX}\leq L$ if $\|(\x,\y)\| := \|\x\|_\X + \|\y\|_\Y$.
Then, using Lipschitz continuity of the gradient,
%
%
%
%
%
\begin{align}
 \L(\xtt,\y^*)  & \leq  \L(\xt,\y^*)  +  \stepsize \innerProdCompressed{\dt_x}{\nabla_x \L(\xt,\y^*)} \notag \\
                  & \quad+ \stepsize^2 \frac{L \|\dt_x\|^2}{2}.
\end{align}
%
Furthermore, setting $(\x,\y) = (\xt, \y^*)$ and $\y'= \yt$ in Equation~\eqref{eq:cross-lip}, we have
\begin{equation}
\begin{aligned}\label{eq:intermediary_scheme}
 w_{t+1}^{(x)} & \leq w_t^{(x)} 
                  - \stepsize g_t^{(x)} 
                  + \stepsize D_\X L_{XY} \|\yt - \y^*\| \\
                 & \quad + \stepsize^2 \frac{L D_\X^2}{2} \, .
\end{aligned}
\end{equation}
Finally, combining \eqref{eq:intermediary_scheme} and \eqref{eq:close_y_w}, we get
\begin{equation}
 \begin{aligned}\label{eq:intermediary_scheme_2}
   w_{t+1}^{(x)} & \leq w_t^{(x)} 
                  - \stepsize g_t^{(x)} 
                  + \stepsize D_\X L_{XY}\sqrt{\frac{2}{\mu_\Y}} \sqrt{w_t^{(y)}} \\
                  & \quad + \stepsize^2 \frac{L D_\X^2}{2}.
 \end{aligned}
\end{equation}
A similar argument on $-\L(\x^*, \ytt)$ gives a bound on $w_t^{(y)}$ much like~\eqref{eq:intermediary_scheme_2}. Summing both yields:
\begin{align}
    w_{t+1} &\leq w_t - \stepsize g_t + 2\stepsize \max\left\{\tfrac{D_\X L_{XY}}{\sqrt{\mu_\Y}},\tfrac{D_\Y L_{YX}}{\sqrt{\mu_\X}}\right\} \sqrt{w_t}  \notag \\
            & \quad + \stepsize^2 \frac{L D_\X^2+ L D_\Y^2}{2}.
\end{align}
We now apply recent developments in the convergence theory of FW methods for strongly convex objectives. \citet{lacoste2015global} crucially upper bound the square root of the suboptimality error on a convex function with the FW gap if the optimum is in the interior, or with the PFW gap if the set is a polytope (Lemma~\ref{lemme:lingap} in Appendix~\ref{app:gapInequalities}). We continue our proof sketch for case~\eqref{enu:situation1} only:\footnote{The idea is similar for case~\eqref{enu:situation2}, but with the additional complication of possible drop steps.}
\begin{equation}
\begin{aligned}
 \label{eq:def_delta_x}
  & {2 \mu_\X \delta_\X^2}\left(\L(\xt,\yt)-\L(\x^*,\yt)\right) \leq \left(\gap^{(x)}\right)^2  \\
  &\qquad \text{where} \quad \delta_\X :=  \min_{\s \in \partial  \X} \|\x^* -\s\|.
 \end{aligned}
\end{equation}
  We can also get the respective equation on $\y$ with $\delta_\Y :=  \min_{\y \in \partial  \Y} \|\y^* -\y\|$ and sum it with the previous one \eqref{eq:def_delta_x} to get:
  \vspace{-1mm}
\begin{equation}\label{eq:def_delta}
     \delta_\mu \sqrt{2w_t} \leq \gap
     \;\, \text{where} \;\,
      \delta_\mu := \sqrt{\min(\mu_\X\delta_\X^2,\mu_\Y\delta_\Y^2)}.
\end{equation}
%
%
%
%
%
%
%
%
%
%
%
%
Plugging this last equation into \eqref{eq:intermediary_scheme_2} gives us 
\begingroup
\setlength{\thinmuskip}{2mu}
\setlength{\medmuskip}{2mu}
\setlength{\thickmuskip}{2mu}
\begin{equation}
\begin{aligned}\label{eq:cst}
    w_{t+1} & \leq w_t 
                  - \CondNumb\stepsize g_t 
                  + \stepsize^2 {C}
                  \;\;\;\; \text{where} \;\;\;\;  C := \tfrac{ L D_\X^2+ L D_\Y^2}{2}
                  \\
    &\text{and}  \;\;\;\; \CondNumb  := 1-  \tfrac{\sqrt{2}}{\delta_\mu}\max\left\{ \tfrac{D_\X L_{XY}}{\sqrt{\mu_\Y}}, \tfrac{D_\Y L_{YX}}{\sqrt{\mu_\X}}\right\}.
\end{aligned} 
\end{equation}
 \endgroup
%
 %
 %
 %
 %
 %
 %
 %
 %
The recurrence~\eqref{eq:cst} is typical in the FW literature. We can re-apply standard techniques on the sequence~$w_t$ to get a sublinear rate with $\stepsize_t = \frac{2}{2+t}$, or a linear rate with $\stepsize_t= \min\left\{ \stepmax, \frac{\CondNumb g_t}{2C} \right\}$ (which minimizes the RHS of~\eqref{eq:cst} and actually guarantees that $w_t$ will be \emph{decreasing}). Finally, thanks to strong convexity, a rate on $w_t$ gives us a rate on $h_t$ (by~\eqref{eq:relate_h_w}). \qed
  %
%
%
%






%






%
%
%


%
%

\section{SP-FW with strongly convex sets}
\label{sec:strongly_set}
  \paragraph{Strongly convex set.} %
  \label{par:strongly_convex_set}
  One can (roughly) define strongly convex sets as sublevel sets of strongly convex functions~\citep[Prop.~4.14]{vial1983strong}. In this section, we replace the strong convex-concavity assumption on~$\L$ with the assumption that $\X$ and $\Y$ are $\beta$-strongly convex \emph{sets}.
  \begin{definition}[\citet{vial1983strong,polyak1966existence}]\label{def:strong_set}
    A convex set $\X$ is said to be $\beta$-strongly convex with respect to~$\|.\|$ 
    if for any $\x,\y \in \X$ and any $\gamma \in [0,1]$, $B_\beta(\gamma,\x,\y) \subset \X$ where $B_\beta(\gamma,\x,\y)$ is the $\|.\|$-ball of radius ${\gamma(1 - \gamma) \frac{\beta}{2}\|\x-\y\|^2}$ centered at $\gamma \x + (1- \gamma)\y$.
    %
    %
  \end{definition}

  Frank-Wolfe for convex optimization over strongly convex sets has been studied by
  \citet{levitin1966constrained,demyanov1970approximate} and \citet{dunn1979rates}, amongst others. 
  They all obtained a linear rate for the FW algorithm if the norm of the gradient is lower bounded by a constant. 
  More recently, \citet{garber2014faster} proved a sublinear rate $O(1/t^2)$ by replacing the lower bound on the gradient by a strong convexity assumption on the function.
  In the VIP setting \eqref{eq:VIP}, the linear convergence has been proved if the optimization is done under a strongly convex set but this assumption does \emph{not} extend to $\X \times \Y$ which \emph{cannot} be strongly convex if $\X$ or $\Y$ is not reduced to a single element. 
  In order to prove the convergence, we first prove the Lipschitz continuity of the \emph{FW-corner} function $\s(\cdot)$ defined below. 
  A proof of this theorem is given in Appendix \ref{sec:strong_conv_proof}.
  \begin{theorem}\label{thm:s_lip}
  %
   Let $\X$ and $\Y$ be $\beta$-strongly convex sets. 
   If $\min(\|\nabla_{\!x} L(\z)\|_{\X^*}, \|\nabla_{\!y} L(\z)\|_{\Y^*}) \geq\delta>0$ for all $\z \in \M$, then the oracle function $\z \mapsto \s(\z) := \arg\min_{\s \in \M}\innerProd{\s}{\FF(\z)}$ is well defined and is $\frac{4L}{\delta \beta }$-Lipschitz continuous (using the norm $\|(\x,\y)\|_{\X \times \Y} := \|\x\|_\X + \|\y\|_\Y$), where $\FF(\z) := \left(
     \nabla_x \L(\z),
     -\nabla_y \L(\z)
     \right)$.
  \end{theorem}

  \paragraph{Convergence rate.} %
  \label{par:convergence_rate}
  When the FW-corner function~$\s(\cdot)$ is Lipschitz continuous (by Theorem~\ref{thm:s_lip}), we can actually show that the FW gap is decreasing in the FW direction and get a similar inequality as the standard FW one~\eqref{eq:FWstandard}, but, in this case, on the \emph{gaps}: $g_{t+1} \leq \gap (1- \gamma_t) + \gamma_t^2 \|\st - \zt \|^2 C_\delta$. 
  Moreover, one can show that the FW gap on a strongly convex set~$\X$ can be lower-bounded by $\|\s_x^{(t)} - \x^{(t)} \|^2$ (Lemma~\ref{lemma:lower_gap} in Appendix~\ref{sec:strong_conv_proof}), by using the fact that~$\X$ contains a ball of sufficient radius around the midpoint between~$\s_x^{(t)}$ and~$ \x^{(t)}$. 
  From these two facts, we can prove the following linear rate of convergence (\emph{not} requiring any \emph{strong} convex-concavity of~$\L$). 
  %
  %
  %
  %
  %
    \begin{theorem}\label{thm:conv_strong}
     Let $\L$ be a convex-concave function and $\X$ and $\Y$ two compact $\beta$-strongly convex sets. 
     Assume that the gradient of  $\L$ is $L$-Lipschitz continuous and that there exists $\delta>0$ such that $\min(\|\nabla_{\!x} L(\z)\|_*, \|\nabla_{\!y} L(\z)\|_*) \geq \delta \;\, \forall \z \in \M$. Set $C_\delta := 2L + \frac{8L^2}{\beta \delta}$. 
     Then the gap $\gap^{\FW}$~\eqref{eq:gap} of the SP-FW algorithm with step size $\gamma_t = \tfrac{\gap^{\FW}}{\|\st-\zt\|^2 C_\delta}$
     converges linearly as $\gap^{\FW} \leq g_0 \left( 1- \rho \right)^{t}$,
    where $\rho :=  \tfrac{\beta \delta}{16 C_\delta}$.
    \end{theorem}
  %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %


    %

%
%
%







\section{SP-FW in the bilinear setting}\label{sec:bilinear}

\paragraph{Fictitious play.} %
\label{par:the_fictitious_play_}
  In her thesis, \citet[\S~4.3.1]{hammond1984solving} pointed out that for the bilinear setting:
\begin{equation}\label{eq:pb_bilin}
  \min_{\x \in \Delta_p} \max_{\y \in \Delta_q} \x^\top M \y 
\end{equation}
where $\Delta_p$ is the probability simplex on $p$ elements,
the SP-FW algorithm with step size $\gamma_t = 1/ \left( 1+t \right)$ is equivalent to the fictitious play (FP) algorithm introduced by \citet{brown1951iterative}. The FP algorithm has been widely studied in the game literature. Its convergence has been proved by \citet{robinson1951iterative}, while \citet{shapiro1958note} showed that one can deduce from Robinson's proof a $O(t^{-1/(p+q-2)})$ rate.
Around the same time, \citet{karlin1960mathematical} conjectured that the FP algorithm converged at the better rate of $O(t^{-1/2})$, though this conjecture is still open and Shapiro's rate is the only one we are aware of.
Interestingly, \citet{daskalakis2014counter} recently showed that Shapiro's rate is also a lower bound if the tie breaking rule gets the worst pick an infinite number of times.
Nevertheless, this kind of adversarial tie breaking rule does not seems realistic since this rule is a priori defined by the programmer.
In practical cases (by setting a fixed prior order for ties or picking randomly for example), Karlin's Conjecture~\citep{karlin1960mathematical} is still open. Moreover, we always observed an empirical rate of at least $O(t^{-1/2})$ during our experiments, we thus believe the conjecture to be true for realistic tie breaking rules.

%
 \paragraph{Rate for SP-FW.} %
 \label{par:Rate for SP-FW.}
 Via the affine invariance of the FW algorithm and the fact that every polytope with $p$ vertices is the affine transformation of a probability simplex of dimension $p$, any rate for the fictitious play algorithm implies a rate for SP-FW.
\begin{corollary}\label{cor:Bilin}
For polytopes $\X$ and $\Y$ with $p$ and $q$ vertices respectively and $\L(\x,\y) = \x^\top M \y$, the SP-FW algorithm with step size $\gamma_t = \frac{1}{t+1}$ converges at the rate $ h_t = O \left(t^{-\frac{1}{p+q-2}} \right).$
\end{corollary}
 %
 This (very slow) convergence rate is mainly of theoretical interest, providing a safety check that the algorithm actually converges.
 Moreover, if Karlin's strong conjecture is true, we can get a $O (1 /\sqrt{t})$ worst case rate which is confirmed by our experiments. 
  

%
%
%
%
%
%
%
%
%

%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%


    %
    %
    %

\section{Experiments}\label{sec:experiments}

\begin{figure*}
    \centering
    \begin{subfigure}[t]{0.32\linewidth}
        \centering
        \includegraphics[height = .8 \linewidth, width = 1 \linewidth]{figures/SP-FW.pdf}
        \caption{SP in the interior, $d=30$}
        \label{fig:conv_int}
    \end{subfigure}
    \;
    \begin{subfigure}[t]{0.32\linewidth}
        \centering
        \includegraphics[width = 1 \linewidth]{figures/SP-AFW2.pdf}
        \caption{$\M$ is a polytope, $d=30$}
        \label{fig:conv_geom}
    \end{subfigure}
    \;
    \begin{subfigure}[t]{0.32\linewidth}
        \centering
        \includegraphics[height = .8 \linewidth,width = 1 \linewidth]{figures/SP-AFW-heuristic.pdf}
        \caption{$\M$ polytope, $d=30$, $\CondNumb<0$}
        \label{fig:better_step_size}
    \end{subfigure}
    \\
    \begin{subfigure}[t]{0.32\linewidth}
        \centering
        \includegraphics[width = 1 \linewidth]{figures/BIG5.pdf}
        \caption{Graphical games}
        \label{fig:graphical}
    \end{subfigure} %
    \;
    \begin{subfigure}[t]{0.32\linewidth}
        \centering
        \includegraphics[height = .75 \linewidth,width = 1 \linewidth]{figures/ocr_small_beta.pdf}
        \caption{OCR dataset, $R=0.01$.}
        \label{fig:beta_small}
    \end{subfigure}
    \;
    \begin{subfigure}[t]{0.32\linewidth}
        \centering
        \includegraphics[height = .75 \linewidth, width = 1 \linewidth]{figures/ocr_large_beta.pdf}
        \caption{OCR dataset, $R=5$.}
        \label{fig:beta_big}
    \end{subfigure}
    \caption{\small On Figures~\ref{fig:conv_int}, \ref{fig:conv_geom} and~\ref{fig:better_step_size}, we plot on a semilog scale the best gap observed $\min_{s\leq t} g^\FW_s$ as a function of $t$. For experiments~\ref{fig:graphical}, \ref{fig:beta_small} and~\ref{fig:beta_big}, the objective function is bilinear and the convergence is sublinear. An effective pass is one iteration for SP-FW or the subgradient method and $n$ iterations for SP-BCFW or SSG.
    We give more details about these experiments in Appendix~\ref{sec:details_on_the_experiments}.
    }
    \vspace{-2mm}
\end{figure*}
  \paragraph{Toy experiments.} %
  \label{par:experiments_on_toy_examples}
First, we test the empirical convergence of our algorithms on a simple saddle point problem over the unit cube in dimension~$d$ (whose pyramidal width has the explicit value $1/\sqrt{d}$ by Lemma~4 from \citet{lacoste2015global}). 
Thus $\X = \Y := [0,1]^d$ and the linear minimization oracle is simply $\text{LMO}(\cdot)\! = \!-0.5 \cdot(\text{sign}(\cdot)-\bm{1})$. 
We consider the following objective function:
  \begin{equation}
    \frac{\mu}{2}\|\x- \x^*\|_2^2 + (\x-\x^*)^\top M(\y-\y^*) - \frac{\mu}{2} \|\y-\y^*\|_2^2
  \end{equation}
for which we can control the location of the saddle point $(\x^*,\y^*) \in \M$. We generate a matrix $M$ randomly as $M \sim \mathcal U([-0.1,0.1]^{d\times d})$ and keep it fixed for all experiments. For the interior point setup~\eqref{enu:situation1}, we set~$(\x^*,\y^*) \sim \mathcal U ([0.25,0.75]^{2d})$, while we set~$\x^*$ and $\y^*$ to some fixed random vertex of the unit cube for the setup~\eqref{enu:situation2}. With all these parameters fixed, the constant $\CondNumb$ is a function of $\mu$ only. We thus vary the strong convexity parameter $\mu$ to test various $\CondNumb$'s.

We verify the linear convergence expected for the SP-FW algorithm for case~\eqref{enu:situation1} in Figure~\ref{fig:conv_int}, and for the 
SP-AFW algorithm for case~\eqref{enu:situation2} in Figure~\ref{fig:conv_geom}. 
As the adaptive step size (and rate) depends linearly on~$\CondNumb$, the linear rate becomes quite slow for small $\CondNumb$. In this regime (in red), the step size $2/(2+k(t))$ (in orange) can actually perform better, despite its theoretical sublinear rate.
%

Finally, figure \ref{fig:better_step_size} shows that we can observe a linear convergence of SP-AFW even if $\CondNumb$ is negative by using a different step size. In this case, we use the heuristic adaptive step size $\gamma_t := \gap / \tilde{C}$ where 
$\tilde{C} := L D_\X^2+LD_\Y^2+ L_{XY} L_{YX}\left( D_\X^2/\mu_\X + D_\Y^2/\mu_\Y \right) $. Here $\tilde{C}$ takes into account the coupling between the concave and the convex variable and is motivated from a different proof of convergence that we were not able to complete. The empirical linear convergence in this case is not yet supported by a complete analysis, highlighting the need for more sophisticated arguments.


\paragraph{Graphical games.} We now consider a bilinear objective $\L(\x,\y) = \x^\top M\y$ where exact projections on the sets is intractable, but we have a tractable LMO. The problem is motivated from the following setup. We consider a game between 
two universities ($A$ and $B$) that are admitting $s$ students and have to assign pairs of students into dorms.
  %
If students are unhappy with their dorm assignments, they will go to the other university. The game has a payoff matrix $M$ belonging to $\R^{(s(s-1)/2)^2}$ where $M_{ij,kl}$ is the expected tuition that $B$ gets (or $A$ gives up) if $A$ pairs student $i$ with $j$ and $B$ pairs student $k$ with $l$.
  %
Here the actions $\x$ and $\y$ are both in the marginal polytope of all perfect unipartite matchings. Assume that we are given a graph $G=(V,E)$ with vertices $V$ and edges $E$. For a subset of nodes $S \subseteq V$, let the induced subgraph $G(S)=(S,E(S))$. \citet{edmonds1965} showed that any subgraph forming a triangle can contain at most one edge of any perfect matching. This forms an exponential set of linear equalities which define the matching polytope ${\cal P}(G) \subset \mathbb{R}^E $ as
\begingroup
\setlength{\thinmuskip}{1.3mu}
\setlength{\medmuskip}{1.3mu}
\setlength{\thickmuskip}{1.3mu}
\begin{equation} 
\{ \x \,| \, \x_e \geq 0, \hspace{-2.1mm} %
\sum_{e \in E(S)}\hspace{-1.7mm} \x_e \leq k, \, \forall S \subseteq V,\, { |S|=2 k+1 },\forall e \in E \}.
\end{equation}
\endgroup
While this strategy space seems daunting, the LMO can be solved in ${\cal O}(s^3)$ time using the blossom algorithm~\citep{edmonds1965}. We run the SP-FW algorithm with $\stepsize_t = \nicefrac{2}{(t+2)}$ on this problem with $s = 2^j$ students for $j=3, \ldots, 8$ with results given in Figure~\ref{fig:graphical} ($d=s(s-1)/2$ in the legend represents the dimensionality of the $\x$ and $\y$ variables). The order of the complexity of the LMO is then $O(d^{3/2})$.
In Figure~\ref{fig:graphical}, the observed empirical rate of the SP-FW algorithm (using $\stepsize_t = \nicefrac{2}{(t+2)}$) is $O(1/t^2)$. Empirically, faster rates seem to arise if the solution is at a corner (a pure equilibrium, to be expected for random payoff matrices in light of~\citep{barany2007nash}).
 %

 \paragraph{Sparse structured SVM.} %
  \label{par:structured_svm}
  We finally consider a challenging optimization problem arising from structured prediction. We consider the saddle point formulation~\citep{taskar2006dualExtrag} for a $\ell_1$-regularized structured SVM objective that minimizes the primal cost function
%
%
$  p(\bm{w}) :=
  %
    \frac{1}{n} \sum_{i=1}^n \tilde H_i(\bm{w}) 
%
$,
where $\tilde H_i(\bm{w})= \max_{\y \in \mathcal Y_i} L_i(\y) - \innerProd{\bm{w}}{\bm{\psi}_i(\y)}$ is the structured hinge loss (using the notation from~\citet{lacoste2013block}). We only assume access to the linear oracle computing $\tilde H_i(\bm{w})$. Let $M_i$ have $\big(\bm{\psi}_i(\y)\big)_{\y \in \mathcal Y_i}$ as columns.
  We can rewrite the minimization problem as a bilinear saddle point problem:
  \vspace{-2mm}
  \begin{equation}
  \begin{aligned}
   & \qquad \min_{\|\bm{w}\|_1  \leq R} \frac{1}{n} \sum_i \Big( \max_{\y_i \in \mathcal Y_i} \; \bm{L}_i^\top \y_i - \bm{w}^\top M_i \y_i \Big)  \\
   & = \min_{\|\bm{w}\|_1 \leq R} \frac{1}{n} \sum_i \Big( \max_{\bm{\alpha}_i \in \Delta(|\mathcal Y_i|)} \!\!\bm{L}_i^\top \bm{\alpha}_i - \bm{w}^\top M_i \bm{\alpha}_i \Big). \label{eq:structuredSVM}
  \end{aligned}
  \end{equation}
  Projecting onto $\Delta(|\Y_i|)$ is normally intractable as the size of $|\Y_i|$ is exponential, but the linear oracle is tractable by assumption.
  We performed experiments with 100 examples from the OCR
  dataset ($d_\omega \!= \!4028$)~\citep{taskarmax}. We encoded the structure $\Y_i$ of the $i^{th}$ word with a Markov model: its $k^{th}$ character $\Y_i^{(k)}$ only depends on $\Y_i^{k-1}$ and $\Y_i^{k+1}$. In this case, the oracle function is simply the Viterbi algorithm~\citet{viterbi1967error}. 
 The average length of a word is approximately 8, hence the dimension of $\Y_i$ is $d_{\Y_i} \approx 26^2 \cdot 8 = 5408$ leading to a large dimension for $\Y$, $d_\Y := \sum_{i=1}^n d_{\Y_i} \approx  5 \cdot 10^5$.  
 We run the SP-FW algorithm with step size $\gamma_t = 1/(1+t)$ for which we have a convergence proof (Corollary~\ref{cor:Bilin}), and with $\gamma_t = 2/(2+t)$, which normally gives better results for FW optimization. We compare with the projected subgradient method (projecting on the $\ell_1$-ball is tractable here) with step size $O(1/\sqrt{t})$ (the subgradient of $\tilde H_i(\bm{w})$ is $-\bm{\psi}_i(\y_i^*)$). Following~\citet{lacoste2013block}, we also implement a block-coordinate (SP-BCFW) version of SP-FW and compare it with the stochastic projected subgradient method (SSG). As some of the algorithms only work on the primal and to make our result comparable to~\citet{lacoste2013block}, we choose to plot the primal suboptimality error $p(\bm{w}_t)- p^*$ for the different algorithms in Figure~\ref{fig:beta_small} and~\ref{fig:beta_big} (the $\bm{\alpha}_t$ iterates for the SP approaches are thus ignored in this error).
The performance of SP-BCFW is similar to SSG when we regularize the learning problem heavily (Figure~\ref{fig:beta_small}). 
However, under lower regularization (Figure~\ref{fig:beta_big}), SSG (with the correct step size scaling) is faster. This is consistent with the fact that $\bm{\alpha_t} \neq \bm{\alpha}^*$ implies larger errors on the primal suboptimality for the SP methods, but we note that an advantage of the SP-FW approach is that the scale of the step size is automatically chosen.
  %

%
%
%

\paragraph{Conclusion.}\label{sec:conclusion} 
We proposed FW-style algorithms for saddle-point optimization with the same attractive properties as FW, in particular only requiring access to a LMO. We gave the first convergence result for a FW-style algorithm towards a saddle point over polytopes by building on the recent developments on the linear convergence analysis of AFW.
However, our experiments let us believe that the condition $\CondNumb > 0$ is not required for the convergence of FW-style algorithms.
We thus conjecture that a refined analysis could yield a linear rate for the general uniformly strongly convex-concave functions in both cases~\eqref{enu:situation1} and~\eqref{enu:situation2}, paving the way for further theoretical work.

\clearpage
%
\subsubsection*{Acknowledgments} %
\label{par:acknowledgments} 
Thanks to N. Ruozzi and
A. Benchaouine for helpful discussions. Work supported in
part by DARPA N66001-15-2-4026, N66001-15-C-4032 and NSF
III-1526914, IIS-1451500, CCF-1302269.
%
%
%
  %
%
  \bibliographystyle{abbrvnat} 
  {
  \bibliography{bib}
  }

  \clearpage


  %
  %
  %

  \appendix
  \onecolumn
  \fontsize{11}{13}
  \selectfont

%
  {\huge  Appendix}
  \paragraph{Outline.} %
  \label{par:outline}
  Appendix~\ref{sec:away_step_frank_wolfe} provides more details about the saddle point away-step Frank-Wolfe (SP-AFW) algorithm. Appendix~\ref{sec:affine_invariant} is about the affine invariant formulation of our algorithms, therein, we introduce some affine invariant constants and prove relevant bounds. 
  Appendix~\ref{sec:relations_gaps} presents some relationships between the primal suboptimalities and dual gaps useful for the convergence proof.
  Appendix~\ref{appendix:analysis} gives the affine invariant convergence proofs of SP-FW and SP-AFW in the strongly convex function setting introduced in Section~\ref{sec:SP-FW_strong_convexity}. 
  Appendix~\ref{sec:strong_conv_proof} gives the proof of linear convergence of SP-FW in the strongly convex set setting as defined in Section~\ref{sec:strongly_set}.
  Finally, Appendix~\ref{sec:details_on_the_experiments} provides details on the experiments.
  %
  
  \section{Saddle point away-step Frank-Wolfe (SP-AFW)} %
  \label{sec:away_step_frank_wolfe}
  In this section, we describe our algorithms SP-AFW and SP-PFW with a main focus on how the away direction is chosen. We also rigorously define a \emph{drop step} and prove an upper bound on their number. In this section, we will assume that there exist two finites sets $\A$ and $\B$ such that 
  $\X = \conv(\A)$ and $\Y = \conv(\B)$.
  \paragraph{Active sets and away directions.} %
  \label{par:active_set}
   Our definition of \emph{active set} is an extension of the one provided in \citet{lacoste2015global}, we follow closely their notation and their results. Assume that we have the current expansion,
   %
   %
   %
  \begin{equation}
    \xt = \sum_{\vv_x \in \mathcal S_x^{(t)}} \alpha_{\vv_x}^{(t)} \vv_x
    \quad \text{where} \quad
     \mathcal S_x^{(t)} := \left\{ \vv_x \in \A\;;\; \alpha_{\vv_x}^{(t)} >0 \right\},
  \end{equation}
  and a similar one for $\yt$.
   Then, the current iterate has a sparse representation as a convex combination of all possible pairs of atoms belonging to $S_x^{(t)}$ and $S_y^{(t)}$, i.e. 
   \begin{equation} \label{eq:ActiveSetImplicit}
     \zt = \sum_{\vv \in \mathcal S^{(t)}} \alpha_{\vv}^{(t)} \vv
     \quad \text{where} \quad
     \mathcal S^{(t)} := \left\{ \vv \in \Vertices\;;\; \alpha_{\vv}^{(t)}:= \alpha_{\vv_x}^{(t)}\alpha_{\vv_y}^{(t)} > 0\right\}.
   \end{equation}
  The set $\mathcal S^{(t)}$ is the current (implicit) \emph{active set} arising from the special product structure of the domain and it defines potentially more away directions than proposed in~\citep{lacoste2015global} for AFW, and these are the directions that we use in SP-AFW and SP-PFW.
  Namely, for every corner $\vv=(\vv_x,\vv_y)$ and $\vv' = (\vv'_x,\vv'_y)$ already picked, $\x-\vv_x$ is a feasible directions in $\X$ and $\y-\vv_y'$ is a feasible direction in $\Y$.
  Thus the combination $(\x-\vv_x,\y-\vv'_y)$ is a feasible direction even if the particular corners $\vv_x$ and $\vv_y'$ have never been picked together. We thus maintain the iterates on $\X$ and $\Y$ as independent convex combination of their respective active sets of corners (Line~\ref{algLine:activeSet} of Algorithm~\ref{alg:AFW}).
  
  Note that after $t$ iteration, the current iterate $\zt$ is $t$-sparse whereas the size of the active set $\mathcal S^{(t)}$ defined in~\eqref{eq:ActiveSetImplicit} can be of size~$t^2$. Nevertheless, because of the block formulation of the away oracle (Line~\ref{algLine:awayCorner} in Algorithm~\ref{alg:AFW}), the away direction can be found in $O(t)$ since we only need to use $S_x^{(t)}$ and $S_y^{(t)}$ separately to compute the away direction in $\mathcal S^{(t)}$.  %
  Moreover, we only need to track at most $t$ corners in $\A$ and $t$ ones in $\B$ to get this bigger active set.
  %
  We can now define the maximal step size for an away direction.
   %
  %
  %
  %
  \paragraph{Maximal step size.} %
  \label{par:maximal_step size}
 For the standard AFW algorithm,  
 \citet{lacoste2015global} suggest to use the maximum step size $\stepmax = \alpha_{\vv^{(t)}}/(1- \alpha_{\vv^{(t)}})$ when using the away direction $\z^{(t)}-\vv^{(t)}$, to guarantee that the next iterate stays feasible. Because we have a product structure of two blocks, we actually consider more possible away directions by maintaining a separate convex combination on each block in our Algorithm~\ref{alg:AFW} (SP-AFW) and~\ref{alg:PFW} (SP-PFW). More precisely, suppose that we have $\xt = \sum_{\vv_x \in S^{(t)}_x} \alpha^{(t)}_{\vv_x} \vv_x$ and $\yt = \sum_{\vv_y \in S^{(t)}_y} \alpha^{(t)}_{\vv_y} \vv_y$, then the following maximum step size $\gamma_{\max}$ (for AFW) ensures that the iterate $\ztt$ stays feasible:
   \begin{equation} \label{eq:update}
    \ztt := \zt + \gamma_t  \dt_A 
    \quad \text{with} \quad \gamma_t \in [0,\gamma_{\max}]
    \quad \text{and} \quad
     \gamma_{\max} := \min\left\{\frac{\alpha^{(t)}_{\vt_x}}{1 -\alpha^{(t)}_{\vt_x}},\frac{\alpha^{(t)}_{\vt_y}}{1 -\alpha^{(t)}_{\vt_y}} \right\}.
    \end{equation}
    A larger $\stepsize_t$ makes one of the coefficients in the convex combination for the iterate negative, thus no more guaranteeing that the iterate stays feasible. A similar argument can be used to derive the maximal step size for the PFW direction in Algorithm~\ref{alg:PFW}.
  %
  \paragraph{Drop steps.} %
  \label{par:drop_steps}
     A \emph{drop step} is when $\gamma_t=\stepmax$ for the away-step update~\eqref{eq:update} \citep{lacoste2015global}. In this case, at least one corner is removed from the active set. We show later in Lemma~\ref{lemme:Lin} that we can still guarantee progress for this step, i.e. $w_{t+1} < w_t$, but this progress be arbitrarily small since $\stepmax$ can be arbitrarily small. \citet{lacoste2015global} shows that the number of drop steps for AFW is at most half of the number of iterations. Because we are maintaining two independent active sets in our formulation, we can obtain more drop steps, but we can still adapt their argument to obtain that the number of drop steps for SP-AFW is at most two thirds the number of iterations (assuming that the algorithm is initialized with only one atom per active set). In the SP-AFW algorithm, either a FW step is jointly made on both blocks, or an away-step is done on both blocks. Let us call~$A_t$ the number of FW steps (which potentially adds an atom in $S^{(t)}_x$ and $S^{(t)}_y$) and $D_t^{(x)}$ (resp $D_t^{(y)}$) the number of steps that removed at least one atom from $S^{(t)}_x$  ($S^{(t)}_y$). Finally, we call $D_t$ the number of \emph{drop steps}, i.e., the number of \emph{away steps} where at least one atom from $S^{(t)}_x$ or $S^{(t)}_y$ have been removed (and thus $\stepsize_t = \stepmax$ for these). Because a step is either a FW step or an away step, we have:
     \begin{equation} \label{eq:NumberSteps}
     A_t + D_t \leq t \, .
     \end{equation}
     We also have that $D_t^{(x)} + D_t^{(y)} \geq D_t$ by definition of $D_t$. Because a FW step adds at most one atom in an active set while a drop step removes one, we have (supposing that $|S^{(0)}_x|=| S^{(0)}_y|=1$):
    \begin{equation}
      1 + A_t -  D_t^{(x)} \geq |S^{(t)}_x| \quad \text{and} \quad 1 + A_t -  D_t^{(y)} \geq |S^{(t)}_y| .
    \end{equation}
    Adding these two relations, we get:
    \begin{equation}
    2 + 2 A_t \geq |S^{(t)}_x| + |S^{(t)}_y| + D_t^{(x)} + D_t^{(y)} \geq 2 + D_t \, ,
    \end{equation}
    using the fact that each active set as at least one element. We thus obtain
    $D_t \leq 2 A_t$. Combining with~\eqref{eq:NumberSteps}, we get:
    \begin{equation} \label{eq:upper_bound_drop_steps}
    D_t \leq \frac{2}{3} t \, ,
    \end{equation}
  as claimed.
  %

%
%
  
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%

  %
\section{Affine invariant formulation of SP-FW}
  \label{sec:affine_invariant}
  In this section, we define the affine invariant constants of a convex function $f$ and their extension to a convex-concave function $\L$. 
  These constants are important as the FW-type algorithms are affine invariant if their step size are defined using affine invariant quantities. 
  We can upper bound these constants using the non affine invariant constants defined in the main paper. 
  Hence a convergence rate with affine invariant constants will immediately imply a rate with the constant introduced in the main paper.
  \subsection{The Lipschitz constants} %
    \label{sub:the_lipschitz_constants}
    We define the Lipschitz constant~$L$ of the gradient of the function $f$ with respect to the norm~$\| \cdot\|$ by using a dual pairing of norms, i.e. $L$ is a constant such that 
    \begin{equation}
       \label{def:lip}
        \forall \x, \x' \in \X, \qquad \|\nabla f(\x) - \nabla f (\x') \|_* \leq L \| \x - \x'\|,
     \end{equation} 
     where $\|\y\|_* := \sup_{\x \in \R^d, \|\x\| \leq 1} \y^{T} \x$ is the dual norm of $\|\cdot \|$. For a convex-concave function, we also consider the partial Lipschitz constants with respect to different blocks as follows.

     For more generality, we consider the dual pairing of norms $({\|\cdot\|_\X}, {\|\cdot\|_{\X^*}})$ on $\X$, and similarly $({\|\cdot\|_\Y}, {\|\cdot\|_{\Y^*}})$ on $\Y$. We also define the norm on the product space $\X \times \Y$ as the $\ell_1$-norm on the components: $\|(\x,\y)\|_{\X \times \Y} := \|\x\|_\X + \|\y\|_\Y$. We thus have that the dual norm of $\X \times \Y$ is the $\ell_\infty$-norm of the dual norms: $\|(\x,\y)\|_{(\X \times \Y)^*} = \max ( \|\x\|_{\X^*}, \|\y\|_{\Y^*})$.
     The \emph{partial} Lipschitz constants $L_{XX},L_{YY},L_{XY} \text{ and } L_{YX}$ of the gradient of the function $\L$ with respect to these norms are the constants such that for all $\x, \,\x' \in \X$ and $\y, \,\y' \in \Y$,
     \begin{equation}
     \label{def:lipL}
     \begin{aligned}
        \hspace{-3mm} \|\nabla_x \L (\x,\y) - \nabla_x \L(\x',\y) \|_{\X^*} &\leq L_{XX} \| \x - \x'\|_\X, &\! 
        \|\nabla_y \L (\x,\y) - \nabla_y \L(\x,\y') \|_{\Y^*} &\leq L_{YY}\,\| \y - \y'\|_\Y, \\
        \hspace{-3mm} \|\nabla_x \L (\x,\y) - \nabla_x \L(\x,\y') \|_{\X^*} &\leq L_{XY} \| \y - \y'\|_\Y,  
        &\!
        \|\nabla_y \L (\x,\y) - \nabla_y \L(\x',\y) \|_{\Y^*} &\leq L_{YX} \,\| \x - \x'\|_\X.
     \end{aligned}
     \end{equation}
  Note that the cross partial Lipschitz constants $L_{XY}$ and $L_{YX}$ do not necessarily use a dual pairing as $\X$ and $\Y$ could be very different spaces. 
  On the other hand, as the possibilities in~\eqref{def:lipL} are special cases of~\eqref{def:lip} when considering the $\ell_1$-norm of this product domain, one can easily deduce that the partial Lipschitz constants can always be taken to be smaller than the full Lipschitz constant for the gradient of $\L$, i.e., we have that $L \geq \max(L_{XX}, L_{XY}, L_{YX}, L_{YY})$.
  %

    
    %
    %
    \subsection{The curvature: an affine invariant measure of smoothness} \label{sub:curv}
    
    To prove the convergence of the Frank-Wolfe algorithm, the typical affine invariant analysis proof in the FW literature assumes that the curvature of the objective function is bounded, where the curvature is defined by~\citet{jaggi2013revisiting} for example. 
    We give below a slight generalization of this curvature notion in order to handle the convergence analysis of FW with away-steps.\footnote{\label{foot:Cfcomment}The change is to consider the more general directions $\s - \vv$ instead of just $\s - \x$, and also any feasible positive step size. 
    See also Footnote~8 in~\citesup{lacoste2013affine} for a related discussion. A different (bigger) constant was required in~\citep{lacoste2015global} for the analysis of AFW because they used a line-search.} 
    It has the same upper bound as the traditional curvature constant (see Proposition~\ref{prop:C_fbounded}).

    \paragraph{Curvature.}[Slight generalization of~\citet{jaggi2013revisiting}]  
    \label{par:curvature}
    Let $f : \X \to \R$ be a convex function, we define the curvature $C_f$ of $f$ as 
      \begin{equation} 
    \label{CurvB}
      \Cf := 
        \sup_{\substack{\scalebox{0.65}{
        $\begin{matrix}
                    \x, \s, \vv \in \X , \\ \stepsize > 0 \text{ s.t.} \\
                    \x_{\stepsize} := \x + \stepsize \dd \in \X \\ 
                    \text{with } \dd := \s - \vv 
                  \end{matrix}$
          }}}
          \frac{2}{\stepsize^2}( f(\x_{\stepsize})-f(\x)- \stepsize \prodscal{\dd}{ \nabla f(\x)}). 
          \end{equation}
    Note that only the \emph{feasible} step sizes~$\stepsize$ are considered in the definition of~$\Cf$, i.e., $\stepsize$ such that $\x_{\stepsize} \in \X$. 
    If the gradient of the objective function is Lipschitz continuous, the curvature is upper bounded. 
    \begin{proposition}[Simple generalization of Lemma~7 in~\citet{jaggi2013revisiting}]
        \label{prop:C_fbounded}
    Let $f$ be a convex and continuously differentiable function on~$\X$ with its gradient $\nabla f$ L-Lipschitz continuous w.r.t. some norm $\|.\|$ in dual pairing over the domain $\X$. Then
      \begin{equation} \label{eq:CfUpperBound}
      C_f \leq D_\X^2 L \, ,
      \end{equation}
      where $D_\X : = \sup_{\x,\x' \in \X} \|\x - \x'\|$ is the diameter of $\X$.
  \end{proposition}  
  \proof[Lemma 1.2.3 in~\citetsup{nesterov2004introductory}, \citet{jaggi2013revisiting}] 
    Let $\x, \s, \vv \in \X$, set $\dd := \s - \vv$ and $\x_{\stepsize} = \x + \gamma \dd $ for some $\stepsize > 0$ such that $\x_{\stepsize} \in \X$. Then by the fundamental theorem of calculus, 
    \begin{equation}
      f(\x_{\stepsize}) = f(\x) + \int_0^\gamma \prodscal{ \dd }{\nabla f(\x + t \dd )} dt.
    \end{equation}
    Hence, we can write 
    \begin{align}
      f(\x_{\stepsize})-f(\x)- \stepsize \prodscal{\dd}{ \nabla f(\x)}
      & = \int_0^\gamma \prodscal{ \dd }{\nabla f(\x + t \dd) -  \nabla f(\x)} dt  \notag\\
      & \leq \|\dd\| \int_0^\gamma  \| \nabla f(\x + t \dd) - \nabla f(\x)\|_*dt \notag \\
      & \leq D_\X^2 L\int_0^\gamma t dt \notag \\ 
      & \leq \frac{\gamma^2}{2} D_\X^2 L.
    \end{align}
  Thus for all $\x,\s,\vv \in \X$ and $\x_{\stepsize} = \x + \stepsize (\s - \vv)$ for $\stepsize > 0$ such that $\x_{\stepsize} \in \X$, we have
  \begin{equation}
    \frac{2}{\gamma^2}( f(\x_{\stepsize})-f(\x)- \stepsize \prodscal{\s - \vv}{ \nabla f(\x)}) \leq L D_\X^2 .
  \end{equation}
  The supremum is then upper bounded by the claimed quantity. 
  \endproof
  
  \citetsup[Appendix C.1]{osokin2016gapBCFW} illustrate well the importance of the affine invariant curvature constant for Frank-Wolfe algorithms in their paragraph titled ``Lipschitz and curvature constants". They provide a concrete example where the wrong choice of norm for a specific domain~$\X$ can make the upper bound of Proposition~\ref{prop:C_fbounded} extremely loose, and thus practically useless for an analysis.
  
  We will therefore extend the curvature constant to the convex-concave
    function $\L$ by simply defining it as the maximum of the curvatures of the functions
    belonging to the family $\left(\x' \mapsto \L(\x',\y),\y' \mapsto
      -\L(\x,\y')\right)_{\x \in \X, \y \in \Y}$ (see Section~\ref{sub:curvature_and_interior_strong_convexity_constant_for_a_convex_concave_function}). But before that, we review affine invariant analogues of the strong convexity constants that will be useful for the analysis.
    %



  %



  \subsection{Affine invariant measures of strong convexity} %
  \label{sub:affine_invariant_measures_of_strong_convexity}

  In this section, we review two affine invariant measures of strong convexity that were proposed by~\citetsup{lacoste2013affine}\citep{lacoste2015global} for the affine invariant linear convergence analysis of the standard Frank-Wolfe algorithm (using the ``interior strong convexity constant'') or the away-step Frank-Wolfe algorithm (using the ``geometric strong convexity constant''). We will re-use them for the affine invariant analysis of the convergence of SP-FW or SP-AFW algorithms. In a similar way as the curvature constant~$\Cf$ includes information about the constraint set~$\X$ and the Lipschitz continuity of the gradient of~$f$ together, these constants both include the information about the constraint set~$\X$ and the strong convexity of a function~$f$ together.

  %
  \paragraph{Interior strong convexity constant.} %
  \label{par:interior_strong_convexity_constant_}
  [based on \citetsup{lacoste2013affine}]
  Let $\xc$ be a point in the relative interior of $\X$.
   The \emph{interior strong convexity constant} for~$f$ with respect to the reference point~$\xc$ is defined as
    \begin{equation}
  \label{def:int_strong}
    \mu^{\xc}_f :=  \inf_{\substack{\scalebox{0.65}{
        $\begin{matrix}
                  \x \in \X \setminus \{\xc\} \\
                  \s = \bar{\s}(\x,\xc,\X) \\
                  \gamma \in (0,1], \\
                  \z = \x + \gamma(\s-\x)
        \end{matrix}$
        }}}
       \frac{2}{\gamma^2} \left(f(\z)-f(\x)- \prodscal{\z-\x}{ \nabla f(\x)}\right).
       \end{equation}
  Here, we follow the notation of \citetsup{lacoste2013affine} and take the point $\s$ to be the point where the ray from $\x$ to the reference point $\xc$ pinches the boundary of the set $\X$,
    i.e. $\bar{\s}(\x,\xc,\X) := {\rm ray}(\x,\xc) \cap \partial \X$, where $ \partial \X$ is the boundary of the convex set~$\X$.
    
  We note that in the original definition~\citepsup{lacoste2013affine}, $\xc$ was the (unique) optimum point for a strongly convex function~$f$ over~$\X$. The optimality of~$\xc$ is actually not needed in the definition and so we generalize it here to any point~$\xc$ in the relative interior of~$\X$, as this will be useful in our convergence proof for SP-FW.

  For completeness, we include here the important lower bound from~\citepsup{lacoste2013affine} on the interior strong convexity constant in terms of the strong convexity of the function~$f$.

  \begin{proposition}[Lower bound on~$\mu^{\xc}$ from {\citetsup[Lemma~2]{lacoste2013affine}}] \label{prop:delta}
  Let $f$ be a convex differentiable function and suppose that $f$ is strongly convex w.r.t. to some arbitrary norm $\norm{\cdot}$ over the domain~$\X$ with strong-convexity constant $\mu_f > 0$. Furthermore, suppose that the reference point $\xc$ lies in the relative interior of~$\X$, i.e.,
  $\delta_c := \min_{\s \in \partial \X}
    \; ||\s-\xc|| >0$. Then the interior strong convexity constant~$\mu^{\xc}_f$~\eqref{def:int_strong} is lower bounded as follows:
    \begin{equation} \mu^{\xc}_f \geq \mu_f \delta_c^2. \end{equation}
  \end{proposition}



  \proof 
  Let $\x$ and $\z$ be defined as in~\eqref{def:int_strong}, i.e., $\z = \x + \stepsize (\s - \x)$ for some $\stepsize > 0$ and where $\s$ intersects the boundary of $\X$ with the ray going from $\x$ to $\xc$.
  By the strong convexity of~$f$, we have
    \begin{equation} \label{eq:prop7strongConvex}
    f(\z)-f(\x) - \prodscal{\z-\x}{\nabla f(\x)} \geq ||\z-\x||^2 \frac{\mu_f}{2}
    = \stepsize^2 ||\s-\x||^2 \frac{\mu_f}{2}.
    \end{equation}
  From the definition of $\s$, we have that $\xc$ lies between $\x$ and $\s$ and thus:
  $||\s-\x||
  \geq ||\s - \xc|| \geq \delta_c$. Combining with~\eqref{eq:prop7strongConvex}, we conclude
    \begin{equation} f(\z)-f(\x) - \prodscal{\z-\x}{\nabla f(\x)} \geq
   \stepsize^2 \delta_c^2 \frac{\mu_f}{2}, \end{equation}
  and therefore 
    \begin{equation} \mu^{\xc}_f \geq \delta_c^2 \mu_f. \end{equation}
  \endproof
  %
  
  We now present the affine invariant constant used in the global linear convergence analysis of Frank-Wolfe variants when the convex set~$\X$ is a polytope. The \emph{geometric strong convexity constant} was originally introduced by~\citetsup{lacoste2013affine} and~\citep{lacoste2015global}. To avoid any ambiguity, we will re-use their definitions verbatim in the rest of this section, starting first with a few geometrical definitions and then presenting the affine invariant constant. In these definitions, they assume that a \emph{finite} set~$\A$ of vectors (that they call \emph{atoms}) is given such that~$\X = \conv(\A)$ (which always exists when~$\X$ is a polytope).  

  \paragraph{Directional Width.}[\citet{lacoste2015global}]
  The \emph{directional width} of a set $\A$ with respect to a direction $\r$
  is defined as $\dirW(\A,\r) := \max_{\s, \vv \in\A}
  \big\langle \frac{\r}{\normEucl{\r}}, \s - \vv \big\rangle$. The \emph{width} of~$\A$
  is the minimum directional width over all possible directions in its affine
  hull.

  \paragraph{Pyramidal Directional Width.}[\citet{lacoste2015global}] We define the \emph{pyramidal directional
  width} of a set $\A$ with respect to a direction $\r$ and a base point
  $\x \in \domain$ to be
  \begin{equation} \label{eq:TruePdirW}
  \PdirW(\A,\r, \x) := \min_{\SS \in \SS_{\x}} \dirW( \SS \cup
  \{\s(\A,\r) \} , \; \r) = \min_{\SS \in \SS_{\x}} \max_{\s \in
  \A, \vv \in \SS} \textstyle \big\langle \frac{\r}{\normEucl{\r}}, \s - \vv \big\rangle
  ,
  \end{equation}
  where $\SS_{\x} := \{ \SS \, | \, \SS \subseteq \A$ such that $\x$ is a
  proper\footnote{By \emph{proper} convex combination, we mean that all
  coefficients are non-zero in the convex combination.} convex combination of
  all the elements in $\SS\}$, and $\s(\A,\r) := \argmax_{\vv \in
  \A} \innerProdCompressed{\r}{\vv}$ is the FW atom used as a summit, when using the convention in this section that $\r := -\nabla f(\x)$. 


  \paragraph{Pyramidal Width.}[\citet{lacoste2015global}]
  To define the pyramidal width of a set, we take the minimum over the cone of
  possible \emph{feasible} directions~$\r$ (in order to avoid the problem of zero width).\\
  %
  A direction~$\r$ is \emph{feasible} for $\A$ from $\x$ if it points inwards $\conv(\A)$, 
  %
  (i.e. $\r \in \text{cone}(\A-\x)$).\\
  We define the \emph{pyramidal width} of a set $\A$ to be the smallest pyramidal width of all its faces, i.e.
  \begin{equation} \label{eq:Pwidth}
  \PWidth(\A) := \displaystyle \min_{\substack{\Kface \in \textrm{faces}(\conv(\A)) \\
                            \x \in \Kface \\
                            \r \in \text{cone}(\Kface-\x) \setminus \{\0\}} 
                                     } \PdirW(\Kface \cap \A,\r, \x)                   .                               
  \end{equation}
  %



\paragraph{Geometric strong convexity constant.}[\citet{lacoste2015global}] %
\label{par:strong_}
%
   The \emph{geometric strong convexity constant} of~$f$ (over the set of atoms $\A$ which is left implicit) is: 
    \begin{equation} \label{def:geom_strong}
    \mu^{\away}_f := 
            \underset{ \x \in \X}{\inf}
            \inf_{\substack{\scalebox{0.6}{
            $\begin{matrix}
                          \x^* \in \X  \\
                          s.t \;   \prodscal{\nabla f(\x)}{\x^* - \x} <0
             \end{matrix}$
              }}}
          \frac{2}{\gamma^\away(\x,\x^*)^2}( f(\x^*)-f(\x)- \prodscal{\x^*-\x}{ \nabla f(\x)}) 
      \end{equation}
  %
  where $\gamma^\away(\x,\x^*) := \frac{\prodscal{-\nabla f(\x)}{\x^*-\x}}{\prodscal{-\nabla f(\x)}{\s_f(\x)-\vv_f(\x)}}$ and $\X = \conv(\A)$. The quantity $\s_f(\x)$ represents the FW corner picked when running the FW algorithm on~$f$ when at~$\x$; while $\vv_f(\x)$ represents the worst-case possible away atom that AFW could pick (and this is where the dependence on $\A$ appears). We now define these quantities more precisely. Recall that the set of possible active sets is $\SS_{\x} := \{ \SS \, | \, \SS \subseteq \A$ such that $\x$ is a
  proper convex combination of
  all the elements in $\SS\}$.
  For a given set~$\SS$, we write $\vv_{\SS}(\x) := \argmax_{\vv \in \SS }
  \left\langle \nabla f(\x), \vv \right\rangle$ for the away atom in the
  algorithm supposing that the current set of active atoms is $\SS$.
  Finally, we define $\vv_f(\x) := \hspace{-3mm}\displaystyle\argmin_{\{\vv = \vv_{\SS}(\x)
  \,|\, \SS \in \SS_{\x} \}} \textstyle \hspace{-3mm}\left\langle \nabla f(\x), \vv
  \right\rangle$ to be the worst-case away atom (that is, the atom which would
  yield the smallest away descent). An important property coming from this definition that we will use later is that for $\st$ and $\vt$ being possible FW and away atoms (respectively) appearing during the AFW algorithm (consider Algorithm~\ref{alg:AFW} ran only on~$\X$), then we have:
  \begin{equation}
    \label{eq:gammaA}
    \gap^{\PW} := \prodscal{\st - \vt}{- \nabla f (\xt)} \geq \prodscal{\s_f(\xt) - \vv_f(\xt)}{- \nabla f (\xt)}.
  \end{equation}

  The following important theorem from~\citep{lacoste2015global} lower bounds the geometric strong convexity constant of~$f$ in terms of both the strong convexity constant of $f$, as well as the pyramidal width of~$\X = \conv{(\A)}$ defined as $\PWidth(\A)$~\eqref{eq:Pwidth}.

  \begin{proposition}[Lower bound for~$\strongConvAFW$ from {\citet[Theorem 6]{lacoste2015global}}]
  \label{thm:muFdirWinterpretation2}
  Let $f$ be a convex differentiable function and suppose that $f$ is
  $\mu$-\emph{strongly convex} w.r.t. to the Euclidean norm
  $\normEucl{\cdot}$ over the domain $\domain=\conv(\A)$ with strong-convexity constant $\mu
  \geq 0$. Then
  \begin{equation} \label{eq:muDirWidthBound}
  \strongConvAFW \geq \mu \cdot \left( \PWidth(\A) \right)^2.
  \end{equation}
  \end{proposition}
  
  The pyramidal width~\eqref{eq:Pwidth} is a geometric quantity with a somewhat intricate definition. Its value is still unknown for many sets (though always strictly positive for finite sets), but \citet[Lemma~4]{lacoste2015global} give its value for the unit cube in $\R^d$ as $1/{\sqrt{d}}$. 
  


\subsection{Curvature and interior strong convexity constant for a convex-concave function} %
\label{sub:curvature_and_interior_strong_convexity_constant_for_a_convex_concave_function}

  In this subsection, we propose simple convex-concave extensions of the definitions of the affine invariant constants defined introduced in the two previous sections.
    
  To define the convex-concave curvature, we introduce the sets $\mathcal{F}$ and $\mathcal G$ of the marginal convex functions. 
    \begin{equation}\label{eq:F_x}
       \mathcal{F} := \{\x' \mapsto \L(\x',\y)  \}_{\y \in \Y} 
       \quad \text{and} \quad
        \mathcal{G} := \{\y' \mapsto -\L(\x, \y') \}_{\x \in \X} .
    \end{equation}
  Let $\L : \M \to \R$ a convex-concave function, we define the curvature pair $(C_{\L_x},C_{\L_y})$ of $\L$ as 
    \begin{equation} \label{eq:CLx}
    (C_{\L_x},C_{\L_y}) := \left(\underset{f \in \mathcal{F}}
        {\sup}\; C_{f},\underset{g \in \mathcal{G} }
        {\sup}\; C_{g} \right).
      \end{equation}
 and the curvature of $\L$ as
  \begin{equation}\label{eq:CL}
  C_\L := \frac{C_{\L_x}+C_{\L_y}}{2}. 
  \end{equation}
 An upper bound on this quantity follows directly from the upper bound on the convex case (Lemma~7 of \citet{jaggi2013revisiting}, repeated in our Proposition~\ref{prop:C_fbounded}) :


  \begin{proposition}\label{prop:Cbounded}
    Let $\L: \M \to \R$ be a differentiable convex-concave function. If $\X$ and $\Y$ are compact and $\nabla\L$ is Lipschitz continuous, then the curvature of $\L$ is bounded by $\frac{1}{2}(L_{XX} D_\X^2+L_{YY}D_\Y^2)$, where $L_{XX}$ (resp $L_{YY}$) is the largest Lipschitz constant respect to $\x$ ($\y$) of $\x \mapsto \nabla_x \L(\x,\y)$ ($\y \mapsto \nabla_y \L(\x,\y)$).
  \end{proposition}
  \proof
    Let $f$ in $\mathcal{F}$,
    \begin{equation}
     C_f \leq Lip(\nabla f) D_\X^2 \leq L_{XX} D_\X^2.
     \end{equation}
     Similarly, let $g$ in $\mathcal{G}$,
     \begin{equation}
     C_g \leq Lip(\nabla g) D_\Y^2 \leq L_{YY} D_\Y^2.
     \end{equation}
     Consequently,
     \begin{equation}
     C_\L = \frac{1}{2}(\underset{f \in \mathcal{F}}{\sup} C_f + \underset{g \in \mathcal{G}}{\sup} C_g) \leq \frac{1}{2} (L_{XX}D_\X^2 + L_{YY} D_\Y^2).
     \end{equation} 
     Where $D_\X$ and $D_\Y$ are the respective diameter of $\X$ and $\Y$.
  \endproof
  Note that $L_{XX}$ and $L_{YY}$ are upper bounded by the global Lipschitz constant of $\nabla \L$.
  Similarly, we define various notions of strong convex-concavity in the following.
\paragraph{Uniform strong convex-concavity constant.} %
\label{par:uniform_strong_convex_concavity_constant}
%
    The uniform strong convex-concavity constants is defined as 
    \begin{equation}
  \label{def:strongL}
      (\mu_\X,\mu_\Y) := \left(\underset{f \in \mathcal{G}}
        {\inf} \; \mu_f,\underset{g \in \mathcal{G}}
        {\inf} \; \mu_g\right)
    \end{equation}
    where $\mu_f$ is the strong convexity constant of $f$ and $\mu_g$ the strong convexity of $g$.


  Under some assumptions this quantity is positive.

      
  \begin{proposition} \label{prop:strongpos}
    If the second derivative of $\L$ is continuous, $\X$ and $\Y$ are compact and if for all $f \in \mathcal{F}\cup \mathcal G, \,\mu_f >0$, then
     $\mu_\X$ and $\mu_\Y$ are positive.  
  \end{proposition}

  \proof
    Let us introduce $H_x(\x,\y):= \nabla_x^2 \L(\x,\y)$ the Hessian of the function $\x\mapsto \L(\x,\y)$. We want to show that the smallest eigenvalue is uniformly bounded on $\M$. We know that the smallest eigenvalue lower bounds $\mu_\X$,
      \begin{equation} 
        \mu_{\X} \geq \inf_{\substack{\scalebox{0.65}{
                $\begin{array}{ll}
                    \; (\x,\y) \in \M \\
                    \|\u\|_2=1
                \end{array}$
                }}} \prodscal{\u}{H_x(\x,\y) \cdot\u}.
      \end{equation}
    But $H_x(\cdot)$ is continuous (because $\nabla_x^2 \L(\cdot)$ is continuous by assumption) and then the function $(\u,\x,\y) \mapsto \prodscal{\u}{H_x(\x,\y) \cdot\u}$ is continuous. 
    Hence since $\M$ and the unit ball are compact, the infimum is a minimum which can't be 0 by assumption. Hence $\mu_\X$ is positive. 
    Doing the same thing with the smallest eigenvalue of $-\nabla_y^2 \L(\x,\y)$, we get that 
    $\mu_\Y>0$.
  \endproof
  A common  family of saddle point objectives is of the form $f(x) +x^TMy - g(y)$. In this case, we get simply that $\left( \mu_\X, \mu_\Y \right) = (\mu_f,\mu_g).$
  An equivalent definition for the uniform strong convex-concavity constant is: $\L$ is $(\mu_\X,\mu_\Y)$-uniform strongly convex-concave function if
  \begin{equation}
    \left( \x,\y \right) \mapsto\L(\x,\y) - \frac{\mu_\X}{2} \|\x\|^2 + \frac{\mu_\Y}{2} \|\y\|^2
  \end{equation}
  is convex-concave.

  The following proposition relates the distance between the saddle point and the values of the function. It is a direct consequence from the uniform strong convex-concavity definition~\eqref{def:strongL}.
  \begin{proposition}\label{prop:strongL}
    Let $\L$ be a uniformly strongly convex-concave function and $(\x^*,\y^*)$ the saddle point of $\L$. 
    %
    Then we have for all $\x$ in $\X$ and $\y \in \Y$,
    \begin{equation} \sqrt{\L(\x,\y^*) - \L^*} \geq \|\x^*-\x\|\sqrt{\frac{\mu_\X}{2}} \quad  
     \text{and} \quad
     \sqrt{\L^* - \L(\x^*,\y)} \geq \|\y^*-\y\|\sqrt{\frac{\mu_\Y}{2}}.
    \end{equation} 
  \end{proposition}


  \proof 
    The saddle point $(\x^*,\y^*)$ is the optimal point of the two strongly convex functions $\x \mapsto \L(\x,\y^*)$ and the function $\y \mapsto -\L(\x^*,\y)$, so we can use the property of strong convexity on each function and the fact that $\mu_\X$ lower bounds the strong convexity constant of $\L(\cdot,\y^*)$ (and similarly for $\mu_\Y$ with $-\L(\x^*,\y)$) as per the definition~\eqref{def:strongL}, to get the required conclusion.
  \endproof
  Now we will introduce the uniform strong convex-concavity constants relatively to our saddle point.
\paragraph{Interior strong convex-concavity.} %
\label{par:interior_strong_convex_concavity}
%
   The SP-FW interior strong convex-concavity constants (with respect to the reference point $(\xc, \yc)$) are defined as: 
    \begin{equation} 
  \label{def:interior_strongL}
    \left(\mu^{\xc}_\L,\mu^{\yc}_\L \right) := \left(\inf_{f \in \mathcal F} \mu^{\xc}_f, \inf_{g\in \mathcal G} \mu^{\yc}_g \right)
    \end{equation}
    where $\mu^{\xc}_f$ is the  interior strong convexity constant of $f$ w.r.t to the point $\xc$ and $ \mu^{\yc}_g$ is the interior strong convexity constant w.r.t to the point $\yc$. The sets $\mathcal F$ and $\mathcal G$ are defined in~\eqref{eq:F_x}. We also define the smallest quantity of both (with the reference point $(\xc, \yc)$ implicit):
    \begin{equation}
    \muIntL = \min\{\mu^{\xc}_\L, \mu^{\yc}_\L\}.
    \end{equation}
    We can lower bound this constant by a quantity depending on the uniform strong convexity constant and the distance of the saddle point to the boundary.
   The propositions on the strong convex-concavity directly follow from the previous definitions and the analogous proposition on the convex case (Proposition~\ref{prop:delta})

  \begin{proposition}\label{prop:deltaL}
    Let $\L$ be a convex-concave function. If the reference point
    $(\xc,\yc)$ belongs to the relative interior of $\X \times \Y$ and if the function
    $\L$ is strongly convex-concave with a strong convex-concavity constant $\mu > 0$,
    then $\muIntL$ is lower bounded away from zero. More precisely, define $\delta_x :=\min_{\s_x\in \partial\X } \; \|\s_x-\xc\|>0$ and $\delta_y := \min_{\s_y \in \partial\Y}\|\s_y-\yc\|$. Then we have,
    \begin{equation} \mu^{\xc}_\L \geq  \mu_\X \delta_x^2 
    \qquad \text{and}\qquad
    \mu^{\yc}_\L \geq \mu_\Y \delta^2_y. \end{equation}
  \end{proposition}



  \proof 
     Using the Proposition~\ref{prop:delta} we have, 
      \begin{equation}
        \mu^{\xc}_f \geq \mu_f \cdot \delta_x^2 \geq \mu_\X\cdot \delta_x^2,
      \end{equation}
     and,
        \begin{equation}
        \mu^{\yc}_g \geq \mu_g \cdot \delta_y^2 \geq \mu_\Y\cdot \delta_y^2.
      \end{equation}
  \endproof
  When the saddle point is not in the interior of the domain, we define next a constant that takes in consideration the geometry of the sets. If the sets are polytopes, then this constant is positive.
\paragraph{Geometric strong convex-concavity.} %
\label{par:geometric_strong_convex_concavity}
%
   The SP-FW \emph{geometric} strong convex-concavity constants are defined analogously as the interior strong convex-concavity constants,
    \begin{equation} 
  \label{def:geom_strongL}
      \left(\mu^{\away}_{\L_x}, \mu^{\away}_{\L_y}\right) := \left( \min_{f \in \mathcal F} \mu^{\away}_f,  \min_{g \in \mathcal G} \mu^{\away}_g \right) \, ; \quad \mu^{\away}_\L:=\min \left(\mu^{\away}_{\L_x}, \mu^{\away}_{\L_y}\right)  ,
    \end{equation}
    where $\mu^{\away}_f$ is the  geometric strong convexity constant of $f \in \mathcal F$ (over $\A$) as defined in~\eqref{def:geom_strong} (and similarly $\mu^{\away}_g$ is the geometric strong convexity constant of $g \in \mathcal G$ over $\B$).

  It is straightforward to notice that the lower bound on the geometric strong convexity constant (Proposition~\ref{thm:muFdirWinterpretation2}) can be extended to the geometric strong convex-concavity constants (where $\mu_\X$ and $\mu_\Y$ are now assumed to be defined with respect to the Euclidean norm): 
  \begin{equation} \label{eq:muAwayLInequality}
  \mu^{\away}_{\L_x} \geq \mu_\X \PWidth(\A)^2 \quad \text{ and } \quad \mu^{\away}_{\L_y} \geq \mu_\Y \PWidth(\B)^2 .
  \end{equation} 
  %


  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %

  %




%

\subsection{The bilinearity coefficient}

In our proof, we need to relate 
  the gradient at the point $(\xt,\yt)$ with the one at
  the point $(\xt, \y^*)$. We can use the Lipschitz continuity of the gradient for this. We define below affine invariant quantities that can upper bound this difference.
  %
\paragraph{Bilinearity coefficients.} %
\label{par:bilinearity_coefficients}
%
  let $\L$ be a strongly convex-concave function, and let $(\x^*,\y^*)$ be its unique saddle point. We define the bilinearity coefficients $(M_{XY},M_{YX})$ as, 
    \begin{equation}   \label{def:M}
    M_{XY} = \hspace{-2mm} \sup_{\substack{\scalebox{0.65}{
       $ \begin{matrix}
                 \y \in \Y \\
                 \x, \s,\vv \in \X \\
                 \dd = \s -\vv   %
               \end{matrix}$
        }}} \hspace{-3mm} \prodscal{\dd}{\frac{\nabla_x\L(\x,\y^*)-\nabla_x\L(\x,\y)}{\sqrt{\L^*-\L(\x^*,\y)}}} 
    \end{equation}
  and, 
  \begin{equation}
      M_{YX}:=  \hspace{-2mm} \sup_{\substack{\scalebox{0.65}{
       $ \begin{matrix}
                 \x \in \X\\
                 \y, \s,\vv \in \Y \\
               \dd = \s-\vv
          \end{matrix}$
        }}} \hspace{-3mm} \prodscal{\dd}{\frac{\nabla_y\L(\x,\y)-\nabla_y\L(\x^*,\y)}{\sqrt{\L(\x,\y^*)-\L^*}}}.
  \end{equation}
  We also define the global bilinearity coefficient as
  \begin{equation}\label{eq:def_bilin_coef}
    M_\L := \max\{M_{XY},M_{YX}\}.
  \end{equation}
  We can upper bound these affine invariant constants with the Lipschitz constant of the gradient, the uniform strong convex-concavity constants and the diameters of the sets.
  \begin{proposition}\label{Mbounded}
  If  $\X$ and $\Y$ are compact, $\nabla \L$ is Lipschitz continuous and $\L$ is uniformly strongly convex-concave with constants $(\mu_\X, \mu_\Y)$, then
    \begin{equation}
      M_{XY} \leq \sqrt{\frac{2}{\mu_\Y}}L_{XY} \cdot D_\X 
      \quad \text{and} \quad
      M_{YX} \leq \sqrt{\frac{2}{\mu_\X}}L_{YX} \cdot D_\Y
    \end{equation}
  where $L_{XY}$ and $L_{YX}$ are the \emph{partial} Lipschitz constants defined in Equation~\eqref{def:lipL}. The quantity $D_\X$ is the diameter of the compact set $\X$
  and $D_\Y$ is the diameter of $\Y$.
  \end{proposition}
  \proof
   \begin{align*}
   M_{XY}
   &= \sup_{\substack{\scalebox{0.65}{
          $ \begin{matrix}
                    \y \in \Y \\
                    \x, \s,\vv \in \X \\
                    \dd = \s -\vv 
                  \end{matrix}$
     }}} \;\prodscal{\dd}{\frac{\nabla_x\L(\x,\y^*)-\nabla_x\L(\x,\y)}{\sqrt{\L^*-\L(\x^*,\y)}}} \\        
    &\leq \sup_{\substack{\scalebox{0.65}{
              $ \begin{matrix}
                        \y \in \Y \\
                        \x, \s,\vv \in \X \\
                        \dd = \s -\vv 
                      \end{matrix}$
         }}} \; {\frac{ \|\dd\|_\X \, \|\nabla_x\L(\x,\y^*)-\nabla_x\L(\x,\y) \|_{\X^*}}{\sqrt{\L^*-\L(\x^*,\y)}}} \\
    & \leq \sup_{\substack{\scalebox{0.65}{
              $ \begin{matrix}
                        \y \in \Y \\
                        \s,\vv \in \X \\
                        \dd = \s -\vv 
                      \end{matrix}$
         }}} \; {\frac{ \|\dd\|_\X \,  L_{XY} \|\y^*-\y \|_\Y}{\sqrt{\L^*-\L(\x^*,\y)}}} \\
     & \leq  \sup_{\y \in \Y} D_\X L_{XY} \frac{\|\y^*-\y\|_\Y}{\sqrt{\L^*-\L(\x^*,\y)}}.
  \end{align*}
  Then using the relation between $\|\y^*-\y\|_\Y$ and $\sqrt{\L^*-\L(\x^*,\y)}$ due to strong convexity (Proposition~\ref{prop:strongL})
  \begin{equation}
  M_{XY} \leq \sqrt{\frac{2}{\mu_\Y}}L_{XY}\cdot D_\X.
  \end{equation}
  We use a similar argument for $M_{YX}$ which allows us to conclude.
  \endproof


  %


%
%
%


\subsection{Relation between the primal suboptimalities} %
\label{sub:relation_between_the_primal_errors}
In this section, we are going to show that if the objective function $\L$ is uniformly strongly convex-concave, then we have a relation between $h_t$ and $w_t$. First let us introduce affine invariant constants to relate these quantities (in the context of a given saddle point $(\x^*, \y^*)$):
\begin{equation}
 \label{def:constant_relation_primal}
  P_\X := \sup_{\x \in \X} \frac{\prodscal{\nabla_x \L (\x,\hat \y(\x))}{\x-\x^*}}{\sqrt{\L(\x,\y^*)-\L(\x^*,\y^*)}}
\quad
\text{and}
\quad
  P_\Y := \sup_{\y \in \Y} \frac{\prodscal{\nabla_y \L (\hat \x (\y),\y)}{\y-\y^*}}{\sqrt{\L(\x^*,\y^*)-\L(\x^*,\y)}},
\end{equation}
where $\hat \y (\x) : = \argmax_{\y \in \Y} \L(\x,\y)$ and $\hat \x (\y) : = \argmin_{\x \in \X} \L(\x,\y)$. We also define: 
\begin{equation} 
  P_\L := \max \{P_\X, P_\Y\}.
\end{equation}
These constants can be upper bounded by easily computable constants.
\begin{proposition} For any $(\mu_\X,\mu_\Y)$-uniformly convex-concave function $\L$,
\label{prop:upper_bound_P_L}
\begin{equation}
  P_\X \leq \sqrt{\frac{2}{\mu_\X}}\sup_{\z \in \M}\| \nabla_x \L(\z)\|_{\X^*}
\quad
\text{and}
\quad
   P_\Y \leq \sqrt{\frac{2}{\mu_\Y}} \sup_{\z\in \M}\|\nabla_y \L (\z)\|_{\Y^*}.
\end{equation}
\end{proposition}
\proof 
Let us start from the definition of $P_\X$, let $\x \in \X$,
\begin{align*}
  \frac{\prodscal{\nabla_x \L (\x,\hat \y(\x))}{\x-\x^*}}{\sqrt{\L(\x,\y^*)-\L(\x^*,\y^*)}} 
  & \leq \frac{\|\x-\x^*\|_\X \cdot \sup \left(\| \nabla_x \L (\z)\|_{\X^*}\right) }{\sqrt{\L(\x,\y^*)-\L(\x^*,\y^*)}} \\
  & \leq \sqrt{\frac{ 2}{\mu_\X}} \sup_{\z \in \M}\| \nabla_x \L(\z) \|_{\X^*} \qquad \qquad\text{(by strong convexity.)} 
\end{align*}
The same way we can get 
\begin{equation}
  P_\Y \leq \sqrt{\frac{2}{\mu_\Y}} \sup_{\z\in \M}\|\nabla_y \L(\z) \|_{\Y^*}.
\end{equation}
It concludes our proof.
\endproof 
One way to compute an upper bound on the supremum of the gradient is to use any reference point $\bar{\z}$ of the set:
\begin{equation}
  \forall \bar{z} \in \M, \quad \sup_{\z \in \M}\| \nabla_x \L(\z) \|_{\X^*} 
  \leq \nabla_x \L(\bar{\z}) + L_{XX} D_\X + L_{XY} D_\Y.
\end{equation}
We recall that $L_{XX}$ is the largest (with respect to $\y$) Lipschitz constant of $\x \mapsto \nabla_x \L(\x,\y)$. Note that $L_{XX}$ is upper bounded by the global Lipschitz constant of $\nabla \L$. We can compute an upper bound on the supremum of the norm of $\nabla_y \L$ the same way.

With these above defined affine invariant constants,  we can finally relate the two primal suboptimalities as $h_t \leq \mathcal{O}(\sqrt{w_t})$.
\begin{proposition}
\label{prop:relation_primal}
For a $(\mu_\X,\mu_\Y)$-uniformly strongly convex-concave function $\L$,
\begin{equation}
  h_t \leq P_\L \sqrt{2w_t} 
  \quad 
  \text{and} \quad
  P_\L \leq \sqrt{2} \underset{\z \in \M}{\sup} \left\{\frac{\|\nabla_x \L(\z)\|_{\X^*}}{\sqrt{\mu_\X}} ,\frac{\|\nabla_y \L(\z)\|_{\Y^*}}{\sqrt{\mu_\Y}}   \right\}.
\end{equation}
\end{proposition}

\proof We will first work on $h_t^{(x)}$:
  \begin{align*}
   h_t^{(x)} &=\L(\xt,\ytm) -\L^* \\
    &\leq \L(\xt,\ytm) -\L(\x^*,\ytm)\\
    &\leq \prodscal{\xt - \x^*}{\nabla_x \L(\xt,\ytm} 
      \qquad\qquad \text{(by convexity)}\\
    & \leq P_\X \sqrt{\wt^{(x)}} \qquad (\text{def of } P_\X \: \eqref{def:constant_relation_primal}).
  \end{align*}
We can do the same thing for $h_t^{(y)}$ and $w_t^{(y)}$, thus 
\begin{equation}
h_t \leq P_\L \left(\sqrt{\wt^{(x)}} + \sqrt{\wt^{(y)}} \right) \leq P_\L \sqrt{2 w_t} ,    
\end{equation}
where the last inequality uses $\sqrt{a} + \sqrt{b} \leq \sqrt{2 (a+b)}$.
Finally, the inequality on $P_\L$ is from Proposition~\ref{prop:upper_bound_P_L}.
\endproof
%

\section{Relations between primal suboptimalities and dual gaps}\label{sec:relations_gaps}
\subsection{Primal suboptimalities}
  Recall that we introduced $\xtm := \argmin_{\x \in \X} \L(\x, \yt)$ 
  and similarly $\ytm := \argmax_{\y \in \Y} \L(\xt, \y).$
  Then the primal suboptimality is the positive quantity 
    \begin{equation}\label{def:primal}
    h_t := \L(\xt,\ytm)- \L(\xtm,\yt). 
    \end{equation}
  To get a convergence rate, one has to upper bound the primal suboptimality defined in~\eqref{def:primal}, but it is hard to work with the moving quantities $\xtm$ and $\ytm$ in the analysis. This is why we use in our analysis a different merit function that uses the (fixed) saddle point $(\x^*,\y^*)$ of $\L$ in its definition. We recall its definition below.

   \paragraph{Second primal suboptimality.} %
   \label{par:second_primal_gap}
  We define the second primal suboptimality for $\L$ of the iterate $(\xt, \yt)$ with respect to the saddle point $(\x^*,\y^*)$ as the positive quantity: 
    \begin{equation} 
  \label{def:primal2}
    \wt := \L(\xt,\y^*)- \L(\x^*,\yt). 
    \end{equation}
  It follows from $\L(\xt,\ytm) \geq \L(\xt,\y^*)$ and $\L(\x^*,\yt) \geq \L(\xtm,\yt)$ that $w_t \leq h_t$. 
  Furthermore, under the assumption of uniform strong convex-concavity, we proved in Proposition~\ref{prop:relation_primal} that the square root of $w_t$ upper bounds $h_t$ up to a constant.
   %

  %
  %
  %
  %
  %

  %

  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
    
  %


\subsection{Gap inequalities} \label{app:gapInequalities}


  In this section, we will prove the crucial inequalities relating suboptimalities and the gap function.
  %
  %
  Let's recall the definition of $\st$ and $\vt$:
    \begin{equation}\st:=  \argmin_{\s \in \M}
         \prodscal{\s}{
       \rt}
    \quad \text{  and } \quad 
    \vt:=  \argmax_{\vv \in \SS^{t}_x\times \SS^{t}_y}
         \prodscal{\vv}{
        \rt}
         \end{equation}
  where $(\rt)^\top := ( (\rt_x)^\top, (\rt_y)^\top ) :=  \left(
        \nabla_x \L(\xt,\yt),
         -\nabla_y \L(\xt,\yt)
         \right)$. 
  Also, the following various gaps are defined as 
    \begin{equation}\label{eq:gaps} 
    \gap^{\FW} :=\prodscal{\dt_{\FW}}{-\rt} 
    , \qquad
    \gap^{\PW} :=\prodscal{\dt_{\PW}}{-\rt}
    \quad \text{and} \quad
    \gap := \prodscal{\dt}{-\rt}
    \end{equation}
  where $\dt_{\FW} = \st - \zt$ and $\dt_{\PW} = \st - \vt$. The direction $\dt$ is the direction chosen by the algorithm at step $t$: it is always $\dt_{FW}$ for SP-FW, and can be either $\dt_{FW}$ or $\dt_{\away} := \zt - \vt$ for SP-AFW. Even if the definitions of these gaps are different, the formalism for the analysis of the convergence of both algorithms is going to be fairly similar.
  It is straightforward to notice that $\gap^{\PW} \geq \gap$ and one can show that the current gap $\gap$ is lower bounded by half of $\gap^{\PW}$:
  \begin{lemma}\label{lemma:gap_FW_PW}
     For the SP-AFW algorithm, the current gap $\gap$ can be bounded as follows:
     \begin{equation}
      \frac{1}{2} \gap^{\PW} \leq \gap \leq \gap^{\PW}
     \end{equation}
   \end{lemma} 
   \proof
    First let's show the RHS of the inequality, 
    \begin{equation}
      \gap^{\PW} := \prodscal{\dt_{\PW}}{-\rt} = \prodscal{\dt_{\away}}{-\rt} + \prodscal{\dt_{\FW}}{-\rt} \geq \prodscal{\dt}{-\rt}
    \end{equation}
    because both $\prodscal{\dt_{\away}}{-\rt}\geq 0$ and $ \prodscal{\dt_{\FW}}{-\rt} \geq 0$ from their definition.
    For the LHS inequality, we use the fact that $\gap = \max \left\{ \prodscal{\dt_{\away}}{-\rt} , \prodscal{\dt_{\FW}}{-\rt} \right\}$ for SP-AFW and thus:
    \begin{equation}
       \gap^{\PW} = \prodscal{\dt_{\away}}{-\rt} + \prodscal{\dt_{\FW}}{-\rt} \leq 2 \gap .
    \end{equation}
   \endproof
   In the following, we will assume that we are in one of the two following cases: 
   \leqnomode
  \begin{equation}
    \label{case:1}
    \text{The saddle point of } \L \text{ belongs to the relative interior of } \M.
    \tag{I}
  \end{equation}
  \begin{equation}
    \label{case:2}
    \X \text{ and } \Y \text{ are polytopes}, \quad \text{i.e.} \; \exists \A, \B \text{ finite s.t}\;\; \X = \conv(\A), \; \Y = \conv(\B).
    \tag{P}
  \end{equation}
  \reqnomode

  Then either $\muIntL>0$ (case~\ref{enu:situation1}) or $\mu^{\away}_\L>0$ (case~\ref{enu:situation2}).
  Let's write the gap function as the sum of two smaller gap functions:
    \begin{equation} \label{eq:gtGeneralized}
      \gap 
      = \quad \underbrace{\prodscal{ \dt_{(x)}}{-\rt_x}}_{=:\gap^{(x)}} \quad
      + \quad
    \underbrace{\prodscal{ \dt_{(y)}}{ -\rt_y}}_{=:\gap^{(y)}} 
    \end{equation}
  %
  %
  %
  Because of the convex-concavity of $\L$, this scalar product bounds
  the differences between the value of $\L$ at the point
  $(\xt,\yt)$ and the value of $\L$ at another point. Hence this
  gap function upper-bounds $h_t$ and $w_t$ defined in~\eqref{def:primal} and~\eqref{def:primal2}. More concretely, we
  have the following lemma.



  \begin{lemma} \label{lemme:g}
    For all $t$ in $\N$, $\x\in \X$ and $ \y \in \Y$
      \begin{equation} \label{eq:encadre}
      \gap^{\PW} \geq \gap^{\FW} \geq  \L(\xt,\y)-\L(\x, \yt),
      \end{equation}
    and, furthermore,
      \begin{equation} \label{eq:gtInequality_with_ht}
      \gap \geq  h_t  \geq \wt.
      \end{equation}
  \end{lemma}
  \proof 
    First let's show the LHS of \eqref{eq:encadre}, 
    \begin{equation}
      \gap^{\PW} = \prodscal{\dt_{\PW}}{-\rt} = \prodscal{\dt_{\away}}{-\rt} + \prodscal{\dt_{\FW}}{-\rt} \geq \prodscal{\dt_{\FW}}{-\rt} = \gap^{\FW}
    \end{equation}
    because one can easily derive that $\prodscal{\dt_{\away}}{-\rt}\geq 0$ from the definition of the away direction $\dt_{\away}$.
    It follows from convexity of $\x \mapsto \L(\x, \yt)$ that for all $\x$ in $\X$, 
        \begin{align} \label{eq:gapx}
           (\gap^{\FW})_{x} :=\prodscal{ (\dt_{\FW})_x}{ -\gnx}
              & \geq \prodscal{ \x - \xt}{-\gnx}\\
              &\geq \L(\xt,\yt) - \L(\x, \yt). 
        \end{align}
    A similar inequality emerges through the convexity of $\y \mapsto -\L(\xt,\y)$,
      \begin{equation}
        (\gap^{\FW})_{y} := \prodscal{ (\dt_{\FW})_y}{\gny} \geq \L(\xt, \y) - \L(\xt,\yt),
      \end{equation}
    which gives us 
      \begin{equation} 
      \gap^{\FW} \geq \L(\xt, \yt) - \L(\x, \yt) + \L(\xt, \y) - \L(\xt , \yt) ,
      \end{equation}
     which shows~\eqref{eq:encadre}.
     By using $\x = \xtm$ and $\y = \ytm$ in~\eqref{eq:encadre}, we get $\gap^{\FW} \geq h_t$. We also know that $g_t = \max(\gap^\away, \gap^\FW) \geq \gap^{\FW}$ for SP-AFW. So combining with $h_t \geq w_t$ that we already knew, we get~\eqref{eq:gtInequality_with_ht}.
  \endproof
  Next, we recall two lemmas, one from~\citesup{lacoste2013affine} and the other one from \citep{lacoste2015global}. These lemmas upper bound the primal suboptimality with the square of the gap times a constant depending on the geometric (or the interior) strong convexity constant.

  %

  \begin{lemma}[\citet{lacoste2015global}, \citetsup{lacoste2013affine}] \label{lemme:lingap}
   If $f$ is strongly convex, then for any $\xt \in \X$, 
    \begin{equation}
     f(\xt)-f(\xc) \leq \frac{\left( \gap^{\FW} \right)^2}{2 \mu^{\xc}_f}
     \quad
     \text{if } \xc \in \text{ interior of } \X
     \quad \text{\citepsup{lacoste2013affine}} 
     \end{equation}
  and 
     \begin{equation}
     f(\xt)-f^* \leq \frac{\left( \gap^{\PW} \right)^2}{2 \mu^{\away}_f}
     \quad
     \text{if } \X = \conv(\A) 
     \quad
     \text{\citep{lacoste2015global}}
    \end{equation}
  where $\gap^{\FW}= \prodscal{\xt-\st}{\nabla f(\xt)}$, $\gap^{\PW}= \prodscal{\vt-\st}{\nabla f(\xt)}$ and $f^* = \min_{x \in \X} f$.
  \end{lemma}
  Notice once again that in this lemma~\emph{we do not need $\xc$ to be optimal}.
  \proof 
  Let $\xt \neq \xc$. Using the definition of interior strong convexity~\eqref{def:int_strong} and choosing $\gamma$ such that $\xc = \xt + \gamma \left( \bar \s\left(\xt,\xc\right) - \xt \right)$, we get
    \begin{align*}
    f(\xc)-f(\xt) &\geq \gamma\prodscal{\bar\s(\xc,\xt)-\xt}{\nabla f(\xt)} + \gamma^2 \frac{ \mu^{\xc}_f}{2}\\
    & \geq - \gamma\gap^{\FW}+ \gamma^2 \frac{ \mu^{\xc}_f }{2}  \\
    & \geq  \frac{ - \left( \gap^{\FW} \right)^2 }{2\mu^{\xc}_f}.
    \end{align*}
  The last line of this derivation is obtained through the inequality: $-a^2 + 2ab - b^2 \leq 0$. 
  If $\xt = \xc$ the inequality is just the positivity of the gap.

  For the second statement, we will use the definition of the geometric strong convexity constant (Equation~\eqref{def:geom_strong}) at the point $\x = \xt$ and $\x^* \in \argmin_{\x \in \X} f(x)$. 
  Recall that $\gamma^\away(\x,\x^*) = \frac{\prodscal{-\nabla f(\x)}{\x^*-\x}}{\prodscal{-\nabla f(\x)}{\s_f(\x)-\vv_f(\x)}}$.
    \begin{align*}
    f(\x^*)-f(\xt) &\geq \prodscal{\x^*-\xt}{\nabla f(\xt)} + \frac{ \mu^{\away}_f}{2}\gamma^\away(\xt, \x^*)^2 \\
    & = -\gamma^\away(\xt, \x^*) \prodscal{\s_f(\xt) - \vv_f(\xt)}{-\nabla f(\xt)}  + \frac{ \mu^{\away}_f}{2} \gamma^\away(\xt, \x^*)^2  \\
    & \geq - \gamma^\away(\xt, \x^*)\gap^{\PW}+  \frac{ \mu^{\away}_f }{2}\gamma^\away(\xt, \x^*)^2 \qquad \text{(Equation~\eqref{eq:gammaA})}  \\
    & \geq  \frac{ - \left( \gap^{\PW} \right)^2 }{2\mu^{\away}_f}.
    \end{align*}
  \endproof
  Lemma~\ref{lemme:lingap} is useful to understand the following lemma and its proof which is just an extension to
  the convex-concave case.


  \begin{lemma}[Quadratic gap upper bound on second suboptimality for~\eqref{case:1} or~\eqref{case:2}]\label{lemme:gap}
   If $\L$ is a strongly convex-concave function, then for any $(\xt,\yt) \in \M$,
    \begin{equation}
     \wt  \leq \frac{(\gap^{\FW})^2}{2 \muIntL } 
     \quad 
      \text{for \eqref{case:1}}
     \quad \text{and} \quad
     \wt \leq h_t \leq \frac{(\gap^{\PW})^2}{2 \mu^{\away}_\L}
     \quad
     \text{for \eqref{case:2}} %
    \end{equation}
  where the gaps are defined in \eqref{eq:gaps}, $\muIntL := \min \{ \mu^{\x^*}_\L, \mu^{\y^*}_\L\}$ (i.e. using the reference points $(\xc,\yc) := (\x^*,\y^*)$ in the definition~\eqref{def:interior_strongL})
  and $\mu^{\away}_\L$ is the geometric strong convex-concavity of $\L$ over $\Vertices$, as defined in~\eqref{def:geom_strongL}.
  \end{lemma}

  \proof For \eqref{case:1}:

   Let the function $f$ on $\X$ be defined by $f(\x) = \L(\x',\yt)$, and the function $g$ on $\Y$ be $g(\y') = -\L(\xt, \y')$. Then using the Lemma~\ref{lemme:lingap} on the function $f$ with the reference point $\x^*$, and on $g$ with reference point $\y^*$, we get
    \begin{align*}
      \L(\xt, \yt) - \L(\x^*,\yt)  & \leq  \frac{ \prodscal{\st_x - \xt }{-\nabla_x \L(\xt,\yt)}^2 }{2 \mu_f^{\x^*}}  \\
      \L(\xt, \y^*) - \L(\xt,\yt)  & \leq   \frac{ \prodscal{\st_y - \yt }{\nabla_y \L(\xt,\yt)}^2 }{2 \mu^{\y^*}_g} .
    \end{align*}
    As $\muIntL$ is smaller than both $\mu_f^{\x^*}$ and $\mu_g^{\y^*}$ by the definition~\eqref{def:interior_strongL}, we can use it in the denominator of the above two inequalities.
    As we saw from Section~\ref{app:gapInequalities} in~\eqref{eq:gtGeneralized}, the gap can be split as sum of the gap of the block $\X$ and the gap of the block $\Y$, i.e. $\gap^{\FW} =  \prodscal{\st_x- \xt}{-\nabla_x \L(\xt,\yt)} +  \prodscal{\st_y -\yt}{\nabla_y \L(\xt,\yt)} $.
    Then, using the inequality: $a^2+b^2 \leq (a+b)^2$ for $(a  ,b \geq
    0)$, we obtain
      \begin{equation} \label{eq:proof_w_FWgap}
      w_t \leq \frac{ (\gap^{\FW})^2}{2 \muIntL}.
      \end{equation}

      For \eqref{case:2}:

    Using the Lemma~\ref{lemme:lingap} for case~\eqref{enu:situation2} on the same functions $f$ and $g$ defined above, we get
      \begin{align*}
        \L(\xt, \yt) - \L(\xtm,\yt)  & \leq  \frac{  \prodscal{\st_x - \vt_x }{\nabla_x \L(\xt,\yt)}^2 }{2 \mu^{\away}_{f}}  \\
        \L(\xt, \ytm) - \L(\xt,\yt)  & \leq   \frac{  \prodscal{\st_y -\vt_y }{-\nabla_y \L(\xt,\yt)}^2 }{2 \mu^{\away}_{g}}.
      \end{align*}
    Using a similar argument as the one to get~\eqref{eq:proof_w_FWgap}, using that $\mu^{\away}_\L$ is smaller than both $\mu^{\away}_{f}$ and $\mu^{\away}_{g}$, and referring to the separation of the gap~\eqref{eq:gtGeneralized}, we get
      \begin{equation} 
       h_t \leq \frac{ (\gap^{\PW})^2}{2 \mu^{\away}_\L}.
      \end{equation}

  \endproof



%

\section{Convergence analysis}
\label{appendix:analysis}
In this section, we are going to show two important lemmas. The first one shows that under some assumptions we can get a Frank-Wolfe-style induction scheme relating the second suboptimality of the potential update $w_{\gamma}$, the current value of the second suboptimality $w_t$, the gap~$\gap$ and any step size $\gamma \in [0,\stepmax]$. The second lemma will relate the gap and the square root of $w_t$; this relation enables us to get a rate on the gap after getting a rate on $w_t$.

\subsection{First lemmas}
  %
  %
  The first lemma in this section is inspired from the standard FW progress lemma, such as Lemma~C.2 in~\citep{lacoste2013block}, though it requires a non-trivial change due to the compensation phenomenon for~$\L$ mentioned in the main text in~\eqref{eq:oscilation}.
  In the following, we define the possible updated iterate $\z_\stepsize$ for $\stepsize \in [0, \stepmax]$:
  \begin{equation}
  \z_\gamma := (\x_\gamma, \y_\gamma):= \zt + \gamma \dt, \quad \text{ where } \quad \dt \text{ is the direction of the step.}
  \end{equation}
  For a FW step $\dt = \dt_{\FW}:= \st - \zt$ and for an away step $\dt = \dt_{\away} := \zt - \vt$. We also define the corresponding new suboptimality for $\z_\gamma$:
  \begin{equation}
    %
    w_\gamma := \L(\x_\gamma, \y^*) - \L(\x^*,\y_\gamma).
  \end{equation}
  %
  %
  %
  %
  %
  %
  \begin{lemma}[Suboptimality progress for SP-FW and SP-AFW] \label{lemme:ineg}
  Let $\L$ be strongly convex-concave, 
  %

    If we are in case~\eqref{case:1} and $\dt = \dt_{\FW}$ is a FW direction, we have for any $\stepsize \in [0,1]$:
    %
      \begin{equation}  \label{eq:wInequalityCaseI}
      w_\gamma 
       \leq \wt  
      - \CondNumb^{\FW}\gamma \gap^{\FW}
      + \gamma^2 {C_\L},
      \quad \text{where} \quad 
      \CondNumb^{\FW} :=1- \frac{M_\L }{\sqrt{\muIntL}}.
      \end{equation}
    If we are in case~\eqref{case:2} and $\dt$ is defined from a step of SP-AFW (Algorithm~\ref{alg:AFW}), we have for any $\stepsize \in [0, \stepmax]$:
      \begin{equation} \label{eq:wInequalityCaseP} 
      w_\gamma 
       \leq \wt  
      - \CondNumb^{\PW} \gamma \gap^{\PW}
      + \gamma^2 {C_\L},
      \quad \text{where} \quad
      \CondNumb^{\PW} := \frac{1}{2}- \frac{M_\L }{\sqrt{\mu^{\away}_\L}}.
      \end{equation}
  %
 %
  \end{lemma}

  \proof 
    The beginning of the argument works with any direction $\dt$. 
    Recall that $\wt^{(x)} := \L(\xt, \y^*)- \L^*$ and $\wt^{(y)} := -\L(\x^*,\yt) + \L^*$.
    Now writing $\x_\gamma = \xt + \gamma \dt_x$ and using the definition of the curvature $C_f$~\eqref{CurvB} for the function $\x\mapsto f(\x) := \L(\x,\y^*)$, we get
      \begin{align}
      w_\gamma^{(x)} &:= \L(\x_\gamma,\y^*) - \L^* \notag\\
        &\leq \L(\xt, \y^*) - \L^* + \gamma \prodscal{\dt_x}{\nabla_x\L(\xt, \y^*)} + \gamma^2 \frac{C_\L}{2},
      \end{align} 
    since $C_\L \leq C_f$ by definition~\eqref{eq:CL}. 
    Recall that the gap function $\gap$ can be decomposed by~\eqref{eq:gtGeneralized} into two smaller gap functions $\gap^{(x)} := \prodscal{\dt_x}{-\rt_x}$ and $\gap^{(y)} := \prodscal{\dt_y}{-\rt_y}$. We define $\epsilon_t := \prodscal{\dt_x}{\nabla_x\L(\xt, \y^*)- \gnx}$ to be the sequence representing the error between the gradient used for the minimization and the gradient at the point $(\xt, \y^*)$. Then,
       \begin{equation}\label{eq:un}
          w_\gamma^{(x)}   \leq   \wt^{(x)} - \gamma \gap^{(x)} +\gamma \epsilon_t + \gamma^2 \frac{C_\L}{2}.
      \end{equation}
    Now, as $M_{XY}$ is finite (under Lipschitz gradient assumption), we can use the definition of the bilinearity constant~\eqref{def:M} to get
      \begin{equation}\label{eq:grad}
      |\epsilon_t| = \left|\prodscal{\dt_x}{\nabla_x\L(\xt, \y^*)- \gnx}\right| \leq \sqrt{\wt^{(y)}} M_{XY}.
       \end{equation}
    Combining equations \eqref{eq:un} and \eqref{eq:grad} we finally obtain
      \begin{equation}
        w_\gamma^{(x)} \leq \wt^{(x)} - \gamma \gap^{(x)} + \gamma M_{XY} \sqrt{\wt^{(y)}}
          + \gamma^2\frac{ C_\L}{2}.
      \end{equation}
     We can get an analogous inequality for $w_\gamma^{(y)}$,
       \begin{equation}
        w_\gamma^{(y)} \leq \wt^{(y)} - \gamma \gap^{(y)} + \gamma M_{YX} \sqrt{ \wt^{(x)}}
          + \gamma^2 \frac{C_\L}{2}.
      \end{equation}
    Then adding $w_\gamma^{(x)}$ and $w_\gamma^{(y)}$ and using $\sqrt{a} + \sqrt{b} \leq \sqrt{2(a+b)}$ (coming from the concavity of $\sqrt{\cdot}$), we get
      \begin{equation} \label{eq:wgammaInt}
      w_\gamma\leq  \wt - \gamma \gap +\gamma M_{\L} \sqrt{2\wt}+ 2\gamma^2 \frac{C_\L}{2}.
      \end{equation}
    We stress that the above inequality~\eqref{eq:wgammaInt} is valid for any direction $\dt$, using $g_t := \prodscal{\dt}{-\rt}$, and for any feasible step size $\stepsize$ such that $\z_\stepsize \in \X \times \Y$ (the last condition was used in the definition of $C_f$; see also footnote~\ref{foot:Cfcomment} for more information).
      
  To finish the argument, we now use the specific property of the direction $\dt$ and use the crucial Lemma~\ref{lemme:gap} that relates~$w_t$ with the square of the appropriate gap.
  
  For the case~\eqref{case:1} of interior saddle point, we consider $\dt = \dt_{\FW}$ and thus $g_t = \gap^{\FW}$. Then combining Lemma~\ref{lemme:gap} (using the interior strong convexity constant) with~\eqref{eq:wgammaInt}, we get
      \begin{equation}\label{eq:int_strong_usual_gap}w_\gamma\leq  \wt - \gamma \left( 1 - \frac{M_\L}{\sqrt{\muIntL}}\right)\gap^{\FW}+ \gamma^2 C_\L.
      \end{equation}
      
    For the case~\eqref{case:2} of polytope domains, we consider $\dt$ as defined by the SP-AFW algorithm. We thus have $\gap \geq \frac{1}{2} \gap^{\PW}$ by Lemma~\ref{lemma:gap_FW_PW}. Then combining Lemma~\ref{lemme:gap} (using the geometric strong convexity constant) with~\eqref{eq:wgammaInt}, we get
      \begin{equation}\label{eq:geom_strong_pfw_gap}
      w_\gamma\leq  \wt - \gamma \left( \frac{1}{2} - \frac{M_\L}{\sqrt{\mu^{\away}_\L}}\right)\gap^{\PW}+ \gamma^2 C_\L ,
      \end{equation}
      which finishes the proof. Still in the case~\eqref{case:2}, we also present an inequality in terms of the direction gap $\gap$ (which yields a better constant that will be important for the sublinear convergence proof in Theorem~\ref{thm:conv_sublin}) by using instead the inequality $\gap^{\PW} \geq \gap$ (Lemma~\ref{lemma:gap_FW_PW}) with Lemma~\ref{lemme:gap} and~\eqref{eq:wgammaInt}:
      \begin{equation}\label{eq:geom_strong_gt_gap}
      w_\gamma\leq  \wt - \gamma \left( 1 - \frac{M_\L}{\sqrt{\mu^{\away}_\L}}\right)\gap + \gamma^2 C_\L.
      \end{equation}
  \endproof

The above lemma uses a specific update direction~$\dt$ to get a potential new suboptimality~$w_\stepsize$. By using the property that $w_\stepsize \geq 0$ always, we can actually derive an upper bound on the gap in terms of $w_t$ \emph{irrespective of any algorithm} (i.e. this relationship holds for any possible feasible point $(\xt,\yt)$). More precisely, for SP-FW algorithm (case~\eqref{enu:situation1}) the only thing we need to set is a feasible point $\zt$ but for the SP-AFW algorithm (case~\eqref{enu:situation2}) we also need an active set expansion for $\zt$ for which the maximum away step size is larger than $\frac{\CondNumb\gap}{2C_\L}$ (which can potentially not be the active set calculated by an algorithm). 
%
This is stated in the following theorem, which is a saddle point generalization of the gap upper bound given in Theorem~2 of~\citep{lacoste2015global}. 

\begin{theorem}[Bounding the gap with the second suboptimality] \label{thm:subopt_gap}
  If $\L$ is strongly convex-concave and has a finite curvature constant then
  \begin{itemize}
    \item case~\eqref{enu:situation1}: For any $\zt \in \M$,
      \begin{equation}    
        \gap^{\FW} \leq \frac{2}{\CondNumb^{\FW}} \max \left\{\sqrt{C_\L w_t},w_t \right\}.
      \end{equation}
      Since $\zt$ is fixed, this statement is algorithm free.
    \item case~\eqref{enu:situation2}:  For any $\zt \in \M$, if there exists  an active set expansion for $\zt$ for which $\stepmax = 1$ or $\stepmax \geq \frac{\CondNumb^{\PW}\gap^{\PW}}{2C_\L}$ (see~\eqref{eq:maxstep} for the definition of $\stepmax$) then,
      \begin{equation}    
        \gap^{\PW} \leq \frac{2}{\CondNumb^{\PW}} \max \left\{\sqrt{C_\L w_t},w_t \right\}.
     \end{equation}
  \end{itemize}
  Both statement are algorithm free but $\gap^{\PW}$ depends on a chosen expansion of $\zt$:
   \begin{equation}\label{eq:expansion}
    \zt = \sum_{(\vv_x,\vv_y) \in \mathcal S^{(t)}} \alpha_{\vv_x}^{(t)}\alpha_{\vv_y}^{(t)} (\vv_x,\vv_y)
    \quad \text{where} \quad
     \mathcal S^{(t)} := \left\{ (\vv_x,\vv_y) \in \A\times\B\;;\; \alpha_{\vv_x}^{(t)}\alpha_{\vv_y}^{(t)} >0 \right\},
  \end{equation}
    because,
   \begin{equation}
   \begin{aligned}
   &\gap^{\PW} := \innerProdCompressed{-\rt}{\dt_\FW + \dt_\away} \\[1mm] 
    &\text{where} \quad \dt_\away := \zt - \argmax_{\vv \in \Coreset_x \times \Coreset_y} \innerProdCompressed{\rt}{\vv}, \\ 
    &\text{and} \quad \dt_\FW := \argmin_{\vv \in \A \times \B} \innerProdCompressed{\rt}{\vv} - \zt.
    \end{aligned} 
  \end{equation}
  The maximum step size associated with the active set expansion described in Equation~\eqref{eq:expansion} is 
  \begin{equation} \label{eq:maxstep}
     \gamma_{\max} := \left\{ \begin{array}
     {ll}
     1 \quad \text{if} \quad \innerProd{-\rt}{\dt_\away} \leq \innerProd{-\rt}{\dt_\FW},&\\
     \min\left\{\frac{\alpha^{(t)}_{\vt_x}}{1 -\alpha^{(t)}_{\vt_x}},\frac{\alpha^{(t)}_{\vt_y}}{1 -\alpha^{(t)}_{\vt_y}} \right\} \quad\text{otherwise}.&
     \end{array}\right.
  \end{equation}
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  \end{theorem}
  \proof In this proof, we let $(\gap, \CondNumb)$ to stand respectively for $(\gap^{\FW},\CondNumb^{\FW})$ for case~\eqref{case:1} or $(\gap^{\PW},\CondNumb^{\PW})$ for case~\eqref{case:2}.
  %
  %
    We will start from the inequalities~\eqref{eq:wInequalityCaseI} and~\eqref{eq:wInequalityCaseP} in Lemma~\ref{lemme:ineg}. Equation~\eqref{eq:wInequalityCaseI} is valid considering a FW direction $\dt_\FW$, Equation~\eqref{eq:wInequalityCaseP} is valid if we consider the direction that would have be set by the SP-AFW algorithm if it was run at point $\zt$ with the active set expansion described in the theorem statement.
    Since $w_\gamma \geq 0$,  for both cases become :
    \begin{equation}
     0 \leq \wt - \gamma \CondNumb\gap+ \gamma^2 C_\L,
    \end{equation}
    then we can put the gap on the LHS,
    \begin{equation} \label{eq:int_subopt_gap}
    \stepsize\CondNumb\gap - \stepsize^2 C_\L \leq \wt.
    \end{equation}
    This inequality is valid for any $\stepsize \in [0,\stepmax]$.
    In order to get the tightest bound between the gap and the suboptimality, we will maximize the LHS. It can be maximized with $\bar{\gamma} := \frac{\CondNumb\gap}{2C_\L}= \stepsize_t$.
    Now we have two cases: 

    If $\bar{\gamma} \in [0,\stepmax]$, then we get: $\: \CondNumb\gap \leq 2\sqrt{C_\L \, w_t}.$

    And if $\stepmax = 1$ and $\bar{\gamma} = \frac{\CondNumb\gap}{2C_\L} > 1$, then setting $\stepsize=1$ we get: $\: \CondNumb\gap \leq 2 w_t$. By taking the maximum between the two options, we get the theorem statement.
    %
  \endproof
  %
  The previous theorem guarantees that the gap gets small when $w_t$ gets small (only for the non-drop steps if in situation~\eqref{case:2}).
  As we use the gap as a stopping criterion in the algorithm, this is a useful theorem to provide an upper bound on the number of iterations needed to get a certificate of suboptimality.
  
  The following corollary provides a better bound on $h_t$ than the inequality $h_t \leq \textrm{cst} \sqrt{w_t}$ previously shown \eqref{eq:relate_h_w} when the situation is \eqref{enu:situation2}. It will be useful later to get a better rate of convergence for $h_t$ under hypothesis~\eqref{case:2}. 
  \begin{corollary}[Tighter bound on $h_t$ for non-drop steps in situation~\eqref{case:2}]
  \label{cor:h_w}
  Suppose that $\L$ is strongly convex-concave and has a finite curvature constant, and that the domain is a product of polytopes (i.e. we are in situation~\eqref{case:2}). Let $\zt \in \M$  be given. If there exists an active set expansion for $\zt$ for which the maximum step size is larger than $\frac{\CondNumb\gap}{2C_\L}$ (see Theorem~\eqref{thm:subopt_gap} for more details) ,
    \begin{equation}
            h_t \leq \frac{2 \max \{C_\L,w_t\}}{\mu^{\away}_\L (\CondNumb^\PW)^2} w_t ,
    \end{equation}
    where $\CondNumb^\PW$ is defined in~\eqref{eq:wInequalityCaseP}.
  \end{corollary}
  \proof
  By Lemma~\ref{lemme:gap} in situation~\eqref{case:2}, we have:
  \begin{equation*}
  h_t \leq \frac{(\gap^{\PW})^2}{2 \mu^{\away}_\L} \leq \frac{2\max \{C_\L,w_t\}}{\mu^{\away}_\L (\CondNumb^\PW)^2} w_t.
  \end{equation*}
  The last inequality is obtained by applying the upper bound on the gap given in the previous Theorem~\ref{thm:subopt_gap}.
  \endproof

%

\subsection{Proof of Theorem~\ref{thm:conv}}
  \label{subsec:proof_main_thm}
  In this section, we will prove that under some conditions on the constant defined in subsections~\ref{sub:curv} and~\ref{sub:affine_invariant_measures_of_strong_convexity}, the suboptimalities $\wt$
  vanish linearly with the adaptive step size $\gamma_t = \min \left\{\stepmax,\frac{\CondNumb}{2C_\L}\gap\right\}$ or sublinearly with the universal step size $\gamma_t = \min \left\{\stepmax,\frac{2}{2+k(t)}\right\}$.
  \begin{lemma}[Geometric decrease of second suboptimality] 
    \label{lemme:Lin} Let $\L$ be a 
    strongly convex-concave function with a smoothness constant $C_\L$, a positive interior strong convex-concavity constant $\muIntL$~\eqref{def:interior_strongL} or a positive geometric strong convex-concavity $\mu^{\away}_\L$~\eqref{def:geom_strongL}. Let us also define the rate multipliers~$\CondNumb$ as
  \begin{equation}
    \CondNumb^{\FW} :=1- \frac{M_\L }{\sqrt{\muIntL}} \quad\text{and}\quad \CondNumb^{\PW} := \frac{1}{2}- \frac{M_\L }{\sqrt{\mu^{\away}_\L}} \quad(\text{see Equation~\eqref{eq:def_bilin_coef} for the definition of } M_\L).
  \end{equation}
    Let the tuple $(\gap,\CondNumb,\mu_\L)$ refers to
  either $(\gap^{\FW},\CondNumb^{\FW},\muIntL)$ for case~\eqref{case:1} where the algorithm is SP-FW , or $(\gap^{\PW},\CondNumb^{\PW},\mu^{\away}_\L)$ for case~\eqref{case:2} %
  %
  where the algorithm is SP-AFW,

        If $\CondNumb > 0$, then  at each non-drop step (when $\gamma_t < \gamma_{\max}$ or $ \gamma_{\max} \geq 1$ ),
        the suboptimality $w_t$ of the algorithm with
        step size $\stepsize_t = \min(\stepmax, \frac{\CondNumb}{2C_\L}\gap)$ decreases
        geometrically as
          \begin{equation} 
             \wtt \leq (1- \rho_\L) \wt
          \end{equation}
        where $\rho_\L := \frac{\CondNumb^2}{2} \frac{\mu_\L}{C_\L}$.
        Moreover, for case \eqref{enu:situation1} there is no drop step and for case~\eqref{enu:situation2} the number of drop step (when $\gamma_t = \gamma_{\max}$) is upper bounded by two third of the number of iteration (see Section~\ref{par:drop_steps}, Equation~\eqref{eq:upper_bound_drop_steps}), while when we have a drop step, we still have:
        \begin{equation}
         \wtt \leq \wt.
        \end{equation}
  \end{lemma}

  \proof 
  The bulk of the proof is of a similar form for both SP-FW and SP-AFW, and so in the following, we let $(\gap, \mu_\L, \CondNumb)$ to stand respectively for $(\gap^{\FW},\muIntL,\CondNumb^{\FW})$ for SP-FW (case~\eqref{case:1}) or $(\gap^{\PW},\mu^{\away}_\L,\CondNumb^{\PW})$ for SP-AFW (case~\eqref{case:2}). As $\stepsize_t \leq \stepmax$, we can apply the important Lemma~\ref{lemme:ineg} with $\stepsize = \stepsize_t$ (the actual step size that was taken in the algorithm) to get:
  \begin{equation} \label{eq:wtIneq}
  w_{t+1} = w_{\stepsize_t}  
         \leq \wt  
        - \CondNumb \stepsize_t \gap
        + \gamma_t^2 {C_\L} . 
  \end{equation}
  We note in passing that the adaptive step size rule $\stepsize_t = \min(\stepmax, \frac{\CondNumb}{2 C_\L}\gap)$ was specifically chosen to minimize the RHS of~\eqref{eq:wtIneq} among the feasible step sizes. 
  
  If $\frac\CondNumb{2C_\L}\gap\leq \stepmax$, then we have $\gamma_t = \frac\CondNumb{2C_\L}\gap$ and so~\eqref{eq:wtIneq} becomes:
      \begin{equation}
        \wtt \leq \wt - \frac{\CondNumb^2}{ 2C_\L}  (\gap)^2+ \frac{\CondNumb^2}{ 4C_\L}(\gap)^2 = \wt - \frac{\CondNumb^2}{4C_\L} (\gap)^2.
      \end{equation}
    Applying the fact that the square of the appropriate gap upper bounds $w_t$ (Lemma~\ref{lemme:gap} with a similar form for both cases~\eqref{case:1} and~\eqref{case:2}), we directly obtain the claimed geometric decrease
      \begin{equation}
        \wtt \leq \wt \left(1- \frac{\CondNumb^2}{2} \frac{\mu_\L}{C_\L} \right).
      \end{equation}
    If $ \frac\CondNumb{2 C_\L}\gap> \stepmax$, then we have $\gamma_t = \stepmax$ and so~\eqref{eq:wtIneq} becomes:
    \begin{align}
      \wtt 
      &\leq  \wt - \CondNumb \stepmax \gap + \stepmax^2 C_\L & \notag\\
      & \leq \wt - \CondNumb \stepmax \gap + \frac\CondNumb{2} \stepmax \gap &( \text{using}\quad  C_\L < \frac{\CondNumb}{2 \stepmax}\gap) \\
      & \leq \wt - \frac\CondNumb{2} \stepmax \gap &\notag\\
       & \leq \wt \left(1-\frac{\CondNumb}{2}\stepmax\right).& (w_t \leq g_t \text{ by Lemma \ref{lemme:g}})
%
%
    \end{align}
    If $\stepmax \geq 1$ (either we are taking a FW step or an away step with a big step size), then the geometric rate is at least $(1-\frac{\CondNumb}{2})$, which is a better rate than $\rho_\L$ since $\CondNumb^2 \leq \CondNumb$ as $\CondNumb \leq 1$, and one can show that $\frac{\mu_\L}{C_\L} \leq 1$ always (see Remark~7 in Appendix D of~\citep{lacoste2015global} for case~\eqref{enu:situation2} and use a similar argument for case~\eqref{enu:situation1}). Thus $\rho_\L$ is valid both when $\stepsize_t < \stepmax$ or $\stepmax \geq 1$, as claimed in the theorem.
    
    When $\stepsize_t = \stepmax < 1$, we cannot guarantee sufficient progress as $\stepmax$ could be arbitrarily small (this can only happen for an away step as $\stepmax = 1$ for a FW step). These are the problematic \emph{drop steps}, but as explained in Appendix~\ref{par:drop_steps} with Equation~\eqref{eq:upper_bound_drop_steps}, they cannot happen too often for SP-AFW.
    
    Finally, to show that the suboptimality cannot increase during a drop step ($\gamma_t= \gamma_{\max}$), we point out that 
    the function $\gamma \mapsto \wt - \gamma \CondNumb^{\PW}\gap^{\PW}+ \gamma^2 C_\L$ is a convex function that is minimized by $\bar{\gamma} = \frac{\CondNumb^{\PW}}{2C_\L} \gap^{\PW}$ and so is decreasing on $[0, \bar{\stepsize}]$. When $\stepsize_t = \stepmax$, we have that $\stepmax \leq \bar{\gamma}$, and thus the value for $\gamma = \gamma_{\max}$ is lower than the value for $\gamma = 0$, i.e.
      \begin{equation}
            \wtt \leq \wt - \stepmax \CondNumb^{\PW}\gap^{\PW}+ \gamma_{\max}^2 C_\L \leq \wt.
      \end{equation}
  \endproof
  %
    The previous lemma (Lemma~\ref{lemme:Lin}), the fact that the gap upper bounds the suboptimality (Lemma~\ref{lemme:g}) and the primal suboptimalities analysis lead us directly
    to the following theorem. This theorem is the affine invariant formulation with adaptive step size of Theorem~\ref{thm:conv}.

  \begin{theorem}\label{thm:conv_affine_invariant}
  Let $\L$ be a 
    strongly convex-concave function with a finite smoothness constant $C_\L$, a positive interior strong convex-concavity constant $\muIntL$~\eqref{def:interior_strongL} or a positive geometric strong convex-concavity $\mu^{\away}_\L$~\eqref{def:geom_strongL}. Let us also define the rate multipliers~$\CondNumb$ as
  \begin{equation}\label{eq:thm24_state_multiplier}
    \CondNumb^{\FW} :=1- \frac{M_\L }{\sqrt{\muIntL}} \quad\text{and}\quad \CondNumb^{\PW} := \frac{1}{2}- \frac{M_\L }{\sqrt{\mu^{\away}_\L}} \quad(\text{see Equation~\eqref{eq:def_bilin_coef} for the definition of } M_\L).
  \end{equation}
    Let the tuple $(\gap,\CondNumb,\mu_\L)$ refers to
  either $(\gap^{\FW},\CondNumb^{\FW},\muIntL)$ for case~\eqref{case:1} where the algorithm is SP-FW , or $(\gap^{\PW},\CondNumb^{\PW},\mu^{\away}_\L)$ for case~\eqref{case:2} where the algorithm is SP-AFW,

  If $\CondNumb>0$, then the suboptimality $h_t$ of the iterates of the algorithm with
  step size $\stepsize_t = \min(\stepmax, \frac{\CondNumb}{2C_\L}\gap)$ decreases
  geometrically\footnote{For a non-drop step one can use Corollary~\ref{cor:h_w} to get the better rate on $h_t$ losing the square root but with a potentially worse constant $ h_t \leq \frac{2\max \{C_\L,w_0(1-\rho_\L)^{k(t)}\}}{\mu^{\away}_\L (\CondNumb^\PW)^2} w_0(1-\rho_\L)^{k(t)}$.} as,
        \begin{equation} 
          h_t \leq P_\L \sqrt{2w_0}(1- \rho_\L)^{k(t)/2}
        \end{equation}
  where $\rho_\L := \frac{\CondNumb^2\mu_\L }{2C_\L}$ and $k(t)$ is the number of non-drop step after $t$ steps. For SP-FW, $k(t)=t$ and for SP-AFW, $k(t)\geq t/3$. Moreover we can also upper bound the minimum gap observed, for all $T \in \N$
  \begin{equation}
    \min_{t \leq T} \gap \leq \frac{2\max\{\sqrt{C_\L},\sqrt{w_0} (1-\rho_\L)^{k(T)/2}\}}{\CondNumb{}} \sqrt{w_0} \left( 1- \rho_\L \right)^{k(T)/2}.
  \end{equation}
      %
      %
      %
      %
      %
  The Theorem~\ref{thm:conv} statement can be deduced from this theorem using the lower and upper bounds on the affine invariant constant of this statement. More precisely, one can upper bound $C_\L$, $M_\L$, $P_\L$ respectively with Propositions~\ref{prop:Cbounded}, \ref{Mbounded} and \ref{prop:upper_bound_P_L} and lower bound $\muIntL$ and $\mu^{\away}_\L$ respectively with Proposition~\ref{prop:deltaL} and Equation~\eqref{eq:muAwayLInequality}.\footnote{Note that only the definition of $\delta_\mu$ in~Theorem~\ref{thm:conv} for case~\eqref{case:2} requires to use the Euclidean norm (because inequality~\eqref{eq:muAwayLInequality} with the pyramidal width only holds for the Euclidean norm). On the other hand, any norm could be used for (separately) bounding $C_\L$, $M_\L$ and $P_\L$.} 
  If we apply these bounds to the rate multipliers in~\eqref{eq:thm24_state_multiplier}, it gives the smaller rate multipliers $\CondNumb$ stated in Theorem~\ref{thm:conv}. 
  \end{theorem}
  \proof
  We uses the Lemma~\ref{lemme:Lin} giving a geometric scheme, with a straightforward recurrence we prove that,
    \begin{equation}
      w_t \leq w_0 (1-\rho_\L)^{k(t)},
    \end{equation}
    where $k(t)$ is the number of non-drop step steps. This number is equal to $t$ for the SP-FW algorithm and it is lower bounded by $t/3$ for the SP-AFW algorithm (see Section~\ref{par:drop_steps} Equation~\eqref{eq:upper_bound_drop_steps}).
    Then by using Proposition \eqref{prop:relation_primal}  relating $h_t$ and the square root of $w_t$ we get the first statement of the theorem,
      \begin{equation}
       h_t \leq   P_\L \sqrt{2w_0}(1- \rho_\L)^{k(t)/2}.
      \end{equation}
    To prove the second statement of the theorem we just use Theorem~\ref{thm:subopt_gap} for the last \emph{non-drop step} after $T$ iterations (let us assume it was at step $t_0$),
    \begin{align}
     g_{t_0} &\leq   \frac{2 \max\{\sqrt{C_\L},\sqrt{w_0} (1-\rho_\L)^{k(t_0)/2}\}}{\CondNumb{}} \sqrt{w_0}(1- \rho_\L)^{k(t_0)/2} &\\
          &=\frac{2\max\{\sqrt{C_\L},\sqrt{w_0} (1-\rho_\L)^{k(T)/2}\}}{\CondNumb{}} \sqrt{w_0}(1- \rho_\L)^{k(T)/2}. & (\text{because } k(t_0)= k(T))
    \end{align}
    The minimum of the gaps observed is smaller than the gap at time $t_0$ then,
    \begin{equation}
      \min_{t\leq T}\gap \leq g_{t_0} \leq \frac{2\max\{\sqrt{C_\L},\sqrt{w_0} (1-\rho_\L)^{k(T)/2}\}}{\CondNumb{}} \sqrt{w_0}(1- \rho_\L)^{k(T)/2}.
    \end{equation}
  \endproof
  The affine invariant formulation with the universal step size $\stepsize_t = \min \left\{\stepmax,\frac{2}{2+k(t)}\right\}$ of Theorem~\ref{thm:conv} also follows from Lemma~\ref{lemme:Lin} by re-using standard FW proof patterns.
   \begin{theorem}\label{thm:conv_sublin}
   Let $\L$ be a 
    strongly convex-concave function with a finite smoothness constant $C_\L$, a positive interior strong convex-concavity constant $\muIntL$~\eqref{def:interior_strongL} or a positive geometric strong convex-concavity $\mu^{\away}_\L$~\eqref{def:geom_strongL}. Let us also define the rate multipliers~$\CondNumb$ as
  \begin{equation}
    \CondNumb^{\FW} :=1- \frac{M_\L }{\sqrt{\muIntL}} \quad\text{and}\quad \tilde \CondNumb^{\PW} := 1- \frac{
        M_\L}{\sqrt{\mu^{\away}_\L}} \quad(\text{see Equation~\eqref{eq:def_bilin_coef} for the definition of } M_\L).
  \end{equation} 
    Let $\CondNumb$ refers to
  either $\CondNumb^{\FW}$ for case~\eqref{case:1} where the algorithm is SP-FW, or $\tilde\CondNumb^{\PW}$ for case~\eqref{case:2} where the algorithm is SP-AFW,

  If $\CondNumb>\frac{1}{2}$, then the suboptimality $w_t$ of the iterates of the algorithm with universal step size $\stepsize_t = \min\left\{\stepmax,\frac{2}{2+k(t)}\right\}$ (see Equation~\eqref{eq:update} for more details about $\stepmax$) has the following decreasing upper bound:
        \begin{equation}\label{eq:sub_rate_w}
          w_t \leq \frac{C}{{2+k(t)}}
        \end{equation}
      where $C = 2 \max\left(w_0,\frac{2 C_\L}{2 \CondNumb-1}\right)$ and $k(t)$ is the number of non-drop step after $t$ steps. For SP-FW, $k(t)=t$ and for SP-AFW, $k(t)\geq t/3$. 
      Moreover we can also upper bound the minimum FW gap observed for $T\geq 1$,
  \begin{equation}
    \min_{t \leq T} \gap^\FW \leq \frac{5C}{\CondNumb{}(k(T)+1)}.
  \end{equation}
    Note that in this theorem the constant $\tilde \CondNumb^{\PW}$is slightly different from the constant $\CondNumb^{\PW}$ in Theorem~\ref{thm:conv_affine_invariant}.
  \end{theorem}
  \proof
    We can put both the recurrence~\eqref{eq:int_strong_usual_gap} for the SP-FW algorithm and the recurrence~\eqref{eq:geom_strong_gt_gap} for the SP-AFW algorithm (from the proof of Lemma~\ref{lemme:ineg}) in the following form by using our unified notation introduced in the theorem statement:
          \begin{equation}\label{eq:proof_sublinear2}
         \wtt 
       \leq \wt  
      - \stepsize_t \CondNumb \gap
      + \stepsize_t^2 {C_\L}. 
      \end{equation}
    Note that the gap $\gap$ is the one defined in Equation~\eqref{eq:gaps} and depends on the algorithm.
    Let $(\CondNumb)$ to stand respectively for $(\CondNumb^{\FW})$ for SP-FW (case~\eqref{case:1}) or $(\tilde \CondNumb^{\PW})$ for SP-AFW (case~\eqref{case:2}). With this notation, the inequality $\gap \geq w_t$ leads to,
      \begin{equation}\label{eq:def_f}
          \wtt 
       \leq \wt \left( 1- \CondNumb\gamma_t \right)   
      + \gamma_t^2 {C_\L}. 
      \end{equation}
      Our goal is to show by induction that 
      \begin{equation}\label{eq:induction_sublin}\tag{$\star$}
      w_t \leq \frac{C}{2+k(t)} \quad \text{ where } C := 2\max \left( w_0, \frac{2C_\L}{2 \CondNumb -1} \right).
      \end{equation}
      Let us first define the convex function $f_t :\gamma \mapsto  \wt \left( 1- \CondNumb\gamma \right)
      + \gamma^2 {C_\L}$. 
      We will show that under~\eqref{eq:induction_sublin}, the function $f_t$ has the following property:
      \begin{equation}
        f_t\left(\frac{2}{2+k(t)}\right) \leq \frac{C}{3+k(t)} .
      \end{equation}
      This property is due to a simple inequality on integers; let $k = k(t)$, from the crucial induction assumption, we get:
      \begin{equation}
        f_t\left(\frac{2}{2+k}\right) 
       = 
          w_{t} \frac{2+k - 2 \CondNumb}{2+k}+ \frac{4}{(2+k)^2} {C_\L} 
       \leq 
          \frac{C}{3+k} \left[ \frac{(3+k)(k+1 - (2 \CondNumb - 1) +\frac{4C_\L}{C})}{(2+k)^2} \right], 
      \end{equation}
      but $(2 \CondNumb - 1) \geq \frac{4C_\L}{C}$ and $(3+k)(1+k)<(2+k)(2+k)$ for any $k$, thus 
      \begin{equation}\label{eq:sublin_f}
       f_t\left(\frac{2}{2+k}\right)  \leq \frac{C}{3+k}.
      \end{equation}
      Equation~\eqref{eq:sublin_f} is crucial for the inductive step of our recurrence.
      \begin{itemize}
        \item 
      Hypothesis~\eqref{eq:induction_sublin} is true for $t=0$ because $k(0)=0$. 
        \item 
      Now let us assume that \eqref{eq:induction_sublin} is true for a $t \in \N$.
      We set the stepsize $\stepsize_t := \min \left\{\stepmax,\frac{2}{2+k(t)}\right\}$.

      If $k(t+1)=k(t)+1$, it means that $\stepsize_t = \frac{2}{2+k(t)}$ and then by~\eqref{eq:def_f} and~\eqref{eq:sublin_f},
      \begin{equation} \label{eq:proof_w_sublin}
         w_{t+1} 
         \leq 
          f_t\left(\frac{2}{2+k(t)}\right) 
         \leq \frac{C}{3+k(t)}
         =  \frac{C}{2+k(t+1)}.
      \end{equation}

      If $k(t+1)= k(t)$, then it means that $0 \leq \stepsize_t < \frac{2}{2+k(t)}$. Hence, the convexity of the function $f_t$ leads us to the inequality
      \begin{align} \notag 
       w_{t+1} \leq f_t(\stepsize_t) 
       &\leq \max \left\{ f_t(0),f_t\left(\frac{2}{2+k(t)}\right) \right\} \\
       & = \max \left\{ w_t,f_t\left(\frac{2}{2+k(t)}\right) \right\}\label{eq:ineg_wt_wtt} \\
       &\leq \max \left\{ \frac{C}{2+k(t)},\frac{C}{3+k(t)} \right\} \\
       &\leq \frac{C}{2+k(t)}.
       \end{align} 
      where we used~\eqref{eq:sublin_f} and the induction hypothesis~\eqref{eq:induction_sublin} to get the penultimate inequality~\eqref{eq:ineg_wt_wtt}. Since we assumed that $k(t+1)= k(t)$, we get
      \begin{equation}
        w_{t+1} \leq  \frac{C}{2+k(t+1)}, 
      \end{equation}
      completing the induction proof for~\eqref{eq:sub_rate_w}.
      \end{itemize}
      



      In case \eqref{case:1}, $k(t) = t$ and in case \eqref{case:2}, $k(t) \geq t/3$ (see Equation~\eqref{eq:upper_bound_drop_steps}), leading us to the first statement of our theorem.

The proof of the second statement is inspired by the proof of Theorem C.3 from~\citep{lacoste2013block}.

With the same notation as the proof of Lemma~\ref{lemme:ineg}, we start from Equation~\eqref{eq:proof_sublinear2} where we isolated the gap $\gap$ to get the crucial inequality
  \begin{equation}
   \gap
         \leq \frac{\wt  -w_{t+1}}{\CondNumb \stepsize_t }
        + \gamma_t \frac{C_\L}{\CondNumb}. 
  \end{equation}
  Since the gap $\gap$ is the one depending on the algorithm  defined by $\gap := \innerProd{-\rt}{\dt}$, we have $\gap = \gap^\FW$ for SP-FW and $\gap = \max \left( \gap^\FW, \gap^\away \right)  \geq \gap^\FW$ for SP-AFW. Thus,
     \begin{equation} \label{eq:Ineg_g_wt_wtt}
    \gap^\FW \leq \gap
         \leq \frac{\wt  -w_{t+1}}{\CondNumb \stepsize_t }
        + \gamma_t \frac{C_\L}{\CondNumb}. 
  \end{equation}
  In the following in order not to be too heavy with notation we will work with de FW gap and note $\gap$ for $\gap^\FW$. 

  The proof idea is to take a convex combination of the inequality~\eqref{eq:Ineg_g_wt_wtt} to obtain a new upper-bound on a convex combination of the gaps computed from step $0$ to step $T$. Let us introduce the convex combination weight $\rho_t := \frac{\stepsize_t\cdot  k(t)( k(t) +2) }{S_T}$ where $k(t)$ is the number of non-drop steps after $t$ steps and $S_T$ is the normalization factor. Let us  also call $N_T := \{ t \leq T \, | \, t \text{ is a non-drop step}\}$.  Taking the convex combination of~\eqref{eq:Ineg_g_wt_wtt}, we get
  \begin{equation} \label{eq:Ineg_mean_g_wt_wtt}
        \sum_{t=0}^T \rho_t \gap \leq 
        \sum_{t=0}^T \rho_t \frac{\wt  -w_{t+1}}{\CondNumb \stepsize_t }+ 
        \sum_{t=0}^T \rho_t \gamma_t \frac{C_\L}{\CondNumb}. 
  \end{equation}
  By regrouping the terms and ignoring the negative term, we get
  \begin{equation} \label{eq:sum_gt_wt_rhot}
        \sum_{t=0}^T \rho_t \gap \leq 
        \frac{w_0 \rho_0}{\CondNumb \stepsize_0} +
        \frac{1}{\CondNumb}\sum_{t=0}^{T-1} w_{t+1}\left( \frac{\rho_{t+1}}{ \stepsize_{t+1} }   -\frac{\rho_t}{ \stepsize_t}\right)+ 
        \sum_{t=0}^{T} \rho_t \gamma_t \frac{C_\L}{\CondNumb}. 
  \end{equation}
 %
 %
 %
 %
 %
 %
 %
 %
 %
 %
 %
 %
  By definition $\frac{\rho_t}{\stepsize_t} := \frac{k(t)(k(t)+2)}{S_T}$ and notice that $\rho_0 = 0$. We now consider two possibilities: if $\gamma_t$ is a drop step, then $k(t+1) = k(t)$ and so
  \begin{equation}
   \frac{\rho_{t+1}}{ \stepsize_{t+1}} - \frac{\rho_{t}}{ \stepsize_{t}} = 0.
  \end{equation}
  If $\stepsize_t$ is a non-drop step, then $k(t+1) = k(t) + 1$ and thus we have
%
  \begin{align}
     \frac{\rho_{t+1}}{ \stepsize_{t+1}} - \frac{\rho_{t}}{ \stepsize_{t}} 
     &=   \frac{(k(t)+1)(k(t)+3)}{S_T} -  \frac{k(t)(k(t)+2)}{S_T} \\
     &= \frac{2k(t)+3}{S_T}. \label{eq:rho/gamma} 
     %
     %
  \end{align}
  As $\stepsize_t \leq \frac{2}{k(t)+2}$, we also have $\rho_t \stepsize_t \leq \frac{4 k(t)}{S_T (k(t)+2))} $.
  The normalization factor $S_T$ to define a convex combination is equal to 
  \begin{equation}
    S_T := \sum_{u=0}^T \stepsize_u \cdot k(u)( k(u) +2)  
    \geq  \sum_{\small\substack{u=0 \\ u \in N_{T} }}^T  \frac{2}{2 +k(u)} \cdot k(u)( k(u) +2) 
    = \sum_{k = 0}^{k(T)} 2 k
    = k(T)(K(T)+1).  
  \end{equation}
  Plugging this and~\eqref{eq:rho/gamma} in the inequality~\eqref{eq:sum_gt_wt_rhot} with the rate~\eqref{eq:induction_sublin} shown by induction gives us,
  \begin{align}
       \sum_{t=0}^T \rho_t \gap 
        &\leq 
        0 +
        \frac{1}{\CondNumb} \sum_{\small\substack{t=0 \\ t \in N_{T} }}^{T-1} \frac{C}{2+k(t+1)}\frac{2k(t)+3}{k(T)(k(T)+1)} + 
         \sum_{t=0}^T \frac{4k(t)}{k(T)(k(T)+1)(k(t)+2)}\frac{C_\L}{\CondNumb} \\
        & \leq \frac{2C}{\CondNumb{}} \frac{1}{k(T)(k(T)+1)} \Big( \sum_{\small\substack{t=0 \\ t \in N_{T} }}^{T-1}  1 + \frac{2C_\L}{C} \sum_{t=1 }^T  1 \Big) \\
        & \leq \frac{2C}{\CondNumb (k(T)+1)} (1 + \frac{\CondNumb}{2}\frac{T}{k(T)}) \leq \frac{5C}{\CondNumb (k(T)+1)},
  \end{align}
  by using $k(T) \geq T/3$.
  Finally, the minimum of the gaps is always smaller than any convex combination, so we can conclude that (for $T\geq1$):
  \begin{equation}
    \min_{0 \leq t \leq T} \gap \leq \frac{5C}{\CondNumb{}(k(T)+1)}.
  \end{equation}
  \endproof
  
  %
  %
  %

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
  %


\section{Strongly convex sets}
  \label{sec:strong_conv_proof}
  In this section, we are going to prove that the function $\s(\cdot)$ is Lipschitz continuous when the sets $\X$ and $\Y$ are strongly convex and when the norm of the two gradient components are uniformly lower bounded. We will also give the details of the convergence rate proof for the strongly convex sets situation.
  Our proof uses similar arguments as~\citet[Theorem~3.4 and~3.6]{dunn1979rates}.
  \begin{reptheorem}{thm:s_lip}  
   Let $\X$ and $\Y$ be $\beta$-strongly convex sets. 
   If $\min(\|\nabla_{\!x} L(\z)\|_{\X^*}, \|\nabla_{\!y} L(\z)\|_{\Y^*}) \geq\delta>0$ for all $\z \in \M$, then the oracle function $\z \mapsto \s(\z) := \arg\min_{\s \in \M}\innerProd{\s}{F(\z)}$ is well defined and is $\frac{4L}{\delta \beta }$-Lipschitz continuous (using the norm $\|(\x,\y)\|_{\X \times \Y} := \|\x\|_\X + \|\y\|_\Y$), where $F(\z) := \left(
     \nabla_x \L(\z),
     -\nabla_y \L(\z)
     \right)$.
  \end{reptheorem}
 \proof First note that since the sets are strongly convex, the minimum is reached at a unique point. Then, we introduce the following lemma which can be used to show that each component of the gradient is Lipschitz continuous irrespective of the other set.
    \begin{lemma} \label{lemma:FxLip}
    Let $F_x:\X\times \A \to \R^d$ be a $L$-Lipschitz continuous function (i.e. $\|F_x(\z)-F_x(\z')\|_{\X^*} \leq L \|\z - \z'\|_{\X \times \A}$) and $\X$ a $\beta$-strongly convex set. If $\forall \z \in \X\times \A, \; \|F_x(\z)\|_{\X^*}\geq \delta > 0$, then $\s_x:\z \mapsto \argmin_{\s \in \X} \prodscal{\s}{F_x(\z)}$ is $ \frac{2L}{\delta \beta }$-Lipschitz continuous.
    \end{lemma}
    \proof
    Let $\z, \, \z' \in \X\times \A$ and let $\bar \x = \frac{\s_x(\z)+ \s_x(\z')}{2}$, then
    \begin{align} 
      \prodscal{\s_x(\z)-\s_x(\z')}{-F_x(\z)} 
        & = 2\prodscal{\s_x(\z)-\bar \x}{-F_x(\z)} \notag \\
        & \geq  2\prodscal{\x-\bar \x}{-F_x(\z)}. \quad \forall \x \in \X \quad \text{(by definition of $s_x$)} \label{eq:Sinequality}
    \end{align}
    Now~\eqref{eq:Sinequality} holds for any $\x \in B_\beta\left(\tfrac{1}{2},\s_x(\z),\s_x(\z') \right)$ as this set is included in $\X$ by $\beta$-strong convexity of~$\X$.
    Then since $\bar \x$ is the center of $B_\beta\left(\tfrac{1}{2},\s_x(\z),\s_x(\z') \right)$, we can choose a $\x$ in this ball such that $\x - \bar \x$ is in the direction which achieves the dual norm of $-F_x(\z)$.\footnote{For the Euclidean norm, we choose $\x - \bar \x$ proportional to $-F_x$; but for general norms, it could be a different direction.} More specifically, we have that:
    $$
    \|-F_x(\z)\|_{\X^*} = \sup_{\| \vv \|_{\X} \leq 1} \prodscal{-F_x(\z)}{\vv}.
    $$
    As we are in finite dimensions, this supremum is achieved by some vector $\vv$. So choose $\x := \bar{\x} + \frac{\vv}{\|\vv\|_\X} \frac{\beta}{8} \|\s_x(\z)-\s_x(\z')\|_\X^2 \in B_\beta\left(\tfrac{1}{2},\s_x(\z),\s_x(\z') \right)$ and plug it in~\eqref{eq:Sinequality}:
    
    \begin{align}
      \prodscal{\s_x(\z)-\s_x(\z')}{-F_x(\z)} 
       & \geq \frac{\beta\|\s_x(\z)-\s_x(\z')\|_\X^2}{4 \|\vv\|_\X} \prodscal{\vv}{-F_x(\z)} \\
        & =  \frac{\beta\|\s_x(\z)-\s_x(\z')\|_\X^2}{4 \|\vv\|_\X}  \|F_x(\z)\|_{\X^*} \\
        & \geq \frac{\beta}{4} \|\s_x(\z)-\s_x(\z')\|_\X^2 \|F_x(\z)\|_{\X^*} . \label{eq:s_lip1}
    \end{align}
    Switching $\z$ and $\z'$ and using a similar argument, we get,
    \begin{equation} \label{eq:s_lip2}
         \prodscal{\s_x(\z')-\s_x(\z)}{-F_x(\z')} \geq \frac{\beta}{4} \|\s_x(\z)-\s_x(\z')\|_\X^2 \|F_x(\z')\|_{\X^*}  .
    \end{equation}
    Hence summing \eqref{eq:s_lip1} and \eqref{eq:s_lip2},
    \begin{align}\label{eq:s_lip_final}
       \frac{\beta}{4} (\|F_x(\z)\|_{X^*} +\|F_x(\z')\|_{\X^*}) \, \|\s_x(\z)-\s_x(\z')\|_\X^2 
       &\leq 
       \prodscal{\s_x(\z)-\s_x(\z')}{F_x(\z')-F_x(\z)} \notag\\
       &\leq
       \|\s_x(\z)-\s_x(\z')\|_\X \|F_x(\z')-F_x(\z)\|_{\X^*} \\
       & \leq L \|\s_x(\z)-\s_x(\z')\|_\X \|\z'-\z\|_{\X \times \Y} \;\;\;\text{(Lip. cty. of  $F_x$)}\notag
    \end{align}
    and finally
    \begin{equation}
      \|\s_x(\z)-\s_x(\z')\|_\X \leq \frac{4L}{\beta \left( \|F_x(\z)\|_{\X^*} +\|F_x(\z')\|_{\X^*} \right) } \|\z-\z'\|_{\X \times \Y} \leq \frac{2L}{\delta \beta } \|\z-\z'\|_{\X \times \Y}.
    \end{equation}
    \endproof
    To prove our theorem, we will notice that for the saddle point setup,
    the oracle function $\s(\cdot) := \argmin_{\s \in \M} \prodscal{\s}{F(\cdot)}$ can be decomposed as $\s(\cdot)=(\s_x(\cdot),\s_y(\cdot))$
    where $\s_x(\cdot) := \argmin_{\s \in \X} \prodscal{\s}{F_x(\cdot)}$ 
    and   $\s_y(\cdot) := \argmin_{\s \in \Y} \prodscal{\s}{F_y(\cdot)}$. 
    Then applying our lemma, the function $\s_x(\cdot)$ is Lipschitz continuous. 
    The same way $\s_y(\cdot)$ is Lipschitz continuous. Then, for all $\z,\z'$ in $\M$
    \begin{equation}
      \|\s(\z)-\s(\z')\|_{\X \times \Y} = \|\s_x(\z)-\s_x(\z')\|_\X + \|\s_y(\z)-\s_y(\z')\|_\Y \leq \frac{4L}{\delta\beta} \|\z-\z'\|_{\X \times \Y},
    \end{equation}
    which gives the definition of the Lipschitz continuity of our function and proves the theorem.
    \endproof
    In this theorem,  we introduced the function $F$. This function is monotone in the following sense:
  \begin{equation}\label{eq:nondecreasing}
        \forall \z, \z' \quad \prodscal{\z - \z'}{F(\z)-F(\z')} \geq 0. 
  \end{equation}
  Actually this property follows directly from the convexity of $\L(\cdot,\y)$ and the concavity of $\L(\x,\cdot)$.
  We can also prove that when the sets $\X$ and $\Y$ are strongly convex and when the gradient is uniformly lower bounded, we can relate the gap and the distance between $\zt$ and $\st$. 
   \begin{lemma}\label{lemma:lower_gap}
   If $\X$ is a $\beta$-strongly convex set and if $\|\nabla f\|_{\X^*}$ is uniformly lower bounded by $\delta$ on $\X$, then
  \begin{equation}
    \max_{\s \in \X} \prodscal{\s - \x}{- \nabla f(\x)}\geq \frac{\beta}{4}  \delta \|\s(\x) - \x\|^2,
  \end{equation}
  where $\s(\x) = \argmax_{\s \in \X}\prodscal{\s-\x}{-\nabla f (\x)}$.
  \end{lemma}
  \proof Let $\x$ and $\s(\x)$ be in $\X$. We have $B_\beta \left( \frac{1}{2}, \s(\x), \x \right) \subset \X$ by $\beta$-strong convexity. So as in the proof of Lemma~\ref{lemma:FxLip}, let $\vv$ be the vector such that $\|\vv\|_\X \leq 1$ and $\prodscal{-\nabla f(\x)}{\vv} = \|\nabla f(\x)\|_{\X^*}$. Let
  \begin{equation}
    \bar \s :=\frac{\s(\x)+ \x}{2} + \frac{\beta}{8} \| \s(\x) - \x\|^2 \frac{\vv}{\|\vv\|_\X} \in \X.
  \end{equation}
  Then 
  \begin{align}
    \prodscal{\s(\x) - \x}{- \nabla f(\x)} 
    & \geq \prodscal{ \bar \s - \x}{- \nabla f(\x)} \notag \\
    &= \frac{1}{2}\prodscal{\s(\x) - \x}{- \nabla f(\x)} +  \frac{\beta}{8}  \|\s(\x) - \x\|^2 \frac{\|\nabla f (\x)\|_{\X^*}}{\|\vv\|_\X} \notag \\
    &\geq \frac{1}{2}\prodscal{\s(\x) - \x}{- \nabla f(\x)} +  \frac{\beta}{8} \delta \|\s(\x) - \x\|^2 
  \end{align}
  which leads us to the desired result.
  \endproof
  From this lemma, under the assumption that $\min(\|\nabla_{\!x} L(\z)\|_{\X^*}, \|\nabla_{\!y} L(\z)\|_{\Y^*}) \geq \delta$ $\forall \z \in \X \times \Y$, it directly follows that 
  \begin{align}
    \gap^\FW = \gap^{(x)} + \gap^{(y)} &\geq \frac{\beta}{4} \delta \left( \|\st_x - \xt\|_\X^2 + \| \st_y - \yt\|_\Y^2\right) \notag \\ 
    &\geq \frac{\beta}{8} \delta \left( \|\st_x - \xt\|_\X + \| \st_y - \yt\|_\Y\right)^2 = \frac{\beta}{8} \delta  \|\st - \zt\|_{\X \times \Y}^2. \label{eq:gap_lower_dist}
  \end{align}
  Now we recall the convergence theorem for strongly convex sets from the main text, Theorem~\ref{thm:conv_strong}:
  \begin{reptheorem}{thm:conv_strong}
       Let $\L$ be a convex-concave function and $\X$ and $\Y$ two compact $\beta$-strongly convex sets. 
       Assume that the gradient of  $\L$ is $L$-Lipschitz continuous and that there exists $\delta>0$ such that $\min(\|\nabla_{\!x} L(\z)\|_{\X^*}, \|\nabla_{\!y} L(\z)\|_{\Y^*}) \geq \delta \;\, \forall \z \in \M$. Set $C_\delta := 2L + \frac{8L^2}{\beta \delta}$. 
       Then the gap $\gap^{\FW}$~\eqref{eq:gap} of the SP-FW algorithm with step size $\gamma_t = \tfrac{\gap^{\FW}}{\|\st-\zt\|^2 C_\delta}$
       converges linearly as 
     \begin{equation}
       \gap^{\FW} \leq g_0 \left( 1- \rho \right)^{t}    
     \end{equation}
    where $\rho :=  \frac{\beta \delta}{16 C_\delta}$. The initial gap $g_0$ is cheaply computed during the first step of the SP-FW algorithm. Alternatively, one can use the following upper bound to get uniform guarantees:
    \begin{equation}  
     g_0 \leq \sup_{\z\in \M} \|\nabla_x \L(\z)\|_{\X^*} D_\X + \sup_{\z\in \M} \|\nabla_y \L(\z)\|_{\Y^*} D_\Y.
     \end{equation} 
    \end{reptheorem}

     \proof
      We compute the following relation on the gap:
      \begin{align}
      g_{t+1} 
      & = \prodscal{\ztt - \stt}{F(\ztt)} \notag\\
      & = \prodscal{\zt - \stt}{F(\ztt)} + \stepsize_t\prodscal{\st - \zt}{F(\ztt)}\notag\\
      & = \prodscal{\zt - \stt}{F(\zt)} +  \prodscal{\zt - \stt}{F(\ztt)-F(\zt)} \notag\\
      &\quad + \stepsize_t\prodscal{\st - \zt}{F(\zt)} + \stepsize_t \prodscal{\st-\zt}{F(\ztt)-F(\zt)} \notag\\
      & \leq \prodscal{\zt - \stt}{F(\zt)} +  \prodscal{\zt - \stt}{F(\ztt)-F(\zt)} \notag\\
      &\quad + \stepsize_t\prodscal{\st - \zt}{F(\zt)} + \stepsize_t^2 \|\st-\zt\|^2 L \label{al:lip gradient}
      \end{align}
      where in the last line we used the fact that the function $F(\cdot)$ is Lipschitz continuous. Then using that $\prodscal{\zt - \stt}{F(\zt)} \leq \prodscal{\zt - \st}{F(\zt)}$ (by definition of $\st$), we get
      \begin{align}
      g_{t+1} 
      & \leq \gap(1- \stepsize_t) + \prodscal{\zt - \stt}{F(\ztt)-F(\zt)} +\stepsize_t^2 \|\st-\zt\|^2 L  \notag\\
      & \leq \gap(1- \stepsize_t) + \prodscal{\st - \stt}{F(\ztt)-F(\zt)} +\stepsize_t^2 \|\st-\zt\|^2 L. \label{eq:183}
      \end{align}
      The last line uses the fact that $F$ is monotone by convexity (Equation~\eqref{eq:nondecreasing}). Finally, using once again the Lipschitz continuity of $F$ and the one of $\s(\cdot)$ (by Theorem~\ref{thm:s_lip}), we get
      \begin{align}
      \prodscal{\st- \stt}{F(\ztt) - F(\zt)} 
      & \leq \| \st - \stt\| L \| \ztt - \zt\| \notag\\
      & \leq \frac{4L^2}{\beta \delta} \|\ztt - \zt \|^2 \qquad\quad (\text{Lipschitz continuity of } \s)\notag\\
      & = \frac{4L^2}{\beta \delta} \stepsize_t^2 \|\st - \zt \|^2. \label{eq:184} 
      \end{align}
      Combining~\eqref{eq:184} with~\eqref{eq:183}, we get
      \begin{equation}
        \label{eq:ineg_strongly convex_sets}
      g_{t+1} \leq \gap (1- \stepsize_t) + \stepsize_t^2 \|\st - \zt \|^2 \frac{C_\delta}{2} \qquad \text{where} \qquad C_\delta := 2L  + \frac{8L^2}{\beta \delta}.
      \end{equation}
      Thus by setting the step size $\stepsize_t = \frac{\gap}{\|\st - \zt\|^2 C_\delta}$,
       we get 
     \begin{equation}
        g_{t+1} \leq \gap - \frac{\gap}{2 C_\delta} \left( \frac{\gap}{\|\st-\zt\|^2} \right) \leq \gap \left( 1- \frac{\beta \delta}{16 C_\delta} \right), 
     \end{equation}
     using the fact that the gap is lower bounded by a constant times the square of the distance between $\st$ and $\zt$ (Equation~\eqref{eq:gap_lower_dist}).
    \endproof
    Note that the bound in this theorem is not affine invariant because of the presence of Lipschitz constants and strong convexity constants of the sets. The algorithm is not affine invariant either because the step size rule depends on these constants as well as on $\|\st-\zt\|$. Deriving an affine invariant step size choice and convergence analysis is still an interesting open problem in this setting.

\section{Details on the experiments} %
\label{sec:details_on_the_experiments}

  \paragraph{Graphical Games.} %
  \label{par:graphical_games}
  The payoff matrix $M$ that we use encodes the following simple
model of competition between universities with their respective benefits: 
\begin{enumerate}
  \item University~1 (respectively University~2) has benefit $b_i^{(1)}$ ($b_i^{(2)}$) to get student $i$. 
  \item Student $i$ ranks the possible
roommates with a permutation $\sigma_i \in \mathcal S_p$. Let $\sigma_i(j)$ represents the rank of $j$ for $i$ (first in the list is the preferred one). 
  \item They go to the university that matched them with their preferred roommate, in case of equality the student chooses randomly.
  \item Supposing that $\x$ encodes the roommate assignment proposed by University~1 (and $\y$ for University~2), then the expectation of the benefit of University~1 is $\x^\top M \y$,
with the following definition for the payoff matrix $M$ indexed by pairs
of matched students. For the pairs $(i,j)$ with $i < j$ and $(k,l)$ with $k < l$ with elements in $1, \ldots, s$, we have:
\begin{enumerate}
  \item $M_{ij,il} = \left\{ 
            \begin{array}{lll}
            b_i^{(1)} & \text{if} \; \sigma_i(j) < \sigma_i(l) \quad \text{\emph{i.e. student $i$ preferred $j$ over $l$}}\\
            -b_i^{(2)} & \text{if} \; \sigma_i(j) > \sigma_i(l) \\
            \frac{b_i^{(1)} - b_i^{(2)}}{2} & \text{otherwise}  \quad (\text{in that case }j = l).
            \end{array} \right.$
  \item $M_{ij,kj} = M_{ji,jk}$ %
  \item $M_{ij,ki} = M_{ij,ik}$
  \item $M_{ij,jl} = M_{ij,lj}$
  \item $M_{ij,kl} = 0 \quad \text{otherwise}$
\end{enumerate}
\end{enumerate}
Note that we need to do unipartite matching here (and not bipartite
matching) since we have to match students together and not students with dorms. 

For our experiments, in order to get a realistic payoff matrix, we set $\mu_i \sim \mathcal U[0,1]$ the \emph{true} value of student $i$. Then we set $b_i^{(U)}\sim \mathcal N(\mu_i, 0.1)$ the value of the student $i$ \emph{observed} by University~$U$. To solve the perfect matching problem, we used Blossom V by \citetsup{kolmogorov2009blossom}. 
  \paragraph{Sparse structured SVM.} %
  \label{par:Structured SVM}
  We give here more details on the derivations of the objective function for the structured SVM problem.
  We first recall the structured prediction setup with the same notation from~\citep{lacoste2013block}. In structured prediction, the goal is to predict a structured object $\y \in \Y(\x)$ (such as a sequence of tags) for a given input $\x \in \X$. For the structured SVM approach, a structured feature map $\bm{\phi} : \X \times \Y \rightarrow \R^d$ encodes the relevant information for input / output pairs, and a linear classifier with parameter $\bm{w}$ is defined by $h_{\bm{w}} = \argmax_{\y \in \Y(\x)} \prodscal{\bm{w}}{\bm{\phi}(\x, \y)}$. We are also given a task-dependent structured error $L(\y', \y)$ that gives the loss of predicting $\y$ when the ground truth is $\y'$. Given a labeled training set $\{(\x^{(i)}, \y^{(i)})\}_{i=1}^n$, the standard $\ell_2$-regularized structured SVM objective in its non-smooth formulation for learning as given for example in Equation~(3) from \citep{lacoste2013block} is:
  \begin{equation}
    \min_{ \bm{w} \in \R^d} \frac{\lambda}{2}\|\bm{w}\|_2^2 + \frac{1}{n} \sum_i \tilde H_i(\bm{w})
  \end{equation}
  where $\tilde H_i(\bm{w}) := \max_{\y \in \mathcal Y_i} \; L_i(\y) - \prodscal{\bm{w}}{\bm{\psi}_i(\y)}$ is the structured hinge loss, and the following notational shorthands were defined: $\Y_i := \Y(\x^{(i)})$, $L_i(\y) := L(\y^{(i)}, \y)$ and $\bm{\psi}_i(\y) := \bm{\phi}(\x^{(i)},\y^{(i)}) - \bm{\phi}(\x^{(i)},\y)$.
  
  In our setting, we consider a sparsity inducing $\ell_1$-regularization instead. Moreover, we use the (equivalent) constrained formulation instead of the penalized one, in order to get a problem over a polytope. We thus get the following challenging problem:
  \begin{equation}
    \min_{\|\bm{w}\|_1  \leq R} \frac{1}{n} \sum_i \tilde H_i(\bm{w}).
  \end{equation}
  To handle any type of structured output space $\Y$, we use the following generic encoding.
  Enumerating the elements of $\Y_i$, we can represent the $j^{th}$ element of $\Y_i$ as $(\overbrace{0,\ldots,0}^{j-1},1,0,\ldots,0 ) \in \R^{|\Y_i|}$. Let $M_i$ have $\big(\bm{\psi}_i(\y)\big)_{\y \in \mathcal Y_i}$ as columns and let $\bm{L}_i$ be a vector of length $|\Y_i|$ with $L_i(\y)$ as its entries. 
  The functions $\tilde H_i(\bm{w})$ can then be rewritten as the maximization of linear functions in $\y$: $\tilde H_i(\bm{w}) = \max_{\y \in \mathcal Y_i} \; \bm{L}_i^\top \y - \bm{w}^\top M_i \y$.
  As the maximization of linear functions over a polytope is always obtained at one of its vertex, we can equivalently define the maximization over the convex hull of $\Y_i$, which is the probability simplex in $\R^{|\Y_i|}$ that we denote $\Delta(|\Y_i|)$: 
  \begin{equation}
    \max_{\y_i \in \mathcal Y_i} \; \bm{L}_i^\top \y_i - \bm{w}^\top M_i \y_i =  \max_{\bm{\alpha}_i \in \Delta(|\mathcal Y_i|)} \!\!\bm{L}_i^\top \bm{\alpha}_i - \bm{w}^\top M_i \bm{\alpha}_i
  \end{equation}
  Thus our equivalent objective is 
  \begin{equation}
   \min_{\|\bm{w}\|_1  \leq R} \frac{1}{n} \sum_i \Big( \max_{\y_i \in \mathcal Y_i} \; \bm{L}_i^\top \y_i - \bm{w}^\top M_i \y_i \Big) 
    = 
    \min_{\|\bm{w}\|_1 \leq R} \frac{1}{n} \sum_i \Big( \max_{\bm{\alpha}_i \in \Delta(|\mathcal Y_i|)} \!\!\bm{L}_i^\top \bm{\alpha}_i - \bm{w}^\top M_i \bm{\alpha}_i \Big) ,
  \end{equation}
  which is the bilinear saddle point formulation given in the main text in~\eqref{eq:structuredSVM}.
  %
%
%
%
%

  

\bigskip
\bigskip
%
\bibliographystylesup{abbrvnat}
\bibliographysup{bib}

\end{document}


\documentclass[tikz,border=10pt]{standalone}
\usepackage{tikz}
\usetikzlibrary{positioning, decorations.pathreplacing, shapes}

\newcommand{\metaaxis}{6}
\newcommand{\centralaxis}{3.5}

\begin{document}
\begin{tikzpicture}

\tikzstyle{processnode} = [draw, rectangle, rounded corners=3pt]
\tikzstyle{datanode} = [draw, ellipse]

\foreach \x in {0,0.5,...,2}
	\draw [->] (\x, 0) -- +(0.5, 0);

\draw (3, 0) node (pathelips) {\(\cdots\)};

\foreach \x in {3.5,4,...,5.5}
	\draw [->] (\x, 0) -- +(0.5, 0);

\draw node [below=of pathelips,yshift=1cm] {standardized path};

\draw [decorate,decoration={brace,amplitude=8}] (0, 0.1) -- +(2, 0) node (firstk) [midway, yshift=0.2cm] {};
\draw [decorate,decoration={brace,mirror,amplitude=8}] (6, 0.1) -- +(-2, 0) node (lastk) [midway, yshift=0.2cm] {};

\path (\metaaxis, 0) -- +(2, 0) node [datanode] (meta) {Metadata};
\path (meta) -- +(0, 1) node [processnode] (embed) {Embeddings};

\draw (\centralaxis, 2) node [processnode] (relus) {Rectifiers};
\path (relus) -- +(0, 1) node [processnode] (softmax) {Softmax};
\path (softmax) -- +(0, 1) node [processnode] (dot) {Centroid};
\path (dot) -- +(2.5, 0) node [datanode] (clusters) {Clusters};
\path (dot) -- +(0, 1) node (predict) {\((latitude, longitude)\)};

\draw [->] (meta) -- (embed);

\draw [->] (firstk) -- (relus);
\draw [->] (lastk) -- (relus);
\draw [->] (embed) -- (relus);

\draw [->] (relus) -- (softmax);
\draw [->] (softmax) -- (dot);
\draw [->] (clusters) -- (dot);
\draw [->] (dot) -- (predict);

\end{tikzpicture}
\end{document}



\newcommand{\taxiglyph}{
	\begin{tikzpicture}[y=0.80pt, x=0.80pt, yscale=-0.030000, xscale=0.030000, inner sep=0pt, outer sep=0pt,
		draw=black,fill=black,line join=miter,line cap=rect,miter limit=10.00,line width=0.800pt]
		\path[fill=black] (284.0625,216.1406) .. controls (284.0625,217.7343) and
		(283.9219,219.9843) .. (283.6406,222.8906) .. controls (283.2656,226.0781) and
		(283.0781,228.3281) .. (283.0781,229.6406) .. controls (282.6094,243.3281) and
		(281.2031,253.6406) .. (278.8594,260.5781) .. controls (275.8594,263.3906) and
		(272.3906,265.0313) .. (268.4531,265.5000) .. controls (269.2031,265.4063) and
		(265.3125,265.3594) .. (256.7812,265.3594) .. controls (248.7187,280.4531) and
		(237.1875,288.0000) .. (222.1875,288.0000) .. controls (208.0312,288.0000) and
		(196.8281,280.4063) .. (188.5781,265.2188) -- (104.7656,265.2188) .. controls
		(97.0781,280.4063) and (85.7344,288.0000) .. (70.7344,288.0000) .. controls
		(54.7969,288.0000) and (43.5937,280.4063) .. (37.1250,265.2188) --
		(19.9688,265.2188) .. controls (19.0313,264.4688) and (17.7656,263.2969) ..
		(16.1719,261.7031) .. controls (13.0781,257.7656) and (11.5312,249.2813) ..
		(11.5312,236.2500) .. controls (11.5312,209.4375) and (21.5156,190.8281) ..
		(41.4844,180.4219) .. controls (43.8281,179.2032) and (46.1250,178.5938) ..
		(48.3750,178.5938) -- (83.5312,178.5938) .. controls (91.5937,169.9687) and
		(103.9219,157.2656) .. (120.5156,140.4844) .. controls (122.6719,138.4219) and
		(124.9219,137.0156) .. (127.2656,136.2656) .. controls (130.5469,135.1406) and
		(138.0937,134.4375) .. (149.9062,134.1562) -- (150.6094,133.4531) .. controls
		(150.6094,133.0781) and (151.6406,123.4687) .. (153.7031,104.6250) --
		(173.9531,104.6250) -- (176.7656,134.4375) -- (199.6875,134.4375) .. controls
		(202.8750,134.4375) and (207.7032,135.3750) .. (214.1719,137.2500) .. controls
		(221.7656,139.4063) and (228.1406,143.3906) .. (233.2969,149.2031) .. controls
		(235.6406,151.9218) and (238.8750,156.8437) .. (243.0000,163.9688) .. controls
		(246.0937,169.1250) and (248.8125,172.0781) .. (251.1562,172.8281) .. controls
		(252.7500,173.2968) and (256.9688,173.5312) .. (263.8125,173.5312) .. controls
		(268.0312,173.5312) and (271.0312,173.8125) .. (272.8125,174.3750) .. controls
		(274.0312,174.7500) and (275.4609,175.8281) .. (277.1016,177.6094) .. controls
		(278.7422,179.3906) and (279.5625,180.8906) .. (279.5625,182.1094) --
		(279.5625,202.5000) .. controls (282.5625,205.3125) and (284.0625,209.8594) ..
		(284.0625,216.1406) -- cycle(271.8281,210.5156) .. controls
		(271.8281,209.2031) and (271.7578,207.1875) .. (271.6172,204.4688) .. controls
		(271.4765,201.7500) and (271.4062,199.7344) .. (271.4062,198.4219) .. controls
		(271.4062,196.0781) and (271.5468,194.0625) .. (271.8281,192.3750) --
		(268.3125,192.3750) .. controls (267.1875,192.3750) and (265.8516,193.7578) ..
		(264.3047,196.5234) .. controls (262.7578,199.2891) and (261.9844,201.3281) ..
		(261.9844,202.6406) .. controls (261.9844,204.4219) and (263.4844,207.0469) ..
		(266.4844,210.5156) -- (271.8281,210.5156) -- cycle(169.5938,134.2969) ..
		controls (169.1250,128.9531) and (168.3750,120.8906) .. (167.3438,110.1094) --
		(159.7500,110.1094) -- (157.5000,134.2969) -- (169.5938,134.2969) --
		cycle(274.6406,236.3906) -- (274.6406,233.8594) -- (253.6875,233.8594) ..
		controls (257.1562,240.5157) and (258.7969,246.7500) .. (258.6094,252.5625) --
		(269.4375,252.5625) .. controls (272.9062,250.4062) and (274.6406,245.0156) ..
		(274.6406,236.3906) -- cycle(269.1562,184.9219) .. controls
		(268.4062,184.3594) and (267.9843,183.7500) .. (267.8906,183.0938) .. controls
		(255.9843,183.0938) and (248.6015,182.6485) .. (245.7422,181.7578) .. controls
		(242.8828,180.8672) and (240.6094,178.9688) .. (238.9219,176.0625) .. controls
		(237.4219,173.4375) and (235.9688,170.8125) .. (234.5625,168.1875) .. controls
		(224.8125,153.3750) and (213.5625,145.9688) .. (200.8125,145.9688) --
		(132.8906,145.9688) .. controls (131.1094,145.9688) and (129.2344,146.7188) ..
		(127.2656,148.2188) .. controls (125.2969,149.7188) and (117.5625,157.2657) ..
		(104.0625,170.8594) .. controls (91.3125,183.7031) and (84.7031,190.1250) ..
		(84.2344,190.1250) -- (50.6250,190.1250) .. controls (47.0625,190.1250) and
		(43.6406,192.0937) .. (40.3594,196.0312) .. controls (42.7031,196.2187) and
		(45.2813,197.2031) .. (48.0938,198.9844) -- (257.3438,198.9844) .. controls
		(257.6250,196.4531) and (259.0312,193.7109) .. (261.5625,190.7578) .. controls
		(264.0938,187.8047) and (266.6250,185.8594) .. (269.1562,184.9219) --
		cycle(244.8281,252.2812) .. controls (244.8281,246.1875) and
		(242.6250,241.0312) .. (238.2188,236.8125) .. controls (233.8125,232.5938) and
		(228.5625,230.4844) .. (222.4688,230.4844) .. controls (216.3750,230.4844) and
		(211.1015,232.5938) .. (206.6484,236.8125) .. controls (202.1953,241.0312) and
		(199.9688,246.1875) .. (199.9688,252.2812) .. controls (199.9688,258.2812) and
		(202.2657,263.5078) .. (206.8594,267.9609) .. controls (211.4531,272.4140) and
		(216.7500,274.6406) .. (222.7500,274.6406) .. controls (228.5625,274.6406) and
		(233.6953,272.3672) .. (238.1484,267.8203) .. controls (242.6015,263.2734) and
		(244.8281,258.0937) .. (244.8281,252.2812) -- cycle(275.7656,221.3438) ..
		controls (275.7656,218.9063) and (275.5312,216.9844) .. (275.0625,215.5781) ..
		controls (271.6875,215.6719) and (270.0000,215.7188) .. (270.0000,215.7188) ..
		controls (262.3125,215.7188) and (258.0938,212.2032) .. (257.3438,205.1719) --
		(49.2188,205.1719) .. controls (48.3750,210.4219) and (45.2812,215.3203) ..
		(39.9375,219.8672) .. controls (34.5938,224.4141) and (29.2031,226.9687) ..
		(23.7656,227.5312) -- (44.5781,227.5312) .. controls (52.3594,219.9375) and
		(61.0781,216.1406) .. (70.7344,216.1406) .. controls (81.8907,216.1406) and
		(90.6094,219.9844) .. (96.8906,227.6719) -- (196.0312,227.6719) .. controls
		(204.0000,219.9844) and (212.8125,216.1406) .. (222.4688,216.1406) .. controls
		(232.1250,216.1406) and (240.8906,219.9844) .. (248.7656,227.6719) --
		(275.6250,227.6719) .. controls (275.6250,226.9219) and (275.6250,225.8438) ..
		(275.6250,224.4375) .. controls (275.7187,222.9375) and (275.7656,221.9063) ..
		(275.7656,221.3438) -- cycle(191.6719,234.0000) -- (101.6719,234.0000) ..
		controls (103.8281,237.0937) and (105.7031,243.2812) .. (107.2969,252.5625) --
		(186.7500,252.5625) .. controls (186.7500,244.4062) and (188.3906,238.2187) ..
		(191.6719,234.0000) -- cycle(45.2812,204.4688) .. controls (45.2812,203.2500)
		and (45.0468,202.3594) .. (44.5781,201.7969) -- (35.7188,201.7969) .. controls
		(31.0313,201.7969) and (26.8594,208.0313) .. (23.2031,220.5000) --
		(25.1719,220.5000) .. controls (26.9532,220.5000) and (28.6407,220.2656) ..
		(30.2344,219.7969) .. controls (32.8594,219.0469) and (36.0235,216.9141) ..
		(39.7266,213.3984) .. controls (43.4297,209.8828) and (45.2812,206.9063) ..
		(45.2812,204.4688) -- cycle(92.8125,252.2812) .. controls (92.8125,246.2812)
		and (90.6328,241.1484) .. (86.2734,236.8828) .. controls (81.9141,232.6172)
		and (76.7344,230.4844) .. (70.7344,230.4844) .. controls (64.5469,230.4844)
		and (59.3203,232.5703) .. (55.0547,236.7422) .. controls (50.7890,240.9141)
		and (48.6562,246.0937) .. (48.6562,252.2812) .. controls (48.6562,258.2812)
		and (50.8359,263.5078) .. (55.1953,267.9609) .. controls (59.5547,272.4140)
		and (64.7344,274.6406) .. (70.7344,274.6406) .. controls (76.5469,274.6406)
		and (81.6797,272.3672) .. (86.1328,267.8203) .. controls (90.5859,263.2734)
		and (92.8125,258.0937) .. (92.8125,252.2812) -- cycle(39.6562,233.8594) --
		(21.5156,233.8594) -- (21.5156,245.1094) .. controls (21.5156,244.9219) and
		(22.1250,247.4062) .. (23.3438,252.5625) -- (34.7344,252.5625) .. controls
		(35.1094,244.3125) and (36.7500,238.0781) .. (39.6562,233.8594) --
		cycle(217.1250,181.9688) .. controls (217.1250,185.7188) and
		(216.1406,188.4375) .. (214.1719,190.1250) -- (167.0625,190.1250) .. controls
		(165.8438,188.8125) and (165.2344,187.2656) .. (165.2344,185.4844) .. controls
		(165.2344,187.0781) and (165.8438,177.2813) .. (167.0625,156.0938) .. controls
		(169.4062,153.5625) and (172.5000,152.2969) .. (176.3438,152.2969) --
		(186.6094,152.2969) .. controls (194.8594,152.2969) and (201.2813,152.8125) ..
		(205.8750,153.8438) .. controls (209.9063,154.7813) and (212.9063,158.7656) ..
		(214.8750,165.7969) .. controls (216.3750,170.9531) and (217.1250,176.3437) ..
		(217.1250,181.9688) -- cycle(159.1875,157.5000) .. controls
		(159.1875,160.5937) and (158.5313,170.6719) .. (157.2188,187.7344) .. controls
		(156.7500,188.3907) and (156.0469,189.1875) .. (155.1094,190.1250) --
		(102.3750,190.1250) .. controls (101.1563,189.0000) and (100.5469,187.9219) ..
		(100.5469,186.8906) .. controls (100.5469,186.1406) and (104.2031,181.9219) ..
		(111.5156,174.2344) .. controls (115.0781,170.0156) and (118.6875,165.8437) ..
		(122.3438,161.7188) .. controls (127.5938,155.8125) and (131.5782,152.5781) ..
		(134.2969,152.0156) .. controls (134.9531,151.9219) and (135.7031,151.8750) ..
		(136.5469,151.8750) .. controls (137.3906,151.8750) and (138.5859,151.9922) ..
		(140.1328,152.2266) .. controls (141.6797,152.4609) and (142.8750,152.5781) ..
		(143.7188,152.5781) .. controls (144.2813,152.5781) and (145.0781,152.5312) ..
		(146.1094,152.4375) .. controls (147.0469,152.4375) and (147.7969,152.4375) ..
		(148.3594,152.4375) .. controls (155.5781,152.4375) and (159.1875,154.1250) ..
		(159.1875,157.5000) -- cycle(171.5625,225.7031) -- (166.7812,225.7031) --
		(166.7812,207.2812) -- (171.5625,207.2812) -- (171.5625,225.7031) --
		cycle(165.0938,225.7031) -- (159.4688,225.7031) -- (155.9531,219.9375) --
		(152.2969,225.7031) -- (142.5938,225.7031) -- (141.3281,221.9062) --
		(135.7031,221.9062) -- (134.2969,225.7031) -- (129.7969,225.7031) --
		(135.9844,207.2812) -- (140.9062,207.2812) -- (147.3750,225.4219) --
		(153.0000,216.2812) -- (147.5156,207.2812) -- (152.7188,207.2812) --
		(156.2344,212.7656) -- (159.4688,207.2812) -- (164.8125,207.2812) --
		(159.0469,216.1406) -- (165.0938,225.7031) -- cycle(131.9062,210.5156) --
		(126.7031,210.5156) -- (126.7031,225.7031) -- (121.9219,225.7031) --
		(121.9219,210.6562) -- (116.4375,210.6562) -- (116.4375,207.2812) --
		(131.9062,207.2812) -- (131.9062,210.5156) -- cycle(140.0625,218.3906) --
		(138.3750,212.7656) -- (136.5469,218.3906) -- (140.0625,218.3906) -- cycle;
	\end{tikzpicture}
}



\documentclass[runningheads,a4paper]{llncs}
\usepackage[utf8]{inputenc}
\usepackage{helvet}
\usepackage{amssymb}
\setcounter{tocdepth}{3}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{hyperref}
\usepackage[T1]{fontenc}
\usepackage{caption}
\captionsetup[table]{skip=10pt}
\usepackage{subcaption}
\usepackage{siunitx}
\usepackage{cite}
\usepackage[section]{placeins}
%\renewcommand*{\bibfont}{\small}

\usepackage{url}
\urldef{\mailsa}\path|alexandre.de.brebisson@umontreal.ca|
\urldef{\mailsb}\path|alex.auvolat@ens.fr|
\urldef{\mailsc}\path|esimon@esimon.eu|
\urldef{\mailsd}\path|vincentp@iro.umontreal.ca|
\newcommand{\keywords}[1]{\par\addvspace\baselineskip
\noindent\keywordname\enspace\ignorespaces#1}

% Vroom vroom
\input{taxiglyph.tex}
\newcommand{\taxi}{ \kern-0.7em \taxiglyph \kern-0.7em }

\makeatletter
\renewcommand*{\@fnsymbol}[1]{\ifcase#1\or {\protect\taxi} \else\@ctrerr\fi}
\makeatother

\begin{document}



\mainmatter % start of an individual contribution

% first the title is needed
\title{Artificial Neural Networks Applied to\\Taxi Destination Prediction}

% a short form should be given in case it is too long for the running head
\titlerunning{Artificial Neural Networks Applied to Taxi Destination Prediction}

% the name(s) of the author(s) follow(s) next
%
% NB: Chinese authors should write their first names(s) in front of
% their surnames. This ensures that the names appear correctly in
% the running heads and the author index.
%
\author{Alexandre de Brbisson\inst{1} \thanks{Random order, this does not reflect the weights of contributions.}
\and tienne Simon\inst{2} \footnotemark[1]
\and Alex Auvolat\inst{3} \footnotemark[1]
\and \\ Pascal Vincent\inst{1}\inst{4} \and Yoshua Bengio\inst{1}\inst{4}.}

%
\authorrunning{Artificial Neural Networks Applied to Taxi Destination Prediction}
% (feature abused for this document to repeat the title also on left hand pages)

\institute{MILA lab, University of Montral,\\
\mailsa, \\ \mailsd\\
\and
ENS Cachan,\\
\mailsc\\
\and
ENS Paris,\\
\mailsb\\
\and
CIFAR.}


\toctitle{Lecture Notes in Computer Science}
\tocauthor{Authors' Instructions}
\maketitle


\begin{abstract}
We describe our first-place solution to the ECML/PKDD discovery challenge on taxi destination prediction. The task consisted in predicting the destination of a taxi based on the beginning of its trajectory, represented as a variable-length sequence of GPS points, and diverse associated meta-information, such as the departure time, the driver id and client information. Contrary to most published competitor approaches, we used an almost fully automated approach based on neural networks and we ranked first out of 381 teams. The architectures we tried use multi-layer perceptrons, bidirectional recurrent neural networks and models inspired from recently introduced memory networks. Our approach could easily be adapted to other applications in which the goal is to predict a fixed-length output from a variable-length sequence.
\end{abstract}

\newpage

\section{Introduction}

The taxi destination prediction challenge was organized by the 2015\linebreak ECML/PKDD conference\footnote{\url{http://www.geolink.pt/ecmlpkdd2015-challenge/}} and proposed as a Kaggle competition\footnote{\url{https://www.kaggle.com/c/pkdd-15-predict-taxi-service-trajectory-i}}. It consisted in predicting the destinations (latitude and longitude) of taxi trips based on initial partial trajectories (which we call \emph{prefixes}) and some meta-information associated to each ride. Such prediction models could help to dispatch taxis more efficiently.

The dataset is composed of all the complete trajectories of 442 taxis running in the city of Porto (Portugal) for a complete year (from 2013-07-01 to 2014-06-30). The training dataset contains 1.7 million datapoints, each one representing a complete taxi ride and being composed of the following attributes\footnote{The exact list of attributes for each trajectory can be found here \url{https://www.kaggle.com/c/pkdd-15-predict-taxi-service-trajectory-i/data}}:

\begin{itemize}
	\item the complete taxi ride: a sequence of GPS positions (latitude and longitude) measured every 15 seconds. The last position represents the destination and different trajectories have different GPS sequence lengths.
	\item metadata associated to the taxi ride:
		\begin{itemize}
		\item if the client called the taxi by phone, then we have a client ID. If the client called the taxi at a taxi stand, then we have a taxi stand ID. Otherwise we have no client identification,
		\item the taxi ID,
		\item the time of the beginning of the ride (unix timestamp).
		\end{itemize}
\end{itemize}

In the competition setup, the testing dataset is composed of 320 partial trajectories, which were created from five snapshots taken at different timestamps. This testing dataset is actually divided in two subsets of equal size: the public and private test sets. The public set was used through the competition to compare models while the private set was only used at the end of the competition for the final leaderboard.

Our approach uses very little hand-engineering compared to those published by other competitors. It is almost fully automated and based on artificial neural networks. Section~\ref{sec_winning} introduces our winning model, which is based on a variant of a multi-layer perceptron (MLP) architecture. Section~\ref{sec_alternatives} describes more sophisticated alternative architectures that we also tried. Although they did not perform as well as our simpler winning model for this particular task, we believe that they can provide further insight on how to apply neural networks to similar tasks. Section~\ref{sec_exp} and Section~\ref{sec_dis} compares and analyses our various models quantitatively and qualitatively on both the competition testing set and a bigger custom testing set. The source code and instructions to reproduce our results can be found online (\url{https://github.com/adbrebs/taxi}).

\newpage

\section{The Winning Approach}\label{sec_winning}

\subsection{Data Distribution}

Our task is to predict the destination of a taxi given a prefix of its trajectory. As the dataset is composed of full trajectories, we have to generate trajectory prefixes by cutting the trajectories in the right way. The provided training dataset is composed of more than 1.7 million complete trajectories, which gives 83 480 696 possible prefixes. The distribution of the training prefixes should be as close as possible as that of the provided testing dataset on which we were eventually evaluated. This test set was selected by taking five snapshots of the taxi network activity at various dates and times. This means that the probability that a trajectory appears in the test set is proportional to its length and that, for each entire testing trajectory, all its possible prefixes had an equal probability of being selected in the test set. Therefore, generating a training set with all the possible prefixes of all the complete trajectories of the original training set provides us with a training set which has the same distribution over prefixes (and whole trajectories) as the test set.

\subsection{MLP Architecture}

A Multi-Layer Perceptron (MLP) is a neural net in which each neuron of a given layer is connected to all the neurons of the next layer, without any cycle. It takes as input fixed-size vectors and processes them through one or several hidden layers that compute higher level representations of the input. Finally the output layer returns the prediction for the corresponding inputs. In our case, the input layer receives a representation of the taxi's prefix with associated metadata and the output layer predicts the destination of the taxi (latitude and longitude). We used standard hidden layers consisting of a matrix multiplication followed by a bias and a nonlinearity. The nonlinearity we chose to use is the Rectifier Linear Unit (ReLU)~\cite{glorot2011deep}, which simply computes $\max(0, x)$. Compared to traditional sigmoid-shaped activation functions, the ReLU limits the gradient vanishing problem as its derivative is always one when $x$ is positive. For our winning approach, we used a single hidden layer of 500 ReLU neurons.


\subsection{Input Layer}

One of the first problems we encountered was that the trajectory prefixes are varying-length sequences of GPS points, which is incompatible with the fixed-size input of the MLP. To circumvent (temporarily, see Section~\ref{rnn}) this limitation, we chose to consider only the first $k$ points and last $k$ points of the trajectory prefix, which gives us a total of $2k$ points, or $4k$ numerical values for our input vector. For the winning model we took $k=5$. These GPS points are standardized (zero-mean, unit-variance). When the prefix of the trajectory contains less than $2k$ points, the first and last $k$ points overlap. When the trajectory prefix contains less than $k$ points, we pad the input GPS points by repeating either the first or the last point.

To deal with the discrete meta-data, consisting of client ID, taxi ID, date and time information, we learn embeddings jointly with the model for each of these information. This is inspired by neural language modeling approaches~\cite{bengio2003neural}, in which each word is mapped to a vector space of fixed size (the vector is called the \emph{embedding} of the word). The table of embeddings for the words is included in the model parameters that we learn and behaves as a regular parameter matrix: the embeddings are first randomly initialized and are then modified by the training algorithm like the other parameters. In our case, instead of words, we have metadata values. More precisely, we have one embedding table for each metadata with one row for each possible value of the metadata. For the date and time, we decided to create higher-level variables that better describe human activity: quarters of hour, day of the week, week of the year (one embedding table is learnt for each of them). These embeddings are then simply concatenated to the $4k$ GPS positions to form the input vector of the MLP. The complete list of embeddings used in the winning model is given in Table~\ref{table_embed}.

\begin{table}
\centering
\caption{Metadata values and associated embedding size.}
\label{table_embed}
\begin{tabular}{l||c|c}
  Metadata & Number of possible values & Embedding size \\
  \hline \hline
  Client ID & 57106 & 10\\
  Taxi ID & 448 & 10\\
  Stand ID & 64 & 10\\
  Quarter hour of the day & 96 & 10\\
  Day of the week & 7 & 10\\
  Week of the year & 52 & 10
\end{tabular}
\end{table}

\subsection{Destination Clustering and Output Layer}

As the destination we aim to predict is composed of two scalar values (latitude and longitude), it is natural to have two output neurons. However, we found that it was difficult to train such a simple model because it does not take into account any prior information on the distribution of the data. To tackle this issue, we integrate prior knowledge of the destinations directly in the architecture of our model: instead of predicting directly the destination position, we use a predefined set $(c_i)_{1\le i \le C}$ of a few thousand destination cluster centers and a hidden layer that associates a scalar value $(p_i)_i$ (similar to a probability) to each of these clusters. As the network must output a single destination position, for our output prediction $\hat{y}$, we compute a weighted average of the predefined destination cluster centers:
\[ \hat{y} = \sum_{i=1}^C p_i c_i. \]
Note that this operation is equivalent to a simple linear output layer whose weight matrix would be initialized as our cluster centers and kept fixed during training.
The hidden values $(p_i)_i$ must sum to one so that $\hat{y}$ corresponds to a centroid calculation and thus we compute them using a softmax layer:
\[ p_i = \frac{\exp(e_i)}{\sum_{j=1}^C \exp(e_j)}, \]
where $(e_j)_j$ are the activations of the previous layer.

The clusters $(c_i)_i$ were calculated with a mean-shift clustering algorithm on the destinations of all the training trajectories, returning a set of $C=3392$ clusters. Our final MLP architecture is represented in Figure~\ref{mlp_fig}.

\begin{figure}
	\centering
	\scalebox{.9}{%
		\def\svgwidth{.8\textwidth}
		\input{diagram_mlp.pdf_tex}
	}
	\caption{Architecture of the winning model.}
	\label{mlp_fig}
\end{figure}


\subsection{Cost Computation and Training Algorithm}

The evaluation cost of the competition is the mean Haversine distance, which is defined as follows ($\lambda_x$ is the longitude of point $x$, $\phi_x$ is its latitude, and $R$ is the radius of the Earth):
\[ d_\mathrm{haversine}(x, y) = 2 R \arctan\left(\sqrt{\frac{a(x, y)}{a(x, y)-1}}\right), \]
where $a(x, y)$ is defined as:
\[ a(x, y) = \sin^2\left(\frac{\phi_y-\phi_x}{2}\right)+\cos(\phi_x)\cos(\phi_y)\sin^2\left(\frac{\lambda_y-\lambda_x}{2}\right). \]
Our models did not learn very well when trained directly on the Haversine distance function and thus, we used the simpler  equirectangular distance instead, which is a very good approximation at the scale of the city of Porto:
\[ d_\mathrm{equirectangular}(x, y) = R \sqrt{\left((\lambda_y-\lambda_x)\cos\left(\frac{\phi_y-\phi_x}{2}\right)\right)^2+(\phi_y-\phi_x)^2}. \]

We used stochastic gradient descent (SGD) with momentum to minimise the mean equirectangular distance between our predictions and the actual destination points. We set a fixed learning rate of 0.01, a momentum of 0.9 and a batch size of 200.


\section{Alternative Approaches}\label{sec_alternatives}

The models that we are going to present in this section did not perform as well for our specific destination task on the competition test set but we believe that they can provide interesting insights for other problems involving fixed-length outputs and variable-length inputs.

\subsection{Recurrent Neural Networks}\label{rnn}

\begin{figure}[!ht]
	\centering
	\scalebox{.9}{%
		\def\svgwidth{.8\textwidth}
		\input{diagram_rnn.pdf_tex}
	}
	\caption{RNN architecture, in which the GPS points are read one by one in the forward direction of the prefix. (The rest of the architecture is the same as before and is omitted for clarity.)}
	\label{fig_rnn}
\end{figure}

As stated previously, a MLP is constrained by its fixed-length input, which prevents us from fully exploiting the entire trajectory prefix. Therefore we naturally considered recurrent neural net (RNN) architectures, which can read all the GPS points one by one, updating a fixed-length internal state with the same transition matrix at each time step. The last internal state of the RNN is expected to summarize the prefix with relevant features for the specific task. Such recurrent architectures are difficult to train due in particular to the problem of vanishing and exploding gradients~\cite{bengio1994learning}. This problem is partially solved with long short-term memory (LSTM) units~\cite{hochreiter1997long}, which are crucial components in many state of the art architectures for tasks including handwriting recognition~\cite{graves2009novel, doetsch2014fast}, speech recognition~\cite{graves2013speech, sak2014long}, image captioning~\cite{xu2015show} or machine translation~\cite{bahdanau2014neural}.

We implemented and trained a LSTM RNN that reads the trajectory one GPS point at a time from the beginning to the end of each input prefix. This architecture is represented in Figure~\ref{fig_rnn}. Furthermore, in order to help the network to better identify short-term dependencies (such as the velocity of the taxi as the difference between two successive data points), we also considered a variant in which the input of the RNN is not anymore a single GPS point but a window of 5 successive GPS points of the prefix. The window shifts along the prefix by one point at each RNN time step.


\subsection{Bidirectional Recurrent Neural Networks}

We noticed that the most relevant parts of the prefix are its beginning and its end and we therefore tried a bidirectional RNN~\cite{graves2005bidirectional} (BRNN) to focus on these two particular parts. Our previously described RNN reads the prefix forwards from the first to the last known point, which leads to a final internal state containing more information about the last points (the information about the first points is more easily forgotten). In the BRNN architecture, one RNN reads the prefix forwards while a second RNN reads the prefix backwards. The two final internal states of the two RNNs are then concatenated and fed to a standard MLP that will predict the destination in the same way as in our previous models. The concatenation of these two final states is likely to capture more information about the beginning and the end of the prefix. Figure~\ref{fig_bidirnn} represents the BRNN component of our architecture.

\begin{figure}
	\centering
	\scalebox{.9}{%
		\def\svgwidth{.8\textwidth}
		\input{diagram_bidirnn.pdf_tex}
	}
	\caption{Bidirectional RNN architecture.}
	\label{fig_bidirnn}
\end{figure}

\subsection{Memory Networks}

Memory networks~\cite{weston2014memory} have been recently introduced as an architecture that can exploit an external database by retrieving and storing relevant information for each prediction. We have implemented a slightly related architecture, which is represented in Figure~\ref{fig_memnet} and which we will now describe. For each prefix to predict, we extract $m$ entire trajectories (which we call \emph{candidates}) from the training dataset. We then use two neural network encoders to respectively encode the prefix and the candidates (same encoder for all the candidates). The encoders are the same as those of our previous architectures (either feedforward or recurrent) except that we stop at the hidden layer instead of predicting an output. This results into $m+1$ fixed-length representations in the same vector space so that they can be easily compared. Then we compute similarities by taking the dot products of the prefix representation with all the candidate representations. Finally we normalize these $m$ similarity values with a softmax and use the resulting probabilities to weigh the destinations of the corresponding candidates. In other words, the final destination prediction of the prefix is the centroid of the candidate destinations weighted by the softmax probabilities. This is similar to the way we combine clusters in our previously described architectures.

\begin{figure}
	\centering
	\scalebox{.9}{%
		\def\svgwidth{.8\textwidth}
		\input{diagram_memnet.pdf_tex}
	}
	\caption{Memory network architecture. The \emph{encoders} are generic bricks that take as input the trajectory points with metadata and process them through a feedforward or recurrent neural net in order to return a fixed-size representations $\vec{r}$ and $(\vec{r})_i$.}
	\label{fig_memnet}
\end{figure}
\vspace{-2em}

As the trajectory database is very large, such an architecture is quite challenging to implement efficiently. Therefore, for each prefix (more precisely for each batch of prefixes), we naively select $m=10000$ random candidates. We believe that more sophisticated retrieving functions could significantly improve the results, but we did not have time to implement them. In particular, one could use a pre-defined (hand-engineered) similarity measure to retrieve the most similar candidates to the particular prefix.

The two encoders that map prefixes and candidates into the same representation space can either be feedforward or recurrent (bidirectional). As RNNs are more expensive to train (both in terms of computation time and RAM consumption), we had to limit ourselves to a MLP with one single hidden layer of 500 ReLUs for the encoder. We trained the architecture with a batch size of 5000 examples, and for each batch we randomly pick 10000 candidates from the training set.

\section{Experimental Results}\label{sec_exp}

\subsection{Custom Validation Set}

As the competition testing dataset is particularly small, we can not reliably compare models on it. Therefore, for the purpose of this paper, we will compare our models on two bigger datasets: a validation dataset composed of 19427 trajectories and a testing dataset composed of 19770 trajectories. We obtained these new testing and validation sets by extracting (and removing) random portions of the original training set. The validation dataset is used to early-stop our training algorithms for each model based on the best validation score, while the testing dataset is used to compare our different trained models.

\begin{table}[bh]
\centering
\caption{Testing errors of our models. Contrary to model 1, model 2 predicts the destination directly without using the clusters. Model 3 only uses the prefix as input without any metadata. Model 4 only uses metadata as input without the prefix. While the BRNN of model 6 takes a single GPS point at each time step, the BRNN of model 7 takes five consecutive GPS points at each time step.}
\label{table_results}
\begin{tabular}{l l||c|c|c}
  & Model & Custom Test & Kaggle Public & Kaggle Private \\
  \hline \hline
  1 & MLP, clustering (winning model) & 2.81 & \textbf{2.39} & \textbf{1.87}\footnotemark[8] \\
  2 & MLP, direct output & 2.97 & 3.44 & 3.88 \\
  3 & MLP, clustering, no embeddings & 2.93 & 2.64 & 2.17 \\
  4 & MLP, clustering, embeddings only & 4.29 & 3.52 & 3.76 \\
  5 & RNN & 3.14 & 2.49 & 2.39 \\
  6 & Bidirectional RNN & 3.01 & 2.65 & 2.33 \\
  7 & Bidirectional RNN with window & \textbf{2.60} & 3.15 & 2.06 \\
  8 & Memory network & 2.87& 2.77 & 2.20 \\
   & Second-place team & - & 2.36 & 2.09 \\
   & Third-place team & - & 2.45 & 2.11 \\
   & Average competition scores\footnotemark[9] & - & 3.02 & 3.11 \\
\end{tabular}
\end{table}

\footnotetext[8]{our winning submission on Kaggle scored 2.03 but the model had not been trained until convergence}
\footnotetext[9]{average over 381 teams, the submissions with worse scores than the public benchmark (in which the center of Porto is always predicted) have been discarded}


\subsection{Results}

Table~\ref{table_results} shows the testing scores of our various models on our custom testing dataset as well as on the competition ones. The different hyperparameters of each model have been tuned.

\section{Analysis of the results}\label{sec_dis}

Our winning ECML/PKDD challenge model is the MLP model that uses clusters of the destinations. However it is not our best model on our custom test, which is the BRNN with window. As our custom test set is considerably larger than the competition one, the scores on it are significantly more confident and we can therefore assert that our overall best model is the BRNN with window.

The results also prove that embeddings and clusters significantly improve our models. The importance of embeddings can also be confirmed by visualizing them. Figure~\ref{fig_tsne} shows 2D t-SNE~\cite{van2008visualizing} projections for two of these embeddings and clear patterns can be observed, proving that quarters of hour and weeks of the year are important features for the prediction.

The reported score of the memory network is lower than the others but this might be due to the fact that it was not trained until convergence (we stopped after one week on a high end GPU).

\begin{figure}[!b]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{_model_contextembedder_qhour_of_day_lookup.pdf}
  \label{fig_qhour}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{_model_contextembedder_week_of_year_lookup.pdf}
  \label{fig_week}
\end{subfigure}
\vspace{-2em}
\caption{t-SNE 2D projection of the embeddings. \textbf{Left:} quarter of hour at which the taxi departed (there are 96 quarters in a day, so 96 points, each one representing a particular quarter). \textbf{Right:} week of the year during which the taxi departed (there are 52 weeks in a year, so 52 points, each one representing a particular week).}
\label{fig_tsne}
\end{figure}

The scores on our custom test set are higher than the scores on the public and private test set used for the competition. This suggests that the competition testing set is composed of rides that took place at very specific dates and times with very particular trajectory distributions. The gap between the public and private test sets is probably due to the fact that their size is particularly small. In contrast, our validation and test sets are big enough to obtain more significant statistics.

All the models we have explored are very computationally intensive and we thus had to train them on GPUs to avoid weeks of training. Our competition winning model is the least intensive and can be trained in half a day on GPU. On the other hand, our recurrent and memory networks are much slower and we believe that we could reach even better scores by training them longer.

\section*{Conclusion}

We introduced an almost fully-automated neural network approach to predict the destination of a taxi based on the beginning of its trajectory and associated metadata. Our best model uses a recurrent bidirectional neural network to encode the prefix, several embeddings to encode the metadata and destination clusters to generate the output.

One potential limitation of our clustering-based output layer is that the final prediction can only fall in the convex hull of the clusters. A potential solution would be to learn the clusters as parameters of the network and initialize them either randomly or from the mean-shift clusters.

Concerning the memory network, one could consider more sophisticated ways to extract candidates, such as using an hand-engineered similarity measure or even the similarity measure learnt by the memory network. In this latter case, the learnt similarity should be used to extract only a proportion of the candidates in order let a chance to candidates with poor similarities to be selected. Furthermore, instead of using the dot product to compare prefix and candidate representations, more complex functions could be used (such as the concatenation of the representations followed by non-linear layers).

\subsection*{Acknowledgments}
We would like to thank the developers of Theano \cite{theano0, theano1}, Blocks and Fuel \cite{blocksfuel} for developing such convenient tools.
We acknowledge the support of the following organizations for research funding and computing support: Samsung, NSERC, Calcul Quebec, Compute Canada, the Canada Research Chairs and CIFAR.

\bibliography{taxi}
\bibliographystyle{unsrt}

\end{document}



\documentclass{article} % For LaTeX2e
%\usepackage{nips14submit_e,times}
\usepackage{iclr2015,times}
\usepackage{hyperref}
\usepackage{url}

\iclrconference
\iclrfinalcopy

\usepackage{natbib}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{relsize}

\usepackage{float}
\usepackage[pdftex]{graphicx}
\graphicspath{{figures/}}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

\renewcommand{\vec}[1]{{\mathbf{#1}}}
\newcommand{\Bernoulli}{\mathcal{B}}
\newcommand{\DD}{{\cal D}}
\newcommand{\LL}{{\cal L}}

\newcommand{\E}[2]{\mathop{\mathlarger{\mathbb{E}} }_{#1}\left[#2\right]}
\newcommand{\prob}[2]{p\left(#1 \, | \, #2\right)}
\newcommand{\qrob}[2]{q\left(#1 \, | \, #2\right)}

\newcommand{\xVec}{\vec{x}}
\newcommand{\hVec}{\vec{h}}
\newcommand{\yVec}{\vec{y}}
\newcommand{\aVec}{\vec{a}}
\newcommand{\bVec}{\vec{b}}

\newcommand{\hess}{\mathbf{H}}
\newcommand{\jacob}{\mathbf{J}}
\newcommand{\params}{\mathbf{\theta}}
\newcommand{\cost}{\mathcal{L}}
\newcommand{\lout}{\mathbf{r}}
\newcommand{\louti}{r}
\newcommand{\outi}{y}
\newcommand{\out}{\mathbf{y}}
\newcommand{\gauss}{\mathbf{G_N}}
\newcommand{\eye}{\mathbf{I}}
\newcommand{\softmax}{\phi}
\newcommand{\targ}{\mathbf{t}}
\newcommand{\metric}{\mathbf{G}}
\newcommand{\sample}{\mathbf{z}}

\newcommand{\bmx}[0]{\begin{bmatrix}}
\newcommand{\emx}[0]{\end{bmatrix}}
\newcommand{\qt}[1]{\left<#1\right>}
\newcommand{\qexp}[1]{\left<#1\right>}
\newcommand{\qlay}[1]{\left[#1\right]}
\newcommand{\vect}[1]{\mathbf{#1}}
\newcommand{\vects}[1]{\boldsymbol{#1}}
\newcommand{\matr}[1]{\mathbf{#1}}
\newcommand{\var}[0]{\operatorname{Var}}
\newcommand{\cov}[0]{\operatorname{Cov}}
\newcommand{\diag}[0]{\operatorname{diag}}
\newcommand{\matrs}[1]{\boldsymbol{#1}}
\newcommand{\va}[0]{\vect{a}}
\newcommand{\vb}[0]{\vect{b}}
\newcommand{\vc}[0]{\vect{c}}
\newcommand{\vh}[0]{\vect{h}}
\newcommand{\vv}[0]{\vect{v}}
\newcommand{\vx}[0]{\vect{x}}
\newcommand{\vw}[0]{\vect{w}}
\newcommand{\vs}[0]{\vect{s}}
\newcommand{\vf}[0]{\vect{f}}
\newcommand{\vy}[0]{\vect{y}}
\newcommand{\vg}[0]{\vect{g}}
\newcommand{\vm}[0]{\vect{m}}
\newcommand{\vu}[0]{\vect{u}}
\newcommand{\vL}[0]{\vect{L}}
\newcommand{\mW}[0]{\matr{W}}
\newcommand{\mG}[0]{\matr{G}}
\newcommand{\mX}[0]{\matr{X}}
\newcommand{\mQ}[0]{\matr{Q}}
\newcommand{\mU}[0]{\matr{U}}
\newcommand{\mV}[0]{\matr{V}}
\newcommand{\mA}{\matr{A}}
\newcommand{\mD}{\matr{D}}
\newcommand{\mS}{\matr{S}}
\newcommand{\mI}{\matr{I}}
\newcommand{\td}[0]{\text{d}}
\newcommand{\vsig}[0]{\vects{\sigma}}
\newcommand{\valpha}[0]{\vects{\alpha}}
\newcommand{\vmu}[0]{\vects{\mu}}
\newcommand{\vzero}[0]{\vect{0}}
%\newcommand{\tf}[0]{\text{f}}
\newcommand{\tf}[0]{\text{m}}
\newcommand{\tdf}[0]{\text{dm}}
\newcommand{\TT}[0]{{\vects{\theta}}}
\newcommand{\grad}[0]{\nabla}
\newcommand{\alert}[1]{\textcolor{red}{#1}}
\newcommand{\N}[0]{\mathcal{N}}
% \newcommand{\LL}[0]{\mathcal{L}}
\newcommand{\HH}[0]{\mathcal{H}}
\newcommand{\RR}[0]{\mathbb{R}}
\newcommand{\II}[0]{\mathbb{I}}
\newcommand{\Scal}[0]{\mathcal{S}}
\newcommand{\sigmoid}{\sigma}
%\newcommand{\E}[0]{\mathbb{E}}
\newcommand{\enabla}[0]{\ensuremath{%
    \overset{\raisebox{-0.3ex}[0.5ex][0ex]{%
    \ensuremath{\scriptscriptstyle e}}}{\nabla}}}

\newcommand{\tred}[1]{\textcolor{red}{#1}}
\newcommand{\todo}[1]{{\Large\textcolor{red}{#1}}}
\newcommand{\done}[1]{{\Large\textcolor{green}{#1}}}

\DeclareMathOperator*{\argmax}{\arg \max}
\DeclareMathOperator*{\argmin}{\arg \min}
\newcommand{\newln}{\\&\quad\quad{}}

\title{Reweighted Wake-Sleep}

%Important Sampling Inference Networks -- Stochastic Backpropagation Training for Directed Graphical Models 
%with Intractable Posterior}

%\title{Training Directed Models using Importance Sampling}

%$^{1}$ 
%$^{2}$\\
%$^1$CIFAR Global Scholar, $^2$CIFAR Senior Fellow \\
\author{
J\"org Bornschein and 
Yoshua Bengio \thanks{J\"org Bornschein is a CIFAR Global Scholar; Yoshua Bengio is a CIFAR Senior Fellow} \\
Department of Computer Science and Operations Research\\
University of Montreal\\
Montreal, Quebec, Canada\\
%\texttt{find.us@on.the.web}\\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}


\maketitle

\begin{abstract}
Training deep directed graphical models with many hidden variables and performing inference
remains a major challenge. Helmholtz machines and deep belief networks are such
models, and the wake-sleep algorithm has been proposed to train them.
The wake-sleep algorithm relies on training not just the directed generative model
but also a conditional generative model (the {\em inference network})
that runs backward from visible to latent,
estimating the posterior distribution of latent given visible.
We propose a novel interpretation of the wake-sleep algorithm which suggests
that better estimators of the gradient can be obtained by sampling latent
variables multiple times from the inference network.
This view is based on importance sampling as an estimator of the likelihood,
with the approximate inference network as a proposal distribution. 
This interpretation is confirmed experimentally, showing that better likelihood
can be achieved with this {\em reweighted wake-sleep} procedure.
Based on this interpretation, we propose that a sigmoidal belief network is not
sufficiently powerful for the layers of the inference network in order to
recover a good estimator of the posterior distribution of latent variables. Our
experiments show that using a more powerful layer model, such as NADE, yields
substantially better generative models.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

Training directed graphical models -- especially models with multiple layers
of hidden variables -- remains a major challenge.  This is unfortunate
because, as has been argued previously~\citep{Hinton06,Bengio-2009-book}, a
deeper generative model has the potential to capture high-level
abstractions and thus generalize better. The exact log-likelihood gradient
is intractable, be it for Helmholtz machines~\citep{Hinton95,Dayan-et-al-1995},
sigmoidal belief networks (SBNs), or deep belief networks
(DBNs)~\citep{Hinton06}, which are directed models with a restricted Boltzmann machine (RBM) as top-layer.
Even obtaining an unbiased estimator of the gradient of the DBN
or Helmholtz machine log-likelihood is not something that has been achieved in
the past. Here we show that it is possible to get an unbiased estimator of
the likelihood (which unfortunately makes it a slightly biased estimator of
the log-likelihood), using an importance sampling approach. Past proposals
to train Helmholtz machines and DBNs rely on maximizing a variational bound
as proxy for the log-likelihood~\citep{Hinton95,Kingma+Welling-ICLR2014,Rezende-et-al-ICML2014}.
The first of these is the {\em wake-sleep} algorithm~\citep{Hinton95}, which 
relies on combining a ``recognition'' network (which we call an {\em approximate
inference network}, here, or simply {\em inference network}) with a generative network.
In the wake-sleep algorithm, they basically provide targets for each other.
We review these previous approaches and introduce a novel approach that
generalizes the wake-sleep algorithm. Whereas the original justification
of the wake-sleep algorithm has been questioned (because we are optimizing
a KL-divergence in the wrong direction), a contribution of this paper is
to shed a different light on the wake-sleep algorithm, viewing it as a special case of
the proposed reweighted wake-sleep (RWS) algorithm, i.e., as reweighted
wake-sleep with a single sample. 
This shows that wake-sleep corresponds to optimizing
a somewhat biased estimator of the likelihood
gradient, while using more samples makes the estimator
less biased (and asymptotically unbiased as more samples are considered).
We empirically show that effect, with clearly better results obtained with $K=5$ samples
than with $K=1$ (wake-sleep), and 5 or 10 being sufficient to achieve good results.
Unlike in the case of DBMs, which rely on a Markov chain to get samples
and estimate the gradient by a mean over those samples, here the samples
are i.i.d., avoiding the very serious problem of mixing between modes that
can plague MCMC methods~\citep{Bengio-et-al-ICML2013} when training
undirected graphical models.

Another contribution of this paper regards the architecture of the deep
approximate inference network. We view the inference network as estimating the
posterior distribution of latent variables given the observed input. With this
view it is plausible that the classical architecture of the inference network
(a SBN, details below) is inappropriate and we test this hypothesis
empirically. We find that more powerful parametrizations that can represent 
non-factorial posterior distributions yield better results.
% each layer as a conditional probability model yields better results.
%In the classical sigmoidal belief network (SBN) (e.g., in the DBN
%and Helmholtz machine), the conditional distribution of each layer of the
%inference network, given the previous layer, is a factorized Bernoulli (where
%the probability for each bit is computed as in a logistic regression with the
%previous layer bits as inputs). 
\iffalse
Here we propose a method to train and evaluate directed graphical models
with binary latent variables according to their estimated log likelihood.
In contrast to most previously proposed methods we do not optimize a
variational bound.  We use an importance sampling based estimator to
perform inference and to compute learning gradients.  The proposal
distribution has the form of a multilayer directed network that runs in
parallel but inverted to the generative network $p(x,h)$.  In parallel to
learning the generative model $p(x)$, we train the proposal distribution
$q(\xVec|\hVec)$ to perform inference / provide a minimum variance
estimator $p(x)$.
\fi

\section{Reweighted Wake-Sleep}

\subsection{The Wake-Sleep Algorithm}

The wake-sleep algorithm was proposed as a way to train Helmholtz machines,
which are deep directed graphical models $p(\xVec,\hVec)$ over visible variables $\xVec$
and latent variables $\hVec$, where the latent variables are organized in
layers $\hVec_k$. %, with the $k$-th layer taking as input the random vector generated by the previous layer in the generating sequence, $\hVec_{k+1}$.
In the Helmholtz machine~\citep{Hinton95,Dayan-et-al-1995}, the top layer
$\hVec_L$ has a factorized unconditional distribution, so that ancestral
sampling can proceed from $\hVec_L$ down to $\hVec_1$ and then the generated
sample $\xVec$ is generated by the bottom layer, given $\hVec_1$. In the
deep belief network (DBN)~\citep{Hinton06}, the top layer is instead
generated by a RBM, i.e., by a Markov chain,
while simple ancestral sampling is used for the others.  Each intermediate
layer is specified by a conditional distribution parametrized 
as a stochastic sigmoidal layer (see section~\ref{sec:SBN} for details).

The wake-sleep algorithm is a training procedure for such generative
models, which involves training an auxiliary network, called the
{\em inference network}, that takes a visible vector $\xVec$ as input and
stochastically outputs samples $\hVec_k$ for all layers $k=1$ to $L$. 
The inference network outputs samples from a distribution that
should estimate the conditional probability of the latent variables of the
generative model (at all layers) given the input. Note that in these kinds of directed models 
%(and this is generally the case with latent variables), 
exact inference, i.e., sampling from $p(\hVec|\xVec)$ is intractable.

The wake-sleep algorithm proceeds in two phases. In the {\em wake phase},
an observation $\xVec$ is sampled from the training distribution $\DD$ and
propagated stochastically up the inference network (one layer at a time), thus sampling latent
values $\hVec$ from $q(\hVec | \xVec)$. Together with $\xVec$, the sampled $\hVec$
forms a target for training $p$, i.e., one performs a step of gradient ascent update with respect to maximum
likelihood over the generative model $p(\xVec, \hVec)$, with the data $\xVec$ and the
inferred $\hVec$. This is useful
because whereas computing the gradient of the marginal likelihood
$p(\xVec)=\sum_\hVec p(\xVec,\hVec)$ is intractable, 
computing the gradient of the complete log-likelihood $\log p(\xVec,\hVec)$
is easy. In addition, these updates decouple all the layers (because both the input and the target of each layer
are considered observed). In the {\em sleep-phase}, a ``dream'' sample is obtained from the
generative network by ancestral sampling from $p(\xVec, \hVec)$ and is used
as a target for the maximum likelihood training of the inference
network, i.e., $q$ is trained to estimate $p(\hVec | \xVec)$.

The justification for the wake-sleep algorithm that was originally proposed is based on the following variational bound,
\[
  \log p(\xVec) \geq \sum_{\hVec} q(\hVec|\xVec) \log \frac{p(\xVec,\hVec)}{q(\hVec|\xVec)}
\]
that is true for any inference network $q$, but the bound becomes tight as $q(\hVec|\xVec)$ approaches $p(\hVec|\xVec)$.
Maximizing this bound with respect to $p$ corresponds to the wake phase update.
The update with respect to $q$ should minimize $KL(q(\hVec|\xVec) || p(\hVec|\xVec))$ (with $q$ as the reference)
but instead the sleep phase update minimizes the reversed KL divergence, $KL(p(\hVec|\xVec) || q(\hVec|\xVec))$
(with $p$ as the reference).

\subsection{An Importance Sampling View yields Reweighted Wake-Sleep}
\label{sec:pxest}

If we think of $q(\hVec|\xVec)$ as estimating $p(\hVec|\xVec)$ and train it accordingly
(which is basically what the sleep phase of wake-sleep does), then we can reformulate
the likelihood as an importance-weighted average:
%
%\begin{align}  
%  p(\xVec) &= \sum_{\hVec} p(\xVec, \hVec) = \sum_{\hVec} \qrob{\hVec}{\xVec}
%                   \frac{p(\xVec, \hVec)}{\qrob{\hVec}{\xVec}} \nonumber \\
%                &= \E{\hVec \sim \qrob{\hVec}{\xVec}}{\frac{p(\xVec, \hVec)}{\qrob{\hVec}{\xVec}}} 
%                \simeq \frac{1}{K} \sum_{\substack{k=1 \\ \hVec^{(k)} \sim \qrob{\hVec}{\xVec}} }^K
%                   \frac{p(\xVec, \hVec^{(k)})}{\qrob{\hVec^{(k)}}{\xVec}}          \label{eq:pxest}
%\end{align}
\begin{align}  
  p(\xVec) %&= \sum_{\hVec} p(\xVec, \hVec) 
            = \sum_{\hVec} \qrob{\hVec}{\xVec} \frac{p(\xVec, \hVec)}{\qrob{\hVec}{\xVec}}
                &= \E{\hVec \sim \qrob{\hVec}{\xVec}}{\frac{p(\xVec, \hVec)}{\qrob{\hVec}{\xVec}}} 
                \simeq \frac{1}{K} \hspace{-0.3cm} \sum_{\substack{k=1 \\ \hVec^{(k)} \sim \qrob{\hVec}{\xVec}} }^K \hspace{-0.3cm} 
                   \frac{p(\xVec, \hVec^{(k)})}{\qrob{\hVec^{(k)}}{\xVec}}          \label{eq:pxest}
\end{align}
%
Eqn. \eqref{eq:pxest} is a consistent and unbiased estimator for the marginal
likelihood $p(\xVec)$.  The optimal $q$ that results in a minimum variance
estimator is $q^{*}(\hVec\,|\,\xVec)=\prob{\hVec}{\xVec}$.  In fact we can show
that this is a zero-variance estimator, i.e., the best possible one that will
result in a perfect $p(x)$ estimate even with a single arbitrary sample 
$\hVec \sim \prob{\hVec}{\xVec}$: 
%
\begin{align}
 \E{h \sim p(\hVec\,|\,\xVec)}
      {\frac{\prob{\hVec}{\xVec}p(\xVec)}{p(\hVec\,|\,\xVec)}} 
%      {\frac{p(\xVec, \hVec)}{p(\hVec\,|\,\xVec)}} 
    = p(\xVec) \; \E{h \sim p(\hVec\,|\,\xVec)}{1} = p(\xVec)
\end{align}
%
Any mismatch between $q$ and $\prob{\hVec}{\xVec}$ will increase the variance
of this estimator, but it will not introduce any bias.  In practice however, we
are typically interested in an estimator for the {\em log}-likelihood. Taking
the logarithm of \eqref{eq:pxest} and averaging over multiple datapoints will
result in a conservative biased estimate and will, on average, underestimate
the true log-likelihood due to the concavity of the logarithm. Increasing the
number of samples will decrease both the bias and the variance.  Variants of
this estimator have been used in e.g. \citep{Rezende-et-al-ICML2014,KarolAndriyDaan14}
to evaluate trained models.

\iffalse
for set of datapoints $\DD=\xVec^{(n)}$. By Jensens's
inequality, we get 
In this case, in expectation over samples,
we get a lower bound on the log-likelihood, by Jensen's inequality: the
expected value over samples of the log of the importance weighted likelihood
estimator is less or equal than the log of the expected value, i.e., less or
equal to the true log-likelihood.
 This is good because it gives us a conservative
estimator of the log-likelihood, in average, i.e., it tends to underestimate the
ground truth.
From the point of view of the log-likelihood gradient, it means
that our estimator is the gradient of a lower bound on the log-likelihood.
This is also true of variational methods such as described below (section~\ref{sec:var-AE}).
However, unlike with these methods, here {\em the bound can be made 
arbitrarily tighter by simply using more samples}, because the inner
average over samples (before applying the log) converges to its expectation.

\begin{align}
  \label{eq:llbound}
  \text{LL bound} &=
  avg_{\xVec \in \DD} \; \E{\hVec^{(k)}\sim q(\hVec|\xVec)}
    {\log p(\xVec, \hVec) - \log q(\hVec | \xVec)} \\
  \label{eq:llest}
  \text{LL} &=
  avg_{\xVec \in \DD} \log \left( \E{\hVec^{(k)}\sim q(\hVec|\xVec)}
    {\frac{ p(\xVec, \hVec) }{ q(\hVec | \xVec)}} \right)  \\
  \text{LL bound} &\leq   \text{LL}
\end{align}

\fi

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\subsection{Training by Reweighted Wake-Sleep}

We now consider the models $p$ and $q$ parameterized with parameters $\theta$ and $\phi$ respectively.

%\subsubsection{Updating $p_{\theta}$ for given $q_\phi$}
{\bf Updating $p_{\theta}$ for given $q_\phi$}:
%
We propose to use an importance sampling estimator based on eq. \eqref{eq:pxest}
to compute the gradient of the marginal log-likelihood 
$\LL_p(\theta, \xVec) = \log p_{\theta}(\xVec)$: 
%
\begin{align}
  \frac{\partial}{\partial \theta} \LL_p(\theta, \xVec \sim \DD) 
%    &= \frac{\partial}{\partial \theta} \log p_{\theta}(\xVec) 
%     = \frac{1}{p(\xVec)} \frac{\partial}{\partial \theta} p(\xVec) \\
%     = \frac{1}{p(\xVec)} \frac{\partial}{\partial \theta} \sum_{\hVec} p(\xVec, \hVec) \nonumber \\
%    &= \frac{1}{p(\xVec)} \sum_{\hVec} p(\xVec, \hVec) 
%            \frac{\partial}{\partial \theta} \log  p(\xVec, \hVec) \nonumber \\
%    &= \frac{1}{p(\xVec)} \sum_{\hVec} \qrob{\hVec}{\xVec} \frac{p(\xVec, \hVec)}{\qrob{\hVec}{\xVec}}
%            \frac{\partial}{\partial \theta} \log  p(\xVec, \hVec) \nonumber \\
    &= \frac{1}{p(\xVec)} \E{\hVec \sim \qrob{\hVec}{\xVec}}{\frac{p(\xVec, \hVec)}{\qrob{\hVec}{\xVec}}
            \frac{\partial}{\partial \theta} \log p(\xVec, \hVec)} \nonumber \\
  \label{eq:grad_p}
%    &\simeq \frac{1}{\sum_k \omega_k} \sum_{k=1}^K \omega_k
%            \frac{\partial}{\partial \theta} \log p(\xVec, \hVec^{(k)}) \\
    &\simeq \sum_{k=1}^K \tilde{\omega}_k \frac{\partial}{\partial \theta} \log p(\xVec, \hVec^{(k)}) 
    \;\;\text{with}\; \hVec^{(k)} \sim \qrob{\hVec}{\xVec}  \\ 
 \text{and the importance} & \text{ weights}\; 
    \tilde{\omega}_k = \frac{\omega_k}{\sum_{k'=1}^K \omega_{k'}} ; \;  
    \omega_k = \frac{p(\xVec, \hVec^{(k)})}{\qrob{\hVec^{(k)}}{\xVec}} \text{.}\nonumber
    %&= \frac{1}{p(\xVec)} \sum_{\hVec} \prob{\hVec}{\xVec}
\end{align}
%
See the supplement for a detailed derivation.
Note that this is a biased estimator because it implicitly contains a division by the estimated $p(x)$.
Furthermore, there is no guarantee that $q=\prob{\hVec}{\xVec}$ results in a minimum
variance estimate of this gradient. But both, the bias and the variance, decrease as the number of samples 
is increased. Also note that the wake-sleep algorithm uses a gradient that is equivalent to using only $K=1$ sample.
Another noteworthy detail about eq. \eqref{eq:grad_p} is that the importance weights $\tilde{\omega}$ are automatically 
normalized such that they sum up to one.

%Equation \eqref{eq:grad_p} is a biased but consistent (asymptotically unbiased)
%.estimator of the gradient.

{\bf Updating $q_{\phi}$ for given $p_\theta$}:
%
In order to minimize the variance of the estimator \eqref{eq:pxest} we would like
$\qrob{\hVec}{\xVec}$ to track $\prob{\hVec}{\xVec}$. We propose to train $q$ using maximum 
likelihood learning with the loss $\LL_q(\phi, \xVec, \hVec) = \log q_\phi(\xVec|\hVec)$.
There are at least two reasonable options how to obtain training data for $\LL_q$: % $(\xVec, \hVec)$ pairs:
1) maximize $\LL_q$ under the empirical training distribution $\xVec \sim {\cal D}$, $\hVec \sim \prob{\hVec}{\xVec}$, or
2) maximize $\LL_q$ under the generative model $(\xVec, \hVec) \sim p_{\theta}(\xVec, \hVec)$.
We will refer to the former as {\em wake phase q-update} and to the latter as {\em sleep phase q-update}.
In the case of a DBN (where the top layer is generated by an RBM), there is an intermediate solution called 
contrastive-wake-sleep, which has been proposed in~\citep{Hinton06}. 
In contrastive wake-sleep we sample $\xVec$ from the training distribution, propagate it stochastically
into the top layer and use that $\hVec$ as starting point for a short Markov chain in the RBM, then 
sample the other layers in the generative network $p$ to generate the rest of $(\xVec,\hVec)$. The
objective is to put the inference network's capacity where it matters most, i.e., near the input configurations
that are seen in the training set. 

Analogous to eqn. \eqref{eq:pxest} and \eqref{eq:grad_p} we use importance sampling
to derive gradients for the wake phase q-update: 
%
\begin{align}
  \frac{\partial}{\partial \phi} \LL_q(\phi, \xVec \sim \DD) 
%    &= \frac{\partial}{\partial \phi} \sum_{\hVec} 
%        p(\xVec, \hVec) {\log q_{\phi}(\hVec | \xVec)}  \nonumber \\
%    &= \frac{1}{p(\xVec)} \E{\hVec \sim \qrob{\hVec}{\xVec}}{\frac{p(\xVec, \hVec)}{\qrob{\hVec}{\xVec}}
%            \frac{\partial}{\partial \phi} \log q_{\phi}(\hVec | \xVec)}  \nonumber \\
    \label{eq:grad_q}
%    &\simeq \frac{1}{\sum_k \omega_k} \sum_{k=1}^K \omega_k
%            \frac{\partial}{\partial \phi} \log q_{\phi}(\hVec^{(k)} | \xVec)
    &\simeq \sum_{k=1}^K \tilde{\omega}_k \frac{\partial}{\partial \phi} \log q_{\phi}(\hVec^{(k)} | \xVec)
%\text{with}\;&\omega_k = \frac{p(\xVec, \hVec^{(k)})}{\qrob{\hVec^{(k)}}{\xVec}} \;
%\text{and}\; \hVec^{(k)} \sim \qrob{\hVec}{\xVec} \nonumber
%    &= \cdots \\
%    &= \E{\xVec \sim \DD}{ \E{\hVec \sim \qrob{\hVec}{\xVec)}} 
%     { \omega \; \frac{\partial}{\partial \phi} \log \qrob{\hVec}{\xVec}} }
%     \; \text{with the importance weights } \omega = \frac{\prob{\hVec}{\xVec}}{\qrob{\hVec}{\xVec}}
\end{align}
%
with the same importance weights $\tilde{\omega}_k$ as in \eqref{eq:grad_p} 
(the details of the derivation can again be found in the supplement).
Note that this is equivalent to optimizing $q$ so as to minimize $KL(p(\cdot|\xVec)
\parallel q(\cdot|\xVec))$. 
%
For the sleep phase q-update we consider the model distribution $p(\xVec, \hVec)$ a fully observed system 
and can thus derive gradients without further sampling:
%
\begin{align}
  \frac{\partial}{\partial \phi} \LL_q(\phi, (\xVec, \hVec) ) 
%  &= \frac{\partial}{\partial \phi} {\log q_{\phi}(\hVec | \xVec)}  \\
%  &= \E{\xVec, \hVec \sim p(\xVec, \hVec)}{
%     \frac{\partial}{\partial \phi} {\log \qrob{\hVec}{\xVec} } } \\
%  &\simeq 
    \label{eq:grad_q_sleep}
  & = \frac{\partial}{\partial \phi} \log q_{\phi}(\hVec | \xVec) 
    \text{ with } \xVec, \hVec \sim p(\xVec, \hVec)
\end{align}
%
This update is equivalent to the sleep phase update in the classical wake-sleep algorithm.

\begin{algorithm}[ht]
\caption{Reweighted Wake-Sleep training procedure and likelihood estimator.
$K$ is the number of approximate inference samples and controls the trade-off
between computation and accuracy of the estimators (both for the gradient
and for the likelihood). We typically use a large value ($K=100,000$) for test set likelihood
estimator but a small value ($K=5$) for estimating gradients. Both the
wake phase and sleep phase update rules for $q$ are optionally included (either one or both can be used,
and best results were obtained using both).
The original wake-sleep algorithm has $K$=1 and only uses the sleep phase update of $q$. 
To estimate the log-likelihood at test time, only the computations up to $\widehat{\LL}$ are required.
% to estimate the log-likelihood, and a larger value of $K$ (500 in the experiments) is preferable to obtain a more accurate estimator.
}
\begin{algorithmic}
\label{alg:RWS}
\FOR{number of training iterations}
  \STATE{$\bullet$ Sample example(s) $\xVec$ from the training distribution}
  \FOR{$k=1$ to $K$}
    \STATE{$\bullet$ Layerwise sample latent variables $\hVec^{(k)}$ from $q(\hVec|\xVec)$}
     %(first layer above $\xVec$, second layer, etc. up to top hidden layer).}
    \STATE{$\bullet$ Compute $q(\hVec^{(k)}|\xVec)$ and $p(\xVec,\hVec^{(k)})$}
  \ENDFOR
  \STATE{$\bullet$ Compute unnormalized weights $\omega_k = \frac{p(\xVec, \hVec^{(k)})}{\qrob{\hVec^{(k)}}{\xVec}}$}
  \STATE{$\bullet$ Normalize the weights $\tilde{\omega}_k = \frac{\omega_k}{\sum_{k'} \omega_{k'}}$}
  \STATE{$\bullet$ Compute unbiased likelihood estimator $\hat{p}(\xVec) = {\rm average}_k \; \omega_k$}
  \STATE{$\bullet$ Compute log-likelihood estimator $\widehat{\LL}(\xVec) = \log {\rm average}_k \; \omega_k$}
  %\STATE{$\bullet$ {\bf Wake-phase update of $p$}. Compute asymptotically unbiased estimator of log-likelihood gradient w.r.t. $p$, 
  %$\sum_k \tilde{\omega}_k \frac{\partial \log p(\xVec,\hVec^{(k)})}{\partial \theta}$,
  %and perform an update of $p$'s parameters using it}
  \STATE{$\bullet$ {\bf Wake-phase update of $p$}: Use gradient estimator $\sum_k \tilde{\omega}_k \frac{\partial \log p(\xVec,\hVec^{(k)})}{\partial \theta}$}
  \STATE{$\bullet$ Optionally, {\bf wake phase update of $q$}: Use gradient estimator $\sum_k \tilde{\omega}_k \frac{\partial \log q(\hVec^{(k)}|\xVec)}{\partial \phi}$}
  \STATE{$\bullet$ Optionally, {\bf sleep phase update of $q$}: Sample $(\xVec',\hVec')$ from $p$ and use gradient $\frac{\partial \log q(\hVec'|\xVec')}{\partial \phi}$}
\ENDFOR
\end{algorithmic}
\end{algorithm}



\subsection{Relation to Wake-Sleep and Variational Bayes}
\label{sec:var-AE}

Recently, there has been a resurgence of interest in algorithms related to the Helmholtz machine 
and to the wake-sleep algorithm for directed graphical models containing either continuous or discrete
latent variables:

In Neural Variational Inference and Learning \citep[NVIL, ][]{Andriy-Karol-2014} the
authors propose to maximize the variational lower bound on the
log-likelihood to get a joint objective for both $p$ and $q$. It
was known that this approach results in a gradient estimate of very high variance
for the recognition network $q$~\citep{Dayan1996varieties}. In the NVIL paper, the authors
therefore propose variance reduction techniques such as {\em baselines}
to obtain a practical algorithm that enhances significantly over the original
wake-sleep algorithm. 
In respect to the computational complexity we note that while we draw $K$ samples from 
the inference network for RWS, NVIL on the other hand draws only a single sample from $q$ but 
maintains, queries and trains an additional auxiliary baseline estimating network. With RWS and a typical value of $K=5$ we thus require at least 
twice as many arithmetic operations, but we do not have to store the baseline network and 
do not have to find suitable hyperparameters for it.


%Compared to NVIL, RWS requires $K$-times as much computations the computations to 
%calculate gradients for $p$ and $q$, but we do not maintain and train an auxiliary
%baseline estimating network. For $K=5$ we thus expect to require roughly twice the 
%computational complexity than NVIL.
% XXX

Recent examples for continuous latent variables include
the auto-encoding variational Bayes~\citep{Kingma+Welling-ICLR2014} and
stochastic backpropagation papers~\citep{Rezende-et-al-ICML2014}. In both
cases one maximizes a variational lower bound on the log-likelihood that is
rewritten as two terms: one that is log-likelihood reconstruction
error through a stochastic encoder (approximate inference) - decoder
(generative model) pair, and one that regularizes the output of the
approximate inference stochastic encoder so that its marginal distribution
matches the generative prior on the latent variables (and the latter is
also trained, to match the marginal of the encoder output).
%
%One of the contributions of this paper is that we find wake-sleep to be a
%special case of reweighted wake-sleep, with a single sample, $K$=1. This makes
%the regular wake-sleep update a clearly biased estimator of the likelihood
%gradient. It also suggests that reweighted wake-sleep can only improve with
%respect to wake-sleep, since both bias and variance must decrease as we
%consider more samples. Experiments described below confirm that hypothesis.
%
Besides the fact that these variational auto-encoders are only for continuous
latent variables, another difference with the reweighted wake-sleep
algorithm proposed here is that in the former, a single sample from the approximate inference
distribution is sufficient to get an unbiased estimator of the gradient
of a proxy (the variational bound). Instead, with the reweighted wake-sleep,
a single sample would correspond to regular wake-sleep, which gives a biased
estimator of the likelihood gradient. On the other hand, as the number of
samples increases, reweighted wake-sleep provides a less biased (asymptotically
unbiased) estimator of the log-likelihood and of its gradient.
Similar in spirit, but aimed at a structured output prediction task is the
method proposed by \cite{Tang+Salakhutdinov-NIPS2013}. The authors optimize the
variational bound of the log-likelihood instead of the direct IS estimate but
they also derive update equations for the proposal distribution that resembles
many of the properties also found in reweighted wake-sleep.


\section{Component Layers}

Although the framework can be readily applied to continuous variables, we
here restrict ourselves to distributions over binary visible and binary
latent variables.  We build our models by combining probabilistic components,
each one associated with one of the layers of the generative network or of the
inference network.  The generative model can therefore be written as 
$p_{\theta}(\xVec, \hVec) = p_0(\xVec|\,\hVec_1) \, p_{1}(\hVec_{1}|\,\hVec_2) \, \cdots \, p_L(\hVec_L)$, 
while the inference network has the form 
$q_\phi(\hVec\,|\,\xVec) = q_1(\hVec_1\,|\,\xVec) \cdots q_L(\hVec_L\,|\,\hVec_{L-1})$.
For a distribution $P$ to be a suitable component we must have a method to efficiently 
compute $P(\xVec^{(k)}|\,\yVec^{(k)})$ given $(\xVec^{(k)}$ , $\yVec^{(k)})$, 
and we must have a method to efficiently draw i.i.d. samples 
$\xVec^{(k)} \sim P(\xVec\,|\,\yVec)$ for a given $\yVec$. 
%
In the following we will describe experiments containing three kinds of layers:

\label{sec:SBN}
{\bf Sigmoidal Belief Network (SBN) layer:} A SBN layer~\citep{Saul+96} is a directed 
graphical model with independent variables $x_i$ given the parents $\yVec$:
%
\begin{align}
  P^{\text{SBN}}(x_i=1\,|\,\yVec) = \sigma( W^{i,:} \, \yVec + b_i )
\end{align}
%
Although a SBN is a very simple generative model given $\yVec$, 
performing inference for $\yVec$ given $\xVec$ is in general intractable.

{\bf Autoregressive SBN layer (AR-SBN, DARN):}
%
If we consider $x_i$ an ordered set of observed variables and introduce 
directed, autoregressive links between all previous $\xVec_{<i}$ and a given $x_i$, 
we obtain a fully-visible sigmoid belief network 
\cite[FVSBN, ][]{Frey98,Bengio+Bengio-NIPS99}. When we additionally condition a FVSBN 
on the parent layer's $\yVec$ we obtain a layer model that was first used
in Deep AutoRegressive Networks \citep[DARN, ][]{KarolAndriyDaan14}:
%is similar to a SBN layer but with the output units $x_i$
%not being independent of each other, given the layer's input $\yVec$. Instead, their dependency
%is captured by a fully connected directed acyclic graph where the $x_i$
%
\begin{align}
  P^{\text{AR-SBN}}(x_i=1\,|\,\xVec_{<i}, \yVec) = \sigma( W^{i,:} \, \yVec + S^{i,<i} \xVec_{<i} + b_i)
\end{align}
%
We use $\xVec_{<i} = (x_1, x_2, \cdots, x_{i-1})$ to refer to the vector
containing the first $i$-1 observed variables. The matrix $S$ is a lower
triangular matrix that contains the autoregressive weights between the 
variables $x_i$, and with $S^{i,<j}$ we refer to the first $j$-1 elements of the $i$-th row
of this matrix. 
In contrast to a regular SBN layer, the units $x_i$ are thus not independent of each other but 
can be predicted like in a logistic regression in terms of its predecessors
$\xVec_{<i}$ and of the input of the layer, $\yVec$.

{\bf Conditional NADE layer:}
%
The Neural Autoregressive Distribution Estimator \citep[NADE, ][]{Larochelle+Murray-2011} is a model
that uses an internal, accumulating hidden layer to predict variables $x_i$ given the vector containing all previously variables $\xVec_{<i}$.
Instead of logistic regression in a FVSBN or an AR-SBN, the dependency between the variables $x_i$ 
is here mediated by an MLP~\citep{Bengio+Bengio-NIPS99}:
%DARN is thus a special case of NADE without a (deterministic) hidden layer to mediate the
%conditional dependency between $x_i$ and its predecessors. Instead of a logistic
%regression, that dependency is mediated 
%
\begin{align}
  P(x_i=1\,|\,\xVec_{<i}) = \sigma( V^{i,:} \sigma( W^{:,<i} \, \xVec_{<i} + \aVec ) + b_i))
\end{align}
%
With $W$ and $V$ denoting the encoding and decoding matrices for the NADE hidden layer.
For our purposes we condition this model on the random variables $\yVec$:
%
\begin{align}
  P^{\text{NADE}}(x_i=1\,|\,\xVec_{<i}, \yVec) 
    = \sigma( V^{i,:} \sigma( W^{:,<i} \, \xVec_{<i} + U_a \, \yVec + \aVec ) + U_b^{i,:} \, \yVec + b_i))
\end{align}
%
Such a conditional NADE has been used previously for modeling musical
sequences~\citep{Boulanger+al-ICML2012-small}.

For each layer distribution we can construct an unconditioned distribution by
removing the conditioning variable $\yVec$. We use such
unconditioned distributions as top layer $p(\hVec)$ for the generative network $p$.
%By removing $\yVec$ from a SBN layer we obtain a factorized Bernoulli
%distribution, by removing $\yVec$ from a AR-SBN layer we obtain a fully-visible 
%sigmoid belief network, and by removing $\yVec$ from a conditional NADE layer we obtain a regular, unconditioned NADE. 

%\subsubsection{parameterizations}

%Given the formula (\ref{learn_p}) and (\ref{learn_q}), there are plenty of options 
%in parameterizing $p_{\theta}(h^i_1,h^i_2,x)$ and $q_{\phi}(h^i_1,h^i_2|x)$. 
%
%For $p_{\theta}$, Sigmoid Belief Networks \citep{Saul+96} is most straightforward. 
%Apart from using the simple Sigmoid Belief Networks,  
%The other choices for those conditionals are conditional NADEs. For the prior $p(h_2)$ 
%there are also many possibilities, such as NADEs, mixture models. 

%For $q_{\phi}$, if we assume it further factorizes as 
%$q_{\phi}(x,h_1,h_2)=q_{\alpha}(h_1|x)q_{\beta}(h_2|h_1)$, then all sorts of 
%parameterizations, such as, conditional NADE, are possible.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments}

Here we present a series of experiments on the MNIST and the CalTech-Silhouettes
datasets. The supplement describes additional experiments on 
smaller datasets from the UCI repository. With these experiments we 
1) quantitatively analyze the influence of the number of samples $K$, 2)
demonstrate that using a more powerful layer-model for the inference network
$q$ can significantly enhance the results even when the generative model is a
factorial SBN, and 3) show that we approach state-of-the-art performance when using 
either relatively deep models or when using powerful layer models such as a conditional NADE. 
Our implementation is available at \url{https://github.com/jbornschein/reweighted-ws/}.

\vspace{-0.2cm}
\subsection{MNIST}
\vspace{-0.2cm}

\begin{figure}[t]
  \centering
  %\def\svgwidth{0.7\textwidth}
  %\input{figures/mnist1.pdf_tex}
  \includegraphics[width=\linewidth]{mnist1.pdf}
  \vspace{-0.5cm}
  \caption{
    \label{fig:mnist1}
    {\bf A} Final log-likelihood estimate w.r.t. number of samples used during training.
    {\bf B} $L_2$-norm of the bias and standard deviation of the low-sample estimated 
     $p_\theta$ gradient relative to a high-sample (K=5,000) based estimate.
  }
\end{figure}

\begin{table}
\centering
\small
\begin{tabular}{|l|r||c|c||c|c|}
  \hline 
  \small
          &             & NVIL    & wake-sleep    &          RWS &           RWS \\
  P-model & size        &         &               & Q-model: SBN & Q-model: NADE \\

  \hline 
  SBN    & 200          & (113.1) & 116.3 (120.7) & 103.1 &  95.0 \\
  SBN    & 200-200      &  (99.8) & 106.9 (109.4) &  93.4 &  91.1 \\
  SBN    & 200-200-200  &  (96.7) & 101.3 (104.4) &  90.1 &  88.9 \\
  \hline
  AR-SBN & 200          &       &                 &        &  89.2 \\
  AR-SBN & 200-200      &       &                 &        &  92.8 \\
  \hline
  NADE   & 200          &       &                 &        &  86.8 \\
  NADE   & 200-200      &       &                 &        &  87.6 \\
  \hline 
\end{tabular}
\caption{
MNIST results for various architectures and training methods.  In the 3rd
column we cite the numbers reported by \cite{Andriy-Karol-2014}.  Values in
brackets are variational NLL bounds, values without brackets report NLL estimates
(see section \ref{sec:pxest}). 
}
\label{tab:mnist}
\end{table}

%\begin{table}
%\centering
%\small
%\begin{tabular}{|l|r||c|c||c|c|c|}
%  \hline 
%  \small
%          &      & NVIL         & wake-sleep  & wake-sleep &          RWS &           RWS \\
%  P-model & size & (NLL bound)  & (NLL bound) & (NLL est.) & Q-model: SBN & Q-model: NADE \\
%
%  \hline 
%  SBN    & 200          & 113.1 & 120.7 & 116.3 & 105.7 &  96.7 \\
%  SBN    & 200-200      &  99.8 & 109.4 & 106.9 &  98.3 &  92.3 \\
%  SBN    & 200-200-200  &  96.7 & 104.4 & 101.3 &  94.7 &  91.8 \\
%%  SBN    & 10-200-200   &       &       &  97.8 &  91.9 &  88.9 \\
%  \hline
%  AR-SBN & 200          &       &        &        &        &  91.8 \\
%  AR-SBN & 200-200      &       &        &        &        &  95.2 \\
%  \hline
%  NADE   & 200          &       &        &        &        &  86.8 \\
%  NADE   & 200-200      &       &        &        &        &  87.6 \\
%  \hline 
%\end{tabular}
%\caption{
%MNIST for different architectures and network depths. In the third column 
%we cite the numbers reported by \cite{Andriy-Karol-2014}. Columns three and
%four report the variational NLL bounds; columns 5 to 8 report the NLL 
%estimates (see section \ref{sec:pxest}).
%}
%\label{tab:mnist}
%\end{table}

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{mnist2.pdf}
  \vspace{-0.5cm}
  \caption{
    \label{fig:mnist2}
    {\bf A} Final log-likelihood estimate w.r.t. number of test samples used. % Solid lines: estimated using the importance sampler; dashed-lines: variational {\em bound} estimator.
    {\bf B} Samples from the SBN/SBN 10-200-200 generative model.  
    {\bf C} Samples from the NADE/NADE 250 generative model. 
    (We show the probabilities from which each pixel is sampled)
  }
\end{figure}

\begin{table}[t]
\centering
\small
\begin{tabular}{|l|c|c|}
  \hline 
  \multicolumn{3}{|c|}{\bf Results on binarized MNIST} \\
  \hline 
  \hline 
         & NLL   & NLL \\
  Method & bound & est. \\
  \hline 
  RWS (SBN/SBN 10-100-200-300-400)   &       & 85.48 \\
  RWS (NADE/NADE 250)                &       & 85.23 \\
  RWS (AR-SBN/SBN 500)$\dagger$      &       & 84.18 \\
%  RWS (SBN/NADE 10-200-200)          &       & 88.47 \\
%                                     &       & \footnotesize $\pm$ 0.43 \\
  \hline 
  NADE (500 units, [1])              &       & 88.35 \\
  EoNADE (2hl, 128 orderings, [2])   &       & 85.10 \\
  DARN (500 units, [3])              &       & 84.13 \\
  RBM (500 units, CD3, [4])          & 105.5 & \\
  RBM (500 units, CD25, [4])         & 86.34 & \\
  DBN (500-2000, [5])                & 86.22 & {\footnotesize 84.55} \\
  \hline 
\end{tabular}
\begin{tabular}{|l|c|c|}
  \hline 
  \multicolumn{2}{|c|}{\bf Results on CalTech 101 Silhouettes} \\
  \hline 
  \hline 
                                 & NLL  \\
  Method                         & est. \\
  \hline 
  RWS (SBN/SBN 10-50-100-300)    & 113.3 \\
%  RWS (SBN/NADE 10-50-100-300)   & 112.6 \\
  RWS (NADE/NADE 150)            & 104.3 \\
                                 & \\
  \hline 
  NADE (500 hidden units)        & 110.6 \\
  RBM (4000 hidden units, [6])   & 107.8 \\
                                 &  \\ 
                                 &  \\
                                 &  \\
                                 &  \\
%                                 &  \\
  \hline 
\end{tabular}
\caption{
Various RWS trained models in
relation to previously published methods: [1] \cite{Larochelle+Murray-2011}, [2]
\cite{Uria+al-ICML2014}, [3] \cite{KarolAndriyDaan14}  [4] \cite{SalMurray08}, [5] \cite{MurraySal09}, 
[6] \cite{Cho2013enhanced}. $\dagger$ Same model as the best performing in [3]: a AR-SBN with {\em deterministic}
hidden variables between the observed and latent. All RWS NLL estimates on MNIST have confidence intervals of $\approx \pm 0.40$.
%In row 4 we report 95\% confidence intervals.
%and to state of the art
%(bold). Left: Results on MNIST; Right: results on CalTech 101 Silhouettes. 
}
\label{tab:results}
\end{table}

We use the MNIST dataset that was binarized according to \cite{MurraySal09} and
downloaded in binarized form from~\citep{LarochelleBinarizedMNIST}. For
training we use stochastic gradient decent with momentum ($\beta$=0.95) and set
mini-batch size to 25. The experiments in this paragraph were run with learning
rates of \{0.0003, 0.001, and 0.003\}. From these three we always report the
experiment with the highest validation log-likelihood. In the majority of our
experiments a learning rate of 0.001 gave the best results, even across
different layer models (SBN, AR-SBN and NADE). If not noted otherwise, we use
$K=5$ samples during training and $K=100,000$ samples to estimate the final
log-likelihood on the test set\footnote{We refer to the lower bound estimates which
can be arbitrarily tightened by increasing the number of test samples as
{\em LL estimates} to distiguish them from the {\em variational LL
lower bounds} (see section \ref{sec:pxest}).}.
To disentangle the influence of the different q updating methods we setup $p$
and $q$ networks consisting of three hidden SBN layers with 10, 200
and 200 units (SBN/SBN 10-200-200). After convergence, the model trained
updating q during the sleep phase only reached a final estimated
log-likelihood of $-93.4$, the model trained with a q-update during the wake
phase reached $-92.8$, and the model trained with both wake and sleep phase
update reached $-91.9$. As a control we trained a model that does not update
$q$ at all. This model reached $-171.4$. We confirmed that combining wake
and sleep phase q-updates generally gives the best results by repeating this
experiment with various other architectures. For the remainder of this paper we
therefore train all models with combined wake and sleep phase q-updates.

Next we investigate the influence of the number of samples used during
training. The results are visualized in Fig. \ref{fig:mnist1} A. Although the
results depend on the layer-distributions and on the depth and width of the
architectures, we generally observe that the final estimated log-likelihood
does not improve significantly when using more than 5 samples during training 
for NADE models, and using more than 25 samples for models with SBN layers.
We can quantify the bias and the variance of the gradient estimator
(\ref{eq:grad_p}) using bootstrapping. While training a SBN/SBN 10-200-200 model
with $K=100$ training samples, we use $K=5,000$ samples to get a high quality
estimate of the gradient for a small but fixed set of 25 datapoints (the size
of one mini-batch). By repeatedly resampling smaller sets of $\{1, 2, 5, \cdots,
5000\}$ samples with replacement and by computing the gradient based on these, we get a measure for
the bias and the variance of the small sample estimates relative the high
quality estimate. These results are visualized in Fig. \ref{fig:mnist1} B.
In Fig. \ref{fig:mnist2} A we finally investigate the quality of the 
log-likelihood estimator (eqn. \ref{eq:pxest}) when applied to the
MNIST test set.

Table \ref{tab:mnist} summarizes how different architectures compare to each
other and how RWS compares to related methods for training directed models. 
We essentially observe that RWS trained models consistently improve over 
classical wake-sleep, especially for deep architectures. We 
furthermore observe that using autoregressive layers (AR-SBN or NADE) for the inference network 
improves the results even when the generative model is composed of factorial SBN layers.
Finally, we see that the best performing models with autoregressive layers in $p$
are always shallow with only a single hidden layer.
In Table \ref{tab:results} (left) we compare some of our best models to
the state-of-the-art results published on MNIST. The deep SBN/SBN
10-100-200-300-400 model was trained for 1000 epochs with $K=5$ training
samples and a learning rate of $0.001$. For fine-tuning we run additional 500 epochs
with a learning rate decay of $1.005$ and 100 training samples.
For comparison we also train the best performing model from the
DARN paper \citep{KarolAndriyDaan14} with RWS, i.e., a single layer AR-SBN with
500 latent variables and a {\em deterministic} layer of hidden variables
between the observed and the latents. We essentially obtain the same final 
testset log-likelihood. For this shallow network we thus do not observe 
any improvement from using RWS.

%training samples
%and additional our model with the largest log-likelihood reaches
%$85.32$ and is a shallow model composed of (conditional) NADEs with 250
%hidden units. This model was trained using 50 epochs with a learning rate of $0.003$
%and $K$=5 samples and another $50$ epochs with a learning rate of $0.001$ and
%$K$=25 training samples. 

\vspace{-0.2cm}
\subsection{CalTech 101 Silhouettes}
\vspace{-0.2cm}

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{caltech1.png}
  \vspace{-0.5cm}
  \caption{
    CalTech 101 Silhouettes:
    {\bf A} Random selection of training data points.
    {\bf B} Random samples from the SBN/SBN 10-50-100-300 generative network.
    {\bf C} Random Samples from the NADE-150 generative network.
    (We show the probabilities from which each pixel is sampled)
  }
  \label{fig:caltech}
\end{figure}

We applied reweighted wake-sleep to the $28 \times 28$ pixel CalTech 101
Silhouettes dataset. This dataset consists of 4,100 examples in the training
set, 2,264 examples in the validation set and 2,307 examples in the test set.  We
trained various architectures on this dataset using the same hyperparameter as
for the MNIST experiments. Table \ref{tab:results} (right) summarizes our results.
Note that our best SBN/SBN model is a relatively deep network with 4 
hidden layers (300-100-50-10) and reaches a estimated LL of -116.9 on the test set.
Our best network, a shallow NADE/NADE-150 network reaches -104.3 and
improves over the previous state of the art ($-107.8$, a RBM with 4000 hidden 
units by~\cite{Cho2013enhanced}).

\vspace{-0.2cm}
\section{Conclusions}
\vspace{-0.2cm}

We introduced a novel training procedure for deep generative models consisting
of multiple layers of binary latent variables. It generalizes and improves over
the wake-sleep algorithm providing a lower bias and lower variance estimator of
the log-likelihood gradient at the price of more samples from the inference
network. During training the weighted samples from the inference network decouple 
the layers such that the learning gradients only propagate within the individual layers.
Our experiments demonstrate that a small number of $\approx 5$ samples 
is typically sufficient to jointly train relatively deep architectures of at least 5
hidden layers without layerwise pretraining and without carefully tuning
learning rates.  The resulting models produce reasonable samples (by visual
inspection) and they approach state-of-the-art performance in terms of
log-likelihood on several discrete datasets. 

We found that even in the cases when the generative networks contain SBN layers
only, better results can be obtained with inference networks composed of more
powerful, autoregressive layers. This however comes at the price of reduced
computational efficiency on e.g. GPUs as the individual variables $h_i \sim q(h|\xVec)$ have to
be sampled in sequence (even though the theoretical complexity is not
significantly worse compared to SBN layers).

We furthermore found that models with autoregressive layers in the generative
network $p$ typically produce very good results. But the best ones were always
shallow with only a single hidden layer. At this point it is unclear if this is
due to optimization problems. 

{
%\small
{\bf Acknowledgments} \;\;
%\vspace{-0.3cm}
%\subsubsection*{Acknowledgments} 
%\vspace{-0.3cm}
We would like to thank Laurent Dinh, Vincent Dumoulin and Li Yao for helpful discussions and the 
developers of Theano~\citep{bergstra+al:2010-scipy,Bastien-Theano-2012} for their powerful software.
We furthermore acknowledge CIFAR and Canada Research Chairs for funding and Compute Canada, and Calcul Qu\'ebec
for providing computational resources.
}

\bibliography{strings,strings-shorter,ml,aigaion,localref}
\bibliographystyle{natbib}
%\bibliographystyle{unsrt}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Supplement}

\subsection{Gradients for $p(\xVec, \hVec)$}
%
\begin{align}
  \label{eq:grad_p_detailed}
  \frac{\partial}{\partial \theta} \LL_p(\theta, \xVec \sim \DD) &=
       \frac{\partial}{\partial \theta} \log p_{\theta}(\xVec) 
%     = \frac{1}{p(\xVec)} \frac{\partial}{\partial \theta} p(\xVec) \\
     = \frac{1}{p(\xVec)} \frac{\partial}{\partial \theta} \sum_{\hVec} p(\xVec, \hVec) \nonumber \\
    &= \frac{1}{p(\xVec)} \sum_{\hVec} p(\xVec, \hVec) 
            \frac{\partial}{\partial \theta} \log  p(\xVec, \hVec) \nonumber \\
    &= \frac{1}{p(\xVec)} \sum_{\hVec} \qrob{\hVec}{\xVec} \frac{p(\xVec, \hVec)}{\qrob{\hVec}{\xVec}}
            \frac{\partial}{\partial \theta} \log  p(\xVec, \hVec) \nonumber \\
    &= \frac{1}{p(\xVec)} \E{\hVec \sim \qrob{\hVec}{\xVec}}{\frac{p(\xVec, \hVec)}{\qrob{\hVec}{\xVec}}
            \frac{\partial}{\partial \theta} \log p(\xVec, \hVec)} \nonumber \\
    &\simeq \frac{1}{\sum_k \omega_k} \sum_{k=1}^K \omega_k
            \frac{\partial}{\partial \theta} \log p(\xVec, \hVec^{(k)}) \\
\text{with}\;&\omega_k = \frac{p(\xVec, \hVec^{(k)})}{\qrob{\hVec^{(k)}}{\xVec}} \;
\text{and}\; \hVec^{(k)} \sim \qrob{\hVec}{\xVec} \nonumber
\end{align}


\subsection{Gradients for the wake phase q update}
%
\begin{align}
  \label{eq:grad_q_detailed}
  \frac{\partial}{\partial \phi} \LL_q(\phi, \xVec \sim \DD) 
    &= \frac{\partial}{\partial \phi} \sum_{\hVec} 
        p(\xVec, \hVec) {\log q_{\phi}(\hVec | \xVec)}  \nonumber \\
    &= \frac{1}{p(\xVec)} \E{\hVec \sim \qrob{\hVec}{\xVec}}{\frac{p(\xVec, \hVec)}{\qrob{\hVec}{\xVec}}
            \frac{\partial}{\partial \phi} \log q_{\phi}(\hVec | \xVec)}  \nonumber \\
    &\simeq \frac{1}{\sum_k \omega_k} \sum_{k=1}^K \omega_k
            \frac{\partial}{\partial \phi} \log q_{\phi}(\hVec^{(k)} | \xVec)
%\text{with}\;&\omega_k = \frac{p(\xVec, \hVec^{(k)})}{\qrob{\hVec^{(k)}}{\xVec}} \;
%\text{and}\; \hVec^{(k)} \sim \qrob{\hVec}{\xVec} \nonumber
%    &= \cdots \\
%    &= \E{\xVec \sim \DD}{ \E{\hVec \sim \qrob{\hVec}{\xVec)}} 
%     { \omega \; \frac{\partial}{\partial \phi} \log \qrob{\hVec}{\xVec}} }
%     \; \text{with the importance weights } \omega = \frac{\prob{\hVec}{\xVec}}{\qrob{\hVec}{\xVec}}
\end{align}
%
Note that we arrive at the same gradients when we set out to minimize
the $KL(p(\cdot|\xVec) \parallel q(\cdot|\xVec)$ for a given datapoint $\xVec$:
%
\begin{align}
  \frac{\partial}{\partial \phi} KL (p_{\theta}(\hVec | \xVec) \parallel q_{\phi}(\hVec | \xVec)) 
  &= \frac{\partial}{\partial \phi} 
     \sum_{\hVec} p_{\theta}(\hVec|x) \log \frac{p_{\theta}(\hVec|\xVec)}{q_{\phi}(\hVec|\xVec)} \nonumber \\
  &= - \sum_{\hVec} p_{\theta}(\hVec|\xVec) \frac{\partial}{\partial \phi} \log q_{\phi}(\hVec|\xVec) \nonumber \\
  &\simeq - \frac{1}{\sum_k \omega_k} \sum_{k=1}^K \omega_k
     \frac{\partial}{\partial \phi} \log q_{\phi}(\hVec^{(k)} | \xVec)
\end{align} 
\\

\newpage

\subsection{Additional experimental results}

\subsubsection{Learning curves for MNIST experiments}
\begin{figure}[h!]
  \centering
  \includegraphics[width=0.7\linewidth]{learning-curves.png}
  \vspace{-0.5cm}
  \caption{
    Learning curves for various MNIST experiments.
  }
\end{figure}

\subsubsection{Bootstrapping based $\log(p(x))$ bias/variance analysis}

Here we show the bias/variance analysis from Fig. 1 B (main paper) 
applied to the estimated $\log(p(x))$ w.r.t. the number of test samples. \\


\begin{figure}[h!]
  \centering
  \includegraphics[width=0.7\linewidth]{log-LL-bias.pdf}
  \vspace{-0.5cm}
  \caption{
    Bias and standard deviation of the low-sample estimated $\log(p(x))$
    (bootstrapping with K=5,000 primary samples from a SBN/SBN 10-200-200 
    network trained on MNIST).
  }
\end{figure}


\newpage

\subsubsection{UCI binary datasets}

We performed a series of experiments on 8 different binary datasets from the
UCI database:

For each dataset we screened a limited hyperparameter space: The learning rate
was set to a value in ${0.001, 0.003, 0.01}$. For SBNs we use $K$=10
training samples and we tried the following architectures: Two hidden layers
with 10-50, 10-75, 10-100, 10-150 or 10-200 hidden units and three hidden
layers with 5-20-100, 10-50-100, 10-50-150, 10-50-200 or 10-100-300 hidden
units. We trained NADE/NADE models with $K$=5 training samples and one hidden
layer with 30, 50, 75, 100 or 200 units in it. 
%
\begin{table}[h!]
\centering
\scriptsize
%\tiny
\begin{tabular}{|l|c|c|c|c|c|c|c|c|}
\hline
Model         & \tiny \bf ADULT & \tiny \bf CONNECT4 & \tiny \bf DNA & \tiny \bf MUSHROOMS & \tiny \bf NIPS-0-12 & \tiny \bf OCR-LETTERS & \tiny \bf RCV1 & \tiny \bf WEB \\
\hline
\hline
{\bf FVSBN}                & 13.17    & 12.39     & 83.64       & 10.27       & 276.88 & 39.30 & 49.84 & 29.35 \\
{\bf NADE$^{*}$}           & 13.19    & 11.99     & 84.81       &  9.81       & 273.08 & 27.22 & 46.66 & 28.39 \\
{\bf EoNADE$^{+}$}         & 13.19    & 12.58     & 82.31       &  9.68       & 272.38 & 27.31 & 46.12 & {\bf 27.87} \\
{\bf DARN$^{3}$}           & 13.19    & 11.91     & {\bf 81.04} &  {\bf 9.55} & 274.68 & 28.17 & 46.10 & 28.83 \\
\hline
{\bf RWS - SBN}            & 13.65    & 12.68     &  90.63      &  9.90       & 272.54    & 29.99      & 46.16     & 28.18 \\
{\tiny hidden units}       & 5-20-100 & 10-50-150 & 10-150      & 10-50-150   & 10-50-150 & 10-100-300 & 10-50-200 & 10-50-300 \\
%{\bf RWS - DARN}           &       &       &       &       &        &       &       & \\
\hline
{\bf RWS - NADE}           & {\bf 13.16} & {\bf 11.68} & 84.26 &  9.71 & {\bf 271.11} & {\bf 26.43} & {\bf 46.09} & 27.92 \\
{\tiny hidden units}       & 30          &  50         &  100  &   50  &  75    & 100   &    &  \\
\hline
\end{tabular}
\caption{
Results on various binary datasets from the UCI repository. The top two rows
quote the baseline results from Larochelle \& Murray (2011); the third row
shows the baseline results taken from Uria, Murray, Larochelle (2014).
(NADE$^{*}$: 500 hidden units; EoNADE$^{+}$: 1hl, 16 ord)
}
\label{tab:other}
\end{table}


\end{document}



\documentclass{article} % For LaTeX2e
%\usepackage{nips11submit_e,times}
\usepackage{times}
%\documentstyle[nips10submit_09,times,art10]{article} % For LaTeX 2.09

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}

% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure} 


% For citations
\usepackage{natbib}

\newif\ifenoughspace
\enoughspacefalse

\newif\ifEnergybasedversion
\Energybasedversionfalse

\title{Implicit Density Estimation by Local Moment Matching to
Sample from Auto-Encoders}

\author{
Yoshua Bengio, Guillaume Alain, and Salah Rifai\\
Department of Computer Science and Operations Research\\
University of Montreal\\
Montreal, H3C 3J7 \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\def\W{{\mathbf W}}
\def\b{{\mathbf b}}
\def\s{{\mathbf s}}
\newcommand{\diag}{\mathop{\mathrm{diag}}}
\newcommand{\sigm}{\mathop{\mathrm{sigmoid}}}
\newcommand{\trace}{\mathop{\mathrm{Tr}}}
\newcommand{\vect}{\mathop{\mathrm{vec}}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\eqdef}{\stackrel{def}{=}}
\newcommand{\N}{\mathbb{N}}

%\newcommand{\argmin}[0]{\operatorname*{arg\,min}}
\newcommand{\argmin}[1]{\operatornamewithlimits{arg\,min}_{#1}}
\newcommand{\argmax}[1]{\operatornamewithlimits{arg\,max}_{#1}}

\newcommand{\E}{\mathbb{E}}
\newcommand{\J}{\mathcal{J}}


%\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}


\maketitle
\vspace*{-2mm}
\begin{abstract}
  Recent work suggests that some auto-encoder variants do a good job of capturing the local manifold structure of the unknown data generating density.  This paper contributes to the mathematical understanding of this phenomenon and helps define better justified sampling algorithms for deep learning based on auto-encoder variants. We consider an MCMC where each step samples from a Gaussian whose mean and covariance matrix depend on the previous state, defines through its asymptotic distribution a target density. First, we show that good choices (in the sense of consistency) for these mean and covariance functions are the local expected value and local covariance under that target density. Then we show that an auto-encoder with a contractive penalty captures estimators of these local moments in its reconstruction function and its Jacobian. A contribution of this work is thus a novel alternative to maximum-likelihood density estimation, which we call local moment matching. It also justifies a recently proposed sampling algorithm for the Contractive Auto-Encoder and extends it to the Denoising Auto-Encoder.
\end{abstract}
\ifEnergybasedversion We also show how this training paradigm
  could in principle be applied to energy-based models in order to avoid
  computing the gradient of the partition function.  \fi


\vspace*{-2mm}
\section{Introduction}
\vspace*{-2mm}

Machine learning is about capturing aspects of the unknown distribution
from which the observed data are sampled (the {\em data-generating
distribution}). For many learning algorithms and in particular
in {\em manifold learning}, the focus is on identifying
the regions (sets of points) in the space of examples where
this distribution concentrates, i.e., which configurations of the
observed variables are plausible. 

Unsupervised {\em representation-learning} algorithms attempt to
characterize the data-generating distribution through the discovery of a
set of features or latent variables whose variations capture most of the
structure of the data-generating distribution.  In recent years, a number
of unsupervised feature learning algorithms have been proposed that are
based on minimizing some form of {\em reconstruction error}, such as
auto-encoder and sparse coding variants~\citep{Bengio-nips-2006-small,ranzato-07-small,
Jain-Seung-08-small,ranzato-08,VincentPLarochelleH2008-small,Koray-08-small,
Rifai+al-2011-small,Salah+al-2011-small,Dauphin-et-al-NIPS2011-small,gregor-nips-11-small}. 
An auto-encoder reconstructs the input through two stages, an encoder function $f$
(which outputs a learned representation $h=f(x)$ of an example $x$) and a decoder
function $g$, such that $g(f(x))\approx x$ for most $x$ sampled from the data-generating
distribution.
These feature learning algorithms
can be {\em stacked} to form deeper and more abstract representations. There
are arguments and much empirical evidence to suggest that when they are
well-trained, such {\em deep learning} algorithms~\citep{Hinton06,Bengio-2009,HonglakL2009,Salakhutdinov2009} can
perform better than their shallow counterparts, both in terms of learning features
for the purpose of classification tasks and for generating higher-quality 
samples.

Here we restrict ourselves to the case of continuous inputs
$x \in \R^d$ with the data-generating distribution being associated with an
unknown {\em target density} function, denoted $p$.
Manifold learning
algorithms assume that $p$ is concentrated in regions of lower 
dimension~\citep{Cayton-2005,Narayanan+Mitter-NIPS2010-short},
i.e., the training examples are by definition located very close to
these high-density manifolds.  In that context, the core objective of
manifold learning algorithms is to identify the local directions of variation,
such that small movement in input space along these directions stays on or near
the high-density manifold. 

Some important questions remain concerning many of these feature learning
algorithms. {\em What is their training criterion learning about the input
  density}? Do these algorithms implicitly learn about the whole density or
only some aspect? If they capture the essence of the target
density, then can we formalize that link and in particular exploit it
to {\em sample from the model}? This would turn these algorithms into
{\em implicit density} models, which only define a density indirectly, e.g., through
a generative procedure that converges to it.  These are the questions to which this
paper contributes.

A crucial starting point for this work is very recent work~\citep{Rifai-icml2012}
proposing a {\em sampling algorithm for Contractive Auto-Encoders}, detailed
in the next section. This algorithm was motivated on geometrical grounds,
based on the observation and intuition that the leading singular vectors
of the Jacobian of the encoder function specify those main directions
of variation (i.e., the tangent plane of the manifold,
the local directions that preserve the high-probability nature of training examples).
Here we make a formal link between the target density and models minimizing
reconstruction error through a contractive mapping, such as the
Contractive Auto-Encoder~\citep{Rifai+al-2011-small} and the Denoising
Auto-Encoder~\citep{VincentPLarochelleH2008-small}. This allows us to
justify sampling algorithms similar to
that proposed by~\citep{Rifai-icml2012}, and apply these ideas to
Denoising Auto-Encoders as well. 

We define a novel alternative to maximum
likelihood training, {\em local moment matching}, which
we find that Contractive and Denoising Auto-Encoders perform. This is achieved
by optimizing a criterion (such as a regularized reconstruction error) such that
the optimal learned reconstruction function (and its derivatives) provide estimators
of the local moments (and local derivatives) of the target density.
These local moments can be used to define an implicit density,
the asymptotic distribution of a particular Markov chain, which can also
be seen as corresponding to an {\em uncountable Gaussian mixture},
with one Gaussian component at each possible location in input space.
%
\ifEnergybasedversion 
As we show, the resulting sampling method could also
be applied to energy-based models, whose density is defined up to a
constant (the partition function), provided that the first and second
derivative of that density function (with respect to the input) can be
conveniently computed.  
\fi

The main novel contributions of this paper are the following.  First, we
show in Section~\ref{sec:CAE-DAE} 
that the Denoising Auto-Encoder with small Gaussian perturbations and
squared error loss is actually a Contractive Auto-Encoder whose contraction
penalty is the magnitude of the perturbation, making the theory developed
here applicable to both, and in particular extending the sampling procedure
used for the Contractive Auto-Encoder to the Denoising
Auto-Encoder as well. Second, we present in Section~\ref{sec:consistency} 
consistency arguments justifying the use
of the first and second estimated local moments to sample from a chain.
Such a sampling algorithm has successfully been used in~\citet{Rifai-icml2012} to sample from a Contractive
Auto-Encoder. With small enough steps, we show that the asymptotic
distribution of the chain has the same similar (smoothed) first and second
local moments as those estimated. Third, we show in Section~\ref{sec:min-rec} that non-parametrically minimizing
reconstruction error with a contractive regularizer yields a reconstruction
function whose value and Jacobian matrix estimate respectively the first
and second local moments, i.e., up to a scaling factor, are the right
functions to use in the Markov chain.  Finally, although the sampling
algorithm was already empirically verified in~\citet{Rifai-icml2012}, we
include in Section~\ref{sec:exp} an experimental validation for the case 
when the model is trained with the denoising criterion.

\vspace*{-2mm}
\section{Contractive and Denoising Auto-Encoders}
\label{sec:CAE-DAE}
\vspace*{-2mm}

The Contractive Auto-Encoder or CAE~\citep{Rifai+al-2011-small} 
is trained to minimize the following regularized
reconstruction error:
\begin{equation}
  {\cal L}_{CAE} = \E\left[ \ell(x,r(x))  + \alpha \left\|\frac{\partial f(x)}{\partial x}\right\|^2_F \right]
\label{eq:cae-crit}
\end{equation}
where $r(x)=g(f(x))$ and $||A||^2_F$ is the sum of the squares of the elements of $A$.
Both the squared loss $\ell(x,r)=\frac{1}{2}||x-r||^2$ and the
cross-entropy loss $\ell(x,r)=-x\log r -(1-x)\log(1-r)$ have been
used, but here we focus our analysis on the squared loss because
of the easier mathematical treatment it allows. Note that success of 
minimizing the above criterion
strongly depends on the parametrization of $f$ and $g$ and in particular
on the tied weights constraint used, with $f(x)=\sigm(W x + b)$
and $g(h)=\sigm(W^T h + c)$. The above regularizing term forces $f$ (as well
as $g$, because of the tied weights) to be contractive, i.e.,
to have singular values less than 1~\footnote{Note that an auto-encoder
without any regularization would tend to find many leading singular values near 1 in
order to minimize reconstruction error, i.e., preserve input norm
in all the directions of variation present in the data.}. Larger values of $\alpha$
yielding more contraction (smaller singular values) where it hurts reconstruction
error the least, i.e., in the local directions where there are only little or no
variations in the data.

The Denoising Auto-Encoder or DAE~\citep{VincentPLarochelleH2008-small}
is trained to minimize the following denoising criterion:
\begin{equation}
 {\cal L}_{DAE} = \E\left[ \ell(x,r(N(x))) \right]
\label{eq:dae-crit}
\end{equation}
where $N(x)$ is a stochastic corruption of $x$ and the expectation is over
the training distribution. Here we consider mostly
the squared loss and Gaussian noise corruption, again because it is easier to handle
them mathematically. In particular, if $N(x)=x+\epsilon$ with
$\epsilon$ a small zero-mean isotropic Gaussian noise vector of variance $\sigma^2$,
then a Taylor expansion around $x$ gives 
$r(x+\epsilon)\approx r(x)+\frac{\partial r(x)}{\partial x}\epsilon$, which
when plugged into ${\cal L}_{DAE}$ gives
\begin{eqnarray}
 {\cal L}_{DAE} &\approx& \E\left[ \frac{1}{2}\left(x-\left(r(x)+\frac{\partial r(x)}{\partial x}\epsilon\right)\right)^T
                                       \left(x-\left(r(x)+\frac{\partial r(x)}{\partial x}\epsilon\right)\right) \right] \nonumber \\
       &=& \frac{1}{2}\left(\E\left[\|x-r(x)\|^2\right] 
         - 2 E[\epsilon]^T \E\left[\frac{\partial r(x)}{\partial x}^T (x-r(x))\right] 
         + Tr\left(\E\left[\epsilon \epsilon^T\right]
              \E\left[\frac{\partial r(x)}{\partial x}^T\frac{\partial r(x)}{\partial x}\right]\right)\right) \nonumber \\
       &=& \frac{1}{2}\left(\E\left[\|x-r(x)\|^2\right] + \sigma^2 \E\left[\left\|\frac{\partial r(x)}{\partial x}\right\|^2_F\right]\right)
\label{eq:dae-crit-approx}
\end{eqnarray}
where in the second line we used the independance of the noise from $x$
and properties of the trace, while in the last line we used
 $\E\left[\epsilon \epsilon^T\right]=\sigma^2 I$
and $\E[\epsilon]=0$ by definition of $\epsilon$.
This derivation shows that {\em the DAE is also a Contractive Auto-Encoder} but
where the contraction is imposed explicitly on the whole reconstruction function
$r(\cdot)=g(f(\cdot))$ rather than on $f(\cdot)$ alone (and $g(\cdot)$ as a side
effect of the parametrization).


\vspace*{-2mm}
\subsection{A CAE Sampling Algorithm}
\label{sec:cae-sampling}
\vspace*{-2mm}

Consider the following coupled Markov chains with elements $M_t$ and $X_t$ respectively:
\begin{eqnarray}
 M_{t+1} &=& \mu(X_t) \nonumber \\
 X_{t+1} &=& M_{t+1} + Z_{t+1}.
\label{eq:2-chain}
\end{eqnarray}
where $M_t,X_t,\mu(X_t) \in \R^d$ and
$Z_{t+1}$ is a sample from a zero-mean Gaussian with covariance $\Sigma(X_t)$.

The basic algorithm for sampling from the CAE, proposed
in~\citet{Rifai-icml2012}, is based on the above Markov chain {\em
  operating in the space of hidden representation} $h=f(x)$, with
$\mu(h)=f(g(h))$ and $Z_{t+1}=\left(\frac{\partial f(x_t)}{\partial x_t}\right)
\left(\frac{\partial f(x_t)}{\partial x_t}\right)^T \varepsilon$, where $\varepsilon$ is
zero-mean isotropic Gaussian noise vector in $h$-space. This defines a chain of hidden
representations $h_t$, and the corresponding chain of input-space
samples is given by $x_t=g(h_t)$. Slightly better results are
obtained with this $h$-space than with the corresponding $x$-space
chain which defines 
$\mu(x)=g(f(x))$ and $Z_{t+1}=\left(\frac{\partial f(x_t)}{\partial x_t}\right)^T
\left(\frac{\partial f(x_t)}{\partial x_t}\right) \varepsilon$ where $\varepsilon$ is
zero-mean isotropic Gaussian noise in $x$-space. We conjecture
that this advantage stems from the fact that moves in $h$-space
are done in a more abstract, more non-linear space. 
\iffalse
Note also
that even though $g(f(\cdot))$ is trained as an auto-encoder
in $x$-space, it also means that $f(g(\cdot))$ is a good
auto-encoder in $h$-space, for the following reason. On most
training samples $x$ we have $g(f(x))\approx x$. Applying $f$
on both sides gives $f(g(f(x)))\approx f(x)$. Letting $h=f(x)$
means that $f(g(h))\approx h$ for high-probability $h$'s, i.e.,
with $h=f(x)$ and $x\sim p$. In what follows we mainly
study the $x$-space variant, but future work should formally
investigate the advantage of the $h$-space variant.
\fi

\vspace*{-2mm}
\section{Local Moment Matching as an Alternative to Maximum Likelihood}
\vspace*{-2mm}
\subsection{Previous Related Work}
\vspace*{-2mm}

Well-known manifold learning (``embedding'') algorithms include Kernel
PCA~\citep{Scholkopf98}, LLE~\citep{Roweis2000-lle},
Isomap~\citep{Tenenbaum2000-isomap}, Laplacian
Eigenmap~\citep{Belkin+Niyogi-2003}, Hessian
Eigenmaps~\citep{Donoho+Carrie-03}, Semidefinite
Embedding~\citep{Weinberger04a-small}, SNE~\citep{SNE-nips15-small} and
t-SNE~\citep{VanDerMaaten08-small} that were primarily developed and used
for data visualization through dimensionality reduction. These algorithms
optimize the hidden representation associated with training points 
in order to best preserve certain properties of an input-space
neighborhood graph.

The properties that we are interested in here are
the {\bf local mean} and {\bf local covariance}. They are defined as the mean and covariance of a density
restricted to a small neighborhood. For example, if we have lots of samples and we only
consider the samples around a point $x_0$, the local mean at $x_0$ would be estimated by the
mean of these neighbors and the local covariance at $x_0$ by the empirical covariance
among these neighbors.
There are previous machine learning algorithms that have been proposed to estimate
these local first and second moments by actually using local 
neighbors~\citep{Brand2003-small,Vincent-Bengio-2003-short,Bengio-Larochelle-NLMP-NIPS-2006-short}.
In Manifold Parzen Windows~\citep{Vincent-Bengio-2003-short} this is literally achieved by estimating
for each test point the empirical mean and empirical covariance of the neighboring
points, with a regularization of the empirical covariance that sets a floor value
for the eigenvalues of the covariance matrix. In Non-Local Manifold 
Parzen~\citep{Bengio-Larochelle-NLMP-NIPS-2006-short},
the mean and covariance are predicted by a neural network that takes $x_0$ as input
and outputs the estimated mean along with a basis for the leading eigenvectors
and eigenvalues of the estimated covariance. The predictor is trained to maximize
the likelihood of the near neighbors under the Gaussian with the predicted mean
and covariance parameters. Both algorithms are manifold learning
algorithms motivated by the objective to discover the local manifold structure
of the data, and in particular predict the manifold tangent planes around 
any given test point. Besides the computational difficulty of having to find the $k$ nearest neighbors
of each training example, these algorithms, especially Manifold Parzen Windows, heavily 
rely on the smoothness of the
target manifolds, so that there are enough samples to teach the model how and
where the manifolds bend.

Note that the term {\em local moment matching} was already used
by~\citet{Gerber-1982} in an actuarial context to match moments of a
discretized scalar distribution. Here we consider the more general problem
of modeling a multivariate density from data, by estimating the
first and second multivariate moments at every possible input point.

\vspace*{-2mm}
\subsection{A Sampling Procedure From Local Moment Estimators}
\vspace*{-2mm}

We first show that mild conditions suffice for the chain to 
converge~\footnote{This is also shown in~\citet{Rifai-icml2012}.}
and then that if local first and second moments have been estimated,
then one can define a plausible sampling algorithm based on a Markov chain
that exploits these local moments at each step. \\

{\bf Convergence of the Chain}\\

This Markov chain $X_1,X_2, \ldots X_t\ldots$ is 
precisely the one that samples a new point by sampling from the Gaussian
with these local first and second moments:
\begin{equation}
 X_{t+1}\sim N(\mu(X_{t}),\Sigma(X_{t}))
\label{eq:chain}
\end{equation}
as in Eq.\eqref{eq:2-chain}, but where we propose to choose
$\mu(X_t)-X_t$ proportional to 
the local mean at $x_0=X_t$ minus $X_t$, and $\Sigma(X_t)$
proportional to the local covariance at $x_0=X_t$. The functions $\mu(\cdot)$
and $\Sigma(\cdot)$ thus define a Markov chain. 

Let us sketch a proof that it converges under mild hypotheses, using the
decomposition into the chains of $M_t$'s and of $X_t$'s.  Assuming that
$\forall x,\;\; \mu(x)\in \cal B$ for some bounded ball $\cal B$, then \mbox{$M_t \in {\cal B}
\;\;\forall t$}. If we further assume that $\Sigma(X_t)$ is always full rank,
then there is a non-zero probability of jumping from any $M_t \in \cal B$
to any $M_{t+1} \in \cal B$, which is sufficient for ergodicity of the
chain and its convergence. Then if $M_t$'s converge, so do their noisy
counterparts $X_t$'s.\\

{\bf Uncountable Gaussian Mixture}\\

If the chain converges, let $\pi$ be the asymptotic distribution of the $X_t$'s.
It is interesting to note that $\pi$ satisfies the operator equation
\begin{equation}
  \pi(x) = \int \pi(\tilde{x}) {\cal N}(x;\mu(\tilde{x}),\Sigma(\tilde{x})) d\tilde{x}
\label{eq:mixture}
\end{equation}
where ${\cal N}(x;\mu(\tilde{x}),\Sigma(\tilde{x}))$ is the density of $x$ under
a normal multivariate distribution with mean $\mu(\tilde{x})$ and covariance $\Sigma(\tilde{x})$.
This can be seen as a kind of {\em uncountable Gaussian mixture} $\pi$ where 
the weight of each component $\tilde{x}$ is given by $\pi(\tilde{x})$ itself
and the functions $\mu(\cdot)$ and $\Sigma(\cdot)$ specify the mean and covariance
of each component.

\vspace*{-2mm}
\subsection{Consistency}
\label{sec:consistency}
\vspace*{-2mm}

From the point of view of learning,
what we would like is that the $\mu(\cdot)$ and $\Sigma(\cdot)$ functions
used in a sampling chain such as Eq.~\eqref{eq:chain} be such that they
yield an asymptotic density $\pi$ close to some target density $p$.
Because the Markov chain makes noisy finite steps, one would expect
that the best one can hope for is that $\pi$ be a smooth approximation
of $p$.

What we show below is that, in the asymptotic regime
of very small steps (i.e. $\Sigma(x)$ is small in magnitude),  a good choice is just
the intuitive one, where $\mu(x_0)-x_0 \propto \E[x|x_0]-x_0$ 
and $\Sigma(x_0) \propto Cov(x|x_0)$.

For this purpose, we formally define the local density $p_\delta(x|x_0)$
of points $x$ in the $\delta$-neighborhood of an $x_0$, as 
\begin{equation}
p_\delta(x|x_0)= \frac{p(x) 1_{||x-x_0||<\delta}}{Z(x_0)}
\label{eq:ptilde}
\end{equation}
where $Z(x_0)$ is the appropriate normalizing constant. Then
we respectively define the local mean and covariance around $x_0$ as simply being 
\begin{eqnarray}
 m_0 \eqdef \E[x|x_0] &=& \int x p_\delta(x|x_0) dx \nonumber \\
 C_0 \eqdef Cov(x|x_0) &=& \int (x-m_0)(x-m_0)^T p_\delta(x|x_0) dx.
\label{eq:mu0-C0}
\end{eqnarray}

Note that we have two scales here, the scale $\delta$ at which we take
the local mean and covariance, and the scale $\sigma=||\Sigma(x_0)||$
of the Markov chain steps. To prove consistency we assume here
that $\sigma \ll \delta$ and that both are small. Furthermore,
we assume that $\mu$ and $\Sigma$ are somehow calibrated, so that
the steps $\mu(x)-x$ are comparable in size to $\sigma$. This means
that $||\mu(x)-x|| \ll \delta$, which we use below.

We want to compute the local mean obtained when we follow the Markov chain,
i.e., in Eq.~\eqref{eq:ptilde} we choose a $p$ equal to $\pi$ 
(which is defined in Eq.~\eqref{eq:mixture}), and we obtain the following:
\begin{eqnarray}
 m_\pi \eqdef E_\pi[x|x_0] &=& \frac{1}{Z(x_0)}\int_x x \int_{\tilde{x}} p(\tilde{x}) {\cal N}(x;\mu(\tilde{x}),\Sigma(\tilde{x})) d\tilde{x}\; 1_{||x-x_0||<\delta} dx \nonumber \\
   &=& \frac{1}{Z(x_0)}\int_{\tilde{x}} p(\tilde{x}) \int_{||x-x_0||<\delta} x  {\cal N}(x;\mu(\tilde{x}),\Sigma(\tilde{x})) dx d\tilde{x}.
\label{eq:m-pi}
\end{eqnarray}
Because the Gaussian sample of the inner integral must be in a small region inside
the $\delta$-ball, the inner integral is approximately the Gaussian mean
if $\mu(\tilde{x})$ is in the $\delta$-ball, and 0 otherwise:
\begin{equation}
\int_{||x-x_0||<\delta} x  {\cal N}(x;\mu(\tilde{x}),\Sigma(\tilde{x})) dx
\approx
\mu(\tilde{x}) 1_{||\mu(\tilde{x})-x_0||<\delta}.\label{eq:first_moment_extracted}
\end{equation}
which gives
\begin{equation}
 m_\pi \approx \int_{\tilde{x}} \frac{p(\tilde{x})}{Z(x_0)} 1_{||\mu(\tilde{x})-x_0||<\delta} \mu(\tilde{x}) d\tilde{x}.
\label{eq:e-mu}
\end{equation}
Now we use the above assumptions, which give $||\mu(x)-x|| \ll \delta$, to conclude
that integrating under the region defined by $1_{||\mu(\tilde{x})-x_0||<\delta}$ is equivalent
to integrating under the region defined by $1_{||\tilde{x}-x_0||<\delta}$. Hence the
above approximation is rewritten
\begin{equation}
 m_\pi \approx \int_{\tilde{x}} \frac{p(\tilde{x})}{Z(x_0)} 1_{||\tilde{x}-x_0||<\delta} \mu(\tilde{x}) d\tilde{x}
\end{equation}
which by the definition of $\E[\cdot | x_0]$ gives the final result:
\begin{equation}
 m_\pi \approx \E[\mu(x)|x_0],
\end{equation}
It means that the local mean under the small-steps Markov chain is a local mean
of the chain's $\mu$'s. This justifies choosing $\mu(x)$ equal to the
local mean of the target density to be represented by the chain, so that
the Markov chain will yield an asymptotic distribution that has 
local moments that are close but smooth versions of those of the target density.

A similar result can be shown for the covariance by observing that the $x$ term in
Eq. (\ref{eq:first_moment_extracted}) produced the first moment of the Gaussian and
that the same reasoning would apply with $xx^T$ instead.

Alternatively, we can follow a shortened version of the above starting with
\begin{eqnarray}
 C_\pi & \eqdef & E_\pi[(x-m_\pi)(x-m_\pi)^T|x_0] \\
 &=& \frac{1}{Z(x_0)}\int_{\tilde{x}} p(\tilde{x}) \int_{||x-x_0||<\delta} (x-m_\pi)(x-m_\pi)^T  {\cal N}(x;\mu(\tilde{x}),\Sigma(\tilde{x})) dx
\end{eqnarray}
where
\begin{eqnarray}
\int_{||x-x_0||<\delta} (x-m_\pi)(x-m_\pi)^T  {\cal N}(x;\mu(\tilde{x}),\Sigma(\tilde{x})) dx \\
\approx
\left(\Sigma(\tilde{x})+(\mu(\tilde{x})-m_\pi)(\mu(\tilde{x})-m_\pi)^T \right) 1_{||\mu(\tilde{x})-x_0||<\delta}.\label{eq:second_moment_extracted}
\end{eqnarray}
By construction the magnitude of the covariance $\Sigma(\tilde{x})$ was made very small, so the following term vanishes
\begin{equation}
\int_{\tilde{x}} \frac{p(\tilde{x})}{Z(x_0)} 1_{||\tilde{x}-x_0||<\delta} \Sigma(\tilde{x}) d\tilde{x} \rightarrow 0
\end{equation}
and we are left with the desired result
\begin{equation}
C_\pi \approx \E\left[\left.(\mu(x)-m_\pi)(\mu(x)-m_\pi)^T\right|x_0\right].
\end{equation}



\vspace*{-2mm}
\subsection{Local Moment Matching By Minimizing Regularized Reconstruction Error}
\label{sec:min-rec}
\vspace*{-2mm}

We consider here an alternative to using nearest neighbors for estimating local moments,
by showing that {\em minimizing reconstruction error with a contractive penalty yields
estimators of the local mean and covariance.}

We start from a training criterion similar to the CAE's but penalizing the
contraction of the whole auto-encoder's reconstruction function, which is also equivalent
to the DAE's training criterion in the case of small Gaussian corruption noise 
(as shown in Eq.~\eqref{eq:dae-crit-approx}):
\begin{equation}
{\cal L}_{\rm global} = \int p(x_0) \left( \left\|x_0 - r(x_0)\right\|^2 + \alpha\left\|\frac{\partial r(x_0)}{\partial x_0}\right\|^2_F \right) dx_0
\label{eq:L-global}
\end{equation}
where $p$ is the target or training distribution. We prove that in a non-parametric 
setting (where $r$ is completely free), the optimal $r$ is such that the local 
mean $m_0$ is estimated by $r_0\eqdef r(x_0)$ while the local covariance 
$C_0$ is estimated\footnote{In practice, i.e., the parametric
case, there is no guarantee that the estimator be symmetric, but
this is easily fixed by symmetrizing it, i.e., using $\frac{J_0+J_0^T}{2}$.} 
by $J_0 \eqdef \left.\frac{\partial r}{\partial x}\right|_{x_0}$.

To find out what the auto-encoder estimates we follow an approach which has
already been used, e.g., to show that minimizing squared prediction error $\E[(f(X)-Y)^2]$ is
equivalent to estimating the conditional expectation, $f(X) \rightarrow \E[Y|X]$. For this
purpose we consider an asymptotic and non-parametric setting corresponding to the limit
where the number of examples goes to infinity (we actually minimize the
expected error) and the capacity goes to infinity: we allow the value $r_0$ and the 
derivative $J_0$ of $r$ at every point $x_0$, to be different, i.e., we
``parametrize'' $r(x)$ in every neighborhood around $x_0$ by
\begin{equation}
  r(x) = r(x_0) + \left.\frac{\partial r}{\partial x}\right|_{x_0} (x-x_0) = r_0 + J_0 (x-x_0)
\label{eq:r-local}
\end{equation}
which is like a Taylor expansion only valid in the neighborhood of $x$'s around $x_0$,
but where we actually consider $r_0$ and $J_0$ to be parameters free to be chosen
separately for each $x_0$.

Armed with this non-parametric formulation, we consider an infinity of such
neighborhoods and the local density $p_\delta(x|x_0)$ (Eq.~\ref{eq:ptilde})
which approaches a Dirac delta function in the limit of $\delta \rightarrow 0$, and
we rewrite ${\cal L}_{\rm global}$ as follows:
\begin{equation}
 {\cal L}_{\rm global} = \lim_{\delta \rightarrow 0} 
\int p(x_0) \left( \left(\int_x ||x - r(x)||^2 p_\delta(x|x_0) dx \right) + \alpha \left\|\frac{\partial r(x_0)}{\partial x_0}\right\|^2_F \right) dx_0.
\label{eq:L-global-dirac}
\end{equation}
The reason for choosing $p_\delta(x|x_0)$ that turns into a Dirac is that
the expectations of $x$ and $xx^T$ arising in the above inner integral
will give rise to the local mean $m_0$ and local covariance $C_0$.
If in the above equation we define $r(x)$ non-parametrically (as per Eq.~\eqref{eq:r-local}),
the minimum can be achieved by considering the separate minimization in each $x_0$ neighborhood
with respect to $r_0$ and $J_0$. We can express the local contribution to the loss at $x_0$ as
\begin{equation}
 {\cal L}_{\rm local}(x_0, \delta) = \int_x ||x - (r_0+J_0(x-x_0))||^2 p_\delta(x|x_0) dx + \alpha ||J_0||^2_F
\label{eq:L-local}
\end{equation}
so that
\begin{equation}
 {\cal L}_{\rm global} = \lim_{\delta \rightarrow 0} \int p(x_0) {\cal L}_{\rm local}(x_0, \delta) dx_0.
\end{equation}
We take the gradient of the local loss with respect to $r_0$ and $J_0$ and set it to 0 (detailed derivation in Appendix) to get
\begin{eqnarray}
 \frac{\partial {\cal L}_{\rm local}(x_0, \delta)}{\partial r_0} &=& 2(r_0 - m_0)+2J_0(m_0-x_0)\nonumber \\
 \frac{\partial {\cal L}_{\rm local}(x_0, \delta)}{\partial J_0} &=&  2\alpha J_0 -2\left(R-m_0 x_{0}^{T}-r_{0}(m_0-x_{0})^{T}\right) \nonumber \\
 & & \hspace{10pt} +2J_0\left(R-m_0 x_{0}^{T}-x_{0}m_0^{T}+x_{0}x_{0}{}^{T}\right).
\label{eq:gradients}
\end{eqnarray}
Solving these equations (detailed derivation in Appendix) gives us the solutions
\begin{eqnarray}
 r_0 &=& (I - J_0)m_0 + J_0 x_0 \nonumber \\
 J_0 &=& C_0 (\alpha I + C_0)^{-1}.
\label{eq:rJ-solutions}
\end{eqnarray}
Note that these quantities $m_0, C_0$ are defined through $p_\delta(x|x_0)$ so they depend implicitly on
$\delta$ and we should consider what happens when we take the limit $\delta \rightarrow 0$.

In particular, when $\delta \rightarrow 0$ we have that $\left\|C_0\right\| \rightarrow 0$ and we can see
from the solutions (\ref{eq:rJ-solutions}) that this forces $\left\|J_0\right\| \rightarrow 0$.
In a practical numerical application, we fix $\delta>0$ to be small and it becomes
interesting to see how these quantities relate asymptotically (in terms of $\delta$ decreasing).
In such a situation, we have the following asymptotically:\footnote{Here, to avoid confusion
with the overloaded $\sim$ notation for sampling, we instead use the $\asymp$ 
notation to denote that the ratio of any coefficient on the left with its corresponding coefficient on the right goes 
to 1 as $\delta \rightarrow 0$.}
\begin{eqnarray}
 r_0 & \asymp & m_0 \nonumber \\
 J_0 & \asymp & \alpha^{-1}C_0.
\label{eq:simple-rJ-solutions}
\end{eqnarray}

Thus, we have proved that {\em the value of the reconstruction and its Jacobian immediately
give us estimators of the local mean and local covariance, respectively.} 

\vspace*{-2mm}
\section{Experimental Validation}
\label{sec:exp}
\vspace*{-2mm}

In the above analysis we have considered the limit of a non-parametric case
with an infinite amount of data. In that limit, unsurprisingly,
reconstruction is perfect, i.e.,
$r(x)\rightarrow x$, $\E[x|x_0]\rightarrow x_0$,
and $||\frac{\partial r(x)}{\partial x}||_F\rightarrow 0$ and $||Cov(x|x_0)||_F\rightarrow 0$, 
at a speed that depends on the scale $\delta$, as we have seen
above. We do not care so much about the magnitudes but instead care about
the directions indicated by $r(x)-x$ (which indicate where to look for increases
in probability density) and by the singular vectors and
relative singular values of $\frac{\partial r(x)}{\partial x}$, which indicate what
directions preserve high density. In
a practical sampling algorithm such as described in Section~\ref{sec:cae-sampling},
one wants to take non-infinitesimal steps. Furthermore,
in the practical experiments
of ~\citet{Rifai-icml2012}, in order to get good generalization with a
limited training set, one typically works with a parametrized model which
cannot perfectly reconstruct the training set (but can generalize). 
This means that the learned reconstruction is not equal to the input,
even on training examples, and nor is the Jacobian of the reconstruction
function tiny (as it would be in the asymptotic non-parametric case).
The mathematical link between the two situations needs to be clarified
in future work, but a heuristic approach which we found to work well
is the following: control the scale of the Markov chain with a hyper-parameter
that sets the magnitude of the Gaussian noise (the variance of $\epsilon$
in Section~\ref{sec:cae-sampling}). 
That hyper-parameter can be optimized by visual inspection or by estimating
the log-likelihood of the samples, using a technique introduced
in~\citep{Breuleux+Bengio-2011} and also used in~\citet{Rifai-icml2012}.
The basic idea is to generate many samples from the model, train a
non-parametric density estimator (Parzen Windows) using these samples as a
training set, and evaluate the log-likelihood of the test set using that
density estimaor.  If the sample generator does not mix well, then some
test examples will be badly covered (far from any of the generated
samples), thus incurring a high price in log-likelihood. If the generator
mixes well but smoothes too much the true density, then the automatically
selected bandwidth of the Parzen Windows will be chosen larger, incurring
again a penalty in test log-likelihood.

In Fig.~\ref{fig:samples}, we show samples of DAEs trained
and sampled similarly as in~\citet{Rifai-icml2012} on both MNIST digits images
and the Toronto Face Dataset (TFD)~\citep{Susskind2010}. These
results and those already obtained in~\citet{Rifai-icml2012}
confirm that the auto-encoder trained either as a CAE or a DAE
estimates local moments
that can be followed in a Markov chain to generate likely-looking
samples (and which have been shown quantitatively to be of high
quality in~\citet{Rifai-icml2012}).
%SAMPLING RESULTS WITH DAE: TO BE ADDED.
\begin{figure*}
\begin{center}
\vspace*{-2mm}
    \includegraphics[width=0.95\textwidth]{samples_tfd.png}
    \includegraphics[width=0.95\textwidth]{samples_mnist.png}
\vspace*{-2mm}
\label{fig:samples}
\end{center}
\caption{Samples generated by a DAE trained on TFD (top 2 rows) and MNIST (bottom 2 rows).}
\end{figure*}   

\ifEnergybasedversion
\section{Extension to Energy-Based Models}
\fi

\vspace*{-2mm}
\section{Conclusion}
\vspace*{-2mm}

This paper has contributed a novel approach to modeling densities: indirectly,
through the estimation of local moments. It has shown that local moments can
be estimated by auto-encoders with contractive regularization. It has justified
a sampling algorithm based on a simple Markov chain when estimators of the local 
moments are available. Whereas auto-encoders are unsupervised learning algorithms
that have been known for many decades, it has never been clear if they captured
everything that can be captured from a distribution. For the first time, this
paper presents a theoretical justification showing 
that they do implicitly perform density estimation
(provided some appropriate regularization is used, and assuming
the training criterion can be minimized).
This provides a more solid footing to the recently proposed algorithm
for sampling Contractive Auto-Encoders~\citep{Rifai-icml2012} and opens the door to other
related learning and sampling algorithms. In particular, it shows that this
sampling algorithm can be applied to Denoising Auto-Encoders as well.

An interesting advantage of modeling data through such training criteria is that 
there is no need to estimate an untractable partition function or its gradient,
and that there is no difficult inference problem associated with these types
of models either.
Future work following up on this paper
should try to answer the more difficult mathematical questions of 
what happens (e.g., with the consistency arguments presented here)
if the Markov chain steps are not tiny, and when we consider a
learner that has parametric constraints, rather than the asymptotic
non-parametric limit considered here.
We believe that the approach presented here can also be applied to
energy-based models (for which the free energy can be computed), and that
the local moments are directly related to the first and second derivative
of the estimated density. Future work should clarify that relationship,
possibly giving rise to new sampling algorithms for energy-based models
(since we have shown here that one can sample from the estimated
density if one can compute the local moments).
Finally, it would be interesting to extend this work in the direction of
analysis that explicitly takes into account the decomposition of the
auto-encoder into an encoder and a decoder. Indeed, we have found
experimentally that sampling in the representation space gives better
results than sampling in the original input space, but a more solid
mathematical treatment of such algorithms is still missing.

\vspace*{-2mm}

{\small
%\bibliography{strings,strings-shorter,ml,myrefs,aigaion-shorter}
\bibliography{strings,ml,aigaion-shorter}
%\bibliographystyle{plain}
\bibliographystyle{natbib}
}

\appendix

\section{Derivation of the local training criterion gradient}

We want to obtain the derivative of the local training criterion,
\begin{equation}
 L_{\rm local} = \lim_{\delta \rightarrow 0} 
 \int_x ||x - (r_0+J_0(x-x_0)||^2 \tilde{p}_\delta(x|x_0) dx + \alpha ||J_0||^2_F.
\label{eq:L-local}
\end{equation}
We use the definitions
\begin{eqnarray*}
 \mu_0 &=& \E[x|x_0]=\int_x x \tilde{p}_\delta(x|x_0) dx  \nonumber \\
 R_0 &=& \E[xx^T|x_0] \nonumber \\
 C_0 &=& Cov(x|x_0)=\E[(x-\mu_0)(x-\mu_0)^T|x_0]=R-\mu_0\mu_0^T.
\end{eqnarray*}
We first expand the expected square error:
\begin{eqnarray*}
 MSE=\E[||x-(r_0+J_0(x-x_0))||^2|x_0] &=& 
 \E[ (x - (r_0+J_0(x-x_0))(x - (r_0+J_0(x-x_0))^T ] \nonumber \\
 &=& \E[ (x - r_0)(x-r_0)^T-2(x-r_0)^TJ_0(x-x_0)] \nonumber \\
 &=& + \E[ (x-x_0)^TJ_0^TJ_0(x-x_0)]. \nonumber
\end{eqnarray*}
Differentiating this with respect to $r_0$ yields 
\[
 \frac{\partial MSE}{\partial r_0} = -2(\mu_0 - r_0)+2J_0(\mu-x_0)
\]
corresponding to Eq.(25) of the paper.

For differentiating with respect to $J_0$, we use the trace properties
\begin{eqnarray*}
  ||A||^2_F &=& Tr(A A^T) \nonumber \\
  Tr(ABC)&=&Tr(BCA) \nonumber \\
  %\frac{\partial Tr(A A^T)}{\partial A} &=& 2 A \nonumber \\
  \frac{\partial Tr(A^TXAZ)}{\partial A} &=& XAZ+X^TAZ^T \nonumber \\
  \frac{\partial Tr(X A)}{\partial A} &=& X^T \nonumber \\
  \frac{\partial Tr(X A^T)}{\partial A} &=& X.
\end{eqnarray*}
We obtain for the regularizer,
\[
 \frac{\partial \alpha ||J_0||^2_F}{\partial J_0}=2\alpha J_0
\]
and for the MSE:
\begin{eqnarray*}
 \frac{\partial MSE}{\partial J_0} &=& -2\E\left[\frac{\partial}{\partial J_0}tr(J_0(x-x_{0})(x-r_{0})^{T})\right]+\E\left[\frac{\partial}{\partial J_0}tr(J_0^{T}J_0(x-x_{0})(x-x_{0})^{T})\right]\\
 & = & -2\E\left[(x-r_{0})(x-x_{0})^{T}\right]+2J_0\E\left[(x-x_{0})(x-x_{0})^{T}\right]\\
 & = & -2\left(R-\mu_0 x_{0}^{T}-r_{0}(\mu_0-x_{0})^{T}\right)+2J_0\left(R-\mu_0 x_{0}^{T}-x_{0}\mu_0^{T}+x_{0}x_{0}{}^{T}\right)
\end{eqnarray*}

% & = & -2\left(R-\mu_0 x_{0}^{T}-\left(\mu_0-J_0(\mu_0-x_{0})\right)(\mu_0-x_{0})^{T}\right)+2J_0\left(R-\mu_0 x_{0}^{T}-x_{0}\mu_0^{T}+x_{0}x_{0}{}^{T}\right)\\
% & = & -2\left(R-\mu_0\mu_0^{T}\right)-2J_0(\mu_0-x_{0})(\mu_0-x_{0})^{T}+2J_0\left(R-\mu_0 x_{0}^{T}-x_{0}\mu_0^{T}+x_{0}x_{0}{}^{T}\right)\\
% & = & -2\left(R-\mu_0\mu_0^{T}\right)+2J_0\left(R-\mu_0\mu_0^{T}\right)\\
% & = & -2\left(I-J_0\right)C_0

%Putting all this together, we obtain
%\[
% \frac{\partial (MSE+\alpha ||J_0||^2_F)}{\partial J_0} = 2 \alpha J_0 - 2\left(I - J_0\right) C_0
%\] 
%as per Eq.(13) of the main text.

\section{Detailed derivation of the minimizers of the local training criterion}

Starting with $r_0$, we solve when the gradient is 0 to obtain
\begin{eqnarray*}
 \frac{\partial MSE}{\partial r_0} & = & (\mu-r_{0})-J_0(\mu-x_{0}) = 0\nonumber \\
\mu-r_{0} & = & J_0(\mu-x_{0})\nonumber \\
r_{0} & = & \mu-J_0(\mu-x_{0}) \nonumber \\
r_{0} & = & (I-J_0)\mu+J_0x_{0}.\nonumber
\end{eqnarray*}

Substituting that value for $r_0$ into the expression for the gradient with respect to $J_0$, we get
\begin{eqnarray*}
 \frac{\partial MSE}{\partial J_0}
 & = & -2\left(R-\mu_0 x_{0}^{T}-\left(\mu_0-J_0(\mu_0-x_{0})\right)(\mu_0-x_{0})^{T}\right)+2J_0\left(R-\mu_0 x_{0}^{T}-x_{0}\mu_0^{T}+x_{0}x_{0}{}^{T}\right)\\
 & = & -2\left(R-\mu_0\mu_0^{T}\right)-2J_0(\mu_0-x_{0})(\mu_0-x_{0})^{T}+2J_0\left(R-\mu_0 x_{0}^{T}-x_{0}\mu_0^{T}+x_{0}x_{0}{}^{T}\right)\\
 & = & -2\left(R-\mu_0\mu_0^{T}\right)+2J_0\left(R-\mu_0\mu_0^{T}\right)\\
 & = & -2\left(I-J_0\right)C_0.
\end{eqnarray*}
Adding the regularizer term and setting the gradient to 0, we get
\begin{eqnarray*}
 \frac{\partial (MSE+\alpha ||J_0||^2_F)}{\partial J_0} &=& -2(I-J_0)C_0+2\alpha J_0 = 0\\
C_0 & = & J_0 C_0 + \alpha J_0 \\
C_0 & = & J_0 (C_0 + \alpha I) \\
J_0 & = & C_0(\alpha I + C_0)^{-1}
\end{eqnarray*}

which altogether gives us Eq.(26) from the main text:
\begin{eqnarray*}
 r_0 &=& (I - J_0)\mu_0 + J_0 x_0 \\
 J_0 &=& (\alpha I + C_0)^{-1} C_0
\end{eqnarray*}

Note that we can also solve for $\mu_0$
\[
\mu = (I-J_0)^{-1}(r_{0}-J_0x_{0})
\]
and for $C_0$:
\begin{eqnarray*}
(I - J_0) C_0 &=& \alpha J_0 \\
C_0 &=& \alpha (I - J_0)^{-1} J_0
\end{eqnarray*}

However, we still have to take the limit as $\delta \rightarrow 0$.
Noting that the magnitude of $C_0$ goes to 0 as $\delta \rightarrow 0$,
it means that $J_0$ also goes to 0 in magnitude. Plugging in the above equations
gives the final results:
\begin{eqnarray*}
r_0 &=& \mu_0 \\
C_0 &=& J_0
\end{eqnarray*}

\end{document}



\begin{abstract}
   We present two techniques to improve landmark localization from partially annotated datasets. Our primary goal is to leverage the common situation where precise landmark locations are only provided for a small data subset, but where class labels for classification tasks related to the landmarks are more abundantly available. We propose a new architecture for landmark localization, where training with class labels acts as an auxiliary signal to guide the landmark localization on unlabeled data. A key aspect of our approach is that errors can be backpropagated through a complete landmark localization model. We also propose and explore an unsupervised learning technique for landmark localization based on having a model predict equivariant landmarks with respect to transformations applied to the image. We show that this technique, used as additional regularization, improves landmark prediction considerably and can learn effective detectors even when only a small fraction of the dataset has labels for landmarks. We present results on two toy datasets and three real datasets, with hands and faces, respectively, showing the performance gain of our method on each.
\end{abstract}


\subsubsection*{Acknowledgment}
We would like to thanks Compute Canada and Calcul Quebec for providing computational resources. This work was partially funded by NVIDIAs NVAIL program.


\section{Conclusion}
We present a new architecture and training procedure for semi-supervised landmark localization. Our contributions are twofold;
We first propose an unsupervised technique for equivariant landmark transformation without requiring labeled landmarks. This technique helped improving the landmark localization on all dataset.
In addition we propose an architecture to improve landmark estimation using auxiliary labels such as classes by backpropagating errors through the landmark localization components of the model. 
Our experiments show we can achieve high accuracy with far less labelled training data. We achieve state of the art performance on public benchmark dataset for fiducial points, 300W, when training from scratch without extra data.


\subsection{300W Dataset}
\label{sec:exp_300W}
In order to compare our architecture with recent models and also evaluate it on natural images in the wild we use 300W \cite{sagonas2013300} dataset. This dataset provides 68 landmarks and is composed of 3148 (337 AFW, 2000 Helen, and 811 LFPW) and 689 (135 IBUG, 224 LFPW, and 330 Helen) images in the training and test sets, respectively. Similar to RCN \cite{honari2016recombinator}, we split the training set into 90\% (2834 images) train-set and 10\% (314 images) valid-set. Since this dataset does not provide any class label, we can evaluate our model in L and L+T cases. We use Seq-MT architecture with 8 convolutional layers (32 feature maps in each except the last one with 68) and kernels 9$\times$9. \footnote{We apply these data augmentations: rotation [-30, 30], and scale and translation [-10\%, 10\%] of the face bounding box.}


In Table~\ref{tab:300W} we compare Seq-MT with other models in the literature. Seq-MT model is outperforming many models including CDM, DRMF, RCPR, CFAN, ESR, SDM, ERT, LBF and CFSS, and is only doing worse than few recent models with complicated architectures, eg. RCN \cite{honari2016recombinator} with multiple branches, RAR \cite{xiao2016robust} with multiple refinement procedure and Lv et. al \cite{Lv_2017_CVPR} with multiple steps. In Table \ref{tab:300W_models} we compare recent models on their training conditions. RAR, TCDCN, \cite{Lv_2017_CVPR}, and CFSS do not use an explicit validation set. This makes comparison with these models more difficult for two reasons: 1) These models do hyper-parameter (HP) selection on the test-set, which makes them overfit on the test-set; and 2) Due to not using any explicit validation set, their affective training size is bigger. The first three models use extra datasets, either through pre-trained models (RAR, \cite{Lv_2017_CVPR}) or additional labeled data (TCDCN). RCN is the only model that performs better than our model and does not leverage extra dataset. It also uses a validation set for HP selection.

Note that the originality of Seq-MT is not in the specific architecture used for the first part of the network that localizes landmarks, but rather in its multi-tasking architecture (specifically in its usage of the class labels to enhance landmark localization) and also leveraging the transformation cost. The landmark localization part of Seq-MT can be replaced with more complex models. To verify this, we use the RCN model, with publicly available code, \cite{honari2016recombinator} and replace the original softmax layer with a soft-argmax layer in order to apply the transformation cost. We refer to this model as RCN$\stackanchor{+}{}$ and it is trained with these hyperparameters: $\beta = 1.0$, $\alpha = 0.5$, $\gamma = 0$, $\lambda = 1.0$. The result is shown as RCN$\stackanchor{+}{}$(L) when using only landmark cost and RCN$\stackanchor{+}{}$(L+T) when using landmark plus the transformation cost. On 300W dataset we apply the transformation cost to samples with or without labelled landmarks to observer how much improvement can be obtained when used on all data. We can further reduce RCN error from 5.54 to 5.1 by applying the transformation cost and soft-argmax. This is a new state of the art without any data-augmentation.

In Table \ref{tab:300W_percent} we compare Seq-MT with Heatmap-MT and Common-MT \footnote{We trained these models with these parameters: $\beta = 0.01$, $\alpha = 2.0$, $\gamma = 10^{-5}$, $\lambda = 2.0$.} on different percentage of labelled landmarks. Seq-MT outperforms the other two models. We also demonstrate the improvement that can be obtained by using RCN$\stackanchor{+}{}$. Note that the transformation cost improves the results when applied to two different landmark localization architectures (Seq-MT, RCN). Moreover, it considerably improves the results on IBUG test-set that contains more difficult examples than the training set. Figure \ref{fig:300W_examples} shows the improvement obtained by using transformation cost on some samples.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[t]
\caption{Performance of different architectures on 300W test-set using 100\% labeled landmarks. The error is Euclidean distance normalized by ocular distance (as a percent; lower is better).}
\label{tab:300W}
\vskip2pt
\centering
\resizebox{.75\linewidth}{!}{
\begin{tabular}{cccc}
\toprule
\textbf{Model} & \textbf{Common} & \textbf{IBUG} & \textbf{Fullset} \\
\midrule
CDM \cite{yu2013} & 10.10 & 19.54 & 11.94 \\
DRMF \cite{asthana2013robust} & 6.65 & 19.79 & 9.22\\
RCPR \cite{burgos2013} & 6.18 & 17.26 & 8.35\\
CFAN \cite{zhang2014coarse} & 5.50 & 16.78 & 7.69\\
ESR \cite{cao2014face} & 5.28 & 17.00 & 7.58\\
SDM \cite{xiong2013} & 5.57 & 15.40 & 7.50 \\
ERT \cite{cao2014face} & & & 6.40 \\
LBF \cite{ren2014face} & 4.95 & 11.98 & 6.32\\
CFSS \cite{zhu2015face} & 4.73 & 9.98 & 5.76 \\
TCDCN* \cite{zhang2015learning} & 4.80 & 8.60 & 5.54\\
RCN \cite{honari2016recombinator} & 4.70 & 9.00 & 5.54 \\
\shortstack{RCN + denoising} \cite{honari2016recombinator} & 4.67 & 8.44 & 5.41 \\
RAR \cite{xiao2016robust} & \textbf{4.12} & 8.35 & \textbf{4.94}\\
Lv et. al \cite{Lv_2017_CVPR} & 4.36 & \textbf{7.56} & 4.99 \\
\midrule
Heatmap-MT (L) & 6.18 & 13.56 & 7.62\\
Comm-MT (L) & 5.68 & 11.04 & 6.73\\
Seq-MT (L) & 4.93 & 10.24 & 5.95\\
Seq-MT (L+T) & 4.84 & 9.53 &	5.74\\
\midrule
RCN$\stackanchor{+}{}$ (L) & 4.47 & 8.47 & 5.26\\
RCN$\stackanchor{+}{}$ (L+T) & 4.34 & 8.20 & 5.10\\
\end{tabular}
}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[t]
\caption{Performance of different architectures on 300W test-set. The error is Euclidean distance normalized by ocular distance (eye-centers). Error is shown as a percent; lower is better.}
\label{tab:300W_percent}
\vskip2pt
\centering
\resizebox{1.\linewidth}{!}{
\begin{tabular}{ccccccc}
\toprule
 & & \multicolumn{5}{c}{\textbf{Percentage of Images with Labeled Landmarks}} \\
\cmidrule{3-7}
& \textbf{Model} & 5\% & 10\% & 20\% & 50\% & 100\% \\
\bottomrule
\hline
\\[-2ex]
\multirow{6}{*}{\textbf{Fullset}}
& Heatmap-MT (L) & 13.47 & 11.68 & 9.85 & 8.18 & 7.62\\
& Comm-MT (L) & 16.73 & 9.66 & 8.61 & 7.39 & 6.73\\
& Seq-MT (L) & 9.82 & 8.30 &	7.26 &	6.28 &	5.95\\  
& Seq-MT (L+T) & 8.23 & 7.28 & 6.62 & 6.10 & 5.74\\
\cmidrule{2-7}
& RCN$\stackanchor{+}{}$ (L) & 7.26 & 6.48 &	5.91 &	5.52 &	5.26\\  
& RCN$\stackanchor{+}{}$ (L+T) & \textbf{7.22} & \textbf{6.32} & \textbf{5.88} & \textbf{5.45} & \textbf{5.10}\\
\bottomrule
\hline
\\[-2ex]
\multirow{6}{*}{\textbf{IBUG}}
& Heatmap-MT (L) & 26.36 & 22.77 & 18.46 & 14.94 & 13.56\\
& Comm-MT (L) & 28.64 & 16.17 & 14.56 & 12.16 & 11.04\\
& Seq-MT (L) & 18.74 & 16.21 &	13.41 &	11.20 &	10.24 \\
& Seq-MT (L+T) & 14.68 & 12.73 & 11.39 & 10.37 & 9.53\\
\cmidrule{2-7}
& RCN$\stackanchor{+}{}$ (L) & 15.36 & 12.74 &	11.82 &	10.12 &	8.47 \\
& RCN$\stackanchor{+}{}$ (L+T) & \textbf{12.54} & \textbf{10.35} & \textbf{9.56} & \textbf{8.67} & \textbf{8.20}\\
\end{tabular}
}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[h]
\begin{center}
\centering
\fboxsep=0mm
\fboxrule=1pt
{\includegraphics[width=0.45\textwidth]
{images/300W_samples.jpg}}
\end{center}
\vskip-6pt
\caption{Examples of our model predictions on 300W test-set. Models used, from top to bottom: 1) \textit{Seq-MT (L)}; 2) \textit{Seq-MT (L+T)}; 3) \textit{RCN $\stackanchor{+}{}$(L)}; 4) \textit{RCN $\stackanchor{+}{}$(L+T)}. The green and red dots show ground truth and model predictions, respectively and the yellow lines show the error. These examples illustrate the improved accuracy obtained by using transformation cost (T). The rectangles indicate the regions that landmarks are mostly improved.}
\label{fig:300W_examples}
%\vspace*{-0.9em}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[t]
\caption{Comparison of recent models on their training conditions. RAR, Lv et. al \cite{Lv_2017_CVPR}, CFSS, and TCDCN do not use an explicit validation subset. This makes hyper-parameter (HP) selection based on the test-set and also brings extra training data (by using the validation-set). RAR and Lv et. al \cite{Lv_2017_CVPR} initialize their models by pre-trained parameters (an ImageNet and a VGG-19 pre-trained model, respectively), which is equivalent to using extra data. TCDCN uses 20,000 extra labelled data for pre-training. Finally, RAR adds manual samples by occluding images with sunglasses, medical masks, phones, etc to make them robust to occlusion. Similar to RCN, Seq-MT and RCN$\stackanchor{+}{}$ both have an explicit validation set for HP selection and therefore use a smaller training set. Neither use any extra data, either through pre-trained models or explicit external data. For data-augmentation we follow RCN and apply only affine transformation on the training set and use a basic occlusion through applying random black rectangles on top of images.}
\label{tab:300W_models}
\vskip2pt
\centering
\resizebox{.8\linewidth}{!}{
\begin{tabular}{ccccccc}
\toprule
 & \multicolumn{6}{c}{\textbf{Model}} \\
\cmidrule{2-7}
\textbf{Feature} & RAR\cite{xiao2016robust} & Lv et. al \cite{Lv_2017_CVPR} & TCDCN\cite{zhang2015learning} & CFSS\cite{zhu2015face} & RCN\cite{honari2016recombinator} & Seq-MT / RCN$\stackanchor{+}{}$ \\
\midrule
Hyper-parameter selection dataset & {\color{red}Test-set} & {\color{red}Test-set} & {\color{red}Test-set} & {\color{red}Test-set} & {\color{green}Valid-set} & {\color{green}Valid-set}\\
Training on entire train-set? &  {\color{red}Yes} & {\color{red}Yes} & {\color{red}Yes} & {\color{red}Yes} & {\color{green}No} & {\color{green}No} \\
Using Extra Dataset? &  {\color{red}Yes} & {\color{red}Yes} & {\color{red}Yes} & {\color{green}No} & {\color{green}No} & {\color{green}No} \\
Manually augmenting the training set? & {\color{red}Yes} & {\color{green}No} & {\color{green}No} & {\color{green}No} & {\color{green}No} & {\color{green}No} \\
\end{tabular}
}
\end{table*}





\subsection{Blocks Dataset}
\label{sec:exp_blocks}
%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t]
\begin{center}
\begin{tabular}{c}
\includegraphics[trim={0 0 16cm 0},clip,width=0.95\linewidth]{images/blocks_classes.png}
\\
\includegraphics[trim={18.5cm 0 0 0},clip,width=0.83\linewidth]{images/blocks_classes.png}
\end{tabular}
   \caption{The fifteen classes of the Blocks dataset. Each class is composed of five squares and one triangle. To create the each $60 \times 60$ image in the dataset, a random scale, translation, and rotation (up to $360$ degrees) is applied to one of the base classes.}
\label{fig:blocks_classes}
\end{center}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%
Our second toy dataset, Blocks, presents additional difficulty: each image depicts a figure composed of a sequence of five white squares with one white triangle at the head. See Fig. \ref{fig:blocks_classes} for all fifteen classes of Block figures. Images in the dataset are created with random rotations of up to $360$ degrees combined with random scale and translation. We split the dataset into train, validation, and test sets, each having $3200$ images. For the sequential multi-tasking architecture, we use six convolution layers with kernels of size $9 \times 9$ followed by two convolutional layers with kernel size $1 \times 1$ before the soft-argmax output. All convolutional layers have eight feature maps, except for the final layer which produces five feature maps, one for each square. The classification section of the network has three fully-connected layers of sizes $256$, $256$, and $15$, respectively, ending with softmax classification.

Initially we trained the model with only cross-entropy on the class labels and evaluated the quality of the resulting landmark assignments. Ideally, the model would consistently assign each landmark to a particular block in the sequence from head (the triangle) to tail (the final square). However, in this more complex setting, the model did not predict landmarks consistently across examples. The addition of the transformation cost induces the model learns relatively consistent landmarks between examples \emph{from the same class}, but this consistency does not extend between different classes. Unlike the Shapes dataset---where there was a consistent, if indirect, mapping between landmarks and the classification task and where there were only a single triangle and square--the correspondence among the classification task, and landmark identities is more tenuous in the Blocks dataset.
Hence, we introduce a labeled set of true landmark locations and evaluate the landmark localization accuracy by having a different percentage of train-set being labelled with true landmarks.

Table~\ref{tab:blocks} compares the results using the sequential multi-tasking model in the following scenarios: 1) using only the landmarks (Seq-MT (L)), which is equivalent to training only the first part of the network, 2) using landmarks and class labels (Seq-MT (L+C)), which trains the whole network on class labels and the first part of the network on landmarks, 3) using landmarks and the the transformation cost (Seq-MT (L+T)), and 4) using three costs together (Seq-MT (L+T+C)). We compare the performance of these models while varying the percentage of images labeled with true landmark locations (L).\footnote{We ensure that the set of examples with labeled landmarks is class-balanced.}

When using the transformation cost in training (scenarios 3 and 4), we only apply it to images that do not provide true landmarks to simulate semi-supervised learning
\footnote{This is done to avoid unfair advantage of our model compared to other models on examples that provide landmarks. However, this approach can be applied to any image, both with and without labeled landmarks.}.

\begin{figure}
\begin{center}
\includegraphics[width=0.48\textwidth]{images/multitask_common_v2.pdf}
\caption{Our implementation of the multi-tasking architecture that is commonly used in the literature \cite{zhang2014facialMTFL, zhang2016MAFL, zhang2014improving, devries2014multi} (Comm-MT). The model takes an image and applies 5 half conv layers (half conv layers maintain the 2D resolution of the feature maps). The conv layer indicated by $\times 4$ is repeated 4 times without weight sharing. The model then applies a $2 \times 2$ pooling layer reducing the size of the 2D maps by half on each dimension. Another half conv layer is applied, followed by a $2 \times 2$ pooling layer. The output of the pooling layer is flattened and given to 2 FC layers with dropout and relu non-linearities. The last FC layer is then connected to two branches, one for the classification task and another for the landmark localization. All conv layers and all FC layers, except that last FC layer, use relu non-linearity.}
\label{fig:common_multitask}
\end{center}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%

As shown in Table \ref{tab:blocks} \footnote{The results are averaged over five seeds. We have tuned L2 separately for these models. $\lambda$ and $\alpha$ coefficient are set to 1 for models that use them. We used $\beta=0.01$ for models with soft-argmax and $25\%$ dropout in FC layers in our experiments.}, the \emph{Seq-MT (L+C)} improves upon \emph{Seq-MT (L)}, indicating that class labels can be used to guide the landmark locations. By adding the transformation cost, we can improve the results considerably. With \emph{Seq-MT (L+T)} better performance is obtained compared to \emph{Seq-MT (L+C)} indicating the unsupervised learning technique can substantially enhance performance. However, the best results are obtained with both class labels and the transformation and landmark costs. See Fig. \ref{fig:blocks_pred} for prediction samples when only $5\%$ of the data are labeled with landmarks.
%%%%%%%%%%%%%%%%%%%%%%%%%

Since our model can be considered to be a multi-tasking network, we contrast it with other multi-tasking architectures that are commonly used in the literature. We compare with two architectures: 1) The ``common'' multi-tasking architecture \cite{zhang2014facialMTFL, zhang2016MAFL, zhang2014improving, devries2014multi} where sub-networks for each task share a common set of initial layers ending in a common fully-connected layer. See an illustration of this architecture in Fig. \ref{fig:common_multitask}.\footnote{We tried other variants of this architecture such as 1) a model that goes directly from the feature maps that have the same size as input image to the FC layer without any pooling layers and 2) a model that has more pooling layers and goes to a lower resolution before feeding the features to FC layers. Both models achieved worse results. Model 1 suffers from an over-abundance of parameters when going to FC layer. Model 2 suffers from pooling layers, and deeper models have slightly worse results on this task.}
We train two variants of this model, one with only landmarks (\emph{Comm-MT (L)}) and another with landmarks and class labels (\emph{Comm-MT (L+C)}) to see whether the class labels improve landmark localization. 2) Heat-map multi-tasking, where to avoid pooling layers we follow the recent trend of maintaining the resolution of feature maps \cite{tompson2015efficient, hariharan2015hypercolumns, long2015fully, honari2016recombinator} and features detected for landmark localization do not pass through a FC layer. With Heat-map multitasking architecture we maintain the resolution of the feature maps for landmark localization and use a softmax on top of it. See Fig. \ref{fig:paral_MT} for an illustration of this architecture. The heatmaps right before the softmax layer are taken as input to the classification model.
We refer to this model as \emph{Heatmap-MT}. Note that this model doesn't have a bottle-neck such as \emph{Seq-MT (L+C)}.

As shown in Table~\ref{tab:blocks} \footnote{The classification output layer in \emph{Comm-MT} model uses sigmoid non-linearity. It worked better than $\mathrm{abs}(\tanh)$ or having no non-linearity. The classification output layer in \emph{Heatmap-MT} model has no-non-linearity.}, the common multi-tasking approach is performing much worse than our sequential architecture for landmark estimation. The drawback of this architecture is the usage of pooling layers which leads to sub-optimal results for landmark estimation. The model trained with extra class information performs better than the model trained only on landmarks. Heatmap-MT performs better than the Comm-MT, indicating better results can be obtained without pooling layers. However, Heatmap-MT performs worse than {Seq-MT}, which is due to using softmax instead of softargmax. The softmax cannot be more accurate than the number of discrete elements in the softmax, however, the soft-argmax can get any real number. Moreover, in {Heatmap-MT} the class label is mostly helping in low percentage of labeled data, but in {Seq-MT} it is helping for all percentage of labeled data. We believe this is due to creating a bottle-neck of landmarks before class label prediction, which causes the class labels to impact on landmarks more directly through back-propagation.
%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[h]
\caption{Error of different architectures on blocks dataset. The error is reported in pixel space. An error of 1 indicates 1 pixel distance to the target landmark location. The first 4 rows show the results of Seq-MT architecture, depicted in Fig. \ref{fig:seq_model}. The 5th and 6th rows show results with common multi-tasking architecture, depicted in \ref{fig:common_multitask}. The last two rows show the results of the heatmap based multi-tasking architecture depicted in Fig. \ref{fig:paral_MT}. Since the transformation cost is applied only to the data that do not provide landmarks, the results of these models for 100\% labeled data would be similar to the ones that do not apply transformation cost.}
\label{tab:blocks}
\vskip2pt
\centering
\resizebox{1.\linewidth}{!}{
\begin{tabular}{ccccccc}
\toprule
& \multicolumn{6}{c}{\textbf{Percentage of Images with Labeled Landmarks}} \\
% \cline{2-7}
\cmidrule{2-7}
\textbf{Model} & 1\% & 5\% & 10\% & 20\% & 50\% & 100\% \\
\midrule
Seq-MT (L) &  8.33 & 3.95 & 3.35 & 1.98 & 1.19 & 0.44\\
Seq-MT (L+C) &  8.02 & 3.45 & 3.20 & 1.67 & 1.05 & \textbf{0.38}\\
Seq-MT (L+T) &  6.42 & 1.94 & 1.37 & 1.16 & 0.85\\
\shortstack{Seq-MT (L+T+C)} & \textbf{6.25} & \textbf{1.70} & \textbf{1.26} & \textbf{1.07} & \textbf{0.74}\\
\midrule
Comm-MT (L) &  12.89 & 11.56 & 10.72 & 9.39 & 5.04 & 3.41\\
Comm-MT (L+C) &  12.28 & 11.19 & 10.36 & 9.01 & 4.21 & 2.97\\
\midrule
\shortstack{Heatmap-MT (L)} &  10.09 & 6.59 & 5.27 & 3.82 & 2.78 & 2.01\\
\shortstack{Heatmap-MT (L+C)} &  9.27 & 6.35 & 5.62 & 3.75 & 3.14 & 2.23\\
\bottomrule
\end{tabular}
}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[h]
\begin{center}
\centering
\fboxsep=0mm
\fboxrule=1pt
{\includegraphics[width=0.35\textwidth]
{images/blocks_pred/1_kpts.png}}\\
\fboxsep=0mm
\fboxrule=1pt
{\includegraphics[width=0.35\textwidth]
{images/blocks_pred/2_kpts_class.png}}\\
\fboxsep=0mm
\fboxrule=1pt
{\includegraphics[width=0.35\textwidth]
{images/blocks_pred/3_trans_kpts.png}}\\
\fboxsep=0mm
\fboxrule=1pt
{\includegraphics[width=0.35\textwidth]
{images/blocks_pred/4_trans_kpts_class.png}}
\end{center}
\vskip-6pt
    \caption{Sample landmark prediction on blocks dataset using sequential multi-tasking models when only 5\% of data is labeled with landmarks. The rows shows the model predictions in this order from top to bottom: 1) Seq-MT (L), 2) Seq-MT (L+C), 3) Seq-MT (L+T), and 4) Seq-MT (L+T+C). The green cross shows the true landmark location and the red cross shows the model's prediction. Best viewed in color with zoom.
}
\label{fig:blocks_pred}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t]
\begin{center}
\includegraphics[width=0.47\textwidth]{images/heatmap_mt.pdf}
\end{center}
   \caption{Multi-tasking architecture using heatmaps (Heatmap-MT). Landmarks are detected using conv layers without sub-sampling or pooling layers. A softmax layer is used for landmark prediction in the output layer. Landmark heatmaps right before softmax layer are fed to a series of pool and conv layers which is then passed to FC layers. The last FC layer is fed to softmax for classification. All layers use relu non-linearity except the layers right before the softmax. The first two FC layers use dropout.}
\label{fig:paral_MT}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%



\section{Experiments}
To validate our proposed model, we begin with two toy datasets, Shapes and Blocks, in order to verify to what extent the class labels can be used to guide the landmark localization regardless of the complexity of the dataset. We show the results on these datasets in Sections \ref{sec:exp_shapes} and \ref{sec:exp_blocks}. Later, we evaluate the performance of our model on three real datasets, Polish sign-language dataset \cite{kawulok2014PolishHands} in Section \ref{sec:exp_hands}, and two fiducial points datasets: Multi-PIE \cite{gross2010multi} in Section \ref{sec:exp_MultiPIE}, and 300W \cite{sagonas2013300} in Section \ref{sec:exp_300W}. All the models are implemented in Theano \cite{2016arXiv160502688short}.
\input{exp_shapes} 
\input{exp_blocks}
\input{exp_hands}
\input{exp_multiPIE} 
\input{exp_300W} 


%!TEX root = paper.tex
\subsection{Hands Dataset}
\label{sec:exp_hands}
\begin{figure}[h]

\begin{tabular}{m{0.43\linewidth}m{0.47\linewidth}}
\setlength{\tabcolsep}{0pt}
{\includegraphics[trim={0cm 0cm 0cm 0cm},clip,width=1.0\linewidth]
{images/hands_kpts.jpg}}
& 
\begin{tabular}{cc}
\includegraphics[trim={0cm 0cm 26cm 0cm},clip,width=0.95\linewidth]
{images/hands_class_sample.png}
\\
\includegraphics[trim={11.2cm 0 14.8cm 0},clip,width=0.95\linewidth]
{images/hands_class_sample.png}
\end{tabular}

\end{tabular}
\caption{Left: 25 landmarks of the HGR1 hands dataset. Right: sample classes from the HGR1 dataset.}
%\vskip-8pt
\label{fig:hands_class}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[h]
\begin{tabular}{c}
{\includegraphics[width=0.45\textwidth]
{images/hands_3col.png}}
\\
\includegraphics[width=0.145\textwidth]
{images/img096_output.png}
\includegraphics[width=0.145\textwidth]
{images/img115_output.png}
\includegraphics[width=0.145\textwidth]
{images/img156_output.png}
\end{tabular}
\caption{Examples of our model predictions on the test set of the HGR1 dataset. GT represents ground-trust annotations, while numbers 100, 50, and 20 indicate which percentage of the training set with labeled landmarks used for training. Results are computed with Seq-MT (L+T+C) model (denoted *) and Seq-MT (L). Best viewed in color with zoom.}
\label{fig:hands_examples}
\end{figure}

Our first experiment on real data is landmark estimation on hands images captured with color sensors. Most common image datasets with landmarks on hands such as NYU \cite{tompson2014NYUhands} and ICVL \cite{tang2014ICVLhands} do not provide class labels. Also, most of the prior work in landmark estimation for hands is based on depth data \cite{tompson2014NYUhands, tang2014ICVLhands, sridhar2013interactive, sun2015cascaded, sinha2016deephand}. We use the Polish hand dataset (HGR1) \cite{kawulok2014PolishHands, nalepa2014PolishHands}, which provides 898 RGB images with 25 landmarks and 27 gestures from Polish sign language captured with uncontrolled lightning and uncontrolled background from 12 subjects.

Figure \ref{fig:hands_class} shows the correspondence of 25 landmarks with example gestures from different classes in the HGR1 dataset. Due to occlusion, different classes have different number of landmarks in range [17, 25]. To train the model properly, we mask out occluded landmarks during training and evaluation. The transformation cost, however, does not use the mask and will operate even on occluded landmarks. We divide images into train, validation and test sets by id (with no overlap in subjects between sets): train (ids 1 to 8), validation (ids 11 and 12), and test (ids 9 and 10). We end up with 573, 163, and 162 images for train, validation and test sets, respectively. 

\begin{table}[t!]
\caption{Performance of architectures on HGR1 hands dataset. The error is Euclidean distance normalized by wrist width. Results are shown as percent; lower is better.}
\label{tab:hands}
\vskip2pt
\centering
\resizebox{1.\linewidth}{!}{
\begin{tabular}{ccccccc}
\toprule
 & \multicolumn{6}{c}{\textbf{Percentage of Images with Labeled Landmarks}} \\ 
\cmidrule{2-7}
\textbf{Model} & 5\% & 10\% & 20\% & 50\% & 100\% \\
\midrule
Seq-MT (L) &	52.6 &	44.7 &	32.1 &	22.1 &	18.8 \\
Seq-MT  (L+C) &	50.0 &	40.2 &	31.4 &	22.3 &	\textbf{18.4} \\
Seq-MT  (L+T) &	45.4 &	32.1 &	25.8 &	\textbf{19.6} &\\
Seq-MT  (L+T+C) &\textbf{40.6} &	\textbf{32.0} &	\textbf{24.0} &	19.8 &\\
\midrule
Comm-MT (L) &	77.1 &	62.8 &	52.7 &	41.8 &	35.7\\
Comm-MT (L+C) &	53.4 &	39.3 &	35.5 &	26.9 &	24.1\\
\midrule
Heatmap-MT (L) &	66.5 &	51.9 &	42.4 &	30.9 &	25.5\\
Heatmap-MT (L+C) &	64.8 &	54.9 &	43.2 &	30.5 &	26.7\\
\bottomrule
\end{tabular}
}
\end{table}


We use the architecture in Fig.~\ref{fig:seq_model} on this dataset with 6 convolutional layers of size $9 \times 9$, each having $64$ maps, with the final layer producing $25$ maps for landmarks. The input image to the model is $64 \times 64$ and gray-scaled. 
The classification section has 3 FC layers of size $(256, 256, 27)$. The first two layers have relu non-linearities with 50\% dropout. 

For training, we use the following parameters: $\beta = 0.001$, $\alpha=0.3$, $\gamma=10^{-5}$, $\lambda=0.5$.

Accuracy of landmark detection on HGR1 dataset is measured by computing average RMSE metric in the image domain for every landmark and normalizing it by wrist width (the Euclidean distance between landmarks \#1 and \#25). We apply the transformation cost only on the images that do not have ground truth landmarks.

Table~\ref{tab:hands} shows results for landmark estimation for hands in the HGR1 test set\footnote{To train all these models we apply these data augmentations: rotation [-20, 20], and scale and translation [-10\%, 10\%] of the face bounding box.}.  All results were averaged over 5 seed, where at every seed we randomly select a subset of examples with provided landmarks. We observe: 1) applying sequential multitasking improves results for most experiments compared to using only landmarks (Seq-MT(L)) or other multi-tasking approaches, 2) the equivariant transformation cost (T) significantly improves results for all experiments, and 
3) \textit{Seq-MT (L+T+C)} compared to \textit{Seq-MT (L)} can achieve the same performance with only half provided landmark labels (see 5\%, 10\%, 20\%). 

We show examples of landmark prediction with different models in Figure~\ref{fig:hands_examples}. 
transformation cost and class labels significantly improve results given a smaller fraction of data with annotated landmarks.



%!TEX root = paper.tex
\begin{figure}[h]
\begin{center}
\centering
\fboxsep=0mm
\fboxrule=1pt
{\includegraphics[width=0.45\textwidth]
{images/multipie_test.png}}
\end{center}
\vskip-6pt
\caption{Examples of our model predictions on MultiPIE dataset. Models used, from top to bottom: 1) \textit{Seq-MT (L)} with $100\%$ labeled landmarks; 2) \textit{Seq-MT (L+T)} with $20\%$; 3) \textit{Seq-MT (L+T+C)} with $5\%$; 4) \textit{Seq-MT (L)} with $5\%$. We observe close predictions by 1) and 2) indicating the effectiveness of our method even with only a small amount of labeled landmarks. Comparison between 3) and 4) shows significant improvement with the proposed transformation cost (T). Best viewed in color with zoom.}
\label{fig:hands_examples}
%\vspace*{-0.9em}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Multi-PIE Dataset}
\label{sec:exp_MultiPIE}
We next evaluate our proposed approach on the task of facial landmark estimation. Similar to Hands, most common face datasets including 300W \cite{sagonas2013300}, Helen \cite{le2012Helen}, LFPW \cite{belhumeur2013LFPW}, and AFW \cite{zhu2012AFW} only provide landmark locations and no classes. MTFL \cite{zhang2014facialMTFL} and MAFL \cite{zhang2016MAFL} datasets provide attributes in addition to landmarks, however, these datasets have only 5 landmarks and it would be very difficult to infer the classes, e.g. gender or emotion, using just 5 landmarks, or to use those labels to guide the landmarks. 
Instead we choose Multi-PIE \cite{gross2010multi} since it provides 68 landmark locations in addition to 6 emotions and 15 camera locations under 20 illuminations. The labeled emotions are neutral, surprise, smile, squint, disgust, and scream. We use emotion and camera as class labels to guide landmark prediction. 
We use images from 5 cameras (1 frontal, 2 with $\pm$15 degrees, and 2 with $\pm$30 degrees) and in each case a random illumination is selected.  The images are then divided into subsets by id, with ids 1-150 in the train set, ids 151-200 in the valid set, and ids 201-337 in the test set. As a result, we have 1875, 579, and 1054 images in train, validation, and test sets, respectively. 

We train a similar architecture to that used for Hands with the difference of using 68 feature-maps in the last convolutional layer for landmarks. 
The classification section has two branches, one for emotion and one for camera pose, with each branch receiving landmarks as inputs. Each has 3 FC layers of size $(256,  256, \mathrm{\#\, of\, classes})$. The first two layers have relu non-linearities with 25\% dropout. For training we set the following parameters: $\beta = 0.001$, $\alpha=0.3$, $\gamma=10^{-5}$, $\lambda=2$. Networks are learned with ADAM optimizer for 1000 epochs with early stopping on validation error. We use gray-scale images and apply the same data augmentation as for Hands. We evaluate the performance of this model with the same settings as in the Hands experiment: 1) only landmarks (L), 2) landmarks plus class labels (L+C), 3) landmarks plus transformation cost (L+T), 4) and all three (L+T+C). Similar to the previous experiments, we do not apply the transformation cost on the examples that provide true landmark locations, and only apply it to images that do not provide landmark locations. 

We compare our model with Comm-MT, Heatmap-MT architectures with and without class labels in Table \ref{tab:multiPIE}. The error is Euclidean distance between true and predicted landmarks normalized by the distance between eye centers (inter-ocular distance). We observe: 1) the proposed Seq-MT outperforms other multitasking models with and without class labels, 2) in Seq-MT class labels improve the results for low percentage of labelled landmarks, 3) the transformation cost improves results considerably for all cases, further  confirming the benefit of using the proposed ELT technique on unlabeled data, and 4) the best performance is obtained when the transformation cost and the class labels are used jointly, indicating both techniques can be used together to get the least error.

\begin{table}[t]
\caption{Performance of different architectures on MultiPIE dataset. The error is Euclidean distance normalized by ocular distance (as a percent; lower is better). The first 4 rows show Sequential Multi-tasking results. The last two rows show common multi-tasking results.}
\label{tab:multiPIE}
\vskip2pt
\centering
\resizebox{1.\linewidth}{!}{
\begin{tabular}{cccccc}
\toprule
 & \multicolumn{5}{c}{\textbf{Percentage of Images with Labeled Landmarks}} \\
\cmidrule{2-6}
\textbf{Model} & 5\% & 10\% & 20\% & 50\% & 100\% \\
\midrule
Seq-MT (L) & 8.05	&	6.99 &	6.27 &	5.48 &	\textbf{5.07}\\
Seq-MT (L+C) &  7.81 & 6.95 & 6.20 & 5.50 & 5.13 \\
Seq-MT (L+T) &  6.69 & 6.24 & 5.78 & 5.27 & \\
Seq-MT (L+T+C) & \textbf{6.57} & \textbf{6.16} & \textbf{5.73} & \textbf{5.23} &\\
\midrule
Comm-MT (L) & 9.22	& 7.93 & 7.02	& 6.27	& 5.71\\
Comm-MT (L+C) & 9.11	& 8.00	& 6.92	& 6.20	& 5.68 \\
\midrule
\shortstack{Heatmap-MT (L)} & 10.83  & 9.18 & 8.13 & 7.00 & 6.63 \\
\shortstack{Heatmap-MT (L+C)} & 11.03  & 9.03 & 8.15 & 7.11 & 6.65 \\
%\midrule
%RCN \cite{honari2016recombinator} & 8.04& 7.03 & 6.44 & 5.99	& 5.78\\
\bottomrule
\end{tabular}
}
\end{table}

\begin{comment}
\begin{table}[t]
\caption{Landmark error on MultiPIE using different class labels. Rows 2 to 5 correspond to using different class labels for L+C case.}
\label{tab:multiPIE_class}
\vskip2pt
\centering
\resizebox{1.\linewidth}{!}{
\begin{tabular}{cccccc}
\toprule
 & \multicolumn{5}{c}{\textbf{Percentage of Images with Labeled Landmarks}} \\
\cmidrule{2-6}
\textbf{Model} & 5\% & 10\% & 20\% & 50\% & 100\% \\
\midrule
Seq-MT (L) &  8.05	&	6.99 &	6.27 &	\textbf{5.48} &	\textbf{5.07} \\
Seq-MT (L + id) &  8.24	&	7.04 &	6.26 &	5.54 &	5.15 \\
Seq-MT (L + emotion) &  7.99	&	7.05 &	6.28 &	5.52 & 5.10 \\
Seq-MT (L + camera) & 8.11 &	7.02 &	6.23 &	5.51 &	5.09 \\
Seq-MT (L + camera+emotion) & \textbf{7.83} & \textbf{6.96} & \textbf{6.20} & 5.49 & 5.11 \\
\bottomrule
\end{tabular}
}
\end{table}
\end{comment}

\begin{comment}
\begin{table}[h]
\caption{A comparison of detection rates (landmarks with average error less than 0.1) on MultiPIE for sets of both 68 and 20 facial landmarks, measured in percentages. Our \textit{Seq-MT (L+T+C)} model outperforms the Deep face shape model~\cite{wu2015discriminative} while using only $20\%$ of landmark labeled data for 68 landmarks. It also outperforms LEAR~\cite{martinez2013local} and Deep face~\cite{wu2015discriminative} on 20 landmark comparison. (*denotes the use of only 17 landmarks)}
\label{tab:multiPIE_comparison}
\vskip2pt
\centering
\resizebox{1.\linewidth}{!}{
\centering
\begin{tabular}{m{0.1cm}m{11.5cm}}%
\toprule
\begin{turn}{90}68 landmarks\end{turn} &
\begin{tabular}{cccccccc}
 & \multicolumn{7}{c}{\textbf{Percentage of Images with Labeled Landmarks}} \\ 
%\cline{2-6}
\cmidrule{2-8}
\textbf{Camera} & 5\% & 10\% & 20\% & 50\% & 100\% & 100\% & 100\%\\
%\midrule
%\multicolumn{6}{l}{Sequential multitasking: \textit{kpts only}} \\
%$0^o$  &$84.40\%$&	$89.01\%$&	$94.44\%$&	$97.28\%$&	$97.99\%$\\
%$15^o$ &$80.60\%$&	$87.31\%$&	$94.03\%$&	$97.76\%$&	$99.25\%$\\
%$30^o$ &$45.95\%$&	$67.57\%$&	$79.73\%$&	$89.19\%$&	$98.65\%$\\
\textbf{location} &\multicolumn{5}{c}{Seq-MT (L+T+C)} & Deep face~\cite{wu2015discriminative} & RLMS~\cite{saragih2011deformable} \\
\midrule
$0^o$  &$88.7$	&$89.8$	&$\mathbf{96.2}$	&$97.5$	&$98.1$&	$96.1$&$75.$\\
$15^o$ &$84.3$	&$88.1$	&$\mathbf{96.3}$	&$97.0$	&$99.3$&	$95.9$&-\\
$30^o$ &$56.8$&$64.9$	&$\mathbf{86.5}$	&$93.2$	&$96.0$&$79.7$&-\\
\end{tabular}
\\
\midrule
\begin{turn}{90}20 landmarks\end{turn} &
\begin{tabular}{cccccc}
& \multicolumn{5}{c}{\textbf{Percentage of Images with Labeled Landmarks}} \\
\cmidrule{2-6}
%\cline{2-6}
\textbf{Camera} & 20\% & 50\% & 100\% & 100\% & 100\%\\
\textbf{location} & \multicolumn{3}{c}{Seq-MT (L+T+C)} & LEAR~\cite{martinez2013local} & Deep face~\cite{wu2015discriminative}$^*$\\
\midrule
$0^o$  &$97.75$&	$98.35$&$\mathbf{98.94}$&	$98.0$&$98.6$\\
$15^o$ &$97.01$	&$97.76$	&$\mathbf{99.25}$&$96.0$&$97.56$\\
$30^o$ &$89.19$	&$95.95$	&$\mathbf{98.65}$&$65.0$&$75.36$\\
\end{tabular}
\\
\bottomrule
\end{tabular}
}
\end{table}
\end{comment}




\subsection{Shapes Dataset}
\label{sec:exp_shapes}
%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t]
\begin{center}
\includegraphics[width=0.47\textwidth]{images/shapes_samples.png}
   \caption{Sample images from the Shapes dataset. Each $60 \times 60$ image contains one square and one triangle with randomly sampled location, size, and orientation.}
\end{center}
\label{fig:shapes_samples}
\vspace*{-1.em}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%
To begin, we use a simple toy dataset to demonstrate our method's ability to learn consistent landmarks without direct supervision.
Images in our Shapes dataset (see Figure \ref{fig:shapes_samples} for examples) consist of two white shapes positioned on a black background.
The shapes, a triangle and a square, are each displayed with randomly sampled size, location, and orientation.
The classification task is to identify which shape, $0$ for triangle and $1$ for square, is positioned closer to the upper-left corner of the image.

In keeping with the architecture in Figure \ref{fig:seq_model}, we trained a model with six convolutional layers with $7 \times 7$ kernels, followed by two convolutional layers with $1 \times 1$ kernels, then the soft-argmax (with $\beta=1$) for landmark localization.\footnote{We also experimented with dilated convolutions, but found that landmarks were predicted with noticeably less precision.}
We maintain sixteen feature maps per layer, until the final layer where two landmark feature maps are predicted.
Following the soft-argmax, predicted landmarks input to two fully connected (FC) layers of size $40$ and $2$, respectively.
The model is trained with \emph{only} the cross-entropy cost on the class label \emph{without} labeled landmarks or the unsupervised transformation cost.

Figure \ref{fig:shapes_pred} shows the predictions of the trained model on a few samples from the dataset. In the first row, the green shape corresponds to the shape predicted to be the nearest to the upper-left corner, which was learned with $99\%$ accuracy.
The red and blue crosses correspond to the first soft-argmax and second soft-argmax landmark localizations, respectively.
We observe that the red cross is consistently placed adjacent to the triangle, while the blue cross is near the square. The input maps to each soft-argmax (shown in rows 2 and 3) also clearly indicate the location of a consistent shape.
This experiment shows the sequential architecture proposed here properly guides the first part of the network to find meaningful landmarks on this dataset, based solely on the supervision of the related classification task.
%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t]
\begin{center}
\centering
\fboxsep=0mm
\fboxrule=1pt
{\includegraphics[width=0.06\textwidth]{images/shapes_predict/predict_1.png}}
\fboxsep=0mm
\fboxrule=1pt
{\includegraphics[width=0.06\textwidth]
{images/shapes_predict/predict_2.png}}
\fboxsep=0mm
\fboxrule=1pt
{\includegraphics[width=0.06\textwidth]
{images/shapes_predict/predict_3.png}}
\fboxsep=0mm
\fboxrule=1pt
{\includegraphics[width=0.06\textwidth]
{images/shapes_predict/predict_4.png}}
\fboxsep=0mm
\fboxrule=1pt
{\includegraphics[width=0.06\textwidth]
{images/shapes_predict/predict_5.png}}\\
\vspace{.1em}
\fboxsep=0mm
\fboxrule=1pt
{\includegraphics[width=0.06\textwidth]
{images/shapes_predict/predict_1_tri.png}}
\fboxsep=0mm
\fboxrule=1pt
{\includegraphics[width=0.06\textwidth]
{images/shapes_predict/predict_2_tri.png}}
\fboxsep=0mm
\fboxrule=1pt
{\includegraphics[width=0.06\textwidth]
{images/shapes_predict/predict_3_tri.png}}
\fboxsep=0mm
\fboxrule=1pt
{\includegraphics[width=0.06\textwidth]
{images/shapes_predict/predict_4_tri.png}}
\fboxsep=0mm
\fboxrule=1pt
{\includegraphics[width=0.06\textwidth]
{images/shapes_predict/predict_5_tri.png}}\\
\vspace{.1em}
\fboxsep=0mm
\fboxrule=1pt
{\includegraphics[width=0.06\textwidth]
{images/shapes_predict/predict_1_sqr.png}}
\fboxsep=0mm
\fboxrule=1pt
{\includegraphics[width=0.06\textwidth]
{images/shapes_predict/predict_2_sqr.png}}
\fboxsep=0mm
\fboxrule=1pt
{\includegraphics[width=0.06\textwidth]
{images/shapes_predict/predict_3_sqr.png}}
\fboxsep=0mm
\fboxrule=1pt
{\includegraphics[width=0.06\textwidth]
{images/shapes_predict/predict_4_sqr.png}}
\fboxsep=0mm
\fboxrule=1pt
{\includegraphics[width=0.06\textwidth]
{images/shapes_predict/predict_5_sqr.png}}
\end{center}
\vskip-6pt
    \caption{Sample classification and landmark predictions from the Shapes dataset. The \emph{top row} shows the two predicted landmarks, indicated by red and blue crosses, respectively. Without any explicit landmark supervision, the model has learned to associate the first landmark (red) with the triangle and the second landmark (blue) with the square. Meanwhile, the green shapes indicate the model has correctly classified the type of shape that is closer to the top-left corner of the image.
    The \emph{second and third rows} show the first and second landmark feature maps, respectively, corresponding closely with the location of the respective shapes. (Best viewed in color with zoom.)
}
\label{fig:shapes_pred}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[t!]
\begin{center}
\includegraphics[width=1\textwidth]{images/seq_arch_joint2.png}
\end{center}
\vskip-7pt
   \caption{The architecture of the sequential multi-tasking model. {\bf Top half:} basic sub-components of a sequential multi-tasking model. The first part of the network predicts the landmarks which is then fed into the second part of the network for classification. {\bf Bottom half:}  the specific architecture used in this paper. The landmark localization network is composed of several convolutional layers that preserve the image resolution without any pooling layers. A large kernel size is used to have a receptive field covering the entire image. The final layer generates as many maps as the number of landmarks. A soft-argmax is then applied on the output of the landmark prediction model. The classification network is composed of three fully connected (FC) layers whose final output is fed into a softmax. All FC layers, except the last one use dropout. All layers use rectified linear units (ReLU) non-linearity except the layer right before soft-argmax and the last FC layer right before softmax. These two layers have no non-linearities.}
\label{fig:seq_model}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Landmark detection is a central step when solving many complex vision problems. Examples include emotion recognition \cite{kahou2013combining}, face identity verification \cite{taigman2014deepface, sun2014deep}, hand tracking \cite{hu2010hand, datcu2013free},  gesture recognition \cite{dardas2010hand}, and eye gaze tracking \cite{zhang2015appearance, mora2012gaze}. Reliable landmark estimation can simplify solutions to these problems and is often part of the pipeline for sophisticated, robust vision tools. Neural networks have recently yielded state-of-the art results on numerous landmark estimation problems \cite{tompson2015efficient, honari2016recombinator, xiao2016robust, yu2016deep}. However, neural networks generally need to be trained on a large set of labeled data to be robust to the variations in natural images. Landmark labeling is a tedious manual work where precision is important; as a result, few landmark datasets are large enough to train reliable deep neural networks. On the other hand it is much easier to label an image with a single class label rather than the entire set of precise landmarks, and datasets with labels related to---but distinct from---landmark detection are far more abundant.

In this paper, we use the indirect supervision of class labels to guide classifiers trained to localize landmarks. The class label can be considered a weak label that sends indirect signals about landmarks. For example, a photo of a hand gesture with the label ``waving'' likely indicates that the hand is posed with an open palm and spread fingers, signaling a set of reasonable locations for landmarks on the hand. We leverage class labels that are more abundant or more easily obtainable than landmark labels, putting our proposed method in the category of multi-task learning. A common approach \cite{zhang2014facialMTFL, zhang2016MAFL, zhang2014improving, devries2014multi} to multi-task learning uses a traditional convolutional network, in which  a final common fully-connected (FC) layer feeds into separate branches, each dedicated to the output for a different task. This approach learns shared low-level features across the set of tasks and acts as a regularizer, particularly when the individual tasks have few labeled samples.

There is a fundamental caveat to applying such an approach directly to simultaneous classification and landmark localization tasks, because the two have opposing requirements: classification output needs to be insensitive (invariant) to small deformations such as translations, whereas landmark localization needs to be equivariant to them, i.e. follow them precisely with high sensitivity. To build in invariance, traditional convolutional neural networks for classification problems relied on pooling layers to integrate signals across the input image. However, tasks such as landmark localization or image segmentation require both the global integration of information as well as an ability to retain local, pixel-level details for precise localization.
The goal of producing precise landmark localization has thus led to the development of new layers and network architectures such as dilated convolutions \cite{yu2015multi}, stacked what-where auto-encoders \cite{Zhao2015StackedWA}, recombinator-networks \cite{honari2016recombinator}, fully-convolutional networks \cite{long2015fully}, and hyper-columns \cite{hariharan2015hypercolumns}, each preserving pixel-level information.
These models have however not been developed with multi-tasking in mind.

Current multi-task architectures that predict landmark locations \cite{zhang2014facialMTFL, zhang2016MAFL, zhang2014improving, devries2014multi, ranjan2016hyperface} predict them at the final layer, i.e. \emph{in parallel} with and at the same stage as classification. 
Many tasks ranging from emotion prediction, face identification, and gesture recognition to head-pose prediction can benefit from using landmarks as information \emph{input} to a model.
However, collecting labeled data for landmarks is almost always expensive compared to collecting data for a related classification task.  
\emph{Therefore we propose a novel class of neural architectures which force classification predictions to flow through the intermediate step of landmark localization.}

\textbf{
The main aim of our model is to
leverage auxiliary classification tasks and data, enhancing landmark localization by backpropagating classification errors through the landmark localization layers of the model.} 
Specifically, we propose a sequential architecture in which the first part of the network predicts landmarks via pixel-level heatmaps, maintaining high-resolution feature maps by omitting pooling layers and strided convolutions. The second part of the network computes class labels using predicted landmark locations. To make the whole network differentiable, we use soft-argmax for extracting landmark locations from pixel-level predictions.
Under this model, learning the landmark localizer is more directly influenced by the task of predicting class labels, allowing the classification task to enhance landmark localization learning.

Semi-supervised learning techniques \cite{salimans2016improved, rasmus2015semi, weston2012deep} have been used in deep learning to improve classification accuracy with a limited amount of labeled training data. A recently proposed method \cite{laine2016temporal} creates an implicit temporal ensemble of models by capturing models at different stages of training. The method penalizes variation in class predictions over time and across a variety of data augmentations applied to unlabeled training data, thus leveraging unlabeled data to learn invariant features and more robust models. 
Rather than penalizing variations in class predictions over time, here we also propose and explore an unsupervised learning technique for landmark localization where the model is asked to produce landmark localizations equivariant with respect to a set of transformations applied to the image. In other words, we transform an image during training and ask the model to produce landmarks that are similarly transformed. Importantly, this technique does not require the true landmark locations, and thus can be applied during semi-supervised training to leverage images with unlabeled landmarks.

In this paper we make the following contributions:
1) We propose a novel multi-tasking neural architecture, which a) predicts landmarks as a complete intermediate step before classification in order to use the class labels to improve landmark location learning, b) correspondingly maintains the resolution of feature maps for accurate landmark prediction, and c) uses soft-argmax for a fully-differentiable model in which end-to-end training can be performed with backpropogation, even from examples that do not provide labeled landmarks.
2) We also propose an unsupervised learning technique to learn features that are equivariant with respect to transformations applied to the input image. This technique can be applied on data that lack true landmark annotations.
Combining contributions 1) and 2), we propose a robust landmark estimation technique which learns effective landmark predictors while requiring fewer labeled landmarks compared to current approaches.



\section{Equivariant Landmark Transformation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t!]
\begin{center}
\includegraphics[width=0.5\textwidth]{images/transformer_net3.pdf}
\end{center}
\vskip-10pt
   \caption{The equivariant landmark transformation (ELT) technique. Image $I$ is transformed by affine transformation $T$ to generate image $I'$. The same transformation is applied to predicted landmarks $L$ of image $I$ to generate $\hat{L}$. The model is encouraged to minimize the squared euclidean distance between $\hat{L}$ and the predicted landmarks ${L'}$ of the transformed image ${I'}$. Both networks share the same parameters. Note that this technique can be applied even on images that do not have ground truth landmarks.}
\label{fig:transform}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
To further make the model's prediction consistent with respect to different transformations that are applied to the image, we propose the following unsupervised learning technique. Consider an input image $I$ and the corresponding landmarks $L(I)$ predicted by the network. Now consider a small affine coordinate transformation $T$. We will use $T \odot \ldots$ to denote the application of such a transformation in coordinate space, whether applied to deform an bitmap image or to transform actual coordinates. If we apply this transformation to produce deformed image $I'=T \odot I$ and compute the resulting landmark coordinates $L(I')$ predicted by the network, they should be very close to the result of applying the transformation on landmark coordinates $L(I)$ i.e. we expect to have $L(T\odot I) \approx T \odot L(I)$.

To encourage this consistent behavior and increase robustness of the model's landmark prediction we thus define the following transformation cost: 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
\mathrm{C}_T = \frac{\alpha}{N K}  \sum_{I\in\mathcal{D}}  \sum_{k=1}^{K} \| \underbrace{T \odot L_k(I)}_{\hat{L}_k} - \underbrace{L_k(T \odot I}_{L'_k}) \|_{2}^{2}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
where $\alpha$ is a hyper-parameter controlling the strength of this additional regularization. The architecture for this technique, that we call \emph{equivariant landmark transformation (ELT))} technique is illustrated in Figure \ref{fig:transform}. Multiple instances of $C_T$ can thus be added to the overall training cost, each corresponding to a different transformation $T$\footnote{In all our experiments we used the same transformations (scaling, translation, rotation) and level of transformation as we did when pre-processing the data.}. Note that this can be applied even on images that do not provide labeled landmarks. 


\section{Sequential Multi-Tasking}
\label{sec:model_seq}
We refer to the new architecture that we propose here for leveraging class labels to guide the learning of landmark locations as \emph{sequential multi-tasking}. This architecture first predicts the landmark locations and then uses the predicted landmarks as the input to the second part of the network that does classification (see Figure \ref{fig:seq_model}). In doing so, we create a bottle-neck in the network, forcing it to solve the classification task only through the landmarks. If the goal were to enhance classification, this architecture would have been harmful since such bottle-necks \cite{he2016deep} would hurt the flow of information for classification. However, since our goal is landmark localization, this architecture enforces receiving signal from class labels through back-propagation to enhance landmark locations. This architecture can be applied whenever class labels can reliably be categorized when only considering the landmarks, without observing the image. 

In order to make the whole pipeline trainable end-to-end, even on examples that do not provide any landmarks, we apply soft-argmax on the output of the last convolutional layer in the landmark prediction model. Specifically, let $M(I)$ the stack of $K$ two-dimensional output maps produced by the last convolutional layer (with no non-linearity) for a given network input image $I$. The map associated to the $k^{th}$ landmark will be denoted $M_k(I)$. To obtain from it a single 2d location $L_k=(x,y)$ for the landmark we use the following soft-argmax operation:
%
\begin{eqnarray}
L_k(I) &=& \textrm{soft-argmax}(\beta M_k(I)) \\
&=& \sum_{i,j} \mathrm{softmax}(\beta M_k(I))_{i,j} (i,j) 
\end{eqnarray}
%
where softmax denotes a spatial softmax of the map, i.e. $\mathrm{softmax}(A)_{i,j} = \frac{\exp(A_{i,j})}{\sum_{i',j'} \exp(A_{i',j'})}$. $\beta$ controls the softmax temperature, thus the peakiness of the resulting probability map, and $(i,j)$ iterate over pixel coordinates. 
In short soft-argmax computes landmark coordinates $L_k = (x,y)$ as a weighted average of all pixel coordinate pairs $(i,j)$ where the weights are given by a softmax of landmark map $M_k$.

Predicted landmark coordinates are then fed into the second part of the network for classification. 
We will denote $P(\mathbb{C} = c | {\mathbb{I}}  = {I})$ the probability ascribed by the model to the class $c$ given input image $I$, as computed by the final classification softmax layer.

Using soft-argmax, as opposed to a simple argmax, the model is fully differentiable through its landmark locations and is trainable end-to-end. When the model is trained to maximize the log probability of the true class labels in the second part of the network, this also guides the landmark locations through its gradients and corresponding parameter updates in the first part of the network. 

A more standard end-to-end differentiable model might use a couple of convolution and pooling layers followed by fully connected layers before generating real-valued $(x,y)$ key-point locations directly. But such an architecture gets sub-optimal results as the pooling layers do not maintain the location of the max-pooled features and this deteriorates the results on models that need fine-grained pixel level accuracy. This is our motivation for not using any pooling layers in the first part of the network that performs landmark localization. Our entire model is trained end-to-end to minimize the following cost:
%
\begin{eqnarray}
\mathrm{C}  =  & \frac{1}{N} \sum_{(I,\tilde{L},c) \in \mathcal{D}} \left( - \log P(\mathbb{C} = c | {\mathbb{I}}  = {I}) \, + \right. \nonumber \\
&  \lambda \frac{1}{K} \sum_{k=1}^{K} \left. || \tilde{L}_k - L_k(I) ||_{2}^{2} \right) + \gamma || \mathbb{W} ||_{2}^2,
\end{eqnarray}
%
where $\mathcal{D}$ is the training set containing $N$ triples $(I,\tilde{L},c)$ of input image, labeled landmarks (when available), and true class. $K$ is the number of landmarks. $\tilde{L}_k$ and $L_k(I)$ respectively correspond to the true and predicted landmark location.
$\mathbb{W}$ represents the parameters of the model. 
The first part of the cost is the negative log likelihood on the class labels and affects the entire network. The second part is the squared Euclidean distance between true and estimated landmark locations and is used only when landmark labels are provided. This cost only affects the first part of the network. The last part of the cost is the $\ell_2$-norm on the model's parameters (weight-decay regularization).


\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{slashbox}
\usepackage{array,multirow}
\usepackage{booktabs}
\usepackage{comment}
\usepackage{rotating}
\usepackage{verbatim}
\usepackage{stackengine}
\usepackage[utf8]{inputenc}
\usepackage{color}
\usepackage{xr} % Cross Ref: to allow references between main paper and supplementary
\externaldocument{supplementary}

\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% author notes
\usepackage[dvipsnames,svgnames,x11names]{xcolor}
\def\authornote#1#2#3{{\textcolor{#2}{\textsl{\small#1:[*#3*]}}}}
\newcommand{\shnote}[1]{\authornote{SH}{Cyan}{#1}} % Sina
\newcommand{\pmnote}[1]{\authornote{PM}{Green}{#1}} % Pavlo
\newcommand{\stnote}[1]{\authornote{ST}{Blue}{#1}} % Stephen
\newcommand{\jknote}[1]{\authornote{JK}{Red}{#1}} % Jan
\newcommand{\cpnote}[1]{\authornote{JG}{Brown}{#1}} % Chris
\newcommand{\pvnote}[1]{\authornote{JG}{Orange}{#1}} % Pascal

\begin{document}

%%%%%%%%% TITLE
\title{Improving Landmark Localization with Semi-Supervised Learning}

\author{Sina Honari${^1}$\thanks{Part of this work was done when author was at NVIDIA Research} \,, Pavlo Molchanov${^2}$, Stephen Tyree${^2}$, Pascal Vincent${^{1,4}}$, Christopher Pal${^3}$, Jan Kautz${^2}$\\
 ${^1}$University of Montreal, ${^2}$NVIDIA, ${^3}$Ecole Polytechnique of Montreal, ${^4}$CIFAR\\
{\tt\small ${^1}$\{honaris, vincentp\}@iro.umontreal.ca,} \\
{\tt\small ${^2}$\{pmolchanov, styree, jkautz\}@nvidia.com, ${^3}$christopher.pal@polymtl.ca}}

\maketitle
\pagenumbering{gobble}
\pagenumbering{arabic}


%%%%%%%%% ABSTRACT
\input{abstract}
%%%%%%%%% BODY TEXT
\input{Introduction} 
\input{model_seq} 
\input{model_equivar} 
%%%%%%%%% Experiments
\input{experiments}
%%%%%%%%% Conclusion
\input{conclusion} 
\input{acknowledgment}

{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\clearpage
%%%%%%%%%%%%%%%%%%%%%%%%%%
% Supplimentort Material %
%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{supplementary}
\end{document}



%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%
\twocolumn[{%
 \centering
\textbf{\Large Supplementary Information for \\Improving Landmark Localization with Semi-Supervised Learning\\}
\vspace{3. em}
}]

\subsection{Extra Experiments on Multi-PIE Dataset}
In Table~\ref{tab:multiPIE_class} we show how much the landmark localization accuracy changes using different classes in MultiPIE dataset. We find camera and emotion to be better helping on landmark localization compared to id and therefore we use these two classes for our experiments in Section \ref{sec:exp_MultiPIE}.

Although the focus of this paper is on improving landmark localization, in order to observe the impact of each multi-tasking approach on classification accuracy, we report the classification results on emotion in Table \ref{tab:multiPIE_emotion} and on camera in Table \ref{tab:multiPIE_camera}. Results show that the classification accuracy improves by providing more labeled landmarks, despite having the number of $<$image, class label$>$ pairs unchanged. 
It indicates that improving landmark localization can directly impact the classification accuracy.
Landmarks are especially more helpful in emotion classification. On camera classification, the improvement is small and all models are getting high accuracy. Another observation is that Heatmap-MT performs better on classification tasks compared to the other two multi-tasking approaches. We believe this is due to passing high-level features to the classification network, which can pass more information from the image to the classification network compared to Seq-MT. However, this model is performing worse than Seq-MT on landmark localization. The Seq-MT model benefits from the landmark bottleneck to improve its landmark localization accuracy, which is the focus of this paper. In Tables \ref{tab:multiPIE_emotion} and \ref{tab:multiPIE_camera} by adding the transformation cost the classification accuracy improves (in addition to landmarks) indicating the improved performance in landmark localization can enhance classification performance. 

Figure \ref{fig:MultiPIE_examples_2} provides further localization examples on MultiPIE dataset. It illustrates the improved accuracy obtained with the proposed sequential mutli-tasking and ELT components when providing a small percentage of labeled landmarks.

\subsection{Extra Experiments on hands Dataset}
In Table \ref{tab:hands_class} we show classification accuracy obtained using different multi-tasking techniques. Similar to the MultiPIE dataset, we observe increased accuracy by providing more labeled landmarks, showing the classification would benefit directly from landmarks. Also similar to MultiPIE, we observe better classification accuracy with Heatmap-MT. Comparing Seq-MT models, we observe improved classification accuracy by using the transformation cost. It demonstrates the impact of this component on both landmark localization and classification accuracy.

Figure \ref{fig:hand_examples_2} provides further landmark localization examples on hands dataset.

\subsection{Extra Information on 300W Dataset}
In Figure \ref{fig:RCN} we show the architecture of \textit{RCN $\stackanchor{+}{}$} 
used for 300W dataset. In Figure \ref{fig:300W_examples_supp} we illustrate further samples from 300W dataset. The samples show the improved accuracy obtained in both \textit{Seq-MT} and \textit{RCN $\stackanchor{+}{}$} by using the ELT technique.

%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[t]
\caption{Landmark error on MultiPIE using different class labels. Rows 2 to 8 correspond to using different class labels for L+C case. Camera and emotion classes on average improve better the landmark localization compared to id.}
\label{tab:multiPIE_class}
\vskip2pt
\centering
\resizebox{1.\linewidth}{!}{
\begin{tabular}{cccccc}
\toprule
 & \multicolumn{5}{c}{\textbf{Percentage of Images with Labeled Landmarks}} \\
\cmidrule{2-6}
\textbf{Model} & 5\% & 10\% & 20\% & 50\% & 100\% \\
\midrule
Seq-MT (L) &  8.05	&	6.99 &	6.27 &	5.48 &	\textbf{5.07} \\
\midrule
Seq-MT (L + id) &  8.36	& 7.03 & 6.24 &	5.53 &	5.14 \\
Seq-MT (L + emotion) & 7.86 & \textbf{6.94} & 6.22 &	5.49 & 5.16 \\
Seq-MT (L + camera) & 7.80 & 6.96 &	6.26 &	\textbf{5.44} &	\textbf{5.11} \\
Seq-MT (L + camera+emotion) & 7.81 & 6.95 & \textbf{6.20} & 5.50 & 5.13 \\
Seq-MT (L + id+emotion) &  7.99 & 6.99 & 6.27 &	5.54 & 5.18 \\
Seq-MT (L + id+camera) &  8.32 & 7.07 & 6.27 &	5.50 & 5.15 \\
Seq-MT (L + id+emotion+camera) &  \textbf{7.78} & 7.00 & 6.27 &	5.59 & 5.16 \\
\bottomrule
\end{tabular}
}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[t]
\caption{Emotion classification error on MultiPIE dataset. In percent; higher is better.}
\label{tab:multiPIE_emotion}
\vskip2pt
\centering
\resizebox{1.\linewidth}{!}{
\begin{tabular}{cccccc}
\toprule
 & \multicolumn{5}{c}{\textbf{Percentage of Images with Labeled Landmarks}} \\
\cmidrule{2-6}
\textbf{Model} & 5\% & 10\% & 20\% & 50\% & 100\% \\
\midrule
\midrule
Comm-MT (L+C) &  74.67 & 79.90 & 83.76 & 86.37 & 86.83 \\
Heatmap-MT (L+C) & \textbf{85.14} & \textbf{87.50} & \textbf{86.93} & \textbf{88.16} & \textbf{87.29} \\
\midrule
Seq-MT (L+C) &  78.78 & 82.62 & 84.69 & 84.03 & 84.86 \\
Seq-MT (L+C+T) &  82.90 & 84.57 & 84.85 & 86.48 & \\
\end{tabular}
}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[t]
\caption{Camera classification error on MultiPIE dataset. In percent; higher is better.}
\label{tab:multiPIE_camera}
\vskip2pt
\centering
\resizebox{1.\linewidth}{!}{
\begin{tabular}{cccccc}
\toprule
 & \multicolumn{5}{c}{\textbf{Percentage of Images with Labeled Landmarks}} \\
\cmidrule{2-6}
\textbf{Model} & 5\% & 10\% & 20\% & 50\% & 100\% \\
\midrule
Comm-MT (L+C) &  96.98 & 97.53 & 98.30 & 98.63 & 98.80 \\
Heatmap-MT (L+C) &  \textbf{98.46} & \textbf{98.99} & \textbf{98.99} &	\textbf{98.98} & \textbf{98.98} \\
\midrule
Seq-MT (L+C) &  97.97 & 98.31 &	98.50 &	98.96 & 98.92 \\
eq-MT (L+C+T) &  98.41 & 98.53 &	98.47 &	98.43 & \\
\end{tabular}
}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[t]
\caption{Classification error on hands dataset. In percent; higher is better.}
\label{tab:hands_class}
\vskip2pt
\centering
\resizebox{1.\linewidth}{!}{
\begin{tabular}{cccccc}
\toprule
 & \multicolumn{5}{c}{\textbf{Percentage of Images with Labeled Landmarks}} \\
\cmidrule{2-6}
\textbf{Model} & 5\% & 10\% & 20\% & 50\% & 100\% \\
\midrule
Comm-MT (L+C) &  60.86 & 69.64 & 69.20 & 76.03 & 73.42 \\
Heatmap-MT (L+C) & \textbf{83.74} & \textbf{87.86} & \textbf{87.55} & \textbf{90.29} & \textbf{89.27} \\
\midrule
Seq-MT (L+C) &  69.08 & 70.14 & 72.26 & 77.07 & 75.92 \\
Seq-MT (L+C+T) &  74.64 & 75.01 & 73.90 & 79.10 & \\
\end{tabular}
}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[h]
\begin{center}
\begin{tabular}{c}
\includegraphics[width=0.95\linewidth]{images/tile_multiPIE_1.jpg}
\vspace{.5em}
\\
\includegraphics[width=0.95\linewidth]{images/tile_multiPIE_2.jpg}
\vspace{1.5em}
\end{tabular}
   \caption{Extra examples of our model predictions on MultiPIE dataset. Models used, from top to bottom: 1) \textit{Seq-MT (L)} with $100\%$ labeled landmarks; 2) \textit{Seq-MT (L+T)} with $20\%$; 3) \textit{Seq-MT (L+T+C)} with $5\%$; 4) \textit{Seq-MT (L)} with $5\%$. We observe close predictions by 1) and 2) indicating the effectiveness of our proposed transformation cost even with only a small amount of labeled landmarks.  Comparison between 3) and 4) shows the improvement obtained with both the ELT technique and the sequential multitasking architecture when using a small percentage of labeled landmarks. Note that the model trained with ELT technique preserves better the joint distribution over the landmarks even with a small number of labeled landmarks. The last two examples on the bottom row show examples with high errors. Best viewed in color with zoom.}
\label{fig:MultiPIE_examples_2}
\end{center}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[h]
\begin{center}
\begin{tabular}{c}
\includegraphics[width=0.95\linewidth]{images/tile_hands_1.jpg}
\vspace{.5em}
\\
\includegraphics[width=0.95\linewidth]{images/tile_hands_2.jpg}
\vspace{.5em}
\\
\includegraphics[width=0.95\linewidth]{images/tile_hands_3.jpg}
\vspace{1.5em}
\end{tabular}
   \caption{Extra examples of our model predictions on the test set of the HGR1 dataset. GT represents ground-trust annotations, while numbers 100, 50, and 20 indicate the percentage of the training set with labeled landmarks. Results are computed with Seq-MT (L+T+C) model (denoted *) and Seq-MT (L). Examples illustrate improvement of the landmark prediction by using the class label and the transformation cost in addition to the labeled landmarks. The last three examples on the bottom row show examples with high errors. Best viewed in color with zoom.}
\label{fig:hand_examples_2}
\end{center}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[t]
\begin{center}
\includegraphics[width=.99\textwidth]{images/RCN_short.pdf}
\end{center}
   \caption{The ReCombinator Networks (RCN) \cite{honari2016recombinator} architecture used for experiments on 300W dataset. P indicates a pooling layer. All pooling layers have stride of 2. C indicates a convolutional layer. The number written below C indicates the convolution kernel size. All convolutions have stride of 1. U indicates an upsampling layer, where each feature map is upsampled to the next (bigger) feature map resolution. K indicates concatenation, where the upsampled features are concatenated with features of the same resolution before a pooling is applied to them. The dashed arrows indicate the feature maps are carried forward for concatenation. The solid arrows following each other, e.g. P, C, indicate the order of independent operations that are applied. The number written above feature maps in $n@w \times h$ format indicate number of feature maps $n$ and the width $w$ and height $h$ of the feature maps.}
\label{fig:RCN}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[t]
\begin{center}
\includegraphics[width=.99\textwidth]{images/tiled_300W.jpg}
\end{center}
   \caption{Extra examples of our model predictions on 300W test-set. Models used, from top to bottom: 1) \textit{Seq-MT (L)}; 2) \textit{Seq-MT (L+T)}; 3) \textit{RCN $\stackanchor{+}{}$(L)}; 4) \textit{RCN $\stackanchor{+}{}$(L+T)}.The first two columns depict examples where all models get accurate predictions, The next 5 columns illustrate the improved accuracy obtained by using ELT technique in two different architectures (Seq-MT and RCN). The last two columns show difficult examples where error is high. The rectangles indicate the regions that landmarks are mostly affected. The green and red dots show ground truth (GT) and model predictions (MP), respectively. The yellow lines show the error by connecting GT and MP. Note that the ELT technique improves predictions in both architectures. Best viewed in color with zoom.}
\label{fig:300W_examples_supp}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%



\documentclass[11pt]{article}

\usepackage[utf8]{inputenc}
\usepackage{icml2017,palatino} 
% use Times
\usepackage{times}
% For figures
\usepackage{graphicx} % more modern
\usepackage{float}
%\usepackage{epsfig} % less modern
%\usepackage{subfigure} 
\usepackage{subfig} 
\usepackage{enumitem}
% For citations
\usepackage{natbib}

\usepackage{ulem}
\usepackage{verbatim}


% For algorithms
\usepackage{algorithm}
%\usepackage{algorithmic}
\usepackage[noend]{algpseudocode}

\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{hyperref}
\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}


% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Proceedings of the...''
%\usepackage[accepted]{icml2017}
\title{Independently Controllable Features}

\author{
Emmanuel Bengio \\
McGill University\\
\texttt{ebengi@cs.mcgill.ca} \\
\And
Valentin Thomas \\
cole Polytechnique Fdrale de Lausanne\\
\texttt{valentin.thomas@epfl.ch} \\
\AND
Joelle Pineau \\
McGill University \\
\texttt{jpineau@cs.mcgill.ca} \\
\And
Doina Precup \\
McGill University \\
\texttt{dprecup@cs.mcgill.ca} \\
\And
Yoshua Bengio\\
CIFAR Senior Fellow, Universit de Montral \\
\texttt{yoshua.bengio@umontreal.ca} \\
}
\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document} 

\maketitle

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2017
% package.

% list of affiliations. the first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country


%\icmlcorrespondingauthor{Cieua Vvvvv}{c.vvvvv@googol.com}
%\icmlcorrespondingauthor{Eee Pppp}{ep@eden.co.uk}

% You may provide any keywords that you 
% find helpful for describing your paper; these are used to populate 
% the "keywords" metadata in the PDF but will not be shown in the document

%\vskip 0.3in

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.
%\footnotetext{hi}

\begin{abstract} 
%original abstract: Finding good ways of disentangling explanatory factors of variation of real-world data is an open question. Interactive environments offer an opportunity in that respect, because they provide the opportunity for an agent to test the effect of its actions on its environment. We introduce the idea that a strong clue for disentangling such latent factors is that some of them are controllable by an agent. We propose a naive method to find factors that explain/measure the effect of the actions of a learner, and test whether they yield good learned representations in the sense of separating out some of the underlying explanatory factors.
%Finding good features to represent real-world data is an open question.
Finding features that disentangle the different causes of variation in real data is a difficult task, that has nonetheless received considerable attention in static domains like natural images. Interactive environments, in which an agent can deliberately take actions, offer an opportunity to tackle this task better, because the agent can experiment with different actions and observe their effects. We introduce the idea that in interactive environments, latent factors that control the variation in observed data can be identified by figuring out what the agent can control. We propose a naive method to find factors that explain or measure the effect of the actions of a learner, and test it in illustrative experiments.
\end{abstract} 

\keywords{representation learning, controllable features}
\acknowledgements{The authors gratefully acknowledge financial support for this work by the Samsung Advanced Institute of Technology (SAIT), the Canadian Institute For Advanced Research (CIFAR), as well as the Natural Sciences and Engineering Research Council of Canada (NSERC).}  

\startmain

\section{Introduction}
Whether in static or dynamic environments, decision making for real world problems is often confronted with the hard challenge of finding a ``good'' representation of the problem. %What makes features ``good'' is often then simply whether the associated problem is correctly solved. Such a definition is problematic if only the environment is known in advance, and not the tasks.
In the context of supervised or semi-supervised learning, it has been argued~\citep{Bengio-2009-book} that good representations separate out underlying explanatory factors, which may be causes of the observed data.
%Given only an environment, it is desirable to learn representations that are generally useful across task domains, especially if those tasks are not well known in advance. 
In such problems, feature learning often involves  mechanisms such as  autoencoders \citep{hinton2006reducing}, which find latent features that explain the observed data. In interactive environments, the temporal dependency between successive observations creates a new opportunity to notice structure in data which may not be apparent using only observational studies. The need to experiment in order to discover causal structures has already been well explored in psychology (e.g.~\cite{gopnik}). In reinforcement learning, several approaches explore  mechanisms that push the internal representations of  learned models to be ``good'' in the sense that they provide better control (see Sec.~\ref{sec:related}).

We propose and explore a more direct mechanism, which explicitly links an agent's control over its environment with its internal feature representations. Specifically, we hypothesize that some of the factors explaining  variations in the data correspond to aspects of the world which can be controlled by the agent. For example, an object could be pushed around or picked up independently of others. In such a case, our approach aims to extract object features  from the raw data while learning a policy that controls precisely these features of the data. 
%An interesting way to discover a latent representation of the world can thus be devised based on learning feature-policy pairs that describe independent causal relationships between actions of agents that lie in the world and their effect on it.
In Sec.~\ref{sec:icf} we explain this mechanism and show experimental results in its simplest instantiation. In Sec.~\ref{sec:general} we discuss how this principle could be applied more generally, and what are the research challenges that emerge.

% 3rd-person imitation learning \cite{stadie2017}?
% Auxiliary task in interesting, they also seek to maximize the activations, but without any selectivity


\section{Independently controllable features}
\label{sec:icf}

To make the above intuitions concrete, assume that there are factors of variation underlying the  observations coming from an interactive environment that are ``independently controllable''. That is, for each of these factors of variation, there exists a policy which will modify that factor only, and not the others. 
%From this we do the following conjecture: these explanatory factors are causes of the observed data, in the mathematical sense of causality. 
For example, the object behind a set of pixels could be acted on independently from other objects, which would explain variations in its pose and scale when we move it around. The object in this case is a ``factor of variation".
What makes discovering and mapping such factors into features tricky is that the factors are not explicitly observed. Our goal is to learn these factors, which we call~\textbf{independently controllable features}, along with policies that control them.
%Since an agent cannot possibly control every factor of variation in its environment, we also posit that it should be reasonable to assume  (in our learning algorithm)
%that there are external agents performing policies that change only one factor of variation, e.g. a carpenter changing the leg style of a chair, or a human moving an object around a robot. % or a god-like entity making the sun rise and set. 
While these may seem strong assumptions about the nature of the environment, our point of view is that they are similar to regularizers meant to make a difficult learning problem better constrained.


There are many possible ways to express the desire to learn independently controllable features as an objective. Section \ref{sec:pol-sel} proposes such an objective for a simple scenario. Section \ref{sec:exp-emmanuel} illustrates the effect of this objective when all the features of the environment are simple and controllable by the agent. Section \ref{sec:exp-valentin} explores a slightly harder scenario in which there is redundancy and policies are learned through a reinforcement learning algorithm.

\subsection{Autoencoders}%\protect\footnote{Readers familiar with autoencoders may wish to skip this section.}}
Our approach builds on the familiar framework of 
autoencoders~\citep{hinton2006reducing}, which are defined as a pair of function approximators $f,g$ with parameters $\theta$ such that $f:X\to H$ maps the input space to some \textit{latent space} $H$, and $g:H\to X$ maps back to the input space $X \subset \mathbb{R}^d$. Autoencoders are trained to minimize the discrepancy between $x$ and $g(f(x))$, a.k.a. the reconstruction error, e.g.,:%, essentially learning the identity function:
$$\min_\theta \tfrac{1}{2} \|x-g(f(x))\|_2^2$$
We call $f(x)=h\in H \subset \mathbb{R}^n$ the latent feature representation of $x$, with $n$ features.

It is common to assume that  $n \ll d$. This causes $f$ and $g$ to perform dimensionality reduction of $X$, i.e. \textit{compression}, since there is a dimension bottleneck through which information about the input data must pass. Often, this bottleneck forces the optimization procedure to uncover principal factors of variation of the data on which they are trained. However, this does not necessarily imply that the different
dimensions of $h=f(x)$ are individually meaningful. In fact, note that for any bijective function $r$, we
could obtain the same reconstruction error by
replacing $f$ by $r \circ f$ and $g$ by $r^{-1} \circ g$, so we should not expect any form of disentangling
of the factors of variation unless some additional
constraints or penalties are imposed on $h$. This
motivates the approach we are about to present. Specifically, we will look for
policies which can separately influence one of the dimensions of $h$, and we will prefer representations which make such policies possible.

\subsection{Policy Selectivity}
\label{sec:pol-sel}

%%%%%%% Say that AE can learn any mapping, but we want mapping to make sense wrt to hypothesis.

Consider the following simple scenario: we train an autoencoder $f,g$ producing $n$  latent features,  $f_k,k=1,\dots n$. In tandem with these features we train $n$ policies, denoted $\pi_k$. Autoencoders can learn relatively arbitrary feature representations, but we would like
these features to correspond to controllable
factors in the learner's environment. Specifically, we would like policy $\pi_k$ to cause a change only in $f_k$ and not in any other features. We think of $f_k$ and $\pi_k$ as a feature-policy pair.

In order to quantify the change in $f_k$ when actions are taken according to $\pi_k$, we define the  \emph{selectivity} of a feature as:
\begin{equation}
    sel(s,a,k) = \mathbb{E}_{s'\sim \mathcal{P}_{ss'}^a} \left[ \frac{|f_k(s')-f_k(s)|}{\sum_{k'}|f_{k'}(s')-f_{k'}(s)|} \right] \label{eq:sel}
\end{equation}
where $s$,$s'$ are successive  raw state representations (e.g. pixels), $a$ is the action, $\mathcal{P}_{ss'}^a$ is the environment's transition distribution from $s$ to $s'$ under action $a$. The normalization by the change in all features means that the selectivity of $f_k$ is maximal when \textit{only that single feature} changes as a result of some action. 

By having an objective that maximizes selectivity \textit{and} minimizes the autoencoder objective, we can ensure that the features  learned can both reconstruct the data and  recover 
independently controllable factors. Hence, we define the following objective, which can be minimized via stochastic gradient descent:
%======\\
%\begin{equation}
%     \underbrace{\mathbb{E}_s [\tfrac{1}{2} || s - g(f(s))||^2_2]}_{\text{reconstruction error}}\ -\ \lambda \underbrace{\sum_k \mathbb{E}_{s}[\sum_a \pi_k(a|s) \log  sel(s,a,k) ]}_{\text{disentanglement reward}}  \label{eq:full-objective}
%\end{equation}
%where we define $sel$ as
%\begin{equation}
%    sel(s,a,k) = \mathbb{E}_{s'\sim \mathcal{P}_{ss'}^a} \left[ \frac{|f_k(s')-f_k(s)|}{\sum_{k'}|f_{k'}(s')-f_{k'}(s)|} \right] \label{eq:sel}
%\end{equation}
%with $s$,$s'$ the raw state representations (e.g. pixels), $a$ the actions, $\mathcal{P}_{ss'}^a$ the environment's transition distribution from state $s$ to $s'$ under action $a$, and $\pi_k$ the policy associated with feature $k$.\\
%=====\\
%New proposition\\
%====\\
\begin{equation}
     %\underbrace{\mathbb{E}_s [\tfrac{1}{2} || s - g(f(s))||^2_2]}_{\text{reconstruction error}}\ -\ \lambda \underbrace{\sum_k \mathbb{E}_{s}[\sum_a \pi_k(a|s) R_k(s, a)   ]}_{\text{disentanglement {\color{red} objective}}}  \label{eq:full-objective}
     \underbrace{\mathbb{E}_s [\tfrac{1}{2} || s - g(f(s))||^2_2]}_{\text{reconstruction error}}\ -\ \lambda \underbrace{\sum_k \mathbb{E}_{s}[\sum_a \pi_k(a|s) \log sel(s, a, k)   ]}_{\text{disentanglement objective}}  \label{eq:full-objective}
\end{equation}
% return or objective or reward in above equation??
Here one can think of $\log sel(s,a,k)$ as the reward signal $R_k(s,a)$ of a control problem, and the expected reward $E_{a\sim \pi_k}[R_k]$ is maximized by finding the optimal set of policies $\pi_k$.
%where we define our reward $R_k(s,a) = \log  sel(s,a,k)$
%with $sel$ being
%\begin{equation}
%    sel(s,a,k) = \mathbb{E}_{s'\sim \mathcal{P}_{ss'}^a} \left[ \frac{|f_k(s')-f_k(s)|}{\sum_{k'}|f_{k'}(s')-f_{k'}(s)|} \right] \label{eq:sel}
%\end{equation}

%%====\\

%This objective pushes $\pi_k$ (and $f$) to maximize the difference between $f_k(s)$ and $f_k(s')$ (where $s'$ happens after being in $s$ and following policy $\pi_k$) and minimize the difference between other factors $k'$ (since the optimization pushes the numerator to be large and the denominator small).

% addition ====== small insight on what is sel and the obj we maximize
%$sel_k = sel(\cdot, \cdot, k)$ can be interpreted as the relative change 
%==== 

Note that it is also possible to have \textit{directed} selectivity: by not taking the absolute value of the numerator of \eqref{eq:sel} (and replacing $\log sel$ with $\log (1+sel)$ in \eqref{eq:full-objective}), the policies must learn to increase the learned latent feature rather than simply change it. This may be useful if the policy to gradually increase a feature is distinct from the policy that decreases it.
%Note that in this objective, both $\pi_k$ and $f$ are assumed to be parameterized by the same shared parameters (except for the last layer going to their own space), and that policies and latent features are jointly learned via this objective.

%NOTE: Alternative version with $\pi$ and $f$ own's parametrizations\\
%Note that for this objective $\pi_k$ is parametrized by a variable $\theta_k$ and $f$ is parametrized by $W_f$.
%The total cost we seek to minimize is therefore

% We have to choose one form for the cost here. Second one is more classic, first one is initial cost proposed by Yoshua
%NOTE: add E_s for all these terms

%$$ \underbrace{\mathbb{E}_s [ \tfrac{1}{2} || s - g(f(s))||^2_2]}_{\text{reconstruction error}}\ +\ \lambda\ ( \underbrace{- \sum_k \mathbb{E}_{s}[\sum_a \log \pi_k(a|s) sel(a,s,k) ]}_{\text{disentanglement cost}} )$$
%OR 
%$$ \underbrace{\mathbb{E}_s [\tfrac{1}{2} || s - g(f(s))||^2_2]}_{\text{reconstruction error}}\ +\ \lambda\ ( \underbrace{- \sum_k \mathbb{E}_{s}[\sum_a \pi_k(a|s) \log  sel(a,s,k) %]}_{\text{disentanglement cost}} )$$

\subsection{A first toy problem}
\label{sec:exp-emmanuel}
%\subsection{Selectivity}

Consider the simple environment described in Figure \ref{fig:squares-env-result}(a): the agent sees a $2\times 2$ square of adjacent cells
in the environment, and has 4 actions that move it up, down, left or right. An autoencoder with directed selectivity  (see Figure \ref{fig:squares-env-result}(c,d)) learns latent features that  map to the $(x,y)$ position of the square in the input space, without ever having explicitly access to these values, and while reconstructing the input properly.
An autoencoder without selectivity also reconstructs the input properly but without learning these two latent $(x,y)$ features explicitly. 

In this setting $f$, $g$ and $\pi$ share some of their parameters. We use the following architecture: $f$ has two $16\times 3 \times 3$ ReLU convolutional layers, followed by a fully connected ReLU layer of 32 units, and a $\tanh$ layer of $n=4$ features; $g$ is the transpose architecture of $f$; $\pi_k$ is a softmax policy over 4 actions, computed from the output of the ReLU fully connected layer.
%Right now the environment has a mathching 1 action = 1 effect, it would be nice to actually have state dependent policy, e.g. (1 action, 1 state) = 1 effect which varies with state.

%One problem with selectivity is that it only cares about magnitude of change rather than direction. How to change it so that direction is also taken into account?

%\begin{figure}
%\centering
%\caption{TODO: make figure 1 and 2 a single figure with subfigures a to d. }
%\label{fig:squares-env}
%\end{figure}

\begin{figure}[H]
\centering
%\includegraphics[width=0.75\columnwidth]{factors_ae_ft12.png}
%\caption{An autoencoder+selectivity with 6 latent features trained on the environment of Figure \ref{fig:squares-env}, for which there are 2 real latent features: the $(x,y)$ position of the square. In each graph, each point is a sample $s$ from the dataset, the $x$ axis of the $k$-th row of graphs is $f_k(s)$, the $y$ axis of the left column is the true $x$ position of the square, the $y$ axis of the right column the true $y$ position of the square. We see that feature 1 learns the $x$ position and feature 2 the $y$ position.}
\includegraphics[width=0.5\columnwidth,trim={0 30px 0 0},clip]{recon_049.png}
\includegraphics[width=0.2\columnwidth,trim={75px 0 0 0},clip]{slopes.png}
\includegraphics[width=0.23\columnwidth]{policies.png}

\hspace*{0.1cm}(a)\hspace*{3.5cm}(b)\hspace*{3.9cm}(c)\hspace*{3.7cm}(d)
\caption{(a) \& (b) A simple environment with 4 actions that push a square left, right, up or down. (a) is an example ground truth, (b) is the reconstruction of the model trained with selectivity. (c) The slope of a linear regression of the true features (the real $x$ and  $y$ position of the agent) as a function of each latent feature. White is no correlation, blue and red indicate strong negative or positive slopes respectively. We can see that features 0 and 1 recover $y$ and features 2 and 3 recover $x$. (d) Representation of the learned policies. Each row is a policy $\pi_k$, each column corresponds to an action (left/right/up/down). Each cell $(k,i)$ represents the probability of action $i$ in policy $\pi_k$;  We can see that features 0 and 1 correspond to going down and up ($-y$/$+y$) and features 2 and 3 correspond to going right and left ($+x$/$-x$).}
\label{fig:squares-env-result}
\end{figure}



\subsection{A slightly harder toy problem}
\label{sec:exp-valentin}
In the next experiments, we aim to generalize the model above in a slightly more complex environment. 
Instead of having $f$ and $\pi_k$ parametrized by the same parameters, we now introduce a different set of parameters for each policy and for the encoder, so that each policy can be learned separately.

Additionally, we use a richer action space, in which we added a ``move down and to the right" action as well as an ``increase/decrease color of  square" action. Note also that the first two actions ("move down") are redundant.

Thus, in this setting, $f, g$ and $\pi_k$ are all parametrized by different variables, as follows:
\begin{itemize}[nosep, topsep=-1ex]
\item $f(s) = f(s; W_f)$ a neural net with a $\tanh$ output activation %a fully-connected net with $\tanh$ activation on the last layer
\item $g(h) = g(h; W_g)$ a neural net with a ReLU output activation
\item $\pi_k (a|s) = \pi(a|s ; \theta_k)$ so that $\pi_k( \cdot | s)  = \text{softmax}(\theta_k \cdot s)$
\end{itemize}

\begin{algorithm}                      % enter the algorithm environment
\caption{Training an autoencoder with disentangled factors}          % give the algorithm a caption
\label{alg1}                           % and a label for \ref{} commands later in the document
\begin{algorithmic}[1]                   % enter the algorithmic 
    \For{$t=1..T$}
        \State{Sample $s$}% \sim \mathbb{P}(s)$}
        %\State update Wf, Wg
        \State{$W_f \gets W_f - \eta_{f} \nabla_{W_f} [ \tfrac{1}{2} ||s -g (f(s))||^2_2]$} 
        \State{$W_g \gets W_g -  \eta_{g} \nabla_{W_g} [ \tfrac{1}{2} ||s -g (f(s))||^2_2]$} 
        \For{$k=1..K$}
            %\State{Sample $a \sim \pi_k(\cdot | s)$}
            %\State{update $\theta_k$, $W_f$ again}
            \State{$W_f \gets W_f + \eta_{f} \ \lambda\  \nabla_{W_f} \mathbb{E}_{a \sim \pi_k(\cdot | s)} [ \log  sel(s,a,k) ]$} 
            \State{$\theta_k \gets \theta_k + \eta_{k} \ \lambda \ \nabla_{\theta_k}  \mathbb{E}_{a \sim \pi_k(\cdot | s)} [\log  sel(s,a,k) ]$}
        \EndFor
    \EndFor
\end{algorithmic}
\end{algorithm}

% original paragraph from Valentin:
%The gradients on line 3 and 4 are typically computed exactly via backpropagation. In this specific case, gradients on line 6 and 7 can be estimated by direct differentiation. In a more general case where the environment also provides a reward $r_t$ (so our total reward to maximize becomes $r_t + \log sel_k$), these gradients can be estimated via Monte-Carlo estimation. Note that, in any case, $\nabla_{\theta_k}  \mathbb{E}_{a \sim \pi_k(\cdot | s)} [\log  sel(s,a,k) ]$ can be computed
The gradients on lines 3 and 4 are computed exactly via backpropagation. In our experiments, gradients on lines 6 and 7 are also computed by backpropagation, but %direct differentiation%.
in a more general case in which the environment also provides a reward $r_t$ (so the total reward to maximize becomes $r_t + \log sel_k$), they can be estimated via Monte-Carlo. Note that, in any case, $\nabla_{\theta_k}  \mathbb{E}_{a \sim \pi_k(\cdot | s)} [\log  sel(s,a,k) ]$ can be computed
with the REINFORCE \citep{williams1992simple} estimator: % \citep{glynn1987likelilood,williams1992simple}.
%REINFORCE for SGD update for $\theta_k$:
$$\nabla_{\theta_k} \mathbb{E}_{a \sim \pi_k(\cdot | s)}[ \log sel(s, a, k) ] = \mathbb{E}_{a \sim \pi_k(\cdot | s)}[ \log sel(s, a, k) \cdot \nabla_{\theta_k} \log \pi_k(a|s)]$$


% separate the images, put eps/svg format
%\begin{figure}[!tbp]
\begin{figure}[H]
\centering
%\includegraphics[width=.9\linewidth]{./rsz_mila_figure_noae.png}
\subfloat[]{\includegraphics[width=.3\linewidth]{./pi_figure.png}}
\hfill
\subfloat[]{\includegraphics[width=.3\linewidth]{./sel_figure.png}}
\hfill
\subfloat[]{\includegraphics[width=.3\linewidth]{./pilogsel_figure.png}}
% redoing figure right now
%\hspace*{-1.0cm}(a)\hspace*{3.5cm}(b)\hspace*{5.0cm}(c)
\caption{Policy $\pi_k$, selectivity $sel_k$ and objective $-\pi_k  \log sel_k$ for a random $s$ and feature/policy $k$ after optimizing (2). On the $y$-axis is the index $k$ of the feature, and on the $x$-axis, the $8$ different actions that the agent can select. Initially, $\pi_k$ was initialized to be uniform.
(a): As in the previous toy problem, each $\pi_k$ quickly concentrates on a specific action. %\newline 
(b): However, in this example, the encoder is also simultaneously shaped so that each feature $f_k(s)$ reacts to one specific action with maximal selectivity (L6 of algorithm \ref{alg1})  (c): $- \pi_k \log sel_k$. The autoencoder was able to learn independently controllable features.
Moreover, in (a), we observe that, as the two first actions are redundant, they have the exact same selectivity and therefore $\pi_3$, the policy associated to the "move down" feature, can either choose the first or second action. }% say that whenever sel = 0, $\pi$ must also go to 0}
\end{figure}
%thus adapts to ensure that the selectivity is high for this chosen action and low for others leading to a low total cost $-\pi_k \log sel_k$.
%\newline%\\


%\subsection{Insight of the cost function}
%\begin{figure}[H]
%\centering
%\includegraphics[width=.5\linewidth]{./rsz_drawing}
%\caption{Do you think such a figure (to be done in matplotlib, and plot $- \log sel$ instead of $\log sel$) would be useful to explain how our cost function works?}
%\end{figure}
%%%% This doesn't work unfortunately :D
%\subsection{External independent controllable factors}
%Idea: assume there are only 2 agents in the world; our agent and the environment. We know the action set of our agent, and we can voluntarily (on-policy) sample actions and test hypotheses about the effect of our actions on our observations. Let's assume that we know the number of actions that the environment agent has, but not when they are taken nor their effect. Let's assume that then environment agent can only take one action at a time, and change one ICF at a time. What we're really interested in is mapping changes to actions or policies, but it seems an underspecified problem.
%Say we have (or also learn) a function $P(b|s,s')$ that gives the probability of the environment having taken action $b$ between $s$ and $s'$. Then we can optimize almost the same objective:
%$$\sum_k \log \sum_b P(b|s,s') P_k(b|s) sel_{s'}(b,s,k) $$
%This is like saying, we want to find the unique action $b$ which maximizes the selectivity \emph{and} changes factor $k$.
%In model-based RL one wants to learn $P(s'|s,a)$, which is usually extremely hard, but here I think we have a shot at learning $P(b|s,s')$, because it's easier, after-the-fact reasoning. Usually the hard part in $P(s'|s,a)$ is the fact that $s'$ is a huge state space. If we assume that the number of actions is limited, learning $P(b|s,s')$ makes sense. Even if the $b$ space is large, I think conceptually because we already have the state transition $(s,s')$, "causal" factors should be learned a bit more easily, because in an environment (especially partially observable) the result of an action is usually very stochastic, but the number of actions that can cause a transition is usually relatively deterministic (or maybe not, it could be argued that not, but I think in the scenarios we care about now, it is the case).
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\section{Controlling factors + ambient dynamics}

%$$Loss_{dis}=(f(s_1 \sim P_{\pi_{\Delta h}}(s_1|s_0))-f(s_0)+\Delta h+\hat{T}(s_0))^2$$
\section{Scaling to general environments: controllability and the binding problem}
\label{sec:general}

%{\color{red}Copy paste of Yoshua's discourse answer, \sout{needs edits} somewhat edited:}

In the previous section we used problems in which  the environment is made of a static set of objects. In this case,   if the objective posited in section \ref{sec:pol-sel} is learned correctly, we can assume that feature $k$ of the representation can unambiguously refer to some controllable property of some specific object in the environment. For example, the agent's world might contain only a red circle %, a blue triangle
and a green rectangle, which are only affected bu the actions of the agent (they do not move on their own) and we only change the positions and colours of these objects from one trial 
to the next. Hence, a specific feature $f_k$ can learn to unambiguously refer to the position or the colour of one of these two objects.

In reality, environments are stochastic, and the set of objects  in a given scene is drawn from some distribution. The number of objects may vary and their types may be different. %Maybe one day the agent wakes up in a world with a square and two circles. Another day it wakes up in a world with a rectangle, a circle and two squares. 
It then becomes less obvious how feature $k$ could refer in a clear way to some feature of one of the objects in a particular scene. 
If  we have \textit{instances} of objects of different types, some addressing or naming scheme is required to refer to the particular objects (instances) present in the scene, so as to match the policy with a particular attribute of a particular object to selectively modify. In our simple environments, this task was trivial because we could simply use the integer $k$ to achieve this coordination between the policy $\pi_k$ and the representation elements (feature $f_k(s)$). In the more general case, this is not possible, which gives rise to a representational problem.

This is connected to the binding problem in neuro-cognitive science: how to represent a set of objects, each  having different attributes, so that we don't confuse, for example, the set $\{$red circle, blue square$\}$ with $\{$red square, blue circle$\}$. It is not enough to have a red-detector feature and a blue-detector feature, a square-detector feature and a circle-detector feature. The binding problem has seen some attention in the representation learning literature \citep{minin2012complex,greff2016tagger}, % (and the related problem of scene description \citep{raposo2017discovering})
 but still remains mostly unsolved. %, especially when the ground truth is unknown.
Jointly considering this problem and larning controllable features may prove fruitful.
%maybe https://arxiv.org/pdf/1702.05068v1.pdf i.e. raposo2017discovering relation netwoks by deepmind, very different approach

%\begin{comment}
%For now the only satisfying direction I can think of is that we may let the learner figure out a way to represent a set, as well as a way to represent a policy (so both are represented by a vector of real numbers) and then learn a mapping A from a policy representation and a scene representation to a single scalar attribute. We can think of the policy representation as a description of a recipe for picking a specific attribute of a specific object in the current scene and then A implementing that recipe. Now the policy must be a function of the scene so it can implicitly refer to it. If I am in my bedroom and I know this it becomes easy to specify a policy which will pick a specific object in the bedroom. If I have to specify such a policy in an absolute way without referring to my bedroom it becomes much more tricky in that I almost need to describe the bedroom as well as the set of actions in order to come with the recipe for picking up a particular object in the bedroom.

%So the flow of information would be something like this:

%\begin{itemize}
%\item sample a scene x in which the agent wakes up that day
%\item  compute the representation of the scene, h=f(x)
%\item compute the reconstruction error or other unsupervised representation learning loss
%\item sample a policy representation p=phi(h,random noise)
%\item map the policy and the scene representation to an attribute of an object, a=A(h,p)
%\item compute the selectivity (now that is tricky because we don't have access too the other values of a for all the possible p's for different policy samples)
%\item update f,phi,A to optimize the reconstruction error and selectivity.
%\end{itemize}

%One issue with the above is that the objective function would not prevent the learner from focussing on a single attribute.
%\end{comment}

%{\color{red} say this differently?:} 

These ideas may also lead to interesting ways of performing exploration.
How do humans choose with which object to play? We are attracted to objects which we do not know yet (i.e., %we don't know 
if and how we can control them). The RL exploration process could be driven by a notion of controllability, predicting the interestingness of objects in a scene and choosing features and associated policies with which to attempt control them. Such ideas have only been explored briefly in the literature, e.g.~\cite{ratitch}% (how do we enumerate them) and  sample one (and one of its attributes) and the policy is then represented accordingly.
% 
% In the real world, agents, such as humans tend to ttracted tobe a objects it doesn't know how to control

%%%%%
%Do we want better interpretability of policies? Better control? Just cool features to look at? 


%\subsubsection*{Related work}

\section{Discussion}
\label{sec:related}
There is a large body of work on learning features in RL focusing on indirectly learning good internal representations. In \cite{jaderberg2016reinforcement}, agents learn off-policy to control their pixel inputs, forcing them to learn features that, intuitively, help  control the environment (at least at the pixel level). \cite{oh2015action} propose models that learn to predict the future, conditioned on action sequences, which push the agent %'s internal representations
to capture temporal features.  There are many more works that go in this direction, such as (deep) successor feature representations \citep{dayan1993improving,kulkarni2016deep} or the options framework \citep{sutton1999between,precup2000temporal} when used in conjunction with neural networks \citep{bacon2016option}.

%We hypothesized that an interesting way to learn a latent representation of the world is to learn feature-policy pairs that describe independent causal relationships between the actions of agents and their effect on it. This idea
Our approach is similar in spirit to the Horde architecture~\citep{sutton2009}. In that scenarion, agents learn policies that maximize specific inputs. The predictions for all these policies then become features for the agent. Our objective is defined specifically in the context of autoencoders. Unlike recent work on the predictron~\citep{silver2017}, our approach is not focused on solving a planning task, and the goal is simply to learn how the world works.


%We show that for simple scenarios, objectives following this guideline recover the true latent features that best describe the world without explicit supervision.
%Finally, we motivate the application of the idea of independent controllable features to more complex scenarios.


% Acknowledgements should only appear in the accepted version. 
%\section*{Acknowledgements} 
\vspace*{-10pt}

\bibliography{main}
\bibliographystyle{icml2017}

\end{document} 




\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{lscape}
\usepackage{xspace}
%\usepackage[moderate,tracking=normal,wordspacing=normal,mathspacing=normal,mathdisplays=normal,margins=normal,indent=normal]{savetrees}


% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

% \cvprfinalcopy % *** Uncomment this line for the final submission
\cvprfinalcopy
\def\cvprPaperID{2327} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{2327}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

% Reduce space between paragraph ({\z@}{ --> 1.0ex <--)
\makeatletter
\renewcommand{\paragraph}{%
  \@startsection{paragraph}{4}%
  {\z@}{0.5ex \@plus 0.5ex \@minus .2ex}{-1em}%
  {\normalfont\normalsize\bfseries}%
}
\makeatother


%commands
\newcommand{\GW}{GuessWhat?!\xspace}
\newcommand{\Referit}{ReferIt\xspace}
\newcommand{\oracle}{oracle\xspace}
\newcommand{\Oracle}{Oracle\xspace}
\newcommand{\questioner}{questioner\xspace}
\newcommand{\Questioner}{Questioner\xspace}
%%%%%%%%% TITLE
\title{\GW Visual object discovery through multi-modal dialogue}
% \GW Building multimodal dialogues to locate objects within images

% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both

\author{Harm de Vries\\
University of Montreal\\
{\tt\small mail@harmdevries.com}
\and
Florian Strub\\
Univ. Lille, CNRS, Centrale Lille,\\
Inria, UMR 9189 CRIStAL\\
{\tt\small florian.strub@inria.fr}
\and
Sarath Chandar\\
University of Montreal\\
{\tt\small sarathcse2008@gmail.com}
\and
Olivier Pietquin\\
DeepMind\\   
{\tt\small pietquin@google.com}
%{\tt\small olivier.pietquin@univ-lille1.fr}
%Olivier Pietquin\thanks{Now at Google Deepmind}\\
%Univ. Lille, CNRS, Centrale Lille, \\Inria, UMR 9189 CRIStAL\\   
%{\tt\small olivier.pietquin@univ-lille1.fr}
\and
Hugo Larochelle\\
Twitter\\
{\tt\small hlarochelle@twitter.com}
%\and
\and
Aaron Courville\\
University of Montreal\\
{\tt\small aaron.courville@gmail.com}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
We introduce \GW, a two-player guessing game as a testbed for research on the interplay of computer vision and dialogue systems. The goal of the game is to locate an unknown object in a rich image scene by asking a sequence of questions. 
Higher-level image understanding, like spatial reasoning and language grounding, is required to solve the proposed task. Our key contribution is the collection of a large-scale dataset consisting of 150K human-played games with a total of 800K visual question-answer pairs on 66K images. We explain our design decisions in collecting the dataset and introduce the oracle and questioner tasks that are associated with the two players of the game. We prototyped deep learning models to establish initial baselines of the introduced tasks. 
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
People use natural language as the most effective way to communicate, including when it comes to describe the visual world around them. They often need only a few words to refer to a specific object in a rich scene. Whenever such expressions \emph{unambiguously} point to one object, we speak of a referring expression~\cite{krahmer2012computational}. However, uniquely identifying the referred object is not always possible, as it depends on the listener's state of mind and the context of the scene. Many real life situations, therefore, require multiple exchanges before it is clear what object is referred to:
\begin{quote}
- Did you see that dog?\\
* You mean the one in the corner?\\
- No, the one that's running.\\
* Yes, what's up with that?
\end{quote}

\vspace{-0.3em}
\begin{figure}[t]
\begin{center}
\centering
\includegraphics[width=0.98\linewidth]{./misc/front2.pdf}
\end{center}
\vskip -1em
\caption{An example game. After a sequence of four questions, it becomes possible to locate the object (highlighted by a green bounding box). }
\label{fig:front}
\vskip -0.7em
\end{figure}
A computer vision system able to hold conversations about what it \textit{sees} would be an important step towards intelligent scene understanding. Such systems would be more transparent and interpretable because humans may naturally interact with them, for example by asking clarifying questions about what it perceives. Still, a fundamental challenge remains: how to create models that understand natural language descriptions and ground them in the visual world. 

The last few years has seen an increasing interest from the computer vision community in tasks towards this goal. Thanks to advances in training deep neural networks~\cite{Goodfellow-et-al-2016-Book} and the availability of large-scale classification datasets~\cite{lin2014microsoft,ILSVRC15,zhou2014learning}, automatic object recognition has now reached human-level performance~\cite{lecun2015deep}. As a result,  attention has been shifted toward tasks involving higher-level image understanding. One prominent example is image captioning~\cite{lin2014microsoft}, the task of automatically producing natural language descriptions of an image. Visual Question Answering (VQA)~\cite{antol2015vqa} is another popular task that involves answering single open-ended questions concerning an image. Closer to our work, the \Referit game~\cite{kazemzadeh2014referitgame} aims to generate a single expression that refers to one object in the image. %All of the mentioned tasks involve single-shot understanding of natural language. 

\begin{figure}[t]
\centering
\includegraphics[width=1\linewidth]{./misc/examples2_2.pdf}
\caption{Two example games in the dataset. After a sequence of five questions we are able to locate the object (highlighted by a green mask).}
\label{fig:examples}
\vskip -1em
\end{figure}

On the other hand, there has been a renewed interest in dialogue systems~\cite{lemon2012conversational,serban2015survey}, inspired by the success of data-driven approaches in other areas of natural language processing~\cite{cho2014learning}. Traditionally, dialogue systems have been built through heavy engineering and hand-crafted expert knowledge, despite machine learning attempts for almost two decades~\cite{levin1997stochastic,singh1999reinforcement}.
One of the difficulties comes from the lack of automatic evaluation as -- contrary to machine translation -- there is no evaluation metric that correlates well with human evaluation~\cite{liu2016not}. A promising alternative is goal-directed dialogue tasks~\cite{lemon2012conversational,singh1999reinforcement,weston2015towards,wen2016network} where agents converse to pursue a goal rather than casually chit-chat. The agent's success rate in completing the task can then be used as an automatic evaluation metric. Many tasks have recently been introduced, including the bAbI tasks~\cite{weston2015towards} for testing an agent's ability to answer questions about a short story, the movie dialog dataset~\cite{dodge2015evaluating} to assess an agent's capabilities regarding personal movie recommendation and a Wizard-of-Oz framework~\cite{wen2016network} to evaluate an agent's performance for assisting users in finding restaurants.

In this paper, we bring these two fields together and propose a novel goal-directed task for multi-modal dialogue. The two-player game, called \GW, extends the ReferIt game~\cite{kazemzadeh2014referitgame} to a dialogue setting. To succeed, both players must understand the relations between objects and how they are expressed in natural language. From a machine learning point of view, the \GW challenge is the following: learn to acquire natural language by interaction on a visual task. Previous attempts in that direction~\cite{akinator,wen2016network} do not ground natural language to their immediate environment; instead they rely on an external database through which a conversational agent searches.  

The key contribution of this paper is the introduction of the \GW dataset that contains 155,280 dialogues composed of 831,889 question/answer pairs on 66,537 images extracted from the MS COCO dataset~\cite{lin2014microsoft}. We define three sub-tasks that are based on the \GW dataset and prototype deep learning baselines to establish their difficulty. 
%
The paper is organized as follows. First, we explain the rules of the \GW game in Sec.~\ref{sec:game}. Then, Sec.~\ref{sec:related_work} describes how \GW relates to previous work. In Sec.~\ref{sec:data_collection} we highlight our design decisions in collecting the  dataset, while Sec.~\ref{sec:data_analysis} analyses many aspects of the dataset. Sec.~\ref{sec:baselines} introduces the questioner and oracle tasks and their baseline models. Finally, Sec.~\ref{sec:discussion} provides a final discussion of the \GW game.


%-------------------------------------------------------------------------
\section{\GW game}
\label{sec:game}

    \GW is a cooperative two-player game in which both players see the picture of a rich visual scene with several objects. One player -- the \textbf{oracle} -- is randomly assigned an object (which could be a person) in the scene.  
    This object is not known by the other player -- the \textbf{questioner} -- whose goal it is to locate the hidden object. To do so, the questioner can ask a series of yes-no questions which are answered by the oracle as shown in  Fig~\ref{fig:front}~and~\ref{fig:examples}. Note that the questioner is not aware of the list of objects, they can only see the whole picture.
%
Once the questioner has gathered enough evidence to locate the object, they notify the oracle that they are ready to guess the object. We then reveal the list of objects, and if the questioner picks the right object, we consider the game successful. Otherwise, the game ends unsuccessfully. We also include a small penalty for every question to encourage the questioner to ask informative questions. Fig~\ref{fig:oracle}~and~\ref{fig:questioner} in Appendix~\ref{ap:website} display a full game from the perspective of the oracle and questioner, respectively. 

The oracle role is a form of visual question answering where the answers are limited to \emph{Yes}, \emph{No} and \emph{N/A} (not applicable). The \textit{N/A} option is included to respond even when the question being asked is ambiguous or an answer simply cannot be determined. For instance, one cannot answer the question "\textit{Is he wearing glasses?}" if the face of the selected person is not visible. % -> find a better example
%
The role of the questioner is much harder. They need to generate questions that progressively narrow down the list of possible objects. Ideally, they would like to minimize the number of questions necessary to locate the object. The optimal policy for doing so involves a binary search: eliminate half of the remaining objects with each question. Natural language is often very effective at grouping objects in an image scene. Such strategies depend on the picture, but we distinguish the following types:
\begin{description}
\item[Spatial reasoning] We group objects spatially within the image scene. One may use absolute spatial information -- \textit{Is it on the bottom left of the picture?} -- or relative spatial location -- \textit{Is it to the left of the blue car?}.
\item[Visual properties] We group objects by their size -- \textit{Is it big?}, shape -- \textit{Is it square?} -- or color -- \textit{Is it blue?}.
\item[Object taxonomy] We can use the hierarchical structure of object categories, i.e.  taxonomy, to group objects e.g. \textit{Is it a vehicle?} to refer to both cars and trucks. 
\item[Interaction] We group objects by how we interact with them -- \textit{Can you drive it?}. 
\end{description}

The goal of the \GW task is to enable machines to understand natural descriptions and ground them into the visual world. Note that such higher-level reasoning only occurs when the scene is rich enough i.e. when there are enough objects in the scene. People otherwise tend to fall back to a linear search strategy by simply enumerating objects (often by their category names). 
 

\section{Related work}
\label{sec:related_work}
The \GW game and the data collected from it present opportunities for the extension of current research on image captioning, visual question answering and dialogue systems. In the following, we describe previous work in these areas and relate them to the open challenges offered by \GW. We also mention other relevant work on dataset collection.

\paragraph{Image captioning}
Our work builds on top of the MS COCO dataset~\cite{lin2014microsoft} which consists of 120k images with more than 800k object segmentations. In addition, the dataset provides $5$ captions per image which initiated an explosion of interest from the research community into generating natural language descriptions of images. Several methods have been proposed \cite{karpathy2015deep,vinyals2015show,DBLP:journals/corr/XuBKCCSZB15}, all inspired by the encoder-decoder approach \cite{cho2014learning,sutskever2014sequence} that has proven successful for machine translation. Image captioning research uncovered successful approaches to automatically generate coherent, factual statements about images. Modeling the interactions in \GW requires instead to model the process of asking useful questions about images.

\paragraph{VQA datasets} Visual Question Answering (VQA) tasks form another well known extension of the captioning task. They instead require answering a question given a picture (\emph{e.g. "How many zebras are there in the picture?", "Is it raining outside?"} ). Recently, the VQA challenge \cite{antol2015vqa} has provided a new dataset far bigger than previous attempts \cite{geman2015visual,malinowski2014multi} where, much like in \GW, questions are free-form. 
%It contains around 750K open-ended questions on 250K different images. Multiple-choice answers are provided for each question. 
An extensive body of work has followed from this publication, largely building on the image captioning literature \cite{agrawal2016analyzing,lu2016hierarchical,shih2015look,yang2015stacked}. Unfortunately, many of these advanced methods were shown to marginally improve on simple baselines~\cite{DBLP:journals/corr/JabriJM16}. Recent work~\cite{agrawal2016analyzing} also reports that trained models often report the same answer to a question irrespective of the image, suggesting that they largely exploit predictive correlations between questions and answers present in the dataset. 
%Thus, VQA algorithms usually fail to grasp the specific meaning of the questions. 
The \GW game and dataset attempt to circumvent these issues. Because of the questioner's aim to locate the hidden object, the generated questions are different in nature: they naturally favour spatial understanding of the scene and the attributes of the objects within it, making it more valuable to consult the image. Besides, it only contains binary questions, whose answers we find to be balanced and has twice more questions on average per picture. 
%It also has twice 

%\paragraph{Dialogue systems} A broad range of datasets co-exist in the dialogue system community \cite{serban2015survey}. Gigantic datasets are composed of spontaneous and unconstrained dialogues that are extracted from chat logs \cite{lowe2015ubuntu} or micro-blogging platforms \cite{sordoni2015neural}.
%Conversational agents are then trained by using supervised techniques such as statistical machine translation \cite{ritter2011data} or recurrent neural networks \cite{vinyals2015neural,sordoni2015neural,DBLP:journals/corr/SerbanSBCP15}.  Even if these datasets contains billions of words, the resulting agents remain unsatisfactory \cite{XXX}. Dialogues still require long term planning and cannot be modeled by purely supervised methods such as deep networks. Moreover, it is difficult to assess the quality of such dialogue systems \cite{schatzmann2005quantitative,liu2016not}. As highlighted previously, existing metrics does not always correlate with human evaluations \cite{elliott2014comparing}. Thus, benchmarks often relies on subjective human assessment to infer which model turns out to the best. 

\paragraph{Goal-directed dialogue} \GW is also relevant to the goal-directed dialogue research community. Such systems are aimed at collaboratively achieving a goal with a user, such as retrieving information or solving a problem. %Therefore, the dialogue system assessment is eased by verifying whether the initial goal was successfully achieved. 
%This approach also enables to extend supervised methods with Reinforcement Learning (RL) techniques. 
%While promising results has been obtained on toys datasets \cite{singh1999reinforcement}, such approach remains rare \cite{lemon2007machine} as datasets remains hard to design \cite{weston2015towards}. 
Although goal-directed dialogue systems are appealing, they remain hard to design. Thus, they are usually restricted to specific domains such as train ticket sales, tourist information or call routing~\cite{pietquin2006probabilistic,singh1999reinforcement,young2013pomdp}. % or require constant human feedback \cite{dodge2015evaluating}. %cf twitter bot
Besides, existing dialogue datasets are either limited to fewer than 100k example dialogues~\cite{dodge2015evaluating}, unless they are generated with template formats~\cite{dodge2015evaluating,wen2016network,weston2015towards} or simulation~\cite{pietquin2013survey,schatzmann2006survey} in which case they don't reflect the free-form of natural conversations.
Finally, recent work on end-to-end dialogue systems fail to handle dynamical contexts. For instance,~\cite{wen2016network} intersects a dialogue with an external database to recommend restaurants. Well-known game-based dialogue systems~\cite{20questions,akinator} also rely on static databases.
In contrast, \GW dialogues are heavily grounded by the images. The resulting dialogue is highly contextual and must be based on the content of the current picture rather than an external database. Thus, to the best of our knowledge, the \GW dataset marks an important step for dialogue research, as it is the first large scale dataset that is both goal-oriented and multi-modal.

\paragraph{Human computation games} 
\GW is in line with Von Ahn's seminal work on human computation games~\cite{von2004labeling,von2006peekaboom} who showed that games are an effective way to gather labeled data. The first ESP game~\cite{von2004labeling} was developed to collect image tags, and was later extended to Peekaboom~\cite{von2006peekaboom} to gather object segmentations. These games were developed more than a decade ago, when object recognition was in its infancy and served a different purpose than \GW. 

\paragraph{ReferIt} Probably closest to our work is the \Referit game~\cite{kazemzadeh2014referitgame,mao2015generation,DBLP:journals/corr/YuPYBB16}. In this game, one player observes an annotated object in a scene, for which they need to generate an expression that refers to it (\emph{e.g.\ \"the man wearing the white t-shirt\"}). The other player then receives this expression and subsequently clicks on the location of the object within the image. The original dataset~\cite{kazemzadeh2014referitgame} uses the IMAGEClef dataset~\cite{escalante2010segmented}, while three recent extensions~\cite{mao2015generation,DBLP:journals/corr/YuPYBB16} were built on top of MS COCO. All three databases select images with only $2-4$ objects of the \emph{same} category. In contrast, \GW picks images with $3-20$ objects without further restrictions on the object class, and thus contains three times more images than the \Referit datasets. To further investigate the difference between \Referit and \GW, we compare three samples for the \emph{same} selected object in Fig~\ref{fig:referit} in Appendix~\ref{ap:gw_samples}. While \Referit directly locates the object with a single expression, \GW iteratively narrows down the object by means of positive and \emph{negative} feedback on questions. We also observe that \GW dialogues favor more abstract concepts, such as "\textit{Is it edible?}" or "\textit{Is it on oval plate?}" than \Referit.


\begin{figure*}[t]
\centering
\begin{subfigure}{0.32\linewidth}
\includegraphics[width=\linewidth]{stats/q_d.pdf}
\vskip -0.6em
\caption{}
\label{fig:stat:q_d}
\vskip -1.2em
\end{subfigure}
\begin{subfigure}{0.32\linewidth}
\includegraphics[width=\linewidth]{stats/object_question.pdf}
\vskip -0.6em
\caption{}
\label{fig:stat:o_q}
\vskip -1.2em
\end{subfigure}
\begin{subfigure}{0.32\linewidth}
\includegraphics[width=\linewidth]{stats/cloud2.png}
\vskip -1.3em
\caption{}
\label{fig:cloud}
\vskip -1.2em
\end{subfigure}
\vskip -0.5em
\caption{(a) Number of questions per dialogue (b) Number of questions per dialogue vs the number of objects within the picture (c) Word cloud of \GW vocabulary with each word proportional to its frequency. Words are colored based on a hand-crafted clustering. Uninformative words such as \emph{"it"}, \emph{"is"} are manually removed.}
\label{fig:statistic}
\vskip -1em
\end{figure*}

\section{\GW Dataset}
\subsection{Data collection}
\label{sec:data_collection}

\paragraph{Images} We use a subset of the training and validation images and objects of the MS COCO dataset~\cite{lin2014microsoft}. We first discard objects that are too small ($\text{area} <  500\text{px}^2$) to be decently located by a human observer. Then, we only keep images containing three to twenty objects, to avoid trivial or overly complicated images. In total, we keep 77,973 images with 609,543 objects. We verified that this selection does not significantly alter the original dataset distribution.% (see Fig~\ref{fig:coco_cat} and Fig~\ref{fig:coco_area} in Appendix~\ref{ap:statistics}). 


\paragraph{Amazon Mechanical Turk} The data collection was crowd-sourced on Amazon Mechanical Turk (AMT)~\cite{buhrmester2011amazon}. We created two separate tasks -- known as HITs on AMT -- for the questioner and oracle roles, and rewarded the questioner slightly more than the oracle. 
%The reason is that the oracle role is not as demanding as the questioner role and could be combined with other HITs simultaneously. 
We ensured the quality of the data collection by several means. First, the workers had to go through a qualification round which consisted of successfully completing $10$ games while producing fewer than $4$ mistakes or disconnects.  After qualification, HITs continue to consist of a batch of $10$ successful games. We incentivize the worker to produce as many successful dialogues in a row by providing bonuses for making fewer mistakes. Secondly, players could report on each other and players were banned after a certain number of reports.  Thus, players were incentivized to cooperate. In the end, we only kept dialogues from qualified people and successful dialogues from the qualification round. 
%
In contrast to traditional dataset collection, our game requires an interactive session between two players. Fortunately, we found that the \GW game was highly engaging. A total of more than 10K people participated in our HITs, and our top ten participants played over $2,000$ games each. 
%
Since questions were manually typed, they could contain spelling mistakes. Thus, we retrieved all questions containing words that do not occur in an English dictionary and manually corrected the $1000$ most common words. For the remaining 30k questions, we created two HITs that to correct the spelling mistakes. See Figure~\ref{fig:mistake_fixer} in Appendix~\ref{ap:website} for further details. 

%\paragraph{Further collection} In the long run, we plan to collect more data by releasing a website and its source code, to allow people to freely play the \GW game. 


\subsection{Data analysis}
\label{sec:data_analysis}
In the following, we explore properties of the data we collected using the \GW game. We provide global statistics, examine the vocabulary used by the questioners and highlight the relationship between properties of objects to guess and the odds of having a successful dialogue.

\paragraph{Dataset statistics}
The raw \GW dataset is composed of 155,280 dialogues containing 821,889 question/answer pairs on 66,537 unique images and 134,073 unique objects. The answers are respectively 52.2\% \emph{no}, 45.6\% \emph{yes} and 2.2\% \emph{N/A}. On average, there are 5.2 questions per dialogue and 2.3 dialogues per image. The dialogues contain 3,986,192 word tokens in total, making up 11,465 different words with at least one occurrence and 5,444 words with at least 3 occurrences. Moreover, 84.6\% of the dialogues are successful, 8.4\% are unsuccessful and 7.0\% are not completed (disconnection, timeout etc.). Thus, different subsets co-exist in the \GW dataset, we will refer to the dataset as full, finished and successful when we include all the dialogues, all finished dialogues (successful and unsuccessful) or only successful dialogues, respectively. For more details, the previous statistics are broken down into dataset types in Tab~\ref{tab:dataset_statistics}.

\begin{table}
\centering
\scalebox{0.79}{
\begin{tabular}{|l|c|c|c|}
\hline
 & Full & Finished & Success\\
\hline
\# dialogues         & 155,280   & 144,434    & 131,394  \\
\# questions         & 821,889   & 732,081    & 648,493  \\
\# words             & 3,986,192 & 3,540,497  & 3,125,219\\
\# voc. size         & 11,465    & 10,985     & 10,469   \\
\# voc. size (3+)    & 5,444     & 5,179      & 4,919    \\
\# images          & 66,537    & 65,112     & 62,954   \\
\# objects           & 134,073   & 125,349    & 114,271  \\
\hline
\end{tabular}}
\caption{\GW statistics split by dataset types.}
\label{tab:dataset_statistics}
\vskip -1em
\end{table}


\paragraph{Question distributions} To get a better understanding of the \GW games, we show the number of questions within a dialogue and the average number of questions given the number of objects within a image in Fig~\ref{fig:statistic}. First, the number of questions within a dialogue decreases exponentially, as players tend to shorten their dialogues to speed up the game (and therefore maximize their gains). More interestingly, we observe that the average number of questions given the number of objects within an image appears to follow a function that grows at a rate between logarithmically and linearly. A questioning strategy of simply listing objects (e.g. "\emph{is it the chair}", etc.) would imply linear growth in the number of questions, while the optimal binary search strategy would imply logarithmic growth. Thus the human questioners seem to imply a strategy that is somewhere in between. We conjecture three reasons why humans do not achieve the optimal search strategy. First, the questioner does not have access to the ground truth list of objects in the picture, and might, therefore, overestimate the number of objects. Second, some humans tend to favor a linear search strategy. Finally, the questioner may ask additional questions to confirm that he has located the right object. This can be important in the presence of possible oracle errors.


\paragraph{Vocabulary} To gain insight into the vocabulary used by the questioner, we compute the frequency of words in the \GW corpus and display the most frequent words as a word cloud in Fig~\ref{fig:cloud}. Several key words clearly stand out. As explained in Sec.~\ref{sec:game}, some of those key words refer to abstract object properties such as \emph{person} or \emph{object}, spatial locations such as \emph{right/left} or \emph{side} and visual features such as \emph{red/black/white}. Furthermore, prepositions are also heavily used to express relationships between objects. %We also compute a co-occurrence matrix of words and show these correlations in Fig~\ref{fig:word_cooccurence} in Appendix~\ref{ap:statistics}.
To better understand the sequential aspect of the questions, we study the evolution of the vocabulary at each question round.
%and look the  difference to the previous round in Tab.~\ref{tab:new_words} in Appendix~\ref{ap:statistics}. 
We observe that questioners use abstract object properties such as \emph{human/object/furniture} only at the beginning of the dialogues, and quickly switch to either spatial or visual terms such as \emph{left/right}, \emph{white/red} or \emph{table,chair}. This can be highlighted by applying a Dynamic Topic Model~\cite{blei2006dynamic} to study the evolution of topics over the course of the dialogue as shown in Fig~\ref{fig:DTM} in Appendix~\ref{ap:statistics}. %As a consequence, there are fewer words introduced over the course of a dialogue as shown in Fig~\ref{fig:new_words} in Appendix~\ref{ap:statistics}.


\begin{figure*}[t]
\centering
\begin{subfigure}{0.30\linewidth}
\includegraphics[width=\linewidth]{stats/success_objects.pdf}
\vskip -0.6em
\caption{}
\label{fig:success_objects}
\vskip -1.2em
\end{subfigure}
\begin{subfigure}{0.33\linewidth}
\includegraphics[width=\linewidth]{stats/success_area.pdf}
\vskip -0.6em
\caption{}
\label{fig:success_area}
\vskip -1.2em
\end{subfigure}
\begin{subfigure}{0.32\linewidth}
\includegraphics[width=\linewidth]{stats/yes_no.pdf}
\vskip -0.6em
\caption{}
\label{fig:yes_no}
\vskip -1.2em
\end{subfigure}
\vskip -0.5em
\caption{(a-b) Histogram of absolute/relative successful dialogues with respect to the number of objects and the size of the objects, respectively. (c) Evolution of answer distribution clustered by the dialogue length}
\label{fig:success}
\vskip -1em
\end{figure*}

 
\paragraph{Elements of success} To investigate whether certain object properties favour success, we compute the success ratio of dialogues relative to: the size of the unknown objects in Fig~\ref{fig:success_area}, the number of objects within the images in Fig~\ref{fig:success_objects}, the object category, the location of objects within the images and the size of the dialogues in Fig~\ref{fig:success_category}, Fig~\ref{fig:success_spatial} and Fig~\ref{fig:success_length} in Appendix ~\ref{ap:statistics}, respectively. As one may expect, the more complex the scene is, the lower the success rate is. When there are only 3 objects, the questioner has 95\% success rate, while this ratio drops to around 70\% with 20 objects. Similarly, big objects are almost always found while the smallest one are only found 60\% of the time. Questioners easily find objects in the middle of the picture but have more difficulties to find them on the border. Finally, objects from categories that are often grouped together, e.g. \emph{bananas} or \emph{books}, have a lower success rates. 


\paragraph{Miscellaneous} In Fig~\ref{fig:yes_no} we break down the ratio of yes-no answers within the dialogues. While the first yes-no answers are balanced for small dialogues, they often terminate with a final \emph{yes}. In contrast, long dialogues often start with a higher proportion of negative answers which slowly decrease during the exchange. 

\subsection{Dataset release}
We split the \GW dataset by randomly assigning 70\%, 15\% and 15\% of the \emph{images} and its corresponding dialogues to the training, validation and test set. This way of dividing the data ensures that we evaluate performance on images not seen during training. The \GW dataset is available at \url{https://guesswhat.ai/download}. 


\section{Baselines}\label{sec:baselines}
We now empirically investigate the difficulty of the oracle and questioner tasks. To do so, we trained reasonable baselines for each task and measured their performance. 
% The source code is freely available on Github\footnote{\url{HIDDEN\_FOR\_BLIND\_REVIEW}}.

Formally, a \GW game revolves around an image $I \in \mathbb{R}^{M\times N}$ containing a set of $K$ segmented objects $\{O_1, \dots, O_K\}$. Each object $O_k$ is assigned an object category $c_k \in \{1, \dots, C\}$ and has a pixel-wise segmentation mask $S_k \in \{0, 1\}^{M\times N}$ to specify its location and size. The game further consists of a sequence of questions and answers $D = \{q_1, a_1, \hdots, q_J, a_J\}$, produced by the questioner and oracle. We will use $q_{<j}$ and $a_{<j}$ to refer to the first $j-1$ questions and answers, respectively. Each question $q_j$ contains a sequence of $N_j$ tokens, i.e.\ $q_j = \{w_{j1}, \dots, w_{jN_j}\}$, where $w_{ji}$ is taken from a vocabulary $V$ and represents the token at position $i$ in question $j$. Each answer is either \emph{Yes}, \emph{No} or \emph{N/A}, i.e.\ $a_j \in \{\text{Yes, No, N/A}\}$. Finally, the oracle has access to the identity of the correct object $O_{\text{correct}}$, and the prediction of the questioner will be denoted as $O_{\text{predict}}$. 

\subsection{Oracle baselines}
\begin{figure}[t]
\begin{center}
\includegraphics[width=0.95\linewidth]{./misc/oracle_model.pdf}
\end{center}
   \caption{An schematic overview of the "Image + Question + Crop + Spatial + Category" oracle model. }
\label{fig:oracle_model}
\vskip -1em
\end{figure}

The oracle task requires to produce a yes-no answer for any object within a picture given a natural language question. We first introduce our model and then outline its results to get a better understanding of the \GW dataset. 

\paragraph{Model} 
We propose a simple neural network based approach to this model, illustrated in Fig~\ref{fig:oracle_model}. Specifically, we use an appropriate neural network architecture to embed each of the following information: the image $I$, the cropped object from $S$, its spatial information, its category $c$ and the current question $q$. These embeddings are then concatenated as a single vector and fed as input to a single hidden layer MLP that outputs the final answer distribution using a softmax layer. Finally, we minimize the cross-entropy error during the training and report the classification error at evaluation time.  
%

The details on how we compute the embeddings are as follows.
To embed the full image, it is rescaled to a $224$ by $224$ image and is passed through a pre-trained VGG network to obtain its FC8 features. As for the selected object, it is first cropped by finding the smallest rectangle that encapsulates it, based on its segmentation mask. We then rescale the crop to a $224$ by $224$ square, before obtaining its FC8 features from the pre-trained VGG network. Although we could use the mask to drop out pixels around the selected object, we keep the crop as is since pre-trained VGG networks are exposed to such background noise during their training. %Rescaling the object region also has the advantage to be invariant to the scale of the object.
%
We also embed the spatial information of the crop, to help locate the cropped object within the whole image. To do so, we follow the approach of \cite{hu2016natural,DBLP:journals/corr/YuPYBB16} and extract an 8-dimensional vector of the location of the bounding box:
\begin{align}
x_{spatial} = [ &x_{min}, y_{min}, x_{max}, y_{max},\notag\\ &x_{center}, y_{center}, w_{box}, h_{box}]\label{eq:spatial}
\end{align}
where $w_{box}$ and $h_{box}$ denote the width and height of the bounding box, respectively. We normalize the image height and width such that coordinates range from $-1$ to $1$, and place the origin at the center of the image.
As for the object category, we convert its one-hot class vector into a dense category embedding using a learned look-up table.
Finally, the embedding of the current natural language question $q$ is computed using an Long Short-Term Memory (LSTM) network~\cite{hochreiter1997long} where questions are first tokenized by using the word punct tokenizer from the python nltk toolkit~\cite{bird2009natural}. For simplicity, we decided to ignore the question-answer pairs history $q_{<t}$ in our oracle baseline.


\paragraph{Training setting}
We train all oracle models on the full dataset. During training, we keep the parameters of the VGG network fixed, and optimize the LSTM, object category/word look-up tables and MLP parameters by minimizing the negative log-likelihood of the correct answer. We use ADAM~\cite{DBLP:journals/corr/KingmaB14} for optimization and train for at most $15$ epochs. We use early stopping on the validation set, and report the train, valid and test error. 


\paragraph{Results} 
We report results for several oracle models using a different set of inputs in Table~\ref{tab:oracle_baseline}. We name the model after the input we feed to it. For instance, (Question+Category+Spatial+Image) refers to the network fed with the question $q$, the object category $c$, the spatial features $x_{spatial}$ and the full image $I$. The results of all subsets are reported in Table~\ref{tab:all_oracle_baseline} in Appendix~\ref{ap:statistics}. 

Because the \GW dataset is fairly balanced, simply outputting the most common answer in the training set -- No -- results in a high $50.8\%$ error rate. Solely providing the image or crop features barely improves upon this result. Only using the question slightly improves the error rate to $41.2\%$. We speculate that this small bias comes from questioners that refer to objects that are never segmented or overrepresented categories. As hoped, we observe that the error rate significantly drops ($< 31\%$) when we finally feed information on the object to guess (crop, spatial or category) to the model. We find that crop and category information are redundant: the (Question+Category) and (Question+Crop) model achieve respectively $29.2\%$ and $25.7\%$ error, while the combined model (Question+Category+Crop) achieves $24.7\%$. In general, we expect the object crop to contain additional information, such as color information, beside the object class. However, we find that the object category outperforms the object crop embedding. This might be partly due to the imperfect feature extraction from the crops. Finally, our best performing model combines object category and its spatial features along with the question. 


\begin{table}
\scalebox{0.79}{
\begin{tabular}{|l|l|l|l|}
\hline
Model & Train err & Val err & Test err\\
\hline
\hline
Dominant class (no) & 47.4\% & 46.2\% & 50.9\%\\
Question & 40.2\% & 41.7\% & 41.2\%\\
Image & 45.7\% & 46.7\% & 46.7\%\\
Crop & 40.9\% & 42.7\% & 43.0\%\\
\hline
Question + Crop & 22.3\% & 29.1\% & 29.2\%\\
Question + Image & 37.9\% & 40.2\% & 39.8\%\\
Question + Category & 23.1\% & 25.8\% & 25.7\%\\
Question + Spatial & 28.0\% & 31.2\% & 31.3\%\\
\hline
Question + Category + Spatial & 17.2\% & 21.1\% & \textbf{21.5\%}\\
Question + Category + Crop & 20.4\% & 24.4\% & 24.7\%\\
Question + Spatial + Crop & 19.4\% & 26.0\% & 26.2\%\\
\hline
Question + Category + Spatial + Crop & 16.1\% & 21.7\% & 22.1\%\\
Question + Spatial + Crop + Image & 20.7\% & 27.7\% & 27.9\%\\
Question + Category + Spatial + Image & 19.2\% & 23.2\% & 23.5\%\\
%\hline
\hline
\end{tabular}}
\caption{Classification errors for the oracle baselines on train, valid and test set. The best performing model is "Question + Category + Spatial" and refers to the MLP that takes the question, the selected object class and its spatial features as input.}
\label{tab:oracle_baseline}
\vskip -1em
\end{table}


\subsection{Questioner baselines}
Given an image, the questioner must ask a series of questions and guess the correct object. We separate the questioner task into two different sub-tasks that are trained independently:
\begin{description}
\item[Guesser] Given an image $I$ and a sequence of questions and answers $D_J$, predict the correct object $O_{\text{correct}}$ from the set of all objects $O$.
\item[Question Generator] Given an image $I$ and a sequence of $T$ questions and answers $D_{\leq T}$, produce a new question $q_{T+1}$. 
\end{description}
In general, one also needs a module to determine when to start guessing the object (and stop asking questions). In our baseline, we bypass this issue by fixing the number of questions to $5$ for the question generator model.

\paragraph{Guesser}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{misc/guesser_model2.pdf}
    \caption{Overview of the guesser model for an image with 4 segmented objects. The weights are shared among the MLPs, this allows for an arbitrary number of objects. }
    \label{fig:guesser}
\end{figure}

\begin{table}[t]
\centering
\scalebox{0.79}{
\begin{tabular}{|l|l|l|l|}
\hline
Model & Train err & Val err & Test err\\
\hline
Human & 9.0\% & 9.2\% & 9.2\%\\
Random & 82.9\% & 82.9\% & 82.9\%\\
\hline
LSTM & 27.9\% & 37.9\% & \textbf{38.7}\%\\
HRED & 32.6\% & 38.2\% & 39.0\%\\
\hline
LSTM+VGG & 26.1\% & 38.5\% & 39.5\%\\
HRED+VGG & 27.4\% & 38.4\% & 39.6\%\\
\hline
\end{tabular}
}
\caption{Classification errors for the guesser baselines on train, valid and test set.}
\label{table:guesser_baseline}
\vskip -1em
\end{table}

The role of the guesser model is to predict the correct object. To do so, the guesser has access to the image, the dialogue and the list of objects in the image. We encode the image by extracting its FC8 features from VGG16 network. A dialogue of a \GW game is a sequence on two different levels: there is a variable number of question-answer pairs where each question in turn consists of a variable-length sequence of tokens. This can be encoded into a fixed size vector by using either an LSTM encoder~\cite{hochreiter1997long} or an HRED encoder~\cite{DBLP:journals/corr/SerbanSBCP15}. While the LSTM encoder considers the dialogue as one flat sequence, HRED explicitly models the hierarchy by two different Recurrent Neural Networks (RNN). First, an encoder RNN creates a fixed-size representation of a question or answer by reading in its tokens and taking the last hidden state of the RNN. This representation is then processed by the context RNN to obtain a representation of the current dialogue \emph{state}. For both models, we concatenate the image and dialogue features and do a dot-product with the embedding for all the objects in the image, followed by a softmax to obtain a prediction distribution over the objects. Given  the best performance of the "Question+Category+Spat" oracle model, we represent objects by their category and their spatial features. More precisely, we concatenate the 8-dimensional spatial representation (see Eq.~\ref{eq:spatial}) and the object category look-up and pass it through an MLP layer to get an embedding for the object. Note that the MLP parameters are shared to handle the variable number of objects in the image. See Fig~\ref{fig:guesser} for an overview of the guesser with HRED and LSTM.

Table~\ref{table:guesser_baseline} reports the results for the guesser baselines using human-generated dialogues. As a first baseline, we report the performance of a random guesser which does not use the dialogue information. We split the guesser results based on whether they use the VGG features or not. In general, we find that including VGG features does not improve the performance of the LSTM and HRED models. We hypothesize that the VGG features are a too coarse representation of the image scene, and that most of the visual information is already encoded in the question and the object features. Surprisingly, we find LSTMs to perform slightly better than the sophisticated HRED models. %We observe that HRED overfits faster than LSTM partly because of the additional parameters that are introduced. %As a next step, we may initialize the HRED model with parameters from a pre-trained Question Generator.
%We also experiment with initializing the HRED model with parameters from a pre-trained Question Generator and notice a slight performance improvement when we fine-tune this model. 

\paragraph{Question Generator}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{./misc/question_generation_model3.pdf}
    \caption{HRED model conditioned on the VGG features of the image. To avoid clutter, 
    we here only show the part of the model that defines a distribution over the third question given the first two questions, its answers and the image $P(q_2|q_{<2},a_{<2},I)$.
    The complete HRED model models the distribution over all questions.
    }
    \label{fig:HRED}
    \vskip -1em
\end{figure}

The question generation task is hard for several reasons. First, it requires high-level visual understanding to ask meaningful questions. Second, the generator should be able to handle long-term context to ask a sequence of relevant questions, which is one of the most challenging problems in dialogue systems. Additionally, we evaluate the question generator using the imperfect oracle and imperfect guesser, which introduces compounding errors. 

Hierarchical recurrent encoder decoder (HRED)~\cite{DBLP:journals/corr/SerbanSBCP15} is the current state of the art method for natural language generation tasks. We extend this model by conditioning on the VGG features of the image as illustrated in Fig~\ref{fig:HRED}. Finally, we train our proposed model by maximizing the conditional log-likelihood:
\begin{align}
   \log P(Q|A,I) = & \log \prod_{j=1}^J P(q_{j}| q_{<j}, a_{<j}, I)\\
    =&\log \prod_{j=1}^J \prod_{i=1}^{N_j} P(w_{ji} | w_{j <i}, a_{\leq j}, I)
\end{align}
with respect to the described parameters. At test time, we use a beam-search to approximately find the most probable question $q_j$. 
Evaluating the questioner model requires a pre-trained oracle and a pre-trained guesser model. We use our questioner model to first generate a question which is then answered by the oracle model. We repeat this procedure 5 times to obtain a dialogue. We then use the best performing guesser model to predict the object and report its error as the metric for the QGEN model. Since we use ground truth answers during the QGEN training while we use oracle answers at test time, there is a mismatch between the training and testing procedure. This can be avoided by using the oracle answers also during training time. We call these models QGEN+GT and QGEN+ORACLE respectively. 

Table~\ref{table:questioner_baseline} shows the results. A guesser based on human generated dialogues achieves $38.7\%$ error. The Question Generator models achieve reasonable performance which lies in between the random performance and the performance of the guesser on human dialogues. We observe that using the Oracle's answers while training the Question Generator introduces additional errors which significantly deteriorates performance. Some example dialogues generated by the QGen+GT model are shown in Fig. \ref{fig:correct_samples} and \ref{fig:incorrect_samples}. 

\begin{table}
\centering
\scalebox{0.79}{
\begin{tabular}{|l|l|}
\hline
Model & Error\\
\hline
Human generated dialogue & 38.7\%\\
QGen+GT & 53.2\%\\
QGen+ORACLE & 66.0\%\\
Random & 82.9\%\\
\hline
\end{tabular}
}
\caption{Test error for the question generator models (QGEN) based on VGG+HRED(FT) guesser model. We here report the accuracy error of the guesser model fed with the questions from the QGEN model.}
\label{table:questioner_baseline}
\vskip -1em
\end{table}



\section{Discussion}
\label{sec:discussion}
We introduced the \GW game, a novel framework for multi-modal dialogue. To the best of our knowledge, we present the first large-scale dataset involving images and dialogue. A wide range of challenges may arise from this union as they rely on different fields of machine learning such as natural language understanding, generative models or computer vision. 
\GW turns out to be an engaging game that greatly decreases the cost for collection of a big dataset required for modern algorithms. As a second contribution, we introduced three tasks based on the questioner and oracle role. In each case, we prototyped a neural architecture as a first baseline. We analyzed these results and presented a quantitative description of the \GW dataset.  

We believe \GW could allow for a myriad of other applications that may either be based on the game itself or extending the database to other tasks. For instance, it can be interesting to compute a confidence interval before proceeding to the final guess.  Differently, \GW could be a test bed for one-shot learning~\cite{fei2006one} of guessing new object categories, transfer learning on line-drawing images~\cite{castrejon2016learning} or using questions from another language. Thus, the \GW dataset offers an opportunity to develop original machine learning tasks upon it.

\paragraph{Acknowledgement} The authors would like to acknowledge the stimulating environment provided by the MILA and SequeL labs. We thank all members of the MILA lab who participated in a trial run of the data collection, and all workers of AMT who participated in our HITs. We thank Jake Snell, Mengye Ren, Laurent Dinh, Jeremie Mary and Bilal Piot for helpful discussions. We acknowledge the following agencies for research funding and computing support: NSERC, Calcul Qu\'{e}bec, Compute Canada, the Canada Research Chairs and CIFAR, CHISTERA IGLU and CPER Nord-Pas de Calais/FEDER DATA Advanced data science and technologies 2015-2020. SC is supported by a FQRNT-PBEEE scholarship.

{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\newpage

\appendix

\onecolumn

\section{User interface}
\label{ap:website}
Figure \ref{fig:oracle}, \ref{fig:questioner} presents the instructions for the oracle and questioner before they started their first game. 

\begin{figure}[h!]
\begin{subfigure}{0.32\linewidth}
\includegraphics[width=\linewidth]{oracle_gif/edited-15.png}
\caption{}
\end{subfigure}
\begin{subfigure}{0.32\linewidth}
\includegraphics[width=\linewidth]{oracle_gif/edited-16.png}
\caption{}
\end{subfigure}
\begin{subfigure}{0.32\linewidth}
\includegraphics[width=\linewidth]{oracle_gif/edited-17.png}
\caption{}
\end{subfigure}
\begin{subfigure}{0.32\linewidth}
\includegraphics[width=\linewidth]{oracle_gif/edited-78.png}
\caption{}
\end{subfigure}
\begin{subfigure}{0.32\linewidth}
\includegraphics[width=\linewidth]{oracle_gif/edited-79.png}
\caption{}
\end{subfigure}
\begin{subfigure}{0.32\linewidth}
\includegraphics[width=\linewidth]{oracle_gif/edited-144.png}
\caption{}
\end{subfigure}
\begin{subfigure}{0.32\linewidth}
\includegraphics[width=\linewidth]{oracle_gif/edited-145.png}
\caption{}
\end{subfigure}
\begin{subfigure}{0.32\linewidth}
\includegraphics[width=\linewidth]{oracle_gif/edited-146.png}
\caption{}
\end{subfigure}
\begin{subfigure}{0.32\linewidth}
\includegraphics[width=\linewidth]{oracle_gif/edited-148.png}
\caption{}
\end{subfigure}
\begin{subfigure}{0.32\linewidth}
\includegraphics[width=\linewidth]{oracle_gif/edited-157.png}
\caption{}
\end{subfigure}
\begin{subfigure}{0.32\linewidth}
\includegraphics[width=\linewidth]{oracle_gif/edited-240.png}
\caption{}
\end{subfigure}
\begin{subfigure}{0.32\linewidth}
\includegraphics[width=\linewidth]{oracle_gif/edited-284.png}
\caption{}
\end{subfigure}
\begin{subfigure}{0.32\linewidth}
\includegraphics[width=\linewidth]{oracle_gif/edited-369.png}
\caption{}
\end{subfigure}
\begin{subfigure}{0.32\linewidth}
\includegraphics[width=\linewidth]{oracle_gif/edited-378.png}
\caption{}
\end{subfigure}
\caption{An example game from the perspective of the oracle. Shown from left to right and top to bottom. }
\label{fig:oracle}
\end{figure}

\begin{figure}[h!]
\begin{subfigure}{0.32\linewidth}
\includegraphics[width=\linewidth]{questioner_gif/32.png}
\caption{}
\end{subfigure}
\begin{subfigure}{0.32\linewidth}
\includegraphics[width=\linewidth]{questioner_gif/33.png}
\caption{}
\end{subfigure}
\begin{subfigure}{0.32\linewidth}
\includegraphics[width=\linewidth]{questioner_gif/57.png}
\caption{}
\end{subfigure}
\begin{subfigure}{0.32\linewidth}
\includegraphics[width=\linewidth]{questioner_gif/65.png}
\caption{}
\end{subfigure}
\begin{subfigure}{0.32\linewidth}
\includegraphics[width=\linewidth]{questioner_gif/70.png}
\caption{}
\end{subfigure}
\begin{subfigure}{0.32\linewidth}
\includegraphics[width=\linewidth]{questioner_gif/93.png}
\caption{}
\end{subfigure}
\begin{subfigure}{0.32\linewidth}
\includegraphics[width=\linewidth]{questioner_gif/142.png}
\caption{}
\end{subfigure}
\begin{subfigure}{0.32\linewidth}
\includegraphics[width=\linewidth]{questioner_gif/214.png}
\caption{}
\end{subfigure}
\begin{subfigure}{0.32\linewidth}
\includegraphics[width=\linewidth]{questioner_gif/232.png}
\caption{}
\end{subfigure}
\begin{subfigure}{0.32\linewidth}
\includegraphics[width=\linewidth]{questioner_gif/274.png}
\caption{}
\end{subfigure}
\begin{subfigure}{0.32\linewidth}
\includegraphics[width=\linewidth]{questioner_gif/299.png}
\caption{}
\end{subfigure}
\begin{subfigure}{0.32\linewidth}
\includegraphics[width=\linewidth]{questioner_gif/351.png}
\caption{}
\end{subfigure}
\begin{subfigure}{0.32\linewidth}
\includegraphics[width=\linewidth]{questioner_gif/364.png}
\caption{}
\end{subfigure}
\begin{subfigure}{0.32\linewidth}
\includegraphics[width=\linewidth]{questioner_gif/407.png}
\caption{}
\end{subfigure}
\begin{subfigure}{0.32\linewidth}
\includegraphics[width=\linewidth]{questioner_gif/415.png}
\caption{}
\end{subfigure}\\
\begin{subfigure}{0.32\linewidth}
\includegraphics[width=\linewidth]{questioner_gif/416.png}
\caption{}
\end{subfigure}
\begin{subfigure}{0.32\linewidth}
\includegraphics[width=\linewidth]{questioner_gif/420.png}
\caption{}
\end{subfigure}
\caption{An example game from the perspective of the questioner. Shown from left to right and top to bottom.}
\label{fig:questioner}
\end{figure}

\begin{figure}[t!]
\centering
\begin{subfigure}{0.75\linewidth}
\includegraphics[width=\linewidth]{website/fix.png}
\caption{Interface to fix ill-formatted questions}
\end{subfigure}
\begin{subfigure}{0.75\linewidth}
\includegraphics[width=\linewidth]{website/diff.png}
\caption{Interface to validate the fix  ill-formatted questions}
\end{subfigure}
\caption{In the first task, we ask workers to correct mistakes in the questions. We then ask workers to validate the proposed correction by showing the difference between the original question and its correction. We alternate both tasks till all questions are corrected and validated.}
\label{fig:mistake_fixer}
\end{figure}


\clearpage

\section{\GW samples}
\label{ap:gw_samples}

\begin{figure}[h!]
\centering
\includegraphics[width=0.90\linewidth]{./misc/examples3.pdf}
  \caption{Three other examples of our dataset.}
\label{fig:example2}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=0.90\linewidth]{./misc/examples4.pdf}
  \caption{Same picture but different objects.}
\label{fig:same_picture}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=0.90\linewidth]{./misc/long_example.pdf}
  \caption{A long dialogue example in a very rich environment.}
\label{fig:long_example}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=0.90\linewidth]{./misc/referit2.pdf}
  \caption{Samples illustrating the difference between \GW and \Referit games. As both dataset are constructed on top of MS COCO, we picked identical objects (and images).}
\label{fig:referit}
\end{figure}


\clearpage


\section{Additional database statistics}
\label{ap:statistics}
Figure~\ref{fig:word_cooccurence} presents a word co-occurrence matrix of the \GW dataset. Figure~\ref{fig:coco_cat} and Figure~\ref{fig:coco_area} compares the object size and category distribution of \GW with MS Coco.

\begin{figure*}[h!]
    \centering
        \includegraphics[ width=\linewidth]{stats/word_cooccurence.pdf}
    \caption{Co-occurrence matrix of words. Only the 50 most frequent words are kept. Rows are first normalized before being sorted thanks a hierarchical clustering with an euclidean distance.}
    \label{fig:word_cooccurence}
\end{figure*}


\begin{figure}
\centering
\includegraphics[width=\linewidth]{stats/coco_category.pdf}
\caption{Visualization of the object category distribution of MS COCO and \GW dataset. The person category was removed for clarity (resp. $273469$ and $188204$).}
\label{fig:coco_cat}
\end{figure}

\begin{figure}[t!]
\centering
\begin{subfigure}{0.45\linewidth}
\includegraphics[width=\linewidth]{stats/coco_area.pdf}
\caption{}
\label{fig:coco_area}
\end{subfigure}
\begin{subfigure}{0.45\linewidth}
\includegraphics[width=\linewidth]{stats/top30cat_pie.pdf}
\caption{}
\label{fig:top30}
\end{subfigure}
\caption{(a) Visualization of the object size distribution of MS COCO and \GW dataset. 
%The \GW dataset has a small bias against very small objects compared to the original MS COCO dataset.  
(b)  Distribution of the the 30 (out of 80) prominent object categories in the \GW which represent 71.3\% of the objects.
}
\vskip -1em
\end{figure}


\newpage

\begin{figure}[t]
\centering
\begin{subfigure}{0.45\linewidth}
\includegraphics[width=\linewidth]{stats/w_q.pdf}
\caption{}
\label{fig:new_words}
\end{subfigure}
\begin{subfigure}{0.45\linewidth}
\includegraphics[width=\linewidth]{./stats/new_words.pdf}
\caption{}
\label{fig:w_q}
\end{subfigure}
\caption{(a) Number of words per question. The question length follows a Poisson-like distribution, a finding which is in line with other datasets~\cite{antol2015vqa}. (b) Percentage of apparition of new words along a dialogues. Questioner tends keep using the same words during the dialogues.}
\vskip -1em
\end{figure}


\begin{figure}[t]
  \begin{minipage}{\textwidth}
  \begin{minipage}{0.45\textwidth}
    \centering
    \begin{tabular}{|c|c|}
    \hline
    Topic 1 & Topic 2\\
    \hline
    Abstract words & Descriptive words\\
    \hline
    person & left \\
    food & one \\
    vehicle  & right \\
    human & wearing \\
    car & side\\
    one & white\\
    object & red\\
    animal & table\\
    \hline
    \end{tabular}
  \end{minipage}
  \hfill
  \begin{minipage}{0.45\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{stats/topics.pdf}
    \end{minipage}
  \end{minipage}
\caption{Relative evolution of topics during a dialogue of size 6. We applied Data Topic Models (DTM) \cite{blei2006dynamic} with the python framework \cite{rehurek_lrec} on our dataset. The table reports the two prominent detected topics with their respective key words while the figure display their relative evolution during the dialogue. The topic titles are manually picked.}
\label{fig:DTM}
\end{figure}


\clearpage

\begin{figure}
\centering
\includegraphics[width=1\linewidth]{./stats/success_category.pdf}
\caption{Histogram of success ratio broken down per object category.}
\label{fig:success_category}
\end{figure}

\begin{figure}[t]
\centering
\begin{subfigure}{0.45\linewidth}
\includegraphics[width=\linewidth]{stats/success_spatial.pdf}
\caption{}
\label{fig:success_spatial}
\end{subfigure}
\begin{subfigure}{0.45\linewidth}
\includegraphics[width=\linewidth]{stats/success_length.pdf}
\caption{}
\label{fig:success_length}
\end{subfigure}
\centering
\caption{(a) Heatmap of the success ratio with respect to the spatial location within the picture. (b) Histogram of the success ratio relative to the dialogue length.}
\end{figure}


% \begin{figure*}[h]
% \centering
% \begin{subfigure}{0.45\linewidth}
% \includegraphics[width=\linewidth]{stats/seq_question_total.pdf}
% \caption{}
% \label{fig:seq_question_total}
% \end{subfigure}
% \begin{subfigure}{0.45\linewidth}
% \includegraphics[width=\linewidth]{stats/seq_question_length.pdf}
% \caption{}
% \label{fig:seq_question_length}
% \end{subfigure}
% \caption{(a) Number of inter-dependent questions sorted by hand-crafted rules. We count questions containing: 'One' : the word  \emph{one}, 'Ordinal numbers' :  \emph{first}, \emph{second} etc., 'Possessive Pronouns' : \emph{his}, \emph{her}, \emph{its} etc. 'Object Pronouns' : \emph{him}, \emph{her}, \emph{them} etc. (n) Length of sequential dependant questions clustered by their length (starting from the root question)}
% \label{fig:question_dependency}
% \end{figure*}





\begin{landscape}
\begin{table}
\begin{subtable}[t]{\linewidth}
\small

\begin{tabular}{|l c|l c c |l c c |}
\hline
person & 14.48 &person & 3.20 &=& left & 2.95 &\textbf{\color{green}$\nearrow$}\\ 
food & 1.29 &left & 1.69 & & right & 2.32 &\textbf{\color{green}$\nearrow$}\\ 
animal & 1.16 &wearing & 1.20 &new& person & 2.28 &\textbf{\color{red}$\searrow$}\\ 
human & 1.03 &right & 1.02 &new& wearing & 1.66 &\textbf{\color{red}$\searrow$}\\ 
object & 0.77 &front & 0.97 &new& whole & 1.58 &new\\ 
car & 0.60 &white & 0.91 &new& white & 1.56 &=\\ 
vehicle & 0.57 &red & 0.77 &new& red & 1.26 &=\\ 
cat & 0.41 &car & 0.64 &\textbf{\color{red}$\searrow$}& black & 1.19 &\textbf{\color{green}$\nearrow$}\\ 
alive & 0.37 &black & 0.60 &new& front & 1.14 &\textbf{\color{red}$\searrow$}\\ 
dog & 0.35 &blue & 0.59 &new& blue & 1.10 &=\\ 
\hline
\end{tabular}\caption{Dialogues having 3 questions}
\end{subtable}

\begin{subtable}[t]{\linewidth}
\small
\begin{tabular}{|l c|l c c |l c c |l c c |l c c |}
\hline
person & 8.20 &person & 1.98 &=& left & 1.69 &\textbf{\color{green}$\nearrow$}& left & 1.92 &=& left & 2.13 &=\\ 
food & 1.03 &left & 1.03 &\textbf{\color{green}$\nearrow$}& person & 1.41 &\textbf{\color{red}$\searrow$}& right & 1.77 &\textbf{\color{green}$\nearrow$}& right & 2.04 &=\\ 
human & 0.56 &right & 0.66 &new& right & 1.26 &=& person & 1.20 &\textbf{\color{red}$\searrow$}& white & 1.28 &\textbf{\color{green}$\nearrow$}\\ 
animal & 0.46 &front & 0.59 &new& white & 0.84 &\textbf{\color{green}$\nearrow$}& white & 1.12 &=& person & 1.26 &\textbf{\color{red}$\searrow$}\\ 
vehicle & 0.45 &car & 0.51 &\textbf{\color{green}$\nearrow$}& wearing & 0.82 &\textbf{\color{green}$\nearrow$}& wearing & 0.93 &=& black & 0.90 &\textbf{\color{green}$\nearrow$}\\ 
object & 0.42 &white & 0.48 &new& side & 0.67 &\textbf{\color{green}$\nearrow$}& black & 0.79 &\textbf{\color{green}$\nearrow$}& wearing & 0.85 &\textbf{\color{red}$\searrow$}\\ 
car & 0.36 &wearing & 0.48 &new& red & 0.62 &\textbf{\color{green}$\nearrow$}& red & 0.72 &=& red & 0.80 &=\\ 
furniture & 0.24 &side & 0.43 &new& front & 0.58 &\textbf{\color{red}$\searrow$}& side & 0.69 &\textbf{\color{red}$\searrow$}& whole & 0.80 &new\\ 
left & 0.24 &red & 0.39 &new& black & 0.55 &new& blue & 0.65 &\textbf{\color{green}$\nearrow$}& blue & 0.75 &=\\ 
edible & 0.20 &vehicle & 0.39 &\textbf{\color{red}$\searrow$}& blue & 0.54 &new& front & 0.58 &\textbf{\color{red}$\searrow$}& front & 0.73 &=\\ 
\hline
\end{tabular}\caption{Dialogues having 5 questions}
\end{subtable}

\begin{subtable}[t]{\linewidth}
\small
\begin{tabular}{|l c|l c c |l c c |l c c |l c c |l c c |l c c |}
\hline
person & 5.89 &person & 1.44 &=& left & 1.08 &\textbf{\color{green}$\nearrow$}& left & 1.26 &=& left & 1.33 &=& left & 1.42 &=& left & 1.65 &=\\ 
food & 0.74 &left & 0.73 &\textbf{\color{green}$\nearrow$}& person & 0.96 &\textbf{\color{red}$\searrow$}& right & 1.08 &\textbf{\color{green}$\nearrow$}& right & 1.22 &=& right & 1.39 &=& right & 1.54 &=\\ 
human & 0.38 &right & 0.42 &new& right & 0.89 &=& person & 0.82 &\textbf{\color{red}$\searrow$}& white & 0.81 &\textbf{\color{green}$\nearrow$}& white & 0.88 &=& white & 0.96 &=\\ 
vehicle & 0.30 &table & 0.37 &\textbf{\color{green}$\nearrow$}& side & 0.57 &\textbf{\color{green}$\nearrow$}& white & 0.67 &\textbf{\color{green}$\nearrow$}& person & 0.80 &\textbf{\color{red}$\searrow$}& person & 0.84 &=& person & 0.90 &=\\ 
object & 0.28 &front & 0.36 &new& white & 0.50 &\textbf{\color{green}$\nearrow$}& side & 0.60 &\textbf{\color{red}$\searrow$}& wearing & 0.59 &\textbf{\color{green}$\nearrow$}& black & 0.63 &\textbf{\color{green}$\nearrow$}& red & 0.65 &\textbf{\color{green}$\nearrow$}\\ 
car & 0.26 &food & 0.35 &\textbf{\color{red}$\searrow$}& wearing & 0.48 &\textbf{\color{green}$\nearrow$}& wearing & 0.54 &=& side & 0.57 &\textbf{\color{red}$\searrow$}& red & 0.57 &\textbf{\color{green}$\nearrow$}& black & 0.63 &\textbf{\color{red}$\searrow$}\\ 
animal & 0.26 &side & 0.35 &new& red & 0.41 &new& red & 0.49 &=& red & 0.54 &=& wearing & 0.56 &\textbf{\color{red}$\searrow$}& blue & 0.57 &\textbf{\color{green}$\nearrow$}\\ 
furniture & 0.20 &car & 0.31 &\textbf{\color{red}$\searrow$}& table & 0.39 &\textbf{\color{red}$\searrow$}& table & 0.41 &=& black & 0.51 &\textbf{\color{green}$\nearrow$}& blue & 0.54 &\textbf{\color{green}$\nearrow$}& wearing & 0.52 &\textbf{\color{red}$\searrow$}\\ 
left & 0.14 &wearing & 0.28 &new& front & 0.38 &\textbf{\color{red}$\searrow$}& black & 0.41 &\textbf{\color{green}$\nearrow$}& blue & 0.49 &\textbf{\color{green}$\nearrow$}& side & 0.53 &\textbf{\color{red}$\searrow$}& next & 0.51 &new\\ 
boat & 0.14 &something & 0.28 &new& car & 0.37 &\textbf{\color{red}$\searrow$}& blue & 0.37 &new& front & 0.42 &\textbf{\color{green}$\nearrow$}& front & 0.45 &=& side & 0.51 &\textbf{\color{red}$\searrow$}\\ 
\hline
\end{tabular}\caption{Dialogues having 7 questions}
\end{subtable}
\caption{Proportions of the ten most common words for each depth of questions for sorted by the size of the dialogues}
\label{tab:word_occ_dialogues}
\end{table}

\end{landscape}


\clearpage
\section{All oracle baselines}
\label{ap:oracle_baselines}
\vspace{-0.5em}
\begin{table}[h!]
\begin{center}
\begin{tabular}{|l|l|l|l|}
\hline
Model & Train err & Valid err & Test err\\
\hline
Dominant class (no) & 47.4\% & 46.2\% & 50.9\%\\
Category & 43.0\% & 42.8\% & 43.1\%\\
Question & 40.2\% & 41.7\% & 41.2\%\\
Crop & 40.9\% & 42.7\% & 43.0\%\\
Image & 45.7\% & 46.7\% & 46.7\%\\
Spatial & 43.9\% & 44.1\% & 44.3\%\\
\hline
Category + Spatial & 41.6\% & 41.7\% & 42.1\%\\
Question + Crop & 22.3\% & 29.1\% & 29.2\%\\
Question + Image & 37.9\% & 40.2\% & 39.8\%\\
Question + Category & 23.1\% & 25.8\% & 25.7\%\\
Question + Spatial & 28.0\% & 31.2\% & 31.3\%\\
Spatial + Crop & 41.8\% & 42.4\% & 42.8\%\\
Crop + Image & 41.6\% & 42.1\% & 42.4\%\\
Spatial + Image & 42.2\% & 44.1\% & 44.2\%\\
Category + Crop & 41.0\% & 41.7\% & 42.3\%\\
Category + Image & 42.3\% & 42.7\% & 43.0\%\\
\hline
Category + Crop + Image & 40.6\% & 41.5\% & 41.8\%\\
Category + Spatial + Crop & 40.6\% & 41.6\% & 42.1\%\\
Question + Category + Spatial & 17.2\% & 21.1\% & \textbf{21.5}\%\\
Question + Crop + Image & 23.7\% & 29.9\% & 30.0\%\\
Category + Spatial + Image & 40.4\% & 42.0\% & 42.2\%\\
Question + Category + Image & 23.4\% & 27.1\% & 27.4\%\\
Question + Spatial + Image & 28.4\% & 32.5\% & 32.5\%\\
Spatial + Crop + Image & 41.6\% & 42.1\% & 42.5\%\\
Question + Category + Crop & 20.4\% & 24.4\% & 24.7\%\\
Question + Spatial + Crop & 19.4\% & 26.0\% & 26.2\%\\
\hline
Question + Category + Spatial + Crop & 16.1\% & 21.7\% & 22.1\%\\
Question + Spatial + Crop + Image & 20.7\% & 27.7\% & 27.9\%\\
Category + Spatial + Crop + Image & 40.3\% & 41.4\% & 41.8\%\\
Question + Category + Spatial + Image & 19.2\% & 23.2\% & 23.5\%\\
Question + Category + Crop + Image & 20.0\% & 25.3\% & 25.5\%\\
\hline
Question + Category + Spatial + Crop + Image & 17.8\% & 23.2\% & 23.3\%\\
\hline
\end{tabular}
\caption{Classification errors for all oracle baselines.}
\label{tab:all_oracle_baseline}
\end{center}
\vskip -1em
\end{table}

\clearpage
\section{Guesser generation model}
\label{ap:guesser}

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.60\linewidth]{./misc/gw_correct_samples.pdf}
    \caption{Three samples of QGen+GT model for which the correct object was predicted. }
    \label{fig:correct_samples}
\end{figure*}

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.60\linewidth]{./misc/gw_incorrect_samples.pdf}
    \caption{Three dialogue samples of QGen+GT model for which the wrong object was predicted.}
    \label{fig:incorrect_samples}
\end{figure*}

%  \begin{figure}[h]
%      \centering
%      \includegraphics[width=0.7\linewidth]{./misc/lstm_guesseur.pdf}
%      \caption{Overview of the LSTM guesseur model. }
%      \label{fig:lstm_guesser}    
%      \vskip -1em
%  \end{figure}

%\vspace{-1em}
%\section{Question generation model}
%\label{ap:questioner}
%\vspace{-1em}
%\begin{figure}[h]
%    \centering
%    \includegraphics[width=0.90\linewidth]{./misc/lstm_model2.pdf}
%    \caption{Overview of the LSTM questioner model. 
    %HRED model conditioned on the VGG features of the image. %To avoid clutter, we here only show the part of the model that defines a distribution over the second question given the first question, its answer and the image $P(q_2|q_{<2},a_{<2},I)$. The complete HRED model otherwise models the distribution over all questions.
%    }
%    \label{fig:LSTM}
%\end{figure}


\end{document}




\documentclass[english,twoside,11pt]{article}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

\usepackage{jmlr2e}
%\usepackage{natbib}
\usepackage{amsfonts}
\usepackage{amsmath}
%\usepackage{mathtools}
\usepackage{multicol}
%\usepackage{graphicx}
\usepackage{booktabs}

%\usepackage[justification=centering]{caption}

\usepackage{sidecap}
\usepackage{subfigure}
\usepackage{hyperref}
\hypersetup{
    colorlinks=false,
    pdfborder={0 0 0},
}
%\hypersetup{colorlinks=false}

\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}

% Definitions of handy macros can go here

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}

% Heading arguments are {volume}{year}{pages}{submitted}{published}{author-full-names}

%\jmlrheading{1}{2013}{1-48}{4/00}{10/00}{\c{C}a\u{g}lar G\"{u}l\c{c}ehre and Yoshua Bengio}

\firstpageno{1}
\iftrue
\addtolength{\textwidth}{6mm}
\addtolength{\textheight}{6mm}
\addtolength{\topmargin}{-3mm}
\addtolength{\oddsidemargin}{-3mm}
\addtolength{\evensidemargin}{-3mm}
\fi

\newif\ifLongVersion
\LongVersionfalse

\usepackage{babel}

\begin{document}

\title{Knowledge Matters: Importance of Prior Information for Optimization}

\author{\name \c{C}a\u{g}lar G\"{u}l\c{c}ehre \email gulcehrc@iro.umontreal.ca\\
    \addr D\'epartement d'informatique et de recherche op\'erationnelle\\
    Universit\'e de Montr\'eal,  Montr\'eal, QC, Canada
    \AND
    \name Yoshua Bengio \email bengioy@iro.umontreal.ca \\
    \addr D\'epartement d'informatique et de recherche op\'erationnelle\\
    Universit\'e de Montr\'eal,  Montr\'eal, QC, Canada}

\editor{Not Assigned}

\maketitle

\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file

We explore the effect of introducing prior information into the intermediate
level of deep supervised neural networks for a learning task on which all
the black-box state-of-the-art machine learning algorithms tested have failed to learn. We
motivate our work from the hypothesis that there is an optimization obstacle involved
in the nature of such tasks, and that humans learn useful intermediate
concepts from other individuals via a form of supervision or guidance using
a curriculum. The experiments we have conducted provide positive evidence in favor of this
hypothesis. In our experiments, a two-tiered MLP architecture is trained on
a dataset for which each image input contains three
sprites, and the binary target class is 1 if all three have the same shape.
Black-box machine learning algorithms only got chance on this task.
Standard deep supervised neural networks also failed. However, using
a particular structure and guiding the learner by providing intermediate
targets in the form of intermediate concepts (the presence of each object)
allows to nail the task. Much better than chance but imperfect results
are also obtained by exploring architecture and optimization variants,
pointing towards a difficult optimization task. We hypothesize that the learning
difficulty is due to the {\em composition} of
two highly non-linear tasks.  Our findings are also consistent with hypotheses
on cultural learning inspired by the observations of {\em effective} local minima (possibly
due to ill-conditioning and the training procedure not being able to escape what
appears like a local minimum).

\end{abstract}

\begin{keywords}
    Deep Learning, Neural Networks, Optimization, Evolution of Culture, 
    Curriculum Learning, Training with Hints
\end{keywords}

\section{Introduction}

There is a recent emerging interest in different fields of science for
{\em cultural learning}~\citep{henrich2003evolution} and how groups of
individuals exchanging information can learn in ways superior to individual
learning. This is also witnessed by the emergence of new research fields such
as "Social Neuroscience". Learning from other agents in an environment by the means
of cultural transmission of knowledge with a peer-to-peer communication is 
an efficient and natural way of acquiring or propagating common knowledge. The most
popular belief on how the information is transmitted between individuals is that
bits of information are transmitted by small units, called memes, which 
share some characteristics of genes, such as self-replication, mutation and 
response to selective pressures~\citep{Dawkins-1976}.

This paper is based on the hypothesis (which is further elaborated in~\citet{Bengio-chapter-2013})
that human culture and the evolution of ideas have been crucial to counter an optimization
issue: this difficulty would otherwise make it
difficult for human brains to capture high level knowledge of the world without the
help of other educated humans.
In this paper machine learning experiments are used to investigate some elements of this hypothesis by
seeking answers for the following questions: are there machine learning tasks which are
intrinsically hard for a lone learning agent but that may become very easy
when intermediate concepts are provided by another agent as additional
intermediate learning cues, in the spirit of Curriculum Learning~\citep{Bengio+al-2009-small}?
What makes such learning tasks more difficult? Can
specific initial values of the neural network parameters yield success when
random initialization yield complete failure? Is it possible to verify that the problem being faced 
is an optimization problem or with a regularization problem?
These are the questions
discussed (if not completely addressed) here, which relate to the following 
broader question: how can humans (and potentially one day, machines) learn
complex concepts?

In this paper, results of different machine learning algorithms on an artificial learning task 
involving binary 64$\times$64 images are presented. In that task, each image in the dataset contains 3 Pentomino
tetris sprites (simple shapes). The task is to figure out if all the
sprites in the image are the same or if there are different sprite shapes in the
image. Several state-of-the-art machine learning algorithms have been tested and
none of them could perform better than a random predictor on the test
set. Nevertheless by providing hints about the intermediate concepts
(the presence and location of particular sprite classes), the problem can easily be solved
where the same-architecture neural network without the intermediate concepts
guidance fails. Surprisingly, our attempts at solving this problem with 
unsupervised pre-training algorithms failed solve this problem. 
However, with specific variations in the network architecture or training
procedure, it is found that one can make a big dent in the problem.
For showing the impact of intermediate level guidance,
we experimented with a two-tiered neural network,
with supervised pre-training of the first part to recognize 
the category of sprites independently of their 
orientation and scale, at different locations, 
while the second part learns from the output of the first
part and predicts the binary task of interest.

The objective of this paper is not to propose a novel learning algorithm or
architecture, but rather to refine our understanding of the learning difficulties
involved with composed tasks (here a logical formula composed with the detection
of object classes), in particular the training difficulties involved for
deep neural networks. The results also bring empirical evidence in favor of some
of the hypotheses from ~\citet{Bengio-chapter-2013}, discussed below, as well as 
introducing a particular form of curriculum learning~\citep{Bengio+al-2009-small}.

Building difficult AI problems has a long history in computer science. Specifically 
hard AI problems have been studied to create CAPTCHA's that are easy to solve for humans, but
hard to solve for machines \citep{von2003captcha}. In this paper we are investigating
a difficult problem for the off-the-shelf black-box machine learning algorithms.\footnote{You can
    access the source code of some experiments presented in that paper and their hyperparameters from
    here: \url{https://github.com/caglar/kmatters}}

\subsection{Curriculum Learning and Cultural Evolution Against Effective Local Minima}

What \citet{Bengio-chapter-2013} calls an {\bf effective local minimum} is a point
where iterative training stalls, either because of an actual local minimum
or because the optimization algorithm is unable (in reasonable time) to
find a descent path (e.g., because of serious ill-conditioning). In this paper, it is hypothesized
that some more abstract learning tasks such as those obtained by composing
simpler tasks are more likely to yield effective local minima for neural networks,
and are generally hard for general-purpose machine learning algorithms.

The idea that learning can be enhanced by guiding the learner through intermediate 
easier tasks is old, starting with animal training by {\em shaping}
~\citep{Skinner1958,Peterson2004,Krueger+Dayan-2009}.~\citet{Bengio+al-2009-small}
introduce a computational hypothesis related to a presumed issue with effective local minima
when directly learning the target task: the good solutions correspond to
hard-to-find-by-chance effective local minima, and intermediate tasks prepare the 
learner's internal configuration (parameters) in a way similar to continuation 
methods in global optimization (which go through a sequence
of intermediate optimization problems, starting with a convex one where local
minima are no issue, and gradually morphing into the target task of interest).

In a related vein,
\citet{Bengio-chapter-2013} makes the following inferences based on
experimental observations of deep learning and neural network learning:

\begin{description}
\item Point 1: Training deep architectures is easier when some hints are given about the function that the intermediate levels should compute \citep{Hinton06,WestonJ2008,Salakhutdinov2009,Bengio-2009}. 
{\em The experiments performed here expand in particular on this point.}

\item Point 2: It is much easier to train a neural network with supervision (where examples ar
provided to it of when a concept is present and when it is not present in a variety of examples) 
than to expect unsupervised learning to discover the concept (which may also happen but usually leads to poorer renditions of the concept). 
{\em The poor results obtained with unsupervised pre-training reinforce
that hypothesis}.

\item Point 3: Directly training all the layers of a deep network together not only makes it difficult to exploit all the extra modeling power of a deeper architecture but in many cases it actually yields worse results as the number of {\em required layers} is increased \citep{Larochelle-jmlr-toappear-2008,Erhan+al-2010}. {\em The experiments performed here also reinforce that hypothesis.}

\item Point 4: \citet{Erhan+al-2010} observed that no two training trajectories ended up in the same effective local minimum, out of hundreds of runs, even when comparing solutions as functions from input to output, rather than in parameter space (thus eliminating from the picture the presence of symmetries and multiple local minima due to relabeling and other reparametrizations). This suggests that the number of different effective local minima (even when considering them only in function space) must be huge.

\item Point 5: Unsupervised pre-training, which changes the initial
  conditions of the descent procedure, sometimes allows to reach
  substantially better effective local minima (in terms of generalization error!),
  and these better local minima do not appear to be reachable by chance
  alone \citep{Erhan+al-2010}. {\em The experiments performed here provide
    another piece of evidence in favor of the hypothesis that where random initialization
    can yield rather poor results, specifically targeted initialization can have a
    drastic impact, i.e., that effective local minima are not just numerous but that
    some small subset of them are much better and hard to reach by chance.}\footnote{Recent work
    showed that rather deep feedforward networks can be very successfully
    trained when large quantities of labeled data are
    available~\citep{Ciresan-2010,Glorot+al-AI-2011-small,Krizhevsky-2012}. Nonetheless,
    the experiments reported here suggest that it all depends on the task
    being considered, since even with very large quantities of labeled
    examples, the deep networks trained here were unsuccessful.}
\end{description}

Based on the above points, \citet{Bengio-chapter-2013} then proposed the following hypotheses 
regarding learning of high-level abstractions.
\begin{itemize}
\item \textbf{Optimization Hypothesis:} When it learns, a biological agent performs an approximate optimization with respect to some implicit objective function.
\item \textbf{Deep Abstractions Hypothesis:} Higher level abstractions represented in brains require deeper computations (involving the composition of more non-linearities).
\item \textbf{Local Descent Hypothesis:} The brain of a biological agent relies on approximate local descent and gradually improves itself while learning.
\item \textbf{Effective Local Minima Hypothesis:} The learning process of a single human learner (not helped by others) is limited by effective local minima.
\item \textbf{Deeper Harder Hypothesis:} Effective local minima are more likely to hamper learning as the required depth of the architecture increases.
\item \textbf{Abstractions Harder Hypothesis:} High-level abstractions are unlikely to be discovered by a single human learner by chance, because these abstractions
are represented by a deep subnetwork of the brain, which learns by local descent.
\item \textbf{Guided Learning Hypothesis:} A human brain can learn high level abstractions if guided by the signals produced by other agents that act as hints or
indirect supervision for these high-level abstractions.
\item \textbf{Memes Divide-and-Conquer Hypothesis:} Linguistic exchange, individual learning and the recombination of memes constitute an efficient evolutionary
recombination operator in the meme-space. This helps human learners to {\em collectively} build better internal representations of their environment,
including fairly high-level abstractions.
\end{itemize}

This paper is focused on ``\textit{Point 1}'' and testing the
``\textit{Guided Learning Hypothesis}'', using machine learning algorithms
to provide experimental evidence. The experiments performed also provide
evidence in favor of the ``\textit{Deeper Harder Hypothesis}'' and associated
``\textit{Abstractions Harder Hypothesis}''.
Machine Learning is still far beyond the
current capabilities of humans, and it is important to tackle the remaining obstacles
to approach AI. For this purpose, the question to be answered is why
tasks that humans learn effortlessly from very few examples, while machine
learning algorithms fail miserably?


\section{Culture and Optimization Difficulty}

As hypothesized in the ``\textit{Local Descent Hypothesis}'', human brains
would rely on a local approximate descent, just like a Multi-Layer
Perceptron trained by a gradient-based iterative optimization.
The main argument in favor of this hypothesis relies on the
biologically-grounded assumption that although firing patterns in the brain
change rapidly, synaptic strengths underlying these neural activities change only
gradually, making sure that behaviors are generally consistent across time.
If a learning algorithm is based on a form of local (e.g. gradient-based) descent,
it can be sensitive to effective local minima ~\citep{Bengio-chapter-2013}.

When one trains a neural network, at some point in the training phase the
evaluation of error seems to saturate, even if new examples are 
introduced. In particular~\citet{Erhan+al-2010} find that early examples
have a much larger weight in the final solution. It looks like the learner is
stuck in or near a local minimum. But since it is difficult to verify
if this is near a true local minimum or simply an effect of strong ill-conditioning,
we call such a ``stuck'' configuration an {\em effective local minimum},
whose definition depends not just on the optimization objective but also on the
limitations of the optimization algorithm.

\citet{Erhan+al-2010} highlighted both the issue of effective local minima
and a regularization effect when initializing a deep network with unsupervised
pre-training. Interestingly, as the network gets deeper the difficulty due to effective local
minima seems to be get more pronounced. That might be because of the number of effective local
minima increases (more like an actual local minima issue),
or maybe because the good ones are harder to reach (more like an ill-conditioning
issue) and more work will be needed to clarify this question.

As a result of Point 4 we hypothesize that it is very difficult for an individual's
brain to discover some higher level abstractions by chance only. As mentioned in the ``\textit{Guided
Learning Hypothesis}'' humans get hints from other humans and learn high-level
concepts with the guidance of other humans\footnote{But some high-level concepts
may also be hardwired in the brain, as assumed in the universal grammar hypothesis
~\citep{montague1970universal}, or in nature vs nurture discussions in cognitive science.}.
Curriculum learning  \citep{Bengio+al-2009} and 
incremental learning \citep{solomonoff1989system},
are examples of this. This is done by properly choosing the sequence of examples
seen by the learner, where simpler examples are introduced first and 
more complex examples shown when the learner is ready for them. One of the hypothesis
on why curriculum works states that curriculum learning acts as a continuation method that allows
one to discover a good minimum, by first finding a good minimum of a smoother error function. Recent
experiments on human subjects also indicates that humans {\em teach}
by using a curriculum strategy \citep{Khan+Zhu+Mutlu-2011}.

Some parts of the human brain are known to have a hierarchical organization
(i.e. visual cortex) consistent with the deep architecture studied in machine
learning papers. As we go from the sensory level to higher levels of the
visual cortex, we find higher level areas corresponding to more abstract
concepts. This is consistent with the {\em Deep Abstractions Hypothesis}.

Training neural networks and machine learning algorithms by decomposing
the learning task into sub-tasks and exploiting prior information about the
task is well-established and in fact constitutes the main approach to
solving industrial problems with machine learning.
The contribution of this paper is rather on rendering explicit the effective local minima
issue and providing evidence on the type of problems for
which this difficulty arises.  This prior information and hints given
to the learner can be viewed as inductive bias for a particular task,
an important ingredient to obtain a good generalization error \citep{mitchell1980need}.
An interesting earlier finding in that line of research was done
with Explanation Based Neural Networks (EBNN) in which a neural network
transfers knowledge across multiple learning tasks. An EBNN uses previously
learned domain knowledge as an initialization or search bias (i.e. to
constrain the learner in the parameter space) ~\citep{Sullivan-96-integrating,mitchell1993explanation}.

Another related work in machine learning is mainly focused on reinforcement
learning algorithms, based on incorporating prior knowledge in terms of logical rules to the learning
algorithm as a prior knowledge to speed up and bias learning ~\citep{kunapuliadviceptron,towell1994knowledge}.

As discussed in ``\textit{Memes Divide and Conquer Hypothesis}`` societies can be viewed 
as a distributed computational processing systems. In civilized societies knowledge is
distributed across different individuals, this yields a space efficiency. Moreover computation,
i.e. each individual can specialize on a particular task/topic, is also divided across the individuals
in the society and hence this will yield a computational efficiency. Considering the limitations of the
human brain, the whole processing can not be done just by a single agent in an efficient manner. A recent
study in paleoantropology states that there is a substantial decline in endocranial volume of the brain
in the last 30000 years \cite{henneberg1988decrease}. The volume of the brain shrunk to 1241 ml from
1502 ml \citep{henneberg1993trends}. One of the hypothesis on the reduction of the volume of skull
claims that, decline in the volume of the brain might be related to the functional changes in brain
that arose as a result of cultural development and emergence of societies given that this time period
overlaps with the transition from hunter-gatherer lifestyle to agricultural societies.

\section{Experimental Setup}

Some tasks, which seem reasonably easy for humans to learn\footnote{keeping in mind that
humans can exploit prior knowledge, either from previous learning or innate knowledge.},
are nonetheless appearing almost impossible to learn for current generic state-of-art 
machine learning algorithms.

Here we study more closely such a task, which becomes learnable if one provides hints to the learner
about appropriate intermediate concepts. Interestingly, the task we used in our experiments
is not only hard for deep neural networks but also for non-parametric machine learning
algorithms such as SVM's, boosting and decision trees.

The result of the experiments for varying size of dataset with several off-the-shelf black box
machine learning algorithms and some popular deep learning algorithms are provided in Table
\ref{tab:results_cmp}. The detailed explanations about the algorithms and the hyperparameters
used for those algorithms are given in the Appendix Section \ref{sec:setup}. We also
provide some explanations about the methodologies conducted for the experiments at Section \ref{sec:learn_eva}.

\iffalse
% seems irrelevant?
Human brains can learn vast amount of recognition task in a very short time
from very few examples whereas still machine learning algorithms still
depending on the task require thousands of examples for each object
category and recently this topic became the focus of one-shot learning
\citet{fei2006one}. This rapid learning is a result of knowledge
transferring from one domain to another and prior information that we have
about the recognition task.
\fi

\begin{figure}[htbp!]
\centering
\mbox{
 \subfigure[sprites, not all same type]{
 \includegraphics[scale=0.4]{./pento64x64_fixed_different.png}
 % pento64x64_fixed_different.png: 800x600 pixel, 100dpi, 20.32x15.24 cm, bb=0 0 576 432
 \label{fig:ex_img_different_objects}
 }
 %\quad
 \hspace*{-20mm}
 \subfigure[sprites, all of same type]{
 \includegraphics[scale=0.4]{./pento64x64_fixed_same.png}
 \label{fig:ex_img_same_objects}
 % pento64x64_fixed_different.png: 800x600 pixel, 100dpi, 20.32x15.24 cm, bb=0 0 576 432
 }
}
\caption{Left (a): An example image from the dataset which has {\em a different sprite type} in it. 
    Right (b): An example image from the dataset that has only one type of Pentomino object in it, 
    but with different orientations and scales.}
\label{fig:pento_same_diff}
\end{figure}

\subsection{Pentomino Dataset}

In order to test our hypothesis, an artificial dataset for
object recognition using 64$\times$64 binary images is designed\footnote{The source code for
the script that generates the artificial Pentomino datasets (Arcade-Universe) is available at:
\url{https://github.com/caglar/Arcade-Universe}. This implementation is based on
Olivier Breuleux's bugland dataset generator.}. If the task is two tiered (i.e., with guidance provided),
the task in the first part is to recognize and locate each Pentomino object class\footnote{~A human
learner does not seem to need to be taught the shape categories of each Pentomino
sprite in order to solve the task. On the other hand, humans have lots of previously
learned knowledge about the notion of shape and how central it is in defining
categories.} in the image. The second part/final binary classification task is to figure out
if all the Pentominos in the image are of the same shape class or not. If a neural network
learned to detect the categories of each object at each location in an image, the remaining
task becomes an XOR-like operation between the detected object categories. The types of Pentomino 
objects that is used for generating the dataset are as follows:

\ifLongVersion
\begin{multicols}{2}
\begin{enumerate}
 \small
 \item Pentomino L sprite
 \item Pentomino N sprite
 \item Pentomino P sprite
 \item Pentomino F sprite
 \item Pentomino Y sprite
 \item Pentomino J sprite
 \item Pentomino N2 sprite: Mirror of ``Pentomino N'' sprite.
 \item Pentomino Q sprite
 \item Pentomino F2 sprite: Mirror of ``Pentomino F'' sprite.
 \item Pentomino Y2 sprite: Mirror of ``Pentomino Y'' sprite.
\end{enumerate}
\end{multicols}
\else

Pentomino sprites N, P, F, Y, J, and Q, along with the
Pentomino N2 sprite (mirror of ``Pentomino N'' sprite),
the Pentomino F2 sprite (mirror of ``Pentomino F'' sprite),
and the Pentomino Y2 sprite (mirror of ``Pentomino Y'' sprite).
\fi

\begin{figure}[htbp!]
 \centering
 \vspace*{-1mm}
 \includegraphics[scale=0.8]{./pento_classes.png}
 \vspace*{-2mm}
 \caption{Different classes of Pentomino shapes used in our dataset.}
 \label{fig:pento_classes}
 % error_bars_deep_mlp.png: 600x371 pixel, 72dpi, 21.17x13.09 cm, bb=0 0 600 371
\end{figure}


As shown in Figures \ref{fig:ex_img_different_objects} and
\ref{fig:ex_img_same_objects}, the synthesized images are fairly simple and do not have
any texture. Foreground pixels are ``1'' and background pixels are
``0''. Images of the training and test sets are generated iid. 
For notational convenience, assume that the
domain of raw input images is $X$, the set of sprites is $S$, the set of
intermediate object categories is $Y$ for each possible location in the image
and the set of final binary task outcomes is $Z=\{0,1\}$. 
Two different types of rigid body transformation is performed: sprite
rotation $rot(X, \gamma)$ where $\Gamma=\{\gamma \colon (\gamma=90 \times
\phi)~{\rm \land}~[(\phi \in \mathbb{N}), (0 \leq \phi \leq 3)]\}$ and scaling
$scale(X, \alpha)$ where $\alpha \in \{1,2\}$ is the scaling factor.
The data generating procedure is summarized below.

\begin{description}

 \item Sprite transformations: Before placing the sprites in an empty image,
   for each image $x \in X$, a value for $z \in Z$ is randomly sampled which is to have (or not) the
   same three sprite shapes in the image. Conditioned on the constraint given by $z$,
   three sprites are randomly selected $s_{ij}$ from $S$ without replacement. Using a uniform
   probability distribution over all possible scales, a scale is chosen and accordingly
   each sprite image is scaled. Then rotate each sprite is randomly rotated by a multiple of 90 degrees.

 \item Sprite placement: Upon completion of sprite transformations, a 
 64$\times$64 uniform grid is generated which is divided into 8$\times$8 blocks,
 each block being of size 8$\times$8 pixels, and randomly select three different blocks
 from the 64=8$\times$8 on the grid and place the
 transformed objects into {\em different} blocks (so they cannot overlap, by construction).

\end{description}

Each sprite is centered in the block in which it is located. Thus there is no
object translation inside the blocks. The only translation invariance is due
to the location of the block inside the image.
 
A Pentomino sprite is guaranteed to not overflow the block in which it 
is located, and there are no collisions or overlaps between sprites, making
the task simpler. The largest possible Pentomino sprite can be fit into an 8$\times$4 
mask.

\subsection{Learning Algorithms Evaluated}
\label{sec:learn_eva}
Initially the models are cross-validated by using 5-fold cross-validation.
With 40,000 examples, this gives 32,000 examples for training and 
8,000 examples for testing.
For neural network algorithms, stochastic gradient
descent (SGD) is used for training. The following standard learning
algorithms were first evaluated: decision trees, SVMs with Gaussian kernel,
ordinary fully-connected Multi-Layer Perceptrons, Random Forests,
k-Nearest Neighbors, Convolutional Neural Networks, and Stacked
Denoising Auto-Encoders with supervised fine-tuning. More details
of the configurations and hyper-parameters for each of them
are given in Appendix Section~\ref{sec:setup}. The only better than chance
results were obtained with variations of the Structured Multi-Layer Perceptron
described below.

\subsubsection{Structured Multi-Layer Perceptron (SMLP)}

The neural network architecture that is used to solve this task is called the SMLP (Structured Multi-Layer Perceptron),
a deep neural network with two parts as illustrated in Figure~\ref{fig:net_arch_hints} and \ref{fig:net_arch_nohints}:

The lower part, P1NN (\emph{Part 1 Neural Network}, as it is called in the rest of the paper),
has shared weights and local connectivity, with one identical 
MLP instance of the P1NN for each patch of the image,
and typically an 11-element output vector per patch (unless otherwise noted). 
The idea is that these 11 outputs per patch could represent
the detection of the sprite shape category (or the absence of sprite in the patch).
The upper part, P2NN ({\em Part 2 Neural Network}) is a fully connected one hidden layer MLP
that takes the concatenation of the outputs of all patch-wise P1NNs as input. Note that
the first layer of P1NN is similar to a convolutional layer
but where the stride equals the kernel size, so that windows do not overlap, i.e., P1NN can be
decomposed into separate networks sharing the same parameters but applied
on different patches of the input image, so that each network can actually be trained
patch-wise in the case where a target is provided for the P1NN outputs. The P1NN output
for patch $\mathbf{p_i}$ which is extracted from the image $\mathbf{x}$ is computed as follows:

\begin{equation}
    f_{\theta}(\mathbf{p_i}) = g_2(V g_1(U \mathbf{p_i} + \mathbf{b}) + \mathbf{c})
\end{equation}

where $\mathbf{p_i} \in R^d$ is the input patch/receptive field extracted from location $i$ of a single image.
$U \in R^{d_h \times d}$ is the weight matrix for the first layer of P1NN and 
$\mathbf{b} \in R^d_h$ is the vector of biases for the first layer of P1NN.
$g_1(\cdot)$ is the activation function of the first layer and
$g_2(\cdot)$ is the activation function of the second layer. In many of the experiments, best
results were obtained with $g_1(\cdot)$ 
a rectifying non-linearity (a.k.a. as RELU), which is $max(0,~X)$
~\citep{Jarrett-ICCV2009-small,Hinton2010,Glorot+al-AI-2011-small,Krizhevsky-2012}. 
$V \in R^{d_h \times d_o}$ is the second layer's weights matrix, such that and 
$\mathbf{c} \in R_{d_o}$ are the biases of the second layer of the P1NN, with 
$d_o$ expected to be smaller than $d_h$.

In this way, $g_1(U \mathbf{p_i} + \mathbf{b})$ is an overcomplete representation of the input patch
that can potentially represent all the possible Pentomino shapes for all factors of variations in the
patch (rotation, scaling and Pentomino shape type). On the other hand, when trained with
hints, $f_\theta(\mathbf{p_i})$ is expected to be the lower dimensional representation of a Pentomino
shape category invariant to scaling and rotation in the given patch.

In the experiments with SMLP trained with hints (targets at the output of P1NN),
% YB: THIS SHOULD BE IN THE SMLP-HINTS SECTION! NOT HERE ===> CG: Tried to clarify that part,
% because P1NN is common for both SMLP-hints and SMLP-nohints.
%
the P1NN is expected to perform classification of each 8$\times$8 non-overlapping patches
of the original 64$\times$64 input image without having any prior knowledge of whether
that specific patch contains a Pentomino shape or not. P1NN in SMLP without hints just outputs the
local activations for each patch, and gradients on $f_\theta({\mathbf p_i})$ are backpropagated
from the upper layers. In both cases P1NN produces the input representation  for the Part 2 Neural Net (P2NN). 
Thus the input representation of P2NN is the concatenated output of P1NN across all the 64 patch locations:

$\mathbf{h_o} = [f_{\theta}(\mathbf{p_0}), ..., f_{\theta}(\mathbf{p_i}), ...,
       f_{\theta}(\mathbf{p_{N}}))]$ where $N$ is the number of patches and the $h_o \in R^{d_i}, d_i = d_o \times N$.
       $\mathbf{h_o}$ is the concatenated output of the P1NN at each patch.

There is a standardization layer on top of the output of P1NN that centers the activations and
performs divisive normalization by dividing  by the standard deviation over a minibatch
of the activations of that
layer. We denote the standardization function $z(\cdot)$. Standardization makes use of the
mean and standard deviation computed for each hidden unit such that each hidden unit of 
$\mathbf{h_o}$ will have 0 activation and unit standard deviation on average
over the minibatch. $X$ is the set of
pentomino images in the minibatch, where $X \in R^{d_{in} \times N}$ is a matrix with $N$ images.
$h_o^{(i)}(\mathbf{x_j})$ is the vector of activations of the $i$-th hidden unit of
hidden layer $h_o(\mathbf{x_j})$ for the $j$-th example, with $x_j \in X$.

%$$\mu_{h_o} = \frac{1}{N}\sum_{h_o^{(i)} \in \mathbf{h_o}} h_o^{(i)}$$
\begin{equation}
\label{eq:mu}
\mu_{h_o^{(i)}} = \frac{1}{N}\sum_{\mathbf{x_j} \in X} h_o^{(i)}(\mathbf{x_j})
\end{equation}

\begin{equation}
\label{eq:sigma}
\sigma_{h_o^{(i)}} = \sqrt{\frac{\sum_j^N (h_o^{(i)}(\mathbf{x_j})-\mu_{h_o^{(i)}})^2}{N} +
    \epsilon}
\end{equation}
\begin{equation}
\label{eq:std}
z(h_o^{(i)}(\mathbf{x_j}))=\frac{h_o^{(i)}(\mathbf{x_j}) - \mu_{h_o^{(i)}}}{\max
    (\sigma_{h_o^{(i)}}, \epsilon)}
\end{equation}

where $\epsilon$ is a very small constant, that is used to prevent numerical underflows in the
standard deviation. P1NN is trained on each 8$\times$8 patches extracted from the image.

$\mathbf{h_o}$ is standardized for each training and test sample separately. Different values of $\epsilon$ were used for SMLP-hints and SMLP-nohints.

The concatenated output of P1NN is fed as an input to the P2NN. P2NN is a feedforward MLP with a
sigmoid output layer using a single RELU hidden layer. The task of P2NN is to perform a nonlinear logical
operation on the representation provided at the output of P1NN.

\subsubsection{Structured Multi Layer Perceptron Trained with Hints (SMLP-hints)}

The SMLP-hints architecture exploits a hint about the presence and category
of Pentomino objects, specifying a semantics for the P1NN outputs. P1NN is trained with the
intermediate target $Y$, specifying the type of Pentomino sprite shape present (if any) at each of
the 64 patches (8$\times$8 non-overlapping blocks) of the image. Because a possible answer at a given
location can be ``none of the object types'' i.e., an empty patch, $y_p$ (for patch $p$) can take
one of the 11 possible values, 1 for rejection and the rest is for the Pentomino shape classes,
illustrated in Figure \ref{fig:pento_classes}:

\begin{equation*}
y_p =
    \begin{cases}
        0 & \text{if  patch~} p \text{~is empty~} \\
        s \in S & \text{if the patch~} p \text{~contains a Pentomino sprite~}.
    \end{cases}
\end{equation*}

A similar task has been studied by \citet{fleuret2011comparing} (at SI appendix Problem 17),
who compared the performance of humans vs computers.

The SMLP-hints architecture takes advantage of dividing the task into two subtasks during training
with prior information about intermediate-level relevant factors. 
Because the sum of the training losses decomposes into the loss on each patch, the P1NN can be
pre-trained patch-wise. Each patch-specific component of the P1NN is
a fully connected MLP with 8$\times$8 inputs and 11 outputs with a softmax output layer.
SMLP-hints uses the the standardization given in Equation \ref{eq:sigma} but with $\epsilon=0$.

The standardization is a crucial step for training the SMLP on the Pentomino dataset, and yields
much sparser outputs, as seen on
Figures \ref{fig:ikgnn_unstd_act_bar} and \ref{fig:ikgnn_std_act_bar}. If the standardization is
not used, even SMLP-hints could not solve the Pentomino task. 
In general, the standardization step dampens the small activations and augments larger
ones(reducing the noise). Centering the activations of each feature detector in a neural network has been
studied in \citep{raiko2012deep} and \citep{vatanen2013pushing}. They proposed that transforming the outputs
of each hidden neuron in a multi-layer perceptron network to have zero output and zero slope on average makes
first order optimization methods closer to the second order techniques.

\begin{figure}[htp]
\centering{
\includegraphics[scale=0.6]{ikgnn_unstd_act_bar.png}
}
\caption{Bar chart of concatenated softmax output activations $\mathbf{h_o}$ of P1NN (11$\times$64=704 outputs) in SMLP-hints before standardization, for a selected example. There are
    very large spikes at each location for one of the possible 11 outcome (1 of K representation).}
\label{fig:ikgnn_unstd_act_bar}
\end{figure}


\begin{figure}[htp]
\centering{
\includegraphics[scale=0.6]{ikgnn_std_act_bar.png}
}
\caption{Softmax output activations $\mathbf{h_o}$ of P1NN  at SMLP-hints before standardization.
% YB: for the example of figure 3?
    There are positive spiked outputs at the locations where there is a Pentomino shape. 
    Positive and negative spikes arise because most of the outputs are near an average value.
    Activations are higher at the locations where there is a pentomino shape.}
\label{fig:ikgnn_std_act_bar}
\end{figure}


%No more needed
%\begin{figure}[htbp!]
% \centering
%  \vspace*{-2.5cm}
% \includegraphics[scale=0.4]{./NetworkInformationFlowDiagram.png}
%  \vspace*{-5cm}
% \caption{Information flow diagram for the SMLP. P1NN is trained on the patches with respect to intermediate target labels and P2NN is trained on the concatenated and standardized output of P1NN with respect to the final task's target labels.}
% \label{fig:net_flow_dia}
 % Network Information Flow Diagram.png: 960x720 pixel, 72dpi, 33.87x25.40 cm, bb=0 0 960 720
%\end{figure}

\begin{figure}[htbp!]
 \centering
 \vspace*{-\parskip}
%\vspace*{-1.6cm}
%\includegraphics[scale=0.5]{./StructuredMLP.png}
%\figure{Structured MLP with Hints simple.png}}
\includegraphics[width=\linewidth]{smlp_hints_arch_simple.pdf}
  \vspace*{-0.2cm}
 \caption{Structured MLP architecture, used with hints (trained in two phases,
first P1NN, bottom two layers, then P2NN, top two layers). In SMLP-hints, P1NN is 
trained on each 8x8 patch extracted from the image and the softmax output
probabilities of all 64 patches are concatenated into a 64$\times$11 vector
that forms the input of P2NN. Only $U$ and $V$ are learned in the P1NN and its output on each 
patch is fed into P2NN. 
The first level and the second level neural networks are trained separately, not jointly.
 }\label{fig:net_arch_hints}
 % NNArchitecture _v2.png: 1485x600 pixel, 762dpi, 4.95x2.00 cm, bb=0 0 140 57
\end{figure}


By default, the SMLP uses rectifier hidden units as activation function,  we
found a significant boost by using rectification compared to hyperbolic
tangent and sigmoid activation functions. The P1NN has a highly overcomplete architecture
with 1024 hidden units per patch, and L1 and L2 weight decay regularization 
coefficients on the weights (not the biases) are respectively 1e-6 and 1e-5. The learning rate for
the P1NN is 0.75. 1 training epoch was enough for the P1NN to learn the features of 
Pentomino shapes perfectly on the 40000 training examples. The P2NN has 2048 hidden units.
L1 and L2 penalty coefficients for the P2NN are 1e-6, and the learning rate is 0.1. These were 
selected by trial and error based on validation set error. Both P1NN (for each patch)
and P2NN are fully-connected neural networks, even though P1NN globally is a special
kind of convolutional neural network.

Filters of the first layer of SMLP are shown in Figure \ref{fig:smlp_hints_filters_patches}.
These are the examples of the filters obtained with the SLMP-hints trained with 40k examples,
whose results are given in Table \ref{tab:results_cmp}. Those filters look very noisy but they work perfectly on the Pentomino task.

\begin{figure}[htbp!]
 \centering
 \vspace*{-\parskip}
%\vspace*{-1.6cm}
%\includegraphics[scale=0.5]{./StructuredMLP.png}
%\figure{Structured MLP with Hints simple.png}}
\includegraphics[scale=1.8]{smlp_hints_filters_patches.png}
  %\vspace*{-0.6cm}
 \caption{Filters of Structured MLP architecture, trained with hints on 40k examples.}
 \label{fig:smlp_hints_filters_patches}
 % NNArchitecture _v2.png: 1485x600 pixel, 762dpi, 4.95x2.00 cm, bb=0 0 140 57
\end{figure}


\subsubsection{Deep and Structured Supervised MLP without Hints (SMLP-nohints)}

SMLP-nohints uses the same connectivity pattern (and deep architecture)
that is also used in the SMLP-hints architecture, but without using the intermediate targets ($Y$).
It directly predicts the final outcome of the task ($Z$),
using the same number of hidden units, the same connectivity and the same
activation function for the hidden units as SMLP-hints. 
120 hyperparameter values have been evaluated
by randomly selecting the number of hidden units from $[64, 128, 256, 512, 1024, 1200, 2048]$ and randomly sampling 20 
learning rates uniformly in the log-domain within the interval of $[0.008, 0.8]$. Two fully connected
hidden layers with 1024 hidden units (same as P1NN) per patch is used and 2048 (same as P2NN) for the last hidden layer,
with twenty training epochs. For this network the best results are obtained with a learning rate
of 0.05.\footnote{The source code of the structured MLP is available at the github repository:
    \url{https://github.com/caglar/structured\_mlp}}

\begin{figure}[htbp!]
 \centering
 \vspace*{-\parskip}
 \centering
%\vspace*{-1.6cm}
\includegraphics[width=\linewidth]{smlp_without_hints_arch.pdf}
  \vspace*{-0.2cm}
 \caption{Structured MLP architecture, used without hints (SMLP-nohints). 
It is the same architecture as SMLP-hints (Figure~\ref{fig:net_arch_hints}) but with
both parts (P1NN and P2NN) trained jointly with respect to the final binary
classification task.}
 \label{fig:net_arch_nohints}
 % NNArchitecture _v2.png: 1485x600 pixel, 762dpi, 4.95x2.00 cm, bb=0 0 140 57
\end{figure}

\begin{figure}[htbp!]
 \centering
 \vspace*{-\parskip}
\includegraphics[scale=1.8]{osmlp_nohints_filters_subset.png}
 % \vspace*{-0.6cm}
 \caption{First layer filters learned by 
   the Structured MLP architecture, trained without using hints on 447600 examples
     with online SGD and a sigmoid intermediate layer activation.}
 \label{fig:osmlp_nohints_filters_subset}
 % NNArchitecture _v2.png: 1485x600 pixel, 762dpi, 4.95x2.00 cm, bb=0 0 140 57
\end{figure}

We chose to experiment with various SMLP-nohint architectures and optimization procedures, trying
unsuccessfully to achieve as good results with SMLP-nohint as with SMLP-hints.

\paragraph{Rectifier Non-Linearity}
A rectifier nonlinearity is used for the activations of MLP hidden layers. We observed that using
piecewise linear nonlinearity activation function such as the rectifier can make the optimization more
tractable.

\paragraph{Intermediate Layer}

The output of the P1NN is considered as an intermediate layer of the SMLP. 
For the SMLP-hints, only softmax output activations have been tried at the intermediate layer, and that sufficed
to learn the task. Since things did not work nearly as well with the SMLP-nohints, 
several different activation functions have been tried: softmax$(\cdot)$, tanh$(\cdot)$, sigmoid$(\cdot)$ and linear activation functions.

\paragraph{Standardization Layer}
% YB REVISE THIS WHEN I REALLY UNDERSTAND WHAT IT IS!!!!!

Normalization at the last layer of the convolutional neural networks has been used occasionaly to
encourage the competition between the hidden units. \citep{Jarrett-ICCV2009} used a local contrast
normalization layer in their architecture which performs subtractive and divisive normalization.
A local contrast normalization layer enforces a local competition between adjacent features in the
feature map and between features at the same spatial location in different feature maps. Similarly
\citep{Krizhevsky-2012} observed that using a local response layer that enjoys the benefit of using
local normalization scheme aids generalization.

Standardization has been observed to be crucial for both SMLP trained with or without
hints. In both SMLP-hints and SMLP-nohints experiments, the neural network was not able 
to generalize or even learn the training set without using standardization in the SMLP intermediate layer,
doing just chance performance. More specifically, in the SMLP-nohints architecture, standardization 
is part of the computational graph, hence the gradients are being backpropagated through it.
The mean and the standard deviation is computed for each hidden unit separately at the
intermediate layer as in Equation \ref{eq:std}. But in order to prevent numerical underflows or
overflows during the backpropagation we have used $\epsilon=1e-8$ (Equation \ref{eq:sigma}).

%$$\sigma_{h_o^{(i)}} = \sqrt{\frac{\sum_j^N (h_o^{(i)}(\mathbf{x_j})-\mu_{h_o^{(i)}})^2}{N}}$$

%\begin{equation}
%    \sigma_{h_o^{(i)}} = \sqrt{\frac{\sum_j^N (h_o^{(i)}(\mathbf{x_j})-\mu_{h_o^{(i)}})^2}{N} + \epsilon}
%\end{equation}

%$\mu_{h_o^(i)}$ and $z(h_o^{(i)}(\mathbf{x_j}))$ is computed in the same way as in Equation \ref{eq:mu} and \ref{eq:std}.

%We used $\sigma=1e-18$ in the SMLP results provided in that paper. 


The benefit of having sparse activations may be specifically important for the ill-conditioned 
problems, for the following reasons. When a hidden unit is ``off'', its gradient (the derivative of
the loss with respect to its output) is usually close to 0 as well, as seen here. 
That means that all off-diagonal
second derivatives involving that hidden unit (e.g. its input weights) are also near 0.
This is basically like removing some columns and rows from the Hessian matrix associated with
a particular example. It has been observed that the condition number of the Hessian
matrix (specifically, its largest eigenvalue) increases as the size of the network 
increases~\citep{Dauphin+Bengio-arxiv-2013}, making training considerably slower
and inefficient~\citep{Dauphin+Bengio-arxiv-2013}. Hence one would expect that as sparsity
of the gradients (obtained because of sparsity of the activations) increases, training
would become more efficient, as if we were training a smaller sub-network for each example,
with shared weights across examples, as in dropouts~\citep{Hinton-et-al-arxiv2012}.

\iffalse
% YB: first understand it, then uncomment or revise
The main reason that ill-conditioning might occur in the Pentomino task is that at the locations where SMLP
thinks there is an object it can have very large activations and at the locations where there is no Pentomino
object, the activations can be small.
%As a result of that, condition number which is the ratio of 
%largest and smallest eigenvalue ($\parallel H^{-1} \parallel \parallel H \parallel =
%                                 \frac{\sigma_{max}(H)}{\sigma_{min}(H)}$)  may become large.

%Sparsification has another significance for Pentomino task. Because the input representation is
%sparse as well. But biases of the P1NN can cause its output to become noisy on each patch, 
%even though that location doesn't contain any Pentomino object. This will allow the gradient flow
%to the locations that doesn't contain any object. Because, the only indicators of the correct
%Pentomino target (does the image contains a different object) are the patches that contains Pentomino shape.
\fi

In Figure \ref{fig:unstd_std_nohints}, the activation of each hidden unit in a bar chart is shown:
the effect of standardization is significant, making the activations sparser.

\begin{figure}[htbp!]
 \centering
 \mbox{
 \subfigure[Before standardization.]{
 \includegraphics[width=0.5\linewidth, height=\textheight, keepaspectratio=True]{unstd_act_bar_nohints.png}
 \label{fig:nstd_act_bar_nohints}
 }
 \subfigure[After standardization.]{
 \includegraphics[width=0.5\linewidth, height=\textheight, keepaspectratio=True]{std_act_bar_nohints.png}
 \label{fig:std_act_bar_nohints}
 }
 }
\caption{Activations of the intermediate-level hidden units of an SLMP-nohints for 
 a particular examples (x-axis: hidden unit number, y-axis: activation value).
   Left (a): before standardization.
    Right (b): after standardization.}
\label{fig:unstd_std_nohints}
\end{figure}

In Figure \ref{fig:unstd_std_acts_hints}, one can see the activation histogram of the SMLP-nohints
intermediate layer, showing the distribution of activation values, before and after 
standardization. Again the sparsifying effect of standardization is very apparent.
\begin{figure}[htbp!]
 \centering
 \mbox{
 \subfigure[Before standardization.]{
 \includegraphics[width=0.5\linewidth, height=\textheight, keepaspectratio=True]{unstd_act_hist_nohints.png}
 \label{fig:unstd_act_hist_nohints}
 }
 \subfigure[After standardization.]{
 \includegraphics[width=0.5\linewidth, height=\textheight, keepaspectratio=True]{std_act_hist_nohints.png}
 \label{fig:std_act_hist_nohints}
 }
}
 \caption{Distribution histogram of activation values of SMLP-nohints intermediate layer.
   Left (a): before standardization.
    Right (b): after standardization.}
\label{fig:unstd_std_acts_hints}
\end{figure}

In Figures \ref{fig:unstd_std_acts_hints} and \ref{fig:unstd_std_nohints}, the 
intermediate level activations of
SMLP-nohints are shown before and after standardization. These are
for the same SMLP-nohints architecture whose
results are presented on Table \ref{tab:results_cmp}. 
For that same SMLP,  the Adadelta~\citep{zeiler2012adadelta} adaptive learning
rate scheme has been used, with 512 hidden units for the hidden layer of P1NN and rectifier activation
function. For the output of the P1NN, 11 sigmoidal units have been used while P2NN
had 1200 hidden units with
rectifier activation function. The output nonlinearity of the P2NN is a sigmoid and the 
training objective is the binary crossentropy.

%That part is unnecessary
%For the structured MLP without hints (SMLP), we evaluated the effect of number of hidden units in the
%intermediate layer with number of hidden units in Table \ref{tab:smlp_layersizes}.
%In these experiments we have trained SMLP for 25 epochs on 100k samples. As a result of these
%experiments we could not see any significant effect of changing the number of hidden units in the
%locally connected layer on patches.
%\begin{table}[t]
%    \small
%    \begin{sc}
%    \begin{center}
%\begin{tabular}{l|l|l}
%\textbf{Size of Locally Connected Layer} & \textbf{Training Error} & \textbf{Test Error} \\ \hline
%11 & 0.470 & 0.501 \\ 
%50 & 0.470 & 0.502 \\ 
%100 & 0.496 & 0.493 \\
%\end{tabular}
%    \end{center}
%    \end{sc}
%    \caption{The training and test error rates with different number of hidden units at the output
%        of locally connected layer for structured MLP on the Pentomino dataset.}
%    \label{tab:smlp_layersizes}
%\end{table}
%

\paragraph{Adaptive Learning Rates}
We have experimented with several different adaptive learning rate algorithms. We tried
   rmsprop \footnote{This is learning rate scaling method that is discussed by G. Hinton in his Video Lecture 6.5
       - rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning, 2012.},
   Adadelta \citep{zeiler2012adadelta}, Adagrad~\citep{duchi2010adaptive} 
  and a linearly (1/t) decaying learning rate ~\citep{Bengio-tricks-chapter-2013}. 
   For the SMLP-nohints with sigmoid activation
   function we have found Adadelta\citep{zeiler2012adadelta} converging faster to an effective local minima and usually
   yielding better generalization error compared to the others.

%I think that part is no more relevant.
%\paragraph{Training SMLP on only }
%We also trained the Structured MLP with only 3 patches instead of the 64, by keeping
%only the patches containing an object and without using standardization on the output activations
%of P1NN.

%The model was trained using 100000 examples, for 120 epochs. We have used a rectifier nonlinearity as the output of the
%locally connected layer and tried different numbers of hidden units. The lowest training
%error we have obtained was 37 percent but the best test error we got was still 49.2 percent.
%It was one way to try to make the task simpler, but did not impact the difficulty of training.

\subsubsection{Deep and Structured MLP with Unsupervised Pre-Training}

Several experiments have been conducted using an architecture similar to the
SMLP-nohints, but by using 
unsupervised pre-training of P1NN, with Denoising Auto-Encoder (DAE) and/or
Contractive Auto-Encoders (CAE).
Supervised fine-tuning proceeds as in the deep and
structured MLP without hints. Because an unsupervised learner may not focus
the representation just on the shapes, a larger number of
intermediate-level units at the output of P1NN has been explored:
previous work on unsupervised pre-training generally
found that larger hidden layers were optimal when using unsupervised
pre-training, because not all unsupervised features will be relevant
to the task at hand. Instead of limiting to 11 units per patch,
we experimented with networks with up to 20 hidden (i.e., code) units 
per patch in the second-layer patch-wise auto-encoder. % TODO: is this 64 correct?

In Appendix \ref{sec:bin_rbm_pento} we also provided the result of some experiments
with binary-binary RBMs trained on 8$\times$8 patches from the 40k training dataset.

In unsupervised pretraining experiments in this paper, both 
\emph{contractive auto-encoder}(CAE) with sigmoid nonlinearity
and binary cross entropy cost function and \emph{denoising
auto-encoder}(DAE) have been used. In the second layer, experiments were performed with
a DAE with rectifier hidden units utilizing L1 sparsity and weight decay on the
weights of the auto-encoder. Greedy layerwise unsupervised training
procedure is used to train the deep auto-encoder architecture \citep{Bengio-nips-2006-small}. In
unsupervised pretraining experiments, tied weights have been used.
Different combinations of CAE and DAE for unsupervised pretraining  have been tested, but none of
the configurations tested managed to learn the Pentomino task, as shown
in Table~\ref{tab:results_cmp}.


\begin{table}[htbp!]
{\tiny%
\newcommand{\mc}[3]{\multicolumn{#1}{#2}{#3}}
\begin{center}
\tiny
\begin{sc}
\begin{tabular}{|l|l|l|l|l|l|l|l|p{2cm}|}
\hline 
\mc{1}{|c|}{\textbf{Algorithm}} & \mc{2}{|c|}{\textbf{20k dataset}} & \mc{2}{|c|}{\textbf{40k dataset}} & \mc{2}{|c|}{\textbf{80k dataset}}\\ \hline
 & \mc{1}{|c|}{\textbf{Training}} & \mc{1}{|c|}{\textbf{Test}} & \mc{1}{|c|}{\textbf{Training}} & \mc{1}{|c|}{\textbf{Test}} & \mc{1}{|c|}{\textbf{Training}} & \mc{1}{|c|}{\textbf{Test}}\\ %\hline
 & \mc{1}{|c|}{\textbf{Error}} & \mc{1}{|c|}{\textbf{Error}} & \mc{1}{|c|}{\textbf{Error}} & \mc{1}{|c|}{\textbf{Error}} & \mc{1}{|c|}{\textbf{Error}} & \mc{1}{|c|}{\textbf{Error}}\\ \hline
\textbf{SVM RBF} & 26.2 & 50.2 & 28.2 & 50.2 & 30.2 & 49.6\\ \hline
\textbf{K Nearest Neighbors} & 24.7 & 50.0 & 25.3 & 49.5 & 25.6 & 49.0\\ \hline
\textbf{Decision Tree} & 5.8 & 48.6 & 6.3 & 49.4 & 6.9 & 49.9\\ \hline
\textbf{Randomized Trees} & 3.2 & 49.8 & 3.4 & 50.5 & 3.5 & 49.1\\ \hline
\textbf{MLP} & 26.5 & 49.3 & 33.2 & 49.9 & 27.2 & 50.1\\ \hline
\textbf{Convnet/Lenet5} & 50.6 & 49.8 & 49.4 & 49.8 & 50.2 & 49.8 \\ \hline
\textbf{Maxout Convnet} & 14.5 & 49.5 & 0.0 & 50.1 & 0.0 & 44.6  \\ \hline
\textbf{2 layer sDA} & 49.4 & 50.3 & 50.2 & 50.3 & 49.7 & 50.3 \\ \hline
\textbf{Struct. Supervised MLP w/o hints} & 0.0 & 48.6 & 0.0 & 36.0 & 0.0 & 12.4 \\ \hline
\textbf{Struct. MLP+CAE Supervised Finetuning} & 50.5  &  49.7 & 49.8  & 49.7 & 50.3 & 49.7 \\ \hline
\textbf{Struct. MLP+CAE+DAE, Supervised Finetuning} & 49.1 & 49.7 & 49.4 & 49.7 & 50.1 & 49.7 \\ \hline
\textbf{Struct. MLP+DAE+DAE, Supervised Finetuning} & 49.5 & 50.3 & 49.7 & 49.8 & 50.3 & 49.7  \\ \hline

\hline\hline
\textbf{Struct. MLP with Hints} & \textbf{0.21} & \textbf{30.7} & \textbf{0} & \textbf{3.1} & \textbf{0} & \textbf{0.01}\\ \hline
\end{tabular}
\end{sc}
\caption{The error percentages with different learning algorithms on Pentomino dataset with different number of training examples.}
\label{tab:results_cmp}
\end{center}
}%
\end{table}

\subsection{Experiments with 1 of K representation}

To explore the effect of changing the complexity of the input representation on the difficulty of the
task, a set of experiments have been designed with symbolic representations of the information in each patch.
In all cases an empty patch is represented with a 0 vector. These representation can be
seen as an alternative input for a P2NN-like network, i.e., they were fed as input to an MLP
or another black-box classifier.

The following four experiments have been conducted, each one using one 
using a different input representation for each patch:

\begin{description}
    \item[Experiment 1-Onehot representation without transformations:] In this experiment several
    trials have been done with a 10-input one-hot vector per patch. Each input corresponds to an object category given in clear, i.e., the ideal input for P2NN if a supervised P1NN perfectly did its job.

    \item[Experiment 2-Disentangled representations:] In this experiment, we did trials with
    16 binary inputs per patch, 10 one-hot bits for representing each object category, 4 
    for rotations and 2 for scaling, i.e., the whole information about the input is given, but
    it is perfectly disentangled. This would be the ideal input for P2NN if an unsupervised P1NN
    perfectly did its job.

    \item[Experiment 3-Onehot representation with transformations:] For each of the ten object
    types there are 8 = 4$\times$2 possible transformations. Two objects in two different patches are the considered ``the same'' (for the final task) if their
    category is the same regardless of the transformations. The one-hot representation of a patch
    corresponds to the cross-product between the 10 object shape classes and the 4$\times$2 
    transformations, i.e., one out
    of 80=10$\times 4 \times 2$ possibilities represented in an 80-bit one-hot vector.
    This also contains all the information about the input image patch, but spread out in
    a kind of non-parametric and non-informative (not disentangled) way, 
    like a perfect memory-based unsupervised
    learner (like clustering) could produce. Nevertheless, the shape class would be easier to
    read out from this representation than from the image representation (it would be an OR
    over 8 of the bits).

    \item[Experiment 4-Onehot representation with 80 choices:] This representation has the same
    1 of 80 one-hot representation per patch but the target task is defined differently. 
    Two objects in two different patches are considered the same iff they have exactly the same 80-bit
    onehot
    representation (i.e., are of the same object category with the same transformation applied).

\end{description}

The first experiment is a sanity check. It was conducted with single hidden-layered
MLP's with rectifier and tanh nonlinearity, and 
the task was learned perfectly (0 error on both training and test dataset) 
with very few training epochs.

\begin{figure}[ht]
 \centering
 \mbox{
 \subfigure[Training and Test Errors for Experiment 4]{
 \includegraphics[width=0.5\linewidth, height=\textheight, keepaspectratio=True]{valid_train_onehot80inputs}
 \label{fig:80inputs_test_train}
 }
 \subfigure[Training and Test Errors for Experiment 3]{
 \includegraphics[width=0.5\linewidth, height=\textheight, keepaspectratio=True]{valid_train_onehot80inputs_trans}
 \label{fig:80inputs_trans_test_train}
 }
 }
\caption{Tanh MLP training curves. Left (a): The training and test errors of Experiment 3 over 800 training epochs with 100k
    training examples using Tanh MLP.
    Right (b):The training and test errors of Experiment 4 over 700 training epochs with 100k
           training examples using Tanh MLP.}
\label{fig:test_train_err_plots}
\end{figure}

The results of Experiment 2 are given in Table \ref{tab:disentangled}. 
To improve results, we experimented with the
Maxout non-linearity in a feedforward MLP \citep{Goodfellow_maxout_2013} with two hidden layers. 
Unlike the typical Maxout network mentioned in the original paper, regularizers
have been deliberately avoided in order to focus on the optimization
issue, i.e: no weight decay, norm constraint on the weights, or dropout. 
Although learning from a disentangled representation is more difficult than learning from perfect object detectors, it is feasible
with some architectures such as the Maxout network. Note that this representation
is the kind of representation that one could hope an unsupervised learning algorithm
could discover, at best, as argued in~\cite{Bengio+Courville+Vincent-arxiv2012}.

The only results obtained on the validation set for Experiment 3 and Experiment 4 are shown respectively in Table
\ref{tab:80inputs_withtrans} and Table \ref{tab:80inputs}. In these experiments
a tanh MLP with two hidden layers have been tested with the same hyperparameters. In experiment
3 the complexity of the problem comes from the transformations (8=4$\times$2) 
and the number of object types.

But in experiment 4, the only source of complexity of the task comes from the number of different
object types. These results are in between the complete failure and complete success
observed with other experiments, suggesting that the task could become solvable
with better training or more training examples. Figure~\ref{fig:test_train_err_plots} illustrates the progress of training a tanh MLP, on both
the training and test error, for Experiments 3 and 4. Clearly, something has been learned,
but the task is not nailed yet. On experiment 3 for both maxout and tanh the maxout there was a
long plateau where the training error and objective stays almost same. Maxout did just chance on
the experiment for about 120 iterations on the training and the test set. But after 120th
iteration the training and test error started decline and eventually it was able to solve the task.
Moreover as seen from the curves in Figure \ref{fig:test_train_err_plots}\subref{fig:80inputs_test_train}
and \ref{fig:test_train_err_plots}\subref{fig:80inputs_trans_test_train},
the training and test error curves are almost the same for both tasks. This implies that for onehot inputs,
whether you increase the number of possible transformations for each object or 
the number of object categories, as soon as the number of possible configurations
is same, the complexity of the problem is almost the same for the MLP.

\begin{table}[t]
    \small
    \begin{sc}
    \begin{center}
\begin{tabular}{l|l|l}
\textbf{Learning Algorithm} & \textbf{Training Error} & \textbf{Test Error} \\ \hline
SVM & 0.0 & 35.6 \\
Random Forests  & 1.29 & 40.475\\
        Tanh MLP                         & 0.0         & 0.0 \\
Maxout MLP& 0.0 & 0.0\\
\end{tabular}
    \end{center}
    \end{sc}
    \caption{Performance of different learning algorithms on disentangled representation in Experiment 2.}
    \label{tab:disentangled}
\end{table}

\begin{table}[t]
    \small
    \begin{sc}
    \begin{center}
\begin{tabular}{l|l|l}
\textbf{Learning Algorithm} & \textbf{Training Error} & \textbf{Test Error} \\ \hline
SVM & 11.212 & 32.37 \\
Random Forests  & 24.839 & 48.915\\
Tanh MLP& 0.0 & 22.475\\
        Maxout MLP                       & 0.0         & 0.0 \\
\end{tabular}
    \end{center}
    \end{sc}
    \caption{Performance of different learning algorithms using a dataset with onehot vector and
        80 inputs as discussed for Experiment 3.}
    \label{tab:80inputs_withtrans}
\end{table}

\begin{table}[t]
    \small
    \begin{sc}
    \begin{center}
\begin{tabular}{l|l|l}
\textbf{Learning Algorithm} & \textbf{Training Error} & \textbf{Test Error} \\ \hline
SVM & 4.346 & 40.545 \\ 
Random Forests & 23.456 & 47.345 \\ 
Tanh MLP & 0 & 25.8 \\
\end{tabular}
    \end{center}
    \end{sc}
    \caption{Performance of different algorithms using a dataset with onehot vector and 80 binary inputs as discussed in Experiment 4.}
    \label{tab:80inputs}
\end{table}

\subsection{Does the Effect Persist with Larger Training Set Sizes?}

The results shown in this section indicate that the problem in the Pentomino task clearly is not
just a regularization problem, but rather basically hinges on 
an optimization problem. Otherwise, we would expect test error to decrease as the
number of training examples increases. This is shown first by studying
the online case and then by studying the ordinary training case with a fixed size
training set but considering increasing training set sizes. In the online minibatch
setting, parameter updates are performed as follows:

\begin{equation}
    \theta_{t+1} = \theta_t - \Delta_{\theta_t} 
\end{equation}

\begin{equation}
    \Delta_{\theta_t} = \epsilon \frac{\sum_i^N \nabla_{\theta_t}L(x_t, \theta_t)}{ N}
\end{equation}

where $L(x_t, \theta_t)$ is the loss incurred on example $x_t$ with parameters
$\theta_t$ , where $t \in \mathcal{Z}^+$ and $\epsilon$ is the learning rate.

Ordinary batch algorithms converge linearly to the optimum $\theta^{*}$, however the noisy gradient
estimates in the online SGD will cause parameter $\theta$ to fluctuate near the local optima.
However, online SGD directly optimizes the expected risk, because the examples are drawn iid from the
ground-truth distribution \citep{bottou2010large}. Thus:

\begin{equation}
    L_{\infty}=E[L(x,\theta)] = \int_x L(x, \theta)p(x)d_x
\end{equation}

where $L_{\infty}$ is the generalization error. Therefore online SGD is trying to minimize the
expected risk with noisy updates. Those noisy updates have the effect of regularizer:

\begin{equation}
    \Delta_{\theta_t} = \epsilon \frac{\sum_i^N \nabla_{\theta_t}L(x_t, \theta_t)}{ N} = \epsilon
\nabla_{\theta_t}L(x,
                                                                                               \theta_t)
    + \epsilon \xi_t
\end{equation}

where $\nabla_{\theta_t}L(x, \theta_t)$ is the true gradient and $\xi_t$ is the zero-mean
stochastic
gradient ``noise'' due to computing the gradient over a finite-size minibatch sample.

We would like to know if the problem with the Pentomino dataset is more a regularization or an optimization
problem. An SMLP-nohints model was trained by online SGD
with the randomly generated online Pentomino stream. 
The learning rate was adaptive, with the Adadelta procedure~\citep{zeiler2012adadelta} on minibatches of 100 examples. 
%YB: WHY DO WE NEED TO KNOW THAT?
%Our Pentomino dataset generator 
%generates batches of 500 examples and we train online SGD with minibatches of 100 examples. 
In the online SGD experiments, two SMLP-nohints that is trained with and without standardization at the
intermediate layer with exactly the same hyperparameters  are tested.
The SMLP-nohints P1NN patch-wise submodel has 2048 hidden units and 
the SMLP intermediate layer has $1152=64\times 18$ hidden units.
The nonlinearity that is used for the intermediate layer is 
the sigmoid. P2NN has 2048 hidden units.


SMLP-nohints has been trained either with or without standardization on top of the output units of the P1NN.
The experiments illustrated in Figures 
\ref{fig:std_vs_unstd_test_acc} and \ref{fig:std_vs_unstd_train_acc} are with the same SMLP without hints architecture for which results 
are given in Table \ref{tab:results_cmp}. In those graphs only the results for the training on
the randomly generated 545400 Pentomino samples have been presented. As shown in the plots 
SMLP-nohints was not able to generalize without standardization. Although without 
standardization the training loss seems to decrease initially, it eventually 
gets stuck in a plateau where training loss doesn't change much.

\begin{figure}[htp]
\centering{
\includegraphics[scale=0.6]{std_vs_unstd_test_error.pdf}
}
\caption{Test errors of SMLP-nohints with and without standardization in the intermediate layer. Sigmoid as an intermediate 
    layer activation has been used. Each tick (batch no) in the x-axis represents 400 examples.}
\label{fig:std_vs_unstd_test_acc}
\end{figure}

\begin{figure}[htp]
\centering{
\includegraphics[scale=0.6]{std_vs_unstd_training_losses.pdf}
}
\caption{Training errors of SMLP-nohints with and without standardization in the intermediate
    layer. Sigmoid nonlinearity has been used as an intermediate layer activation function.
        The x-axis is in units of blocks of 400 examples in the training set.}
\label{fig:std_vs_unstd_train_acc}
\end{figure}


%In Table  \ref{tbl:pento_std}, 

Training of SMLP-nohints online minibatch SGD is performed 
using standardization in the intermediate layer and Adadelta learning rate
adaptation, on 1046000 training examples
from the randomly generated Pentomino stream. At the end of the training, 
test error is down to 27.5\%, which is much better than chance but
from from the score obtained with SMLP-hints of near 0 error.

%\begin{table}[htbp!]
%{
%\newcommand{\mc}[3]{\multicolumn{#1}{#2}{#3}}
%\begin{center}
%    \begin{tabular}{l|l|l|l}
%    Precision & Recall & F1 Score & Error \\ \hline
%    0.799      & 0.605   & 0.689     & 0.275 \\
%    \end{tabular}
%\end{center}
%}
%\caption{Different performance metrics for the SMLP-nohints with standardization after training on 1046000 Pentomino examples.}
%\label{tbl:pento_std}
%\end{table}

In another SMLP-nohints experiment {\em without standardization} 
the model is trained with the 1580000 Pentomino
examples using online minibatch SGD. P1NN has 2048 hidden units and 16 sigmoidal outputs per patch.
for the P1NN hidden layer. P2NN has 1024 hidden units for the hidden layer. Adadelta is used to adapt 
the learning rate. At the end of training this SMLP, the test error remained
stuck, at 50.1\%.

%\begin{table}[htbp!]
%{
%    \newcommand{\mc}[3]{\multicolumn{#1}{#2}{#3}}
%    \begin{center}
%    \begin{tabular}{l|l|l|l}
%    Precision & Recall & F1 Score & Error \\ \hline
%    0.495      & 0.392   & 0.438     & 0.499 \\
%    \end{tabular}
%    \end{center}
%}
%\caption{Different performance metrics for online SMLP without standardization after training on 1580000 Pentomino examples.}
%\label{tbl:pento_unstd}

%\end{table}

% YB: THIS SECTION SHOULD BE COUPLED WITH THE PART ABOUT INCREASING TRAINING SET SIZE
% SINCE THE CONCLUSION IS SIMILAR.

\subsubsection{Experiments with Increased Training Set Size}

Here we consider the effect of training different learners with different numbers of training examples.
For the experimental results shown in Table \ref{tab:results_cmp},
3 training set sizes (20k, 40k and 80k examples) had been used. Each dataset was generated with different
random seeds (so they do not overlap). Figure
\ref{fig:error_bars} also shows the error bars for an ordinary MLP
with three hidden layers, for a larger range of training set sizes,
between 40k and 320k examples. The number of training epochs
is 8 (more did not help), and there are three hidden layers with 2048 feature detectors.
The learning rate we used in our experiments is 0.01. The activation function of the MLP is
a tanh nonlinearity, while the L1, L2 penalty coefficients are both 1e-6.

% YB: this seems out of place?
%The P1NN learns to classify patches with respect to the intermediate concepts very
%quickly. According to our experiments, 5,000 training examples are enough
%for the P1NN to learn the intermediate targets perfectly. P2NN is then trained
%of top of P1NN and performs almost perfectly, both on the training set and the
%test set, with 80,000 examples.

Table \ref{tab:results_cmp} shows that, without guiding hints, none of
the state-of-art learning algorithms could perform noticeably better than a random
predictor on the test set.  This shows the importance of intermediate hints
introduced in the SMLP. The decision trees and SVMs can overfit the training
set but they could not generalize on the test set. Note that the numbers reported
in the table are for hyper-parameters selected based on validation set error,
hence lower training errors are possible if avoiding all regularization and taking
large enough models. On the training set, the MLP with two large hidden
layers (several thousands) could reach nearly 0\% training error, but still
did not manage to achieve good test error.

In the experiment results shown in Figure~\ref{fig:error_bars}, we evaluate
the impact of adding more training data for the fully-connected MLP. As mentioned before
for these experiments we have used a MLP with three hidden layers where each layer
has 2048 hidden units. The tanh$(\cdot)$ activation function is used with 0.05 learning rate
and minibatches of size 200.

As can be seen from the figure, adding more training examples did not help
either training or test error (both are near 50\%, with training error slightly
lower and test error slightly higher), 
reinforcing the hypothesis that the difficult encountered 
is one of optimization, not of regularization.

\begin{figure}[htbp!]
 \centering
 \vspace*{-1mm}
 \includegraphics[scale=0.5]{./training_long.png}
 \vspace*{-2mm}
 \caption{Training and test error bar charts for a regular MLP with 3 hidden layers.
     There is no significant improvement on the generalization error of the MLP 
     as the new training examples are introduced.}
 \label{fig:error_bars}
 % error_bars_deep_mlp.png: 600x371 pixel, 72dpi, 21.17x13.09 cm, bb=0 0 600 371
\end{figure}

%In Pentomino dataset there are 
\subsection{Experiments on Effect of Initializing with Hints}

Initialization of the parameters in a neural network can have a big impact on the learning and
generalization \citep{GlorotAISTATS2010}. Previously \cite{Erhan+al-2010} showed that
initializing the parameters of a neural network with unsupervised pretraining guides the
learning towards basins of attraction of local minima that provides 
better generalization from the training dataset. In this section we
analyze the effect of initializing the SMLP with hints and then continuing without hints at the
rest of the training.
For experimental analysis of hints based initialization, SMLP is trained for 1 training epoch using the hints and for 60
epochs it is trained without hints on the 40k examples training set. We also compared the same architecture
with the same hyperparameters, against to SMLP-nohints trained for 61 iterations on the same dataset.
After one iteration of hint-based training SMLP obtained 9\% training error and 39\% test error.
Following the hint based training, SMLP is trained without hints for 60 epochs, but at epoch 18,
it already got 0\% training and 0\% test error. The hyperparameters for this experiment and the
experiment that the results shown for the SMLP-hints in Table \ref{tab:results_cmp} are 
the same. The test results
for initialization with and without hints are shown on Figure \ref{fig:opt_hint_init}. This figure
suggests that initializing with hints can give the same generalization performance but training takes
longer.

\begin{figure}[htp]
\centering{
\includegraphics[scale=0.6]{opt_hint_initialization_test_error.png}
}
\caption{Plots showing the test error of SMLP with random initialization vs initializing with hint based training.}
\label{fig:opt_hint_init}
\end{figure}


\subsubsection{Further Experiments on Optimization for Pentomino Dataset}

With extensive hyperparameter optimization and using standardization in the intermediate level of
the SMLP with softmax nonlinearity, SMLP-nohints was able to get 5.3\% training and 6.7\% test error
on the 80k Pentomino training dataset. We used the 2050 hidden units for the hidden
layer of P1NN and 11 softmax output per patch. For the P2NN, we used 1024 hidden units with
sigmoid and learning rate 0.1 without using any adaptive learning rate method. This SMLP uses 
a rectifier nonlinearity
for hidden layers of both P1NN and P2NN. Considering that architecture uses softmax as the intermediate activation function of SMLP-nohints. It is
very likely that P1NN is trying to learn the presence of specific Pentomino shape in a given patch. This architecture 
has a very large capacity in the P1NN, that probably provides it enough capacity to learn the presence
of Pentomino shapes at each patch effortlessly.

An MLP with 2 hidden layers, each 1024 rectifier units, was trained
using LBFGS (the implementation from the scipy.optimize library) 
on 40k training examples, with gradients computed on batches of 10000 examples at each iteration.
However, after convergence of training, the MLP was still doing chance on the test dataset.

We also observed that using linear units for the intermediate layer yields better generalization
error without standardization compared to using activation functions such as sigmoid, tanh and RELU
for the intermediate layer. SMLP-nohints was able to get 25\% generalization error with linear
units without standardization whereas all the other activation functions that has been tested failed to
generalize with the same number of training iterations without standardization and hints. This suggests that
using non-linear intermediate-level activation functions 
without standardization introduces an optimization
difficulty for the SMLP-nohints, maybe because the intermediate level acts like
a bottleneck in this architecture.

%CG: Removed that part because it is inconclusive:
%\paragraph{Investigating the Deeper Harder Hypothesis}
%In the experiments at Figure \ref{fig:depth_vs_relu}, we explored the effect of increasing depth 
%of the architecture on training difficulty of neural network for Pentomino task. Using unbounded
%nonlinearities such as RELU might yield to very large weights and this can result to very large
%gradient steps. Thus the learning can diverge or never converge, e.g.: for RELU all the incoming weights
%and biases of a layer can become negative. This effect can be more pronounced as the depth of the
%architecture increases.

%We trained 3 MLP's with RELU nonlinearity with different number of hidden units for each layer. All
%the neural networks uses the same hyperparameters. For each layer we used 1024 hidden units
%with linearly increasing momentum and linearly decaying learning rate.

%\begin{figure}[ht]
% \centering
% \mbox{
% \subfigure[Training Errors vs Depth of MLP]{
% \includegraphics[width=0.5\linewidth, height=\textheight, keepaspectratio=True]{relu_vs_depth_train.pdf}
% \label{fig:relu_train_depth}
% }
% \subfigure[Test Errors vs Depth of MLP]{
% \includegraphics[width=0.5\linewidth, height=\textheight, keepaspectratio=True]{relu_vs_depth_test.pdf}
% \label{fig:relu_test_depth}
% }
%}
% \caption{Left (a): This plot shows how the test error changes as the we  train 3 MLP's with different number
%               of layers.
%    Right (b): This plot shows how the generalization error changes as we train 3 MLP's with different number
%               of layers.}

%\label{fig:depth_vs_relu}
%\end{figure}


\section{Conclusion and Discussion}

In this paper we have shown an example of task which seems almost
impossible to solve by standard black-box machine learning algorithms,
but can be almost perfectly solved when one encourages a semantics for the intermediate-level
representation that is guided by prior knowledge. The
task has the particularity that it is defined by the composition
of two non-linear sub-tasks (object detection on one hand, and
a non-linear logical operation similar to XOR on the other hand).

What is interesting is that in the case of the neural network, we can
compare two networks with exactly the same architecture but a different
pre-training, one of which uses the known intermediate concepts to teach an
intermediate representation to the network. With enough capacity and training time they can
{\em overfit} but did not not capture the essence of the task, as seen by
test set performance. 

We know that a structured deep network can learn the task, if it is initialized in the right place,
and do it from very few training examples. Furthermore we have shown that if 
one pre-trains SMLP with
hints for only one epoch, it can nail the task. But the exactly same architecture which started training from random
initialization, failed to generalize.

Consider the fact that even SMLP-nohints with standardization after being trained
using online SGD on 1046000 generated examples and still gets 27.5\% test error. This is an
indication that the problem is not a regularization problem but possibly an inability to
find a {\em good effective local minima of generalization error}.

What we hypothesize is that for most initializations and architectures (in particular the fully-connected ones),
although it is possible to find a {\em good effective local minimum of training error}
when enough capacity is provided, it is difficult (without the proper initialization)
to find a good local minimum of generalization error. On the other hand, when the network architecture
is constrained enough but still allows it to represent a good solution
(such as the structured MLP of our experiments), it seems that the optimization
problem can still be difficult and even training error remains stuck high if the standardization
isn't used. Standardization obviously makes the training objective of the SMLP easier to optimize and helps it
to find at least a {\em better effective local minimum of training error}. This finding suggests that by using specific
architectural constraints and sometimes domain specific knowledge about the problem, one can alleviate
the optimization difficulty that generic neural network architectures face.

It could be that the combination of the network architecture and training procedure
produces a training dynamics that tends to yield into these minima that are
poor from the point of view of generalization error, even when they manage to nail training
error by providing enough capacity. Of course, as the number of examples increases, we would expect this
discrepancy to decrease, but then the optimization problem could still make
the task unfeasible in practice.  Note however that our preliminary
experiments with increasing the training set size (8-fold) for MLPs did not reveal
signs of potential improvements in test error yet, as shown in
Figure~\ref{fig:error_bars}. Even using online training on 545400 Pentomino examples,
the SMLP-nohints architecture was still doing far from perfect in terms of generalization error
(Figure~\ref{fig:std_vs_unstd_test_acc}).

These findings bring supporting evidence to the
``Guided Learning Hypothesis'' and ``Deeper
Harder Hypothesis'' from~\citet{Bengio-chapter-2013}:
higher level abstractions, which are expressed by composing
simpler concepts, are more difficult to learn (with the learner
often getting in an effective local minimum ), but that difficulty
can be overcome if another agent provides hints of the importance
of learning other, intermediate-level abstractions which are relevant
to the task.

Many interesting questions remain open. Would a network without any
guiding hint eventually find the solution with a enough training time
and/or with alternate parametrizations? To what extent is ill-conditioning a core issue?
The results with LBFGS were disappointing but changes in the architectures (such
as standardization of the intermediate level) seem to make training much easier.
Clearly, one can reach good solutions from an
appropriate initialization, pointing in the direction of an issue with local minima,
but it may be that good solutions are also reachable from
other initializations, albeit going through a tortuous ill-conditioned
path in parameter space. Why did our attempts at learning the intermediate
concepts in an unsupervised way fail? Are these results specific to the
task we are testing or a limitation of the unsupervised feature learning algorithm
tested? Trying with many more unsupervised variants and exploring
explanatory hypotheses for the observed failures could help us answer
that. Finally, and most ambitious, can we solve these kinds of problems
if we allow a community of learners to collaborate and collectively
discover and combine partial solutions in order to obtain solutions
to more abstract tasks like the one presented here? Indeed, we would
like to discover learning algorithms that can solve such tasks without
the use of prior knowledge as specific and strong as the one used
in the SMLP here. These experiments could be inspired by and inform
us about potential mechanisms for collective learning through cultural
evolutions in human societies.

\acks{We would like to thank to the ICLR 2013 reviewers for their insightful comments,
and NSERC, CIFAR, Compute Canada and Canada Research Chairs for funding.}

\bibliography{cultrefs,strings,ml,aigaion}

\section{Appendix}

\subsection{Binary-Binary RBMs on Pentomino Dataset}
\label{sec:bin_rbm_pento}
We trained binary-binary RBMs (both visible and hidden are binary)
on 8$\times$8 patches extracted from the Pentomino Dataset using PCD (stochastic
maximum likelihood), a
weight decay of .0001 and a sparsity penalty\footnote{implemented as TorontoSparsity in pylearn2, see the
yaml file in the repository for more details}. 
We used 256 hidden units and trained by SGD with a batch size of 32 and
a annealing learning rate \citep{Bengio-tricks-chapter-2013} starting from 1e-3 with annealing 
rate 1.000015.
The RBM is trained with momentum starting from 0.5. %YB and then what? up to what value of momentum? 
The biases are initialized to -2 in
order to get a sparse representation. 
The RBM is trained for 120 epochs (approximately 50 million updates).

After pretraining the RBM, its parameters are used to initialize
the first layer of an SMLP-nohints network.
As in the usual architecture of the SMLP-nohints on top of P1NN, there is an intermediate layer.
Both P1NN and the intermediate layer have a sigmoid nonlinearity, and the intermediate
layer has 11 units per location. This SMLP-nohints is trained with Adadelta and standardization at the
intermediate layer \footnote{In our auto-encoder experiments we directly fed features
to P2NN without standardization and Adadelta.}.

\begin{figure}[htp]
\centering{
\includegraphics[scale=0.6]{rbm_pre_errors.pdf}
}
\caption{Training and test errors of an SMLP-nohints network whose first layer
is pre-trained as an RBM. Training error
    reduces to 0\% at epoch 42, but test error is still chance.}
\label{fig:rbm_pre_errors}
\end{figure}

\begin{figure}[htp]
\centering{
\includegraphics[scale=0.8]{rbm_pentomino_filters.png}
}
\caption{Filters learned by the binary-binary RBM after training on the 40k examples. 
The RBM did learn the edge structure of Pentomino shapes.}
\label{fig:rbm_pento_filters}
\end{figure}

\begin{figure}[htp]
\centering{
\includegraphics[scale=1.0]{pentomino_samples.png}
}
\caption{100 samples generated from trained RBM. All the generated samples are 
valid Pentomino shapes.}
\label{fig:rbm_pento_samples}
\end{figure}

\iffalse
\begin{figure}[htp]
\centering{
\includegraphics[scale=1.0]{pento_negative_chain.png}
}
\caption{100 Binary RBM Pentomino negative samples.}
\label{fig:rbm_neg_chain}
\end{figure}
\fi

\subsection{Experimental Setup and Hyper-parameters}
\label{sec:setup}

\subsubsection{Decision Trees}

We used the decision tree implementation in the
scikit-learn~\citep{pedregosa2011scikit} python package which is an
implementation of the CART (Regression Trees) algorithm. The CART algorithm
constructs the decision tree recursively and partitions the input space such that
the samples belonging to the same category are grouped together
\citep{olshen1984classification}. We used The Gini index as the impurity
criteria. We evaluated the hyper-parameter configurations with a grid-search. We
cross-validated the maximum depth ($max\_depth$) of the tree (for
preventing the algorithm to severely overfit the training set) and
minimum number of samples required  to create a split ($min\_split$).
20 different configurations of hyper-parameter values were evaluated. 
We obtained the best validation error with $max\_depth=300$ and
$min\_split=8$.

\subsubsection{Support Vector Machines}

We used the ``Support Vector Classifier (SVC)'' implementation from the
scikit-learn package which in turn uses the libsvm's Support Vector
Machine (SVM) implementation. Kernel-based SVMs are non-parametric models that
map the data into a high dimensional space and 
separate different classes with hyperplane(s) such that the support
vectors for each category will be separated by a large margin. We
cross-validated three hyper-parameters of the model using grid-search: $C$,
$\gamma$ and the type of kernel($kernel\_type$). $C$ is the penalty term 
(weight decay) for the SVM and $\gamma$ is a hyper-parameter that controls the width of the
Gaussian for the RBF kernel. For the polynomial kernel, $\gamma$ controls
the flexibility of the classifier (degree of the polynomial)
as the number of parameters increases \citep{hsu2003practical, ben2010user}. 
We evaluated forty-two hyper-parameter configurations. That includes, two kernel types:
$\{RBF,~Polynomial\}$; three gammas: $\{1e-2,~1e-3,~1e-4\}$ for the RBF
kernel, $\{1,2,5\}$ for the polynomial kernel, and seven $C$ values among:
$\{0.1, 1, 2, 4, 8, 10, 16\}$. As a result of the grid search and
cross-validation, we have obtained the best test error by using the RBF kernel,
with $C=2$ and $\gamma=1$.
 
\subsubsection{Multi Layer Perceptron}

We have our own implementation of Multi Layer Perceptron based on the 
Theano~\citep{bergstra+al:2010-scipy-short} machine learning libraries. We have
selected 2 hidden layers, the rectifier activation function, and
2048 hidden units per layer. We cross-validated three hyper-parameters of the 
model using random-search, sampling the learning rates $\epsilon$ in log-domain, and 
selecting $L1$ and $L2$ regularization penalty coefficients in sets of fixed values,
evaluating 64 hyperparameter values. The range of the hyperparameter 
values are $\epsilon \in [0.0001, 1]$, $L1 \in \{0., 1e-6, 1e-5, 1e-4\}$ and 
$L2 \in \{0, 1e-6, 1e-5\}$.
As a result, the following were selected: $L1=1e-6$, $L2=1e-5$ and $\epsilon=0.05$.

\subsubsection{Random Forests}

We used scikit-learn's implementation of ``Random Forests'' decision
tree learning. The Random Forests algorithm creates an ensemble of
decision trees by randomly selecting for each tree a subset of features % using random subspace method???
and applying bagging to combine the individual decision trees \citep{Breiman01}.  We have used grid-search and
cross-validated the $max\_depth$, $min\_split$, and number of trees ($n\_estimators$). We have
done the grid-search on the following hyperparameter values, $n\_estimators\in\{5, 10, 15, 25, 50\}$, 
$max\_depth\in \{100, 300, 600, 900\}$, and $min\_splits\in\{1, 4, 16\}$. We obtained the best validation error
with $max\_depth=300$, $min\_split=4$ and $n\_estimators=10$.

\subsubsection{k-Nearest Neighbors}

We used scikit-learn's implementation of k-Nearest Neighbors (k-NN). k-NN
is an instance-based, lazy learning algorithm that selects the training
examples closest in Euclidean distance to the input query.
It assigns a class label to the test example based on the
categories of the $k$ closest neighbors. The hyper-parameters we have
evaluated in the cross-validation are the number of neighbors ($k$) and
$weights$. The $weights$ hyper-parameter can be either ``uniform'' or
``distance''. With ``uniform'', the value assigned to the query point 
is computed by the majority vote of the nearest neighbors. With ``distance'',
each value assigned to the query point is computed by weighted majority votes
where the weights are computed with the inverse distance between the query
point and the neighbors. We have used $n\_neighbours\in\{1, 2, 4, 6, 8, 12\}$ and
$weights\in\{"uniform", "distance"\}$ for hyper-parameter search. As a result of 
cross-validation and grid search, we obtained the best validation
error with $k=2$ and $weights$=``uniform''.

\subsubsection{Convolutional Neural Nets}

We used a Theano~\citep{bergstra+al:2010-scipy-short} implementation of
Convolutional Neural Networks (CNN) from the deep learning tutorial at
\url{deeplearning.net}, which is based on a vanilla version of a CNN
\citet{lecun1998gradient}. Our CNN has two convolutional layers. Following
each convolutional layer, we have a max-pooling layer. On top of the
convolution-pooling-convolution-pooling layers there is an MLP with one
hidden layer. In the
cross-validation we have sampled 36 learning rates in log-domain in the
range $[0.0001, 1]$ and the number of filters from the range $[10, 20, 30, 40,
  50, 60]$ uniformly. For the first convolutional layer we used 9$\times$9
receptive fields in order to guarantee that each object fits inside the
receptive field.  As a result of random hyperparameter search and doing manual
hyperparameter search on the validation dataset, the following values were selected:
\begin{itemize}
    \item The number of features used for the first layer is 30 and the second layer is 60.
    \item For the second convolutional layer, 7$\times$7 receptive fields.
    The stride for both convolutional layers is 1.
    \item Convolved images are downsampled by a factor of 2$\times$2 at each pooling operation.
    \item The learning rate for CNN is 0.01 and it was trained for 8 epochs.
\end{itemize}

\subsubsection{Maxout Convolutional Neural Nets}

We used the pylearn2 (\url{https://github.com/lisa-lab/pylearn2}) 
implementation of maxout convolutional networks~\citep{Goodfellow_maxout_2013}.
There are two convolutional layers in the selected architecture, 
without any pooling. In the last convolutional
layer, there is a maxout non-linearity. 
The following were selected by cross-validation: learning rate, number of channels for the
both convolution layers, number of kernels for the second layer and number of units and pieces
per maxout unit in the last layer, a linearly decaying learning rate, momentum
starting from 0.5 and saturating to 0.8 at the 200'th epoch. 
Random search for the hyperparameters was used to evaluate 48 different hyperparameter configurations
on the validation dataset. For the first convolutional layer, 8$\times$8 kernels were selected 
to make sure that each Pentomino shape fits into the kernel. Early stopping was used and 
test error on the model that has the best validation error is reported. Using norm constraint
on the fan-in of the final softmax units yields slightly better result on the validation dataset.

As a result of cross-validation and manually tuning the hyperparameters we used the following
hyperparameters:

\begin{itemize}
    \item 16 channels per convolutional layer. 600 hidden units for the maxout layer.
    \item 6x6 kernels for the second convolutional layer.
    \item 5 pieces for the convolution layers and 4 pieces for the maxout layer per maxout units.
    \item We decayed the learning rate by the factor of 0.001 and the initial learning rate is
    0.026367. But we scaled the learning rate of the second convolutional layer by a constant
    factor of 0.6.
    \item The norm constraint (on the incoming weights of each unit) is 1.9365.
\end{itemize}

Figure \ref{fig:maxconvnet_filters} shows the first layer filters of the maxout convolutional net, 
after being trained on the 80k training set for 85 epochs.

\begin{figure}[htp]
\centering{
\includegraphics[scale=1.0]{maxconvnet_filters.png}
}
\caption{Maxout convolutional net first layer filters. 
Most of the filters were able to learn the basic edge structure of the Pentomino shapes.}
\label{fig:maxconvnet_filters}
\end{figure}


\subsubsection{Stacked Denoising Auto-Encoders}
Denoising Auto-Encoders (DAE) are a form of regularized 
auto-encoder~\citep{Bengio-Courville-Vincent-TPAMI2013}. The DAE forces the hidden layer
 to discover more robust features and prevents it from simply learning the identity by 
reconstructing the input from a corrupted version of it~\citep{Vincent-JMLR-2010}. Two DAEs
were stacked, resulting in an unsupervised transformation with two hidden
layers of 1024 units each. Parameters of all layers are then fine-tuned with supervised fine-tuning using logistic regression 
as the classifier and 
SGD as the gradient-based optimization algorithm. The stochastic corruption process
is binomial (0 or 1 replacing each input value, with probability 0.2).
The selected learning rate 
is $\epsilon_0=0.01$ for the DAe and $\epsilon_1=0.1$ 
for supervised fine-tuning. Both L1 and L2 penalty for the DAEs and 
for the logistic regression layer are set to 1e-6.

\paragraph{CAE+MLP with Supervised Finetuning:}

A regularized auto-encoder which sometimes outperforms the DAE is the Contractive
Auto-Encoder (CAE), \citep{Rifai-icml2012}, which penalizes the Frobenius norm 
of the Jacobian matrix of derivatives of the hidden units with respect to the CAE's
inputs. The CAE serves as pre-training for an MLP, and in the supervised fine-tuning state,
the Adagrad method was used to automatically tune the learning rate \citep{duchi2010adaptive}.

After training a CAE with 100 sigmoidal units patch-wise, the features extracted on each patch
are concatenated and fed as input to an MLP. The selected Jacobian penalty coefficient is 2,
the learning rate for pre-training is 0.082 with batch size of 200 and 
200 epochs of unsupervised learning are performed on the training set. For supervised finetuning,
the learning rate is 0.12 over 100 epochs, L1 and L2 regularization penalty terms respectively are 1e-4 and 1e-6,
and the top-level MLP has 6400
hidden units.

%One important point about the supervised finetuning is that only the patches that
%have an object in it gets a gradient. If we assume that $a^{(i)}_{[p_i]}$ is the postsynaptic
%activation for the patch $p_i$ at the ith layer and $z^{(i)}$ is the presynaptic activation at the
%ith layer. Now let's consider that the we have only one layer of auto-encoder with weight matrix
%$U$. The derivative of loss function with respect to $U$ will be:

%$$\frac{\partial L}{\partial U}= \frac{\partial \mathbb{L}}{\partial a^{(2)}_{[p_i]}}
%\frac{\partial a^{(2)}_{[p_i]}}{\partial z^{(1)}} \frac{\partial z^{(1)}}{\partial U}$$

%Since $z_j$ at patch $p_i$ is, $z_j = Up_i+b$, the derivative will be:
%$\frac{\partial z^{(1)}}{\partial U}=p_{i}$ and if $p_i$ is empty, $\frac{\partial
%    \mathbb{L}}{\partial U}=0$.

\paragraph{Greedy Layerwise CAE+DAE Supervised Finetuning:}

For this experiment we stack a CAE with sigmoid non-linearities
and then a DAE with rectifier non-linearities
during the pre-training phase.
As recommended by \citet{Glorot+al-AI-2011} we have used a softplus nonlinearity for reconstruction,
$softplus(x)=log(1+e^x)$.
We used an L1 penalty on the rectifier outputs
to obtain a sparser representation with rectifier non-linearity and L2 regularization 
to keep the non-zero weights small.

The main difference between the DAE and CAE is that the DAE yields more robust reconstruction
whereas the CAE obtains more robust features \citep{Rifai+al-2011-small}.

As seen on Figure \ref{fig:net_arch_nohints} the weights U and V are shared on each patch
and we concatenate the outputs of the last auto-encoder on each patch to feed it as an input to
an MLP with a large hidden layer.

We used 400 hidden units for the CAE and 100 hidden units for DAE. The learning rate used for 
the CAE is
0.82 and for DAE it is 9*1e-3. The corruption level for the DAE (binomial noise) 
is 0.25 and the contraction level for the CAE is 2.0. The L1
regularization penalty for the DAE is 2.25*1e-4 and the L2 penalty is 9.5*1e-5. 
For the supervised finetuning
phase the learning rate used is 4*1e-4 with L1 and L2 penalties respectively 1e-5 and 1e-6. 
The top-level MLP has  6400 hidden units. The auto-encoders are each trained for 150 epochs
while the whole MLP is fine-tuned for 50 epochs.

\paragraph{Greedy Layerwise DAE+DAE Supervised Finetuning:} 
For this architecture, we have trained two layers of denoising auto-encoders greedily 
and performed supervised finetuning after unsupervised pre-training. The motivation for using two 
denoising auto-encoders is the fact that rectifier nonlinearities work well with the
deep networks but it is difficult to train CAEs with the rectifier non-linearity. 
We have used the same type of denoising auto-encoder that is used for the
greedy layerwise CAE+DAE supervised finetuning experiment.

In this experiment we have used 400 hidden units for the first layer DAE and 100
hidden units for the second layer DAE. The other hyperparameters for DAE and supervised
finetuning are the same as with the {\emph{CAE+DAE MLP Supervised Finetuning}} experiment.

\end{document}



\documentclass{article} % For LaTeX2e
\usepackage{nips11submit_e,times}
\usepackage{amsmath, amsthm}
\usepackage{amsfonts}
\usepackage{graphicx} % more modern
\usepackage{subfigure} 
\usepackage{multirow} 
\usepackage{pifont} 
%\documentstyle[nips10submit_09,times,art10]{article} % For LaTeX 2.09
\usepackage[numbers]{natbib} % Numbered citations


\title{On Training Deep Boltzmann Machines}


\author{
Guillaume Desjardins,
%\texttt{desjagui@iro.umontreal.ca} \\
%\And
Aaron Courville,
%\\ \texttt{courvila@iro.umontreal.ca} \\
%\AND
Yoshua Bengio \\
\texttt{\{desjagui,courvila,bengioy\}@iro.umontreal.ca} \\
D\'{e}partement d'informatique et de recherche op\'{e}rationnelle \\
Universit\'{e} de Montr\'{e}al \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\nipsfinalcopy % Uncomment for camera-ready version

\newcommand{\E}{\mathbb{E}}
\newcommand{\Egy}{{\bf E}}
\newcommand{\x}{\mathbf{x}}
\renewcommand{\v}{\mathbf{v}}
\newcommand{\h}{\mathbf{h}}
\newcommand{\nv}{\mathbf{v^{{\bf -}}}}
\newcommand{\nh}{\mathbf{h^{{\bf -}}}}
\newcommand{\s}{\mathbf{s}}
\renewcommand{\b}{\mathbf{b}}
\renewcommand{\c}{\mathbf{c}}
\newcommand{\W}{\mathbf{W}}
\newcommand{\T}{{\bf \mathcal T}}
\newcommand{\B}{{\bf \mathcal B}}
\newcommand{\N}{{\mathcal N}} % normal distribution
\newcommand{\lr}{{\epsilon}}  % learning rate

\begin{document}


\maketitle
\vspace*{-4mm}
\begin{abstract}
The deep Boltzmann machine (DBM) has been an important development in the quest
for powerful ``deep'' probabilistic models. To date, simultaneous or joint
training of all layers of the DBM has been largely unsuccessful with
existing training methods. We introduce a simple regularization scheme that
encourages the weight vectors associated with each hidden unit to have
similar norms. We demonstrate that this regularization can be easily combined with
standard stochastic maximum likelihood to yield an effective
training strategy for the simultaneous training of all layers of the deep
Boltzmann machine.
\end{abstract}

\section{Introduction}

Since its introduction by \citet{Salakhutdinov2009}, the deep Boltzmann
machine (DBM) has been one of the most ambitious attempts to build a probabilistic model with many layers of
latent or hidden variables. 
The DBM shares some characteristics with the earlier deep belief
network (DBN) \cite{Hinton06}. Both models may be viewed as multi-layer or ``deep''
extensions of the popular restricted Boltzmann machine (RBM). However,
unlike the DBN, commonly used DBM approximate inference schemes implement
true feedback mechanisms where the inferred activations of high-level units can
influence the activations of lower-level units. Thus alternative
interpretations of an input are able to compete at all levels of the model
simultaneously. This sort of sophistication in inference has the potential
to lead to more robust and globally coherent inferences, which are likely
to translate to better performance when the model is employed in tasks such
as classification.

Despite its success, the DBM has not yet entirely fulfilled this potential and the
DBN remains the more popular model paradigm for deep probabilistic models.
One potential explanation of why this is so could be tied to the difficulties one encounters when attempting
to estimate the model parameters from data.  Straightforward applications
of gradient-based methods such as stochastic maximum likelihood (SML)
\cite{Younes1999} (also known as persistent contrastive divergence
\cite{Tieleman08}) appear to fall in poor local minima, and fail to
adequately explore the space of model parameters.

One solution, as presented in \cite{Salakhutdinov2009}, is based on a greedy
layer-wise pretraining strategy which appears to overcome these poor local
minima . Each layer is trained as an RBM with the latent activations of the
layer below as its input.  These layers are then recombined by rescaling
the weights to account for the doubling of inputs into each intermediate
layer.  While this procedure seems to work well, it involves many steps and
is more complicated than would be ideal.  Also, more importantly, the
necessity of layer-wise pretraining makes it very difficult for the
organization of upper layers to influence the topological organization of
lower layers. For example, in cases where we would like to learn a DBM that
is not fully connected between each layer, it would potentially be very
desirable for the global connectivity pattern (eg. local receptive fields)
to influence the pattern of learned filters at all layers. Layer-wise
pretraining precludes this possibility.  If, on the other hand, one could
jointly train all layers of a deep Boltzmann machine simultaneously, then
the pattern of activation of the upper layer units would have an
opportunity to influence the weights trained at the lower layers via their
effect on lower-layer units activations.

In this paper, we describe a simple scheme for joint training of all layers of a
deep Boltzmann machine. Our strategy is based on the
observation that the poor local minima in which SML falls are characterized
by high variance in the norms of the weight vectors, particularly those
between the data and the first layer of hidden units. Our solution is to
simply add a regularization term to the standard maximum likelihood
training criterion that penalizes large differences in the norms of the
weight vectors, both within a layer and across neighbouring layers. Our
regularization is based on the intuition that successful models are those
where all units contribute roughly equally to the representation of the
data. This is a common principle that has previously been applied in the
form of sparsity penalties for RBM and DBN training
\cite{HonglakL2008}, where all hidden units are encouraged to be active over
an equal proportion of the data. Our application of this principal is somewhat
different, we do not explicitly control the activation of the hidden units,
instead we encourage equal influence of each \emph{active} hidden unit. 
We demonstrate, with experiments, both the failure of standard SML training
for DBMs and the effectiveness of our regularized-SML DBM training strategy.

\section{Boltzmann Machines}

A Boltzmann machine is defined as a network of
symmetrically-coupled binary stochastic units (random variables). These
stochastic units can be divided into two groups: (1) the \emph{visible}
units $v \in \{0,1\}^D$ that represent the data, and (2) the \emph{hidden}
units $h \in \{0,1\}^N$ that mediate dependencies between the visible units
through their mutual interactions. The pattern of interaction is specified
through the energy function:
\begin{equation}
E_{\mathrm{BM}}(v,h;\theta) = -\frac{1}{2}v^TUv-\frac{1}{2}h^TVh-v^TWh-b^Tv-d^Th,
\label{eq:boltzmann_energy}
\end{equation}
where $\theta = \{U,V,W,b,d\}$ are the model parameters which respectively
encode the visible-to-visible interactions, the hidden-to-hidden
interactions, the visible-to-hidden interactions, the visible
self-connections, and the hidden self-connections (also known as biases). To
avoid over-parametrization, the diagonals of $U$ and $V$ are set to zero.

The energy function
specifies the probability distribution over the joint space $[v,h]$, via the Boltzmann distribution:
\begin{equation}
P(v,h) = \frac{1}{Z(\theta)}\exp\left(-E_{\mathrm{BM}}(v,h;\theta)\right),
\end{equation}
where the partition function $Z(\theta)$ ensures that the density
normalizes:
\begin{equation}
Z(\theta) = \sum_{v_1 = 0}^{v_1 = 1}\cdots \sum_{v_D = 0}^{v_D = 1}
\sum_{h_1 = 0}^{h_1 = 1}\cdots \sum_{h_N = 0}^{h_N = 1} \exp\left(-E_{\mathrm{BM}}(v,h;\theta)\right).
\end{equation}
This joint probability distribution gives rise to the set of conditional
distributions of the form:
\begin{eqnarray}
P(h_i \mid v, h_{\setminus i}) & = &
    \mathrm{sigmoid}
    \left( 
        \sum_{j}W_{ji}v_{j} + \sum_{i' \setminus i} V_{ii'} h_{i'} + d_{i} 
    \right) \\ 
P(v_j \mid h, v_{\setminus j}) & = &
    \mathrm{sigmoid}
    \left( 
        \sum_{i}W_{ji}v_{j} + \sum_{j' \setminus j} U_{jj'} v_{j'} + b_{j} 
    \right).
\end{eqnarray}
In general, inference in the Boltzmann machine is intractable. For example,
computing the conditional probability of $h_i$ given the visibles, $P(h_i
\mid v)$, requires marginalizing over the rest of the hiddens which implies
evaluating a sum with $2^{N-1}$ terms:
\begin{equation}
P(h_i \mid v) = \sum_{h_1 = 0}^{h_1 = 1}\cdots \sum_{h_{i-1} = 0}^{h_{i-1} = 1}
\sum_{h_{i+1} = 0}^{h_{i+1} = 1} \cdots \sum_{h_N = 0}^{h_N = 1} P(h \mid v)
\end{equation}
However with some judicious choices
in the pattern of interactions between the visible and hidden units, more
tractable subsets of the model family are possible.
 
\subsection{Restricted Boltzmann Machines}

The restricted Boltzmann machine (RBM) is likely the most popular subset of
Boltzmann machines. They are defined by restricting the interactions in the
Boltzmann energy function, in Eq. \ref{eq:boltzmann_energy}, to only those
between $h$ and $v$, i.e. for $E_{\mathrm{RBM}}$, $U=\mathbf{0}$ and $V=\mathbf{0}$. As such, the
RBM can be said to form a \emph{bipartite} graph with the visibles and the
hiddens forming two layers of vertices in the graph. With this
restriction, the RBM possesses the useful property that the conditional
distribution over the hidden units factorizes given the visibles:
\begin{equation}
P(h \mid v) = \prod_{i} P(h_{i} \mid v) = \prod_{i} \mathrm{sigmoid}\left(
  \sum_{j} W_{ji} v_{j}+d_{i}\right)
\end{equation}
Likewise, the conditional distribution over the visible units given the
hiddens also factorizes:
\begin{equation}
P(v \mid h) = \prod_{j} P(v_{j} \mid h) = \prod_{j} \mathrm{sigmoid}\left(
  \sum_{i} W_{ji} h_{i}+b_{j}\right)
\end{equation}
This conditional factorization property of the RBM immediately implies that
most inferences we would like make are readily tractable. For example, the
conditional independence of the hiddens implies that posterior marginals of the form
$P(h_{i} \mid v)$ are immediately available.

Importantly, the tractability of the RBM does not extend to its partition
function, which still involves sums with exponential number of terms.
It does imply however that we can limit the number of terms to
$\min\{2^D,2^N\}$. Usually this is still an unmanageable number of terms
and therefore we must resort to approximate methods to deal with its
computation.

Learning in the RBM is also rendered much more tractable in comparison to
the general Boltzmann machine. Typically, learning involves finding a set
of model parameters that approximately  maximizes the log likelihood of a 
training dataset:
\begin{equation}
\sum_{t} log P(v_{:,t}; \theta) = \sum_{t} log \sum_{h_{1,t}=0}^{h_{1,t}=1}\cdots\sum_{h_{N,t}=0}^{h_{N,t}=1}P(v_{:,t}, h_{:,t} ; \theta).
\end{equation}
This can be accomplished via gradient ascent. The gradient of the log likelihood
of the data for the RBM is given by:
\begin{equation}
\frac{\partial}{\partial\theta_{i}}\left(\sum_{t=1}^{T}\log p(v_{:,t})\right)
 = - \sum_{t=1}^{T}\left\langle
     \frac{\partial}{\partial\theta_{i}}E_{\mathrm{RBM}}(v_{:,t},h)\right\rangle _{p(h_{:,t}\mid v_{:,t})} + 
     \sum_{t=1}^{T}\left\langle
     \frac{\partial}{\partial\theta_{i}}E_{\mathrm{RBM}}(v,h)\right\rangle
   _{p(v,h)}, \nonumber
\end{equation}
where we have the expectations with respect to $p(h_{:,t}\mid v_{:,t})$
in the {}``clamped'' condition, and over the full joint $p(v_{:,t},h_{:,t})$
in the {}``unclamped'' condition.
In training, we follow the stochastic maximum likelihood (SML) algorithm (also know as
persistent contrastive divergence or PCD)~\cite{Younes1999,Tieleman08},
i.e., performing only one or a few updates of an MCMC chain between
each parameter update.

\section{Deep Boltzmann Machines}

\begin{figure}
   \centering
   \includegraphics[scale=0.50]{dbm.png}
   \caption[]{\small{\label{fig:dbm} A graphical depiction of a 2-layer DBM.
   Undirected weights connect the visible layer to layer $h^{(1)}$; and layers
   $h^{(1)}$ to $h^{(2)}$. Note that $v$ and $h^{(2)}$ are conditionally
   independent given $h^{(1)}$.}}
\end{figure}

The Deep Boltzmann Machine (DBM) is another
particular subset of the Boltzmann machine family of models where the units
are again arranged in layers. However unlike the RBM, the DBM possesses multiple
layers of hidden units as illustrated in Figure \ref{fig:dbm}. With
respect to the Boltzmann energy function of Eq.~\ref{eq:boltzmann_energy}, the DBM
corresponds to setting $U=0$ and a sparse connectivity structure in both
$V$ and $W$. We can make the structure of the DBM more explicit by specifying
its energy function. For the 2-layer model it is given as:
\begin{equation}
E_{\mathrm{DBM}}(v, h^{(1)}, h^{(2)}; \theta) = 
    -v^T W h^{(1)} - 
    \left.h^{(1)}\right.^T V h^{(2)} -
    \left.d^{(1)}\right.^T h^{(1)} -
    \left.d^{(2)}\right.^T h^{(2)} -
    b^T v ,
\label{eq:boltzmann_energy}
\end{equation}
with $\theta = \{W,V,d^{(1)},d^{(1)},b\}$. The DBM can also be characterized as
a bipartite graph between two sets of vertices, formed by the units in odd and even-numbered
layers (with $v:=h^{(0)}$).
% \begin{figure}[ht!]
%     \centering
%         \includegraphics[scale=0.38]{fig1b_lr=001.pdf}
%     \caption[]{An illustration of a DBM}
%     \label{fig:DBM}
% \end{figure}

\subsection{Mean-field approximate inference} 
A key point of departure from the RBM is that 
the posterior distribution over the hidden units (given the visibles) is no longer
tractable, due to the interactions between the hidden units. \citet{Salakhutdinov2009} resort to a
mean-field approximation to the posterior. Specifically, in the case of the
2-layer model, we wish to approximate $P\left(h^{(1)},h^{(2)} \mid v\right)$ with the factored
distribution $Q_v(h^{(1)},h^{(2)}) =
\prod_{j=1}^{N_1}Q_v\left(h^{(1)}_{j}\right)\
\prod_{i=1}^{N_2}Q_v\left(h^{(2)}_{i}\right)$. such that the KL
divergence $\mathrm{KL} \left( P\left(h^{(1)},h^{(2)} \mid v\right) \Vert
Q_v(h^{1},h^{2}) \right)$ is minimized or equivalently, that a lower
bound to the log likelihood is maximized:
\begin{equation}
\log P(v) > \mathcal{L}(Q_{v}) \equiv
\sum_{h^{(1)}}\sum_{h^{(2)}}Q_v(h^{(1)},h^{(2)})\log\left(\frac{P(v,h^{(1)},h^{(2)})}{Q_v(h^{(1)},h^{(2)})}\right)
\label{eq:variational_lower_bound}
\end{equation}

Maximizing this lower-bound with respect to the mean-field distribution
$Q_v(h^{1},h^{2})$ yields the following mean field update equations:
\begin{align}
    \label{eq:mf1}
    \hat{h}^{(1)}_i &\leftarrow \mathrm{sigmoid}\left( 
        \sum_j W_{ji} v_j + \sum_k V_{ik} \hat{h}^{(2)}_k + d^{(1)}_i
        \right) \\
    \label{eq:mf2}
    \hat{h}^{(2)}_k &\leftarrow \mathrm{sigmoid}\left( 
        \sum_i V_{ik} \hat{h}^{(1)}_i + d^{(2)}_k 
        \right)
\end{align}

Iterating Eq.~(\ref{eq:mf1}-\ref{eq:mf2}) until convergence yields the
parameters used to estimate the ``variational positive phase'' of
Eq.~\ref{eq:variational_gradient}:
\begin{align}
\mathcal{L}(Q_{v}) &=
    \mathbb{E}_{Q_v}\left[ 
    \log P(v,h^{(1)},h^{(2)}) -
    \log Q_v(h^{(1)},h^{(2)}) \right] \nonumber \\
&= \mathbb{E}_{Q_v} \left[ 
    - E_{\mathrm{DBM}}(v,h^{(1)},h^{(2)}) 
    - \log Q_v(h^{(1)},h^{(2)}) \right]
    - \log Z(\theta) \nonumber \\
\label{eq:variational_gradient}
\frac{\partial \mathcal{L}(Q_{v})} {\partial \theta} &= 
    -\mathbb{E}_{Q_v} 
    \left[ 
    \frac{\partial E_{\mathrm{DBM}}(v,h^{(1)},h^{(2)})}{\partial \theta} 
    \right]
    + \mathbb{E}_{P}
    \left[
    \frac{\partial E_{\mathrm{DBM}}(v,h^{(1)},h^{(2)})}{\partial \theta}
    \right]
\end{align}

Note that this variational learning procedure leaves the ``negative phase''
untouched. It can thus be estimated through SML or Contrastive Divergence
\cite{Hinton-PoE-2000} as in the RBM case.

\subsection{Training Deep Boltzmann Machines}
Despite the intractability of inference in the DBM, its training should not, in
theory, be much more complicated than that of the RBM. The major difference
being that instead of maximizing the likelihood directly, we instead choose
parameters to maximize the lower-bound on the likelihood given in
Eq.~\ref{eq:variational_lower_bound}. The SML-based algorithm for maximizing
this lower-bound is as follows:
\begin{enumerate}
\item Clamp the visible units to a training example.
\item Iterate over Eq.~(\ref{eq:mf1}-\ref{eq:mf2}) until convergence.
\item Generate negative phase samples $v^-$, $h^{(1)-}$ and $h^{(2)-}$ through SML.
\item Compute $\partial \mathcal{L}(Q_v) \left/ \partial \theta \right.$
    using the values obtained in steps 2-3.
\item Finally, update the model parameters with a step of gradient ascent.
\end{enumerate}

While the above procedure appears to be a simple extention of the highly
effective SML scheme for training RBMs, as we demonstrate in
Sec.~\ref{sec:exp}, this procedure seems vulnerable to falling in poor
local minima which leave many filters effectively dead (not significantly
different from its random initialization with small norm). 
\label{sec:layerwise}

The failure of the SML joint training strategy was noted by
\citet{Salakhutdinov2009}. As a far more successful alternative, they
proposed a greedy layer-wise training strategy.
This procedure consists in pre-training
the layers of the DBM, in much the same way as the Deep Belief Network: i.e.
by stacking RBMs and training each layer to independently model the output of
the previous layer.
A final joint ``fine-tuning'' is done following the above SML-based procedure.

% The above method may appear suboptimal, in that only the first hidden layer
% $h^{(1)}$ is in direct contact with the data. It is thus conceivable that the
% above procedure would leave the second layer weights $V$ untouched, until the
% weights $W$ were trained sufficiently, so as to present a crude, abstract
% representation of the data to the upper layers. As we shall see in
% Sec.~\ref{sec:joint_training} however, this is actually the opposite of what
% happens in practice.

% This theory however, motivates the greedy layer-wise training procedure
% proposed in \citet{Salakhutdinov2009}. This procedure consists in pre-training
% the layers of the DBM, in much the same way as the Deep Belief Network: i.e.
% by stacking RBMs and training each layer to independently model the output of
% the previous layer. The DBM pre-training however, is modified to account for
% ``future'' top-down connections: layers

\section{Joint Training of a DBM}
\label{sec:joint_training}

While greedy layer-wise training has been shown to be reasonably
successful, a means of joint training a DBM would be highly desireable. Not
only would it be simpler, but it would open the door to local
receptive field learning and more general architectures where we would
like the top-down pattern of connectivity to influence the learning of
lower-level features. In this section we detail our simple proposal for a means
to jointly train all layers of a DBM.

Our strategy is based on the observation that standard SML-based joint
training tends produce high variance in the weight vectors associated with
each hidden unit -- particularly across the first-layer weight vectors that
connect the visible units to the first hidden layer units. 

Our proposal is thus to regularize the maximum likelihood objective, in order
to encourage units to have similar norms (i) within a given layer and (ii) to a
lesser extent, across neighbouring layers. We thus introduce an additional parameter
$\mu^{(l)}$ for each layer of the DBM, representing the average norm of weight
vectors of the $l$-th layer. We then add a regularization term to our objective
$\mathcal{L}(Q_v)$, which penalizes deviations from this mean-value using a
squared-error penalty.  $\mu^{(l)}$'s belonging to adjacent layers are further
constrained to be close (again in the $l2$ sense). This gives rise to the
following regularized objective:
\begin{align}
    \label{eq:regll}
    \max_\theta \mathcal{L}(Q_{v}) +  \
    \alpha^{(1)} \sum_{j=1}^{N_1} \left( \Vert W_{\cdot i} \Vert_2 - \mu^{(1)} \right)^2 +\
    \alpha^{(2)} \sum_{i=1}^{N_2} \left( \Vert V_{\cdot j} \Vert_2 - \mu^{(2)} \right)^2 +\
    \gamma \left( \mu^{(1)} - \mu^{(2)} \right)^2.
\end{align}

One can think of this regularization term as a spring, which ensures that the
system evolves jointly from an initial random weight configuration (where the
columns of $W^{(l)}$ have small norm) to a good model of the input distribution
$P(v)$, which undoubtedly requires weight vectors with larger norms.

\section{Experiments}
\label{sec:exp}

To evaluate the effect of our regularization term, we trained deep Boltzmann
machines on the pervasive MNIST dataset \cite{LeCun98-small}. All our models
were trained for $10^6$ updates, using minibatches of size $50$, varying the
learning rate in $\{10^{-2},10^{-3},10^{-4}\}$.

\paragraph{Direct Approach} 
Our first model was a 3-layer DBM with [500,500,1000] hidden units in the
first, second and third layers (respectively).  The resulting filters are shown
in Figure~\ref{fig:bad_filters}.

\begin{figure}
    \centering
    \subfigure[Layer 1]
    {
        \label{fig:bad_l1}
        \includegraphics[scale=0.5]{dbm_e1M_layer1.png}
    }
    \subfigure[Layer 2]
    {
        \label{fig:bad_l2}
        \includegraphics[scale=0.5]{dbm_e1M_layer2.png}
    }
    \subfigure[Layer 3]
    {
        \label{fig:bad_l3}
        \includegraphics[scale=0.5]{dbm_e1M_layer3.png}
    }
    \caption[]{\small{Random subset of filters obtained by a 784-500-500-1000
    DBM, after $10^6$ updates (trained jointly, using the direct approach). The
    learning rate was set to $10^{-3}$ and the batch size to $50$. Higher-level weights
    are visualized by performing a linear combination of lower-level weights,
    but only picking the $20$ most active connections.
    \label{fig:bad_filters}}}
\end{figure}

% These results came as somewhat of a surprise. Our initial hypothesis was that
% $h^{(2)}$ and $h^{(3)}$ not being directly connected to the data, the first
% layer-weights would take ``all the credit'', and train normally as in a normal
% RBM, leaving the upper layer-weights unchanged. 

As we can see, many of the first-layer
filters are not significantly different from their random initialization, with
very small norm. Based on the difference between these results and the
successful training of an RBM, we speculate that the top-down interactions
prevent the first layer from learning useful filters. We have confirmed that
the high-frequency ``noise'' filters are actually the ones with lowest norm. It
seems as though early top-down input is influencing the activations of these
hidden units, perhaps directly through some form of suppression, or indirectly,
by reinforcing the activation of the subset of filters which become useful early on.
While we plot filters for 3-layer networks, the same results hold for networks
of depth two.

% above procedure would leave the second layer weights $V$ untouched, until the
% weights $W$ were trained sufficiently, so as to present a crude, abstract
% representation of the data to the upper layers. As we shall see in
% Sec.~\ref{sec:joint_training} however, this is actually the opposite of what
% happens in practice.

\paragraph{Joint Training}

Using our regularized objective, we train a 2-layer DBM with 500 hidden
units in each layer. We set the hyper-parameters of Eq.\ref{eq:regll} as
follows: $\alpha^{(1)}=0.1$, $\alpha^{(2)}=0.1$ and $\gamma = 1$. Furthermore,
we dampen the learning rate on the parameters $\mu^{(1)}$ and $\mu^{(2)}$ by a
factor of a thousand, in order for the layer norms to evolve more slowly.
A random subset of filters is shown in Figure~\ref{fig:good_filters}.

\begin{figure}
    \centering
    \subfigure[Layer 1]
    {
        \label{fig:good_l1}
        \includegraphics[scale=0.5]{dbm_exp41_e1M_layer1.png}
    }
    \subfigure[Layer 2]
    {
        \label{fig:good_l2}
        \includegraphics[scale=0.5]{dbm_exp41_e1M_layer2.png}
    }
    \caption[]{\small{Random subset of filters obtained by a 784-500-500 DBM,
    after $10^6$ updates. All layers were trained jointly using the
    regularized cost of Eq.\ref{eq:regll}. The learning rate was set to
    $10^{-2}$ and the batch size to $50$.
    \label{fig:good_filters}}}
\end{figure}

The effect of our regularization term is drastic: the vast majority of first
layer filters seem to train successfully and resemble the pen-stroke detectors
characteristic of features learned on MNIST.  While more difficult to
interpret, the second layer filters are also clearly superior to the ones of
Figure~\ref{fig:bad_filters}, combining lower-level features (pen strokes) into
more global digit-like objects.

%Using Annealed Importance Sampling \cite{Neal-2001,Salakhutdinov+Murray-2008},
%we estimated the test set likelihood of our model to be $-60.05 \pm 5.72$
%($\pm$ indicating standard deviation). This compares favorably to the results
%of \cite{Salakhutdinov2009}, where a 2-layer DBM with [500,1000] units achieved
%$-84.62 \pm .11$.

% our method (results)

%dlogz =  -153.323532104
%var_dlogz =  32.7451445105
%log_za =  1155.49200767
%log_z =  1002.16847557
%var_dlogz =  32.7451445105
%Test set likelihood:  -60.0453232422

% our method (hyperparams)
%"model": !obj:dbm.dbm2.DBM {
    %"seed" : 123141,
    %"n_u": [784,500,500],
    %"lr": 0.01,
    %"lr_timestamp": [0.],
    %"lr_mults": {'mu1': [0.001], 'mu2': [0.001],
                 %'W1':[1.],'W2':[1.],
                 %'bias0': [1.], 'bias1': [1.], 'bias2':[1.]},
    %"pos_mf_steps": 10,
    %"enable": {'split_norm': False},
    %"iscales": {'W1': 0.003, 'W2': 0.003,                         # split_norm=False
                %'norm_w1':0.1, 'norm_w2':0.1,
                %'mu1':0.1, 'mu2':0.1,
                %'bias1': 0.0, 'bias2': 0.0}, # split_norm=True
    %"clip_min": {},
    %"layer_alpha": [0,0.1,0.1],
    %"global_alpha": 1.0,
    %"l1": {'W1': 0.0, 'W2': 0.0},
    %"batch_size": 50,
    %"my_save_path": 'dbm',
    %"save_at": [1000,5000],
    %"save_every": 10000,
%},

\section{Discussion}

We have introduced a simple regularization scheme that appears to prevent
SML from falling into a poor local minimum of our objective function. The
regularizer encourages units to learn filters which have similar norms, both
within and across adjacent layers.
We have empirically demonstrated the failure of joint training of all
layers of a deep Boltzmann machine which leaves many filters in the lower
layer of a 2-layer DBM with very small norm and, consequently, little
contribution to modeling the data. We have also empirically demonstrated
the success of our regularization scheme in simultaneously training both layers of a
2-layer DBM. While we have not shown it here, our scheme also appears to
work well for DBMs with more than 2 layers. In future work, we would like
to determine how many layers can be learnt simultaneously using similar basic
norm-regularizaiton schemes.

\subsubsection*{Acknowledgments}

The authors would like to acknowledge the support of the following agencies for
research funding and computing support: NSERC, Calcul Qu\'{e}bec and CIFAR. We
would also like to thank the developers of Theano \cite{bergstra+al:2010-scipy}.

\small{
\bibliography{strings,strings-shorter,ml,aigaion-shorter}
\bibliographystyle{natbib}
}

\end{document}



\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{subfig}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
%\setcounter{page}{4321}
\begin{document}

%%%%%%%%% TITLE
\title{Using Descriptive Video Services to Create a Large Data Source for Video Annotation Research}
\author{Atousa Torabi\\
Universit\'{e} de Montr\'{e}al\\
{\tt\small atousa.torabi@umontreal.ca}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Christopher Pal\\
\'Ecole Polytechnique de Mont\'eral\\
%First line of institution2 address\\
{\tt\small christopher.pal@polymtl.ca}
\and
Hugo Larochelle\\
Universit\'{e} de Sherbrooke\\
%First line of institution2 address\\
{\tt\small hugo.larochelle@usherbrooke.ca}
\and
Aaron Courville\\
Universit\'{e} de Montr\'{e}al\\
%First line of institution2 address\\
{\tt\small aaron.courville@umontreal.ca}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
%The creation of large high quality datasets for video annotation research has tremendous potential to enable future research advances.
In this work, we introduce a dataset of video annotated with high quality natural language phrases describing the visual content in a given segment of time. Our dataset is based on the Descriptive Video Service (DVS) that is now encoded on many digital media products such as DVDs. DVS is an audio narration describing the visual elements and actions in a movie for the visually impaired. It is temporally aligned with the movie and mixed with the original movie soundtrack. We describe an automatic DVS segmentation and alignment method for movies, that enables us to scale up the collection of a DVS-derived dataset with minimal human intervention. Using this method, we have collected the largest DVS-derived dataset for video description of which we are aware. Our dataset currently includes over 84.6 hours of paired video/sentences from 92 DVDs and is growing.    
\end{abstract}

\section{Introduction}

% The following is longer and a little more detailed and nuanced (maybe too much so, actually I think this is more intro text...)
There has been tremendous recent interest in the task of image annotation. This increased interest is likely due in part to recent innovations in the use of high capacity neural network based techniques for both images processing and for the generation of natural language phrases. Unlike many previous approaches, such techniques are able to produce both syntactically and semantically rich descriptions. However, such techniques typically require significant quantities of paired data to yield high quality results. Recent success have been fueled by the availability of large labeled datasets such as Flickr30k~\cite{Flickr} and MSCOCO~\cite{coco}. 

Video annotated in a similar way represents an even richer source of data for machine learning and artificial intelligence research. However, for video description, there have been no large datasets available. 
% Note to self (CP) Not sure if we want to cast this as 'just' AI research, I think the end goal of DVS is noble in itself

% Does DVS provide a complete solution?
Descriptive Video Service (DVS) is a US-based provider of descriptive narrations that are included as audio tracks in a variety of movies and TV programs distributed on DVDs and other visual media. Their purpose is to make visual media more accessible for the visually impaired. In different regions of the world this type of annotation is known under other names such as Descriptive Video or Described Video in Canada or Audio Description in the UK. Lists of DVDs with DVS audio tracks are available on many websites such as that of the WGBH media group ~\cite{WGBH}. In Canada, the Canadian Radio-television and Telecommunications Commission (CRTC) maintains a list of television channels with described video ~\cite{CRTC}. While the WGBH television station was an early pioneer in the creation of this technology, there are now a number of providers of similar services. In our work here we outline our solution to the semi-automated construction of a large dataset using DVDs with DVS audio narrations. We also present the Montreal Video Annotation Dataset (M-VAD) created using this technique. 
% Could use another name 
% I am open to any name you think is more appropriate 
% Aaron - I like the name but i added a hyphen as a pronounciation aid.
% Chris - Agreed

% Is DVS a term used to refer to the actual audio track? If we use DVS correctly it is technically a registered trademark, ie a brand name like Keenex
%I have used DVS narrations and DVS soundtrack
% http://en.wikipedia.org/wiki/Descriptive_Video_Service
% http://main.wgbh.org/wgbh/pages/mag/description.html
DVS narrations describes key visual elements of the video such as changes in the scene, people's appearance, gestures, actions, and their interaction with each other and the scene's objects in concise but precise language. DVS soundtracks on DVDs are carefully positioned within movies to fit in natural pauses in the dialogue and are mixed with the original movie soundtrack by professional post-production. Video description is written by professional writers using rich natural sentences and their maximum misalignment with the described scene is limited to 2 seconds. For these reasons the use of DVS or other similar audio narrations is therefore appealing in terms of the quality of the narrations and the relatively high degree of alignment. Recently and in parallel to our work, it was shown that DVS on DVDs are a better source of visual description ~\cite{Rohrbach2015} in that they are more precisely aligned with the visual content of a movie compared to movie scripts. Movie scripts are written in advance before movie production and are sometimes changed during movie production. Moreover, many automatic alignment techniques for scripts and movies are imprecise and require heavy manual cleaning if they are to be used as a source of high quality training and evaluation data. As such, the automated use of scripts to construct large sources of paired video and annotations is problematic.

Despite the advantages offered by DVS, creating a completely automated approach for extracting the relevant narration or annotation from the audio track and refining the alignment of the annotation with the scene still poses some challenges. In this paper we discuss our solution. 

One of the main challenges in automating the construction of a video annotation dataset derived from DVS audio is accurately segmenting the DVS output, which is mixed with the original movie soundtrack. In \cite{Rohrbach2015}, DVS segments are detected by exploiting a similarity comparison between the DVS track and the original movie soundtracks, using Fast Fourier Transform (FFT) techniques. While comparisons using similarity measures of this sort provide a partial solution to the alignment problem, we are interested in automating the annotation and video alignment as much as possible.
%Aaron: In above sentence it isn't obvious how the second part follows from the first. we need to say something about why their solution doesn't lead to an automated solution.
% Chris: Indeed, it is not completely clear to me if their solution is any less automatic than ours. Atousa had mentioned that the thresholds involved with the way she had been trying to do this were very sensitive. That may or may not be the case with the way in which they were doing the isolation.

%Atousa : In my opinion it is the case for majority of DVDs. you can find the DVS part roughly but you can not find it's beginning or end with precision less than second. I have not tried on bluray. They only used bluray, unless the bluray DVS mixing is different. Also in their paper they explicitly say that they choose this threshold for each DVD separately one by one. This is manual. Based on my experience, using their approach has lots of false positive DVS detection.

In this work we describe our methods for DVS narration isolation and video alignment. DVS narrations are typically carefully placed within key locations of a movie and edited by a post-production supervisor for continuity. For example, when a scene changes rapidly, the narrator will speak multiple sentences without pauses. Such content should be kept together when describing that part of movie. If a scene changes slowly, the narrator will instead describe the scene in one sentence, then pause for a moment, and later continue the description. By detecting those short pauses, we are able to align a movie with video descriptions automatically.

We have collected a new DVS-derived dataset from 92 DVDs \footnote{List of DVDs in our collection:}
%
Other researchers may contact us to discuss how they may obtain access to the information necessary to replicate a well defined evaluation that follows the experiments with this dataset presented \cite{Li}. We present some qualitative examples of automatically generated descriptions from our work in \cite{Li} in Section 5 of this paper.

% Chris -> Atousa:
% cite the other (ICML) archive paper here
% Could make this a reference as opposed to a hardcoded section 5

%that we will make available to the research community upon request and proof of purchase of the underlying media. 
%
The DVDs used to construct our dataset can be bought from the Amazon~\cite{amazon} or HMV~\cite{hmv} websites.  
% Aaron: We need to take about this part.
% Chris: Sure, mainly I think we are much more safe to say it this way. We can chat about it but I would recommend caution here.

We believe the M-VAD dataset will be useful for a wide range of different research areas, especially for high capacity deep learning models for "in the wild" video description. 
\footnotetext{21 JUMP STREET, 30 MINUTES OR LESS, 40 YEAR OLD VIRGIN, 500 DAYS OF SUMMER, ABRAHAM LINCOLN VAMPIRE HUNTER, A GOOD DAY TO DIE HARD, A THOUSAND WORDS, BAD TEACHER, BATTLE LOS ANGELES, BIG MOMMAS LIKE FATHER LIKE SON, BLIND DATING, BRUNO, BURLESQUE, CAPTAIN AMERICA, CHARLIE ST CLOUD, CHASING MAVERICKS, CHRONICLE, CINDERELLA MAN, COLOMBIANA, DEAR JOHN, DEATH AT A FUNERAL, DINNER FOR SCHMUCKS, DISTRICT 9, EASY A, FLIGHT, FRIENDS WITH BENEFITS, GET HIM TO THE GREEK, GHOST RIDER SPIRIT OF VENGEANCE, GREEN ZONE, GROWN UPS, HANSEL GRETEL WITCH HUNTERS, HOW DO YOU KNOW, HUGO, IDES OF MARCH, INSIDE MAN, IN TIME, IRON MAN2, ITS COMPLICATED, JACK AND JILL, JULIE AND JULIA, JUST GO WITH IT, KARATE KID, KATY PERRY PART OF ME, KNOCKED UP, LAND OF THE LOST, LARRY CROWNE, LIFE OF PI, LITTLE FOCKERS, MORNING GLORY, MR POPPERS PENGUINS, NANNY MCPHEE RETURNS, NO STRINGS ATTACHED, PARENTAL GUIDANCE, PERCY JACKSON LIGHTENING THIEF, PROMETHEUS, PUBLIC ENEMIES, ROBIN HOOD, RUBY SPARKS, SALT, SANCTUM, SNOW FLOWER, SORCERERS APPRENTICE, SOUL SURFER, SPARKLE 2012, SUPER 8, THE ADVENTURES OF TINTIN, THE ART OF GETTING BY, THE BIG YEAR, THE BOUNTY HUNTER, THE CALL, THE DESCENDANTS, THE GIRL WITH THE DRAGON TATTOO, THE GUILT TRIP, THE ROOMMATE, THE SITTER, THE SOCIAL NETWORK, THE VOW, THE WATCH, THINK LIKE A MAN, THIS MEANS WAR, THOR, TITANIC1, TITANIC2, TOOTH FAIRY, TRUE GRIT, UGLY TRUTH, WE BOUGHT A ZOO, WHATS YOUR NUMBER, XMEN FIRST CLASS, YOUNG ADULT, ZOMBIELAND, and ZOOKEEPER.}
%----------------------------------------------------------------------
\begin{figure*}[t]
\center
\includegraphics[width=17cm]{PI.pdf}            
\caption{DVS dataset collection. From the movie "Life of the Pie". Line 2 and 3: Vocal isolation of movie and DVS soundtrack. Second and third rows shows movie and DVS audio signals after voice isolation. The two circles show the DVS segments on the DVS mono channel track. A pause (flat signal) between two DVS narration parts shows the natural DVS narration segmentation while the narrator stops and then continues describing the movie. We automatically segment DVS audio based on these natural pauses. At first row, you can also see the transcription related to first and second DVS narration parts on top of second and third image shots. }
\label{fig:thirdlayers}
\end{figure*}

\begin{table*}[ht]
\caption{M-VAD dataset overal statistics.}
\label{tab1}
\centering
\begin{tabular}{|l|c|c|c|c|c|c|}
\hline
&Movies& \# Words & \# Paragraph & \# Sentences & Avg. video length & Total video length \\     
\hline
 Un-Filtered     & 92 & 531,778  & 52,683 & 59,415 & 6.3 sec. & 91.0 h.\\
 Filtered  	& 92 & 510,933	& 48,986 & 55,904 & 6.2 sec. & 84.6 h.\\
\hline
\end{tabular}
\end{table*}  

\section{Related Work}
Recently, there have been important advances in deep learning for natural language description generation from images~\cite{Kiros2014,donahue2014long,KarpathyCVPR14,vinyals2014show}. Existing, relatively big open-domain image datasets such as Filckr8k~\cite{flickr8k}, Filckr30k~\cite{Flickr}, MSCOCO~\cite{coco}, SBU~\cite{SBUs} have played an important role in recent breakthroughs in image annotation with natural sentences. In contrast, video annotation with natural language descriptions has not been investigated as extensively and with as much success. Recently, \cite{VenugopalanXDRMS14} proposed to use a 2D-ConvNet and an RNN for video description generation by transferring knowledge from 100,000 images with captions. One of the main issue associated to automatic video annotation is the lack of open-domain, paired video/caption datasets. The majority of available video datasets are toy domains with with small word vocabularies such as TaCos~\cite{rohrbach13iccv}, TaCos Multi-Level~\cite{tacoMulti}, YouCook~\cite{youtube}, and MSVD~\cite{chen11}. Recently, work parallel to ours presented a movie description dataset from DVDs, where descriptions come either from DVS from scripts. In their comparative study, they have shown that DVS is a richer and more accurate source of descriptions, while the movie scripts are less reliable in many cases and are not as well temporally aligned with movies~\cite{Rohrbach2015}. In their work, a semi-automatic FFT-based DVS segmentation method is used, that requires human alignment in post-processing. In their collection of 72 movies, the description of 46 of these movies (i.e. 34.7 hours) came from DVS and the rest came from scripts. To the best of our knowledge, our dataset is the largest DVS-derived movie dataset available. We are currently working on further increasing its size.
%----------------------------------------------------------------------
\begin{figure*}[t]
\center
\subfloat(a){\includegraphics[width=17cm]{verb.png}} 
\subfloat(b){\includegraphics[width=17cm]{noun.png}} 
\caption{(a) 40 most frequent verbs and (b) 40 most frequent nouns.}
\label{fig:verbnounstat}
\end{figure*}

\begin{table*}[ht]
\caption{Comparing video annotation datasets.}
\label{tab2}
\centering
\begin{tabular}{|l|c|c|c|c|c|}
\hline
Dataset & Context & Videos & Description source & \# Clips & \# Sentences  \\       
\hline
TACoS ~\cite{rohrbach13iccv}  & cooking  & 123 & crowd  & 7,206 & 18,227 \\
TACoS multilevel ~\cite{tacoMulti}  & cooking	& 273 & crowd	& 14,105 & 52,593\\
MSVD ~\cite{chen11}  & various & - & crowd	& 1,970 & 70,028\\
movie description ~\cite{Rohrbach2015}  & various & 72 & Script + DVS & 54,076 & 54,076\\
movie description (DVS part) ~\cite{Rohrbach2015} & various & 46 & DVS & 30,680 & 30,680\\ 
M-VAD (ours)  &various & 92 & DVS	& 48,986 & 55,904\\
\hline
\end{tabular}
\end{table*} 

\section{DVS-Derived Dataset Collection}
DVS narrations audio track on DVD is a rich source of data that describes videos in professionally written natural sentences. According to the American Council of the Blind (ACB)~\cite{blind} and the WGBH media access group  ~\cite{WGBH}, the number of DVDs featured with DVS is growing rapidly every year. This makes DVS narrations an appealing source of data for collecting large paired video/sentence datasets. The DVS narrations are mixed with original movie sound track. We've found that efficient methods with minimal human intervention were sufficient to build a large DVS-derived dataset by processing the large amount of DVS audio, aligning it with the movies and transcribing it to text. In the subsequent sections, we describe the steps we followed for our data collection in details.   

%----------------------------------------------------------------------
\subsection{DVS Narrations Segmentation Using Vocal Isolation}
Vocal isolation technique boosts vocals including dialogues and DVS narrations while suppressing background movie sound in stereo signals. This technique is used widely in karaoke machines for stereo signals to remove the vocal track by reversing the phase of one channel to cancel out any signal perceived to come from the center while leaving the signals that are perceived as coming from the left or the right. The main reason that we use vocal isolation for DVS segmentation is based on the fact that DVS narration is mixed within natural pauses in the dialogue, when there is DVS narration there is no dialogue. Therefore, in vocal isolated signals when the narrator speaks, the movie signal is almost a flat line relative to DVS signal allowing us to cleanly separate the narration from other dialogue by comparing two signals. Figure~\ref{fig:thirdlayers} illustrates an example from movie "Life of Pi" where in the original movie soundtrack there are the sounds of ocean waves in the background.

Our approach has three main steps: first we isolate vocal including dialogues and DVS narrations, second we separate the DVS narrations from dialogues, and finally we apply a simple thresholding method to extract DVS segment audio tracks.  

We isolate vocals using Adobe Audition's center channel extractor~\cite{audition} implementation to boost DVS narrations and movie dialogues while suppressing movie background sounds on both DVS and movie audio signals. We align the movie and DVS audio signals by taking a fast Fourier transform (FFT) of two audio signals, computing cross-correlation, a measure of similarity vs offset and select the offset which corresponds to peak cross-correlation. After alignment, we apply Least Mean Square (LMS) noise cancellation and subtract the DVS mono squared signal from the movie mono squared signal in order to suppress dialogue in the DVS signal. For the majority of movies, applying LMS results in cleaned DVS narrations for DVS audio signal. Even in cases where the standard movie audio signal and DVS signal shapes are very different - due to the DVS mixing process - our procedure is sufficient for the automatic segmentation of DVS narration.

Finally, we extract the DVS audio tracks by detecting the beginning and end of DVS narration segments in the DVS audio signal (i.e. where the narrator starts and stops speaking) using a simple thresholding method that we applied to all DVDs without changing the threshold value. In contrast, in recent work~\cite{Rohrbach2015}, DVS audio segementation was done by comparing the similarity of the two signals (the standard movie soundtrack and the soundtrack+DVS) in the Fourier domain against a threshold. We have found that with a similar approach, these types of thresholds usually have to be adjusted for each movie. Moreover we have found that such thresholds depend on the intensity of the background audio signal and the level of the DVS narrator's voice. Segmenting the DVS signal without any background audio suppression requires human cleaning.

%----------------------------------------------------------------------
\subsection{Movie/DVS Alignment And Professional Transcription}
DVS audio narration segments are time-stamped based on our automatic DVS narration segmentation. In order to compensate potential 1-2 seconds misalignment between the DVS narrator speaking and the corresponding scene in the movie, we automatically added two seconds to the end of each video clips, without any human intervention. We believe that in ~\cite{Rohrbach2015}, movie/DVS narrations alignment has been done manually partially because of some imprecise detection of the DVS narrations segments in DVS signal as we explained above. 
% Aaron:Below is unnecessarily critical of other work - are we really arguing that DVD is superior to blu-ray?
% Moreover they only collected blu-ray HD disks with DVS narrations audio tracks, we have found that DVS is more widely available on DVDs. We did not limit ourselves to blu-ray disks and we collected any DVD featured with any format of DVS audio tracks (i.e. DVS soundtracks could be either stereo 2.0 or AC3 5.1 channels).  

In order to obtain high quality text descriptions, the DVS audio segments were transcribed with more than 98 percent transcription accuracy, using professional transcription services~\cite{transInc}. These services use a combination of automatic speech recognition techniques and human transcription to produce a high quality transcription. Our audio narration isolation technique allows us to process the audio into small, well defined time segments and reduce the overall transcription effort and cost.

%----------------------------------------------------------------------
\begin{table*}[ht]
\caption{Corpus POS statistics. }
\label{tab3}
\centering
\begin{tabular}{|l|c|c|c|c|}
\hline
\ Total vocabulary  & \# Nouns & \# Verbs & \# Adjective & \# Adverb\\       
\hline
 17,609 & 9,512	& 2,571 & 3,560 & 857\\
\hline
\end{tabular}
\end{table*}  

\begin{figure*}
\begin{center}
\includegraphics[width=17 cm]{LSTMSamples.png} 
\end{center}
\caption{Four samples of generated sentences by our LSTM model and DVS narrations from the movies: A: Charile st. cloud, B: How do you know, C: The roommate, and D: The big year.}
\label{fig:LSTM}
\end{figure*}

\section{DVS-Derived Dataset Statistics and Comparison With Other Datasets}
Our dataset currently contains video clips from 92 DVDs, covering a variety of movie genre. Since in the beginning and end of the movie, the DVS narrator reads any text shown in the movie (e.g. movie credits), we discard sentences that are time-stamped within three minutes at the beginning and 4 minutes at end of the movie. Table ~\ref{tab1} shows the overall statistics for both our un-filtered and filtered dataset. The filtered data contains 48,986 video clips, with an average length of 6.2 seconds (for a total of 84.6 h). The number of sentences (counted based on the number of periods in the dataset) is 55,904, meaning that some clips are paired with more than one sentence. 

In table~\ref{tab2}, we compare our dataset with existing paired sentence/video datasets. To the best of our knowledge, our dataset is the biggest DVS-derived dataset available. The recent movie description dataset of \cite{Rohrbach2015} is the most similar, however the source for the text descriptions is a combination of movie scripts and DVS. The DVS part consists in only 46 DVDs, while our dataset contains 92 DVDs. Other existing datasets have a limited number of videos and domains, where the source of the text description is based on crowdsourcing. In contrast, our DVS-derived dataset has a variety of videos with professionally written captions.

We tagged all words in the dataset corpus using the Standford Part-Of-Speech (POS) tagger toolbox~\cite{pos}. Table~\ref{tab3} illustrates the vocabulary size, number of nouns, verbs, adjective, and adverbs in the DVS dataset. It is interesting that the number of adjectives is larger than the number of verbs, which shows that the DVS is describing the characteristics of visual elements in the movie in some detail. Also, figure~\ref{fig:verbnounstat} shows the 40 most frequent verbs and nouns in our DVS dataset. It is interesting that among the 10 most frequent verbs, 5 of them are synonyms of "seeing" (i.e. gaze, look, stare, watch, and face). 

%----------------------------------------------------------------------
\section{DVS-Derived Dataset Corpus Preparation}
In our DVS dataset there is over 500 proper names. When training a video description model, we are usually not interested in learning how to generate such names. Moreover, our recent work on training an LSTM-based model on this dataset suggests that removing proper names from the dataset is beneficial. In this dataset We have replaced all people's name with a single token word (e.g.\ "SOMEONE").

We also provide an official training/validation/test split for our dataset, consisting of 38,949, 4888 and 5149 video clips respectively. This split balances DVD genres within each set, which is motivated by the fact that the vocabulary used to describe, say, an action movie could be very different from the vocabulary used in a comedy movie. 

Figure~\ref{fig:LSTM} shows some of the preliminary qualitative results of our recent LSTM video description model~\cite{Li} on this dataset, based on our official dataset split. "LSTM" is the generated sentence and "DVS" is the original DVS sentence. In these examples, even though the samples are not necessarily the same as the DVS reference,  they are still meaningful and closely related to the visual context.

%----------------------------------------------------------------------
\section{Conclusion}
In this work, we have introduced a new, large DVS-derived video dataset. It is publicly available for the research community. Our automatic DVS segmentation and alignment method enabled us to collect this dataset with minimal human intervention. We also provide a balanced split of data for defining an official task.

{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}



%% bare_jrnl.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



\documentclass[journal]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[journal]{../sty/IEEEtran}


\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{url}
\usepackage{multirow}
% \usepackage{subcaption}
\usepackage{mwe}
\usepackage{xcolor}
\usepackage{makecell}

\renewcommand\theadalign{cb}
\renewcommand\theadfont{\bfseries}
\renewcommand\theadgape{\Gape[4pt]}
\renewcommand\cellgape{\Gape[4pt]}

\newcommand{\specialcell}[2][c]{%
  \begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}
  
% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.





% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Learning Normalized Inputs for Iterative Estimation in Medical Image Segmentation}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%

\author{Michal Drozdzal, Gabriel Chartrand, Eugene Vorontsov, Lisa Di Jorio, \\ An Tang, Adriana Romero, Yoshua Bengio, Chris Pal, Samuel Kadoury
\thanks{M. Drozdzal was with \'{E}cole Polytechnique de Montr\'{e}al, Montr\'{e}al, Montreal Institute for Learning Algorithms and Imagia Inc., Montr\'{e}al, e-mail:michal.drozdzal@umontreal.ca}% <-this % stops a space
\thanks{G. Chartrand was with Universit\'{e} de Montr\'{e}al, Montr\'{e}al and Imagia Inc., Montr\'{e}al}% <-this % stops a space
\thanks{L. Di Jorio was with Imagia Inc., Montr\'{e}al}% <-this % stops a space
\thanks{A. Tang was with CHUM Research Center, Montr\'{e}al}% <-this % stops a space
\thanks{E. Vorontsov and C. Pal were with \'{E}cole Polytechnique de Montr\'{e}al, Montr\'{e}al and Montreal Institute for Learning Algorithms}% <-this % stops a space
\thanks{A. Romero and Y. Bengio were with Montreal Institute for Learning Algorithms}% <-this % stops a space
\thanks{S. Kadoury was with \'{E}cole Polytechnique de Montr\'{e}al, Montr\'{e}al and CHUM Research Center, Montr\'{e}al}% <-this % stops a space
}

% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
\markboth{}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.




% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\begin{abstract}
In this paper, we introduce a simple, yet powerful pipeline for medical image segmentation that combines Fully Convolutional Networks (FCNs) with Fully Convolutional Residual Networks (FC-ResNets). We propose and examine a design that takes particular advantage of recent advances in the understanding of both Convolutional Neural Networks as well as ResNets. Our approach focuses upon the importance of a trainable pre-processing when using FC-ResNets and we show that a low-capacity FCN model can serve as a pre-processor to normalize medical input data. In our image segmentation pipeline, we use FCNs to obtain normalized images, which are then iteratively refined by means of a FC-ResNet to generate a segmentation prediction. As in other fully convolutional approaches, our pipeline can be used off-the-shelf on different image modalities. We show that using this pipeline, we exhibit state-of-the-art performance on the challenging Electron Microscopy benchmark, when compared to other 2D methods. We improve segmentation results on CT images of liver lesions, when contrasting with standard FCN methods. Moreover, when applying our 2D pipeline on a challenging 3D MRI prostate segmentation challenge we reach results that are competitive even when compared to 3D methods. The obtained results illustrate the strong potential and versatility of the pipeline by achieving highly accurate results on multi-modality images from different anatomical regions and organs.
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Image Segmentation, Fully Convolutionl Networks, ResNets, Computed Tomography, Electron Microscopy, Magnetic Resonance Imaging.
\end{IEEEkeywords}






% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction}
\label{sec:Intro}
\IEEEPARstart{S}{egmentation} is an active area of research in medical image analysis. With the introduction of Convolutional Neural Networks (CNNs), significant improvements in performance have been achieved in many standard datasets. For example, for the EM ISBI 2012 dataset~\cite{EM_data}, PROMISE12 challenge or MS lesions~\cite{MsLesions}, the top entries are built on CNNs \cite{RonnebergerFB15,0011QCH16,HavaeiDWBCBPJL15,Yu17}.

The common view on CNN models is based on a \textit{representation learning} perspective \cite{Greff16,Goodfellow-et-al-2016-Book}. This view assumes that a CNN is built from layers (convolutional operations and non-linearities) that learn increasing levels of abstraction. In \cite{ZeilerF13}, these different levels of abstraction were visualized, showing that in the first layer the network learns simple edge and blob detectors (e.g. Gabor-like filters), the second layer learns combination of these simple features, while the deeper layers learn to represent more complex object contours, such as faces or flowers. Moreover, in \cite{VeitWB16}, it was shown that removing any single layer of the network after training/finetuning can significantly harm the network's performance, implying that the transformations learnt by a CNN layer are very different from an identity mapping.

Recently, a new class of models called Residual Networks (ResNets) have been introduced \cite{HeZRS15,HeZR016}. ResNets are built from hundreds of residual blocks. Each residual block is composed of two paths: the first one applies a series of nonlinear transformations (typically two or three transformations composed of Batch Normalization \cite{SzegedyLJSRAEVR14}, convolution and a ReLu non-linearity), while the second one is an identity mapping. These two paths are summed up at the end of a residual block. This small architectural modification has three important implications. First, the gradient can flow uninterrupted allowing parameters to be updated even in very deep networks. Second, ResNets are robust to layer removal at training time \cite{HuangSLSW16} and at inference time \cite{VeitWB16} implying that the operations applied by a single layer are only a small modification to identity operation. Third, ResNets are robust to layers permutation \cite{VeitWB16}, suggesting that neighboring layers perform similar operations. These characteristics are not shared with traditional CNNs and researchers have attempted to propose possible explanations on the internal behaviors and mechanisms with ResNet-like models. Recently, two possible explanations of ResNets-like models have emerged. The first, explains the behavior of ResNets in terms of an \textit{embedding of relatively shallow networks} \cite{VeitWB16}. The second, suggests that ResNets perform \textit{iterative estimation}, where the input to the model is iteratively modified by small transformations \cite{Greff16,LiaoP16}.

In recent years, state of the art segmentation methods for medical images have been based on Fully Convolutional Networks (FCNs)~\cite{long_shelhamer_fcn, RonnebergerFB15}. While CNNs typically consist of a contracting path composed of convolutional, pooling and fully connected layers, FCNs add an expanding path composed of transposed convolutions or unpooling layers. The expanding path recovers spatial information by merging features skipped from the various resolution levels on the contracting path. Variants of these skip connections are proposed in the literature. In \cite{long_shelhamer_fcn}, upsampled feature maps are summed with feature maps skipped from the contractive path, while \cite{RonnebergerFB15} concatenate them and add convolutions and non-linearities between each upsampling step. These skip connections have been shown to help recover the full spatial resolution at the network's output, making fully convolutional methods suitable for semantic segmentation. Since traditional FCNs are an extension of CNNs, they can be explained by the representation learning perspective on deep learning.

Although deep learning methods have proved their potential in medical image segmentation, their performance strongly depends on the quality of pre-processing and post-processing steps \cite{HavaeiGLJ16}. Thus, traditional image segmentation pipelines based on FCNs are often complemented by pre-processing and post-processing blocks (see Figure\ref{fig:01}). Pre-processing methods vary among different imaging modalities and can include operations like standardization, histogram equalization, value clipping or range normalization (e.g. dividing by maximum intensity value). The tools of choice for post-processing are either based on Conditional Random Fields \cite{Krahenbuhl11} to account for spatial consistency of the output prediction or on morphological operations to clean the output prediction.

In \cite{DrozdzalVCKP16}, the Fully Convolutional Residual Networks (FC-ResNets) are introduced. FC-ResNets incorporate additional shortcut paths and, thus, increase the number of connections within a segmentation network. These additional shortcut paths have been shown not only to improve the segmentation accuracy but also help the network optimization process, resulting in faster convergence of the training. Since FC-ResNets are an extension of ResNets, their behavior should be interpreted in terms of iterative estimation or embedding of relatively shallow networks. Moreover, not surprisingly, these FC-ResNets are more susceptible to image pre-processing than FCNs, their performance is highly dependent on proper data preparation (e.g. data standardization or range normalization). 

\begin{figure}[t!]
\centering
\subfigure[Traditional pipeline]{\includegraphics[width=0.5\textwidth]{Images/Traditional_pipeline.png}\label{fig:01}}\hfill
\subfigure[Our pipeline]{\includegraphics[width=0.35\textwidth]{Images/Pipeline.png}\label{fig:02}}\hfill
\caption{Traditional pipeline for medical image segmentation (a) and proposed pipeline for medical image segmentation (b).}
\label{fig:pipeline}
\vspace{-.5cm}
\end{figure}

In this paper, we take advantage of recent advances in the understanding of both CNNs as well as ResNets and propose a new medical image segmentation pipeline. We use a FCN to obtain pre-normalized images, which are then iteratively refined by means of a FC-ResNet to generate a segmentation prediction (see Figure \ref{fig:02}). 

Thus, in our pipeline, the FCN can be thought of as a pre-processor that is learnt by means of back-propagation, and FC-ResNet can be thought as a powerful classifier that is an ensemble of exponential number of shallow models. This small modification to current segmentation pipelines allows to remove hand-crafted data pre-processing and to build end-to-end systems trained with back-propagation that achieve surprisingly good performance in segmentation tasks for bio-medical images. Our pipeline reaches state-of-the art for 2D methods on electron microscopy (EM) ISBI benchmark dataset \cite{EM_data} and outperforms both standard FCN \cite{long_shelhamer_fcn,RonnebergerFB15} and FC-ResNet \cite{DrozdzalVCKP16} on in-house CT liver lesion segmentation dataset. Moreover, while applying our 2D pipeline off-the-shelf on a challenging 3D MRI prostate segmentation challenge, we reach results that are competitive even when compared to 3D methods.

Thus, the contributions in this paper can be summarized as follows:
\begin{itemize}
\item We combine Fully Convolutional Residual Networks with Fully Convolutional Networks (Section \ref{sec:Method}).
\item We show that a very deep network without any hand-designed pre- or post-processing achieves state-of-the-art performance on the challenging EM ISBI benchmark \cite{EM_data} (Section \ref{sec:EM_data}).
\item We show that our pipeline outperforms other common FCN-based segmentation pipelines on our in-house CT liver lesion dataset (Section \ref{sec:liver_data}).
\item We show that our 2D pipeline can be applied to untreated MR images reaching competitive results on challenging 3D MRI prostate segmentation task, outperforming many 3D based approaches (Section \ref{sec:prostate_data}).
\item We show that a FCN based pre-processor normalizes the data to the values adequate for FC-ResNet (Section \ref{sec:normalizatoin}).
\end{itemize}

\section{Background}
\label{sec:Background}

Recent advances in medical image segmentation often involve convolutional networks. Most of these state-of-the-art approaches are based on either variants of FCNs (FCN8 or UNet) or CNN architectures. FCN architectures process the input image end-to-end and provide a full resolution segmentation map, whereas CNN variants are applied to input patches and aim to solely classify the central pixel of each patch. In many cases, the application of an FCN/CNN is preceded by a pre-processing step and followed by a post-processing step. The former aims to account for the variability in the input images, whereas the latter helps refine the predictions made by the FCN/CNN.

In the remainder of this section, we review segmentation pipelines in medical imaging based on deep neural networks, for different imaging modalities and organs, with a particular focus on pre-processing and post-processing steps proposed to normalize and regularize data, respectively.

\textbf{Electron Microscopy (EM)}. EM is widely used to study synapses and other sub-cellular structures in the mammalian nervous system. EM data is the basis of two medical imaging segmentation challenges: 2D \cite{EM_data} and 3D \cite{EM3D}. The core of the best performing methods is based on a patch-based CNN \cite{Ciresan}, FCN8 \cite{0011QCH16}, UNet \cite{RonnebergerFB15} and FC-ResNets \cite{DrozdzalVCKP16, Quan2016, Fakhry2016}. The methods account for gray scale value variability by employing data augmentation (e.g. intensity shifts \cite{RonnebergerFB15,Quan2016}) or data pre-processing (e.g standardization \cite{DrozdzalVCKP16} or rescaling \cite{Quan2016}). As for post-processing, a variety of FCN prediction refinement techniques have been proposed. In \cite{Beier2016}, a minimum cost multi-cut approach is introduced, in \cite{Fakhry2016, 0011QCH16}, watershed algorithm is used, while in \cite{Quan2016}, median filtering is employed to improve EM segmentation results.

\begin{figure*}[t!]
\centering
\hspace*{\fill}%
\subfigure[FC-ResNet]{\includegraphics[width=0.4\textwidth]{Images/ResUNet.png}\label{fig:1}}\hfill
\subfigure[Bottleneck block]{\includegraphics[width=0.135\textwidth]{Images/bottle.png}\label{fig:2}}\hfill
\subfigure[Simple block]{\includegraphics[width=0.135\textwidth]{Images/init.png}\label{fig:4}}
\hspace*{\fill}%
%\hfill
\caption{An example of residual network for image segmentation. (a) Residual Network with long skip connections built from bottleneck blocks, (b) bottleneck block and (c) simple block. Blue color indicates the blocks where a downsampling is optionally performed, yellow color depicts the (optional) upsampling blocks, dashed arrow in figures (b) and (c) indicates possible long skip connections. Note that blocks (b) and (c) can have a dropout \cite{Srivastava:2014} layer (depicted with dashed line rectangle). The downsampling operation is performed with strided convolution in the bottleneck block and with maxpooling in the simple block.}
\label{fig:ResUNet}
\end{figure*}


%\textbf{Hematoxylin and Eosin (H\&E) stains}.  H\&E stains are the most widely used histology images in medical diagnosis and have been used in several benchmarks in medical image analysis \cite{Sirinukunwattana16,Camelyon16}. Recently, a number of methods addressing the problem of gland segmentation in colon histology imagery have been presented in the literature. The core of these methods relies on variants of FCN8 \cite{Chen16}, UNet \cite{RonnebergerFB15} or patch-based CNNs \cite{Kainz15,Sirinukunwattana16}. Moreover, most of these methods provide a way to account for stain variability, either as a pre-processing or a data augmentation step. Common pre-processing methods for H\&E stains include histogram matching and color deconvolutions \cite{Sirinukunwattana16,Kainz15}. Data augmentation methods randomly change the stain intensity to help the network learn more robust features \cite{Sirinukunwattana16,RonnebergerFB15}. Finally, to refine segmentation predictions, morphological operations are often applied \cite{Sirinukunwattana16,Chen16} as a post-processing step.


\textbf{Magnetic Resonance Imaging (MRI).} MRI is used for detection, classification of lesions and tumor staging. As in many medical imaging applications, deep learning methods are nowadays playing an important role in the segmentation of lesions and organs from MRI scans, consistently improving state-of-the-art performance. Patch-based CNN architectures have been used to segment brain tumors \cite{KamnitsasLNSKMR16,Pandian116} and MS lesions \cite{BirenbaumG16}. Variants of FCN8 and UNets have also been applied to segment brain tumors \cite{Chang16,Lun16,McKinley16,Casamitjana16,Zhao16}, whereas FC-ResNets have been successfully employed to segment white and gray matter in the brain \cite{Chen16_vox} and prostate \cite{Yu17,MilletariNA16}. To account for 3D information, some of the proposed architectures apply convolutions in a 3D fashion \cite{Chen16_vox,Casamitjana16,Pandian116,KamnitsasLNSKMR16,MilletariNA16} or make use of recurrent neural networks (LSTMs or GRUs) \cite{Andermatt2016,StollengaBLS15}. As in the previous modalities, pre-processing techniques are applied to the input of the network. MRI pre-processing techniques include intensity normalization \cite{BirenbaumG16,Chen16_vox}, rescaling \cite{Chang16}, standardization \cite{KamnitsasLNSKMR16,Pandian116,Casamitjana16,Zhao16,Chen16_vox,Andermatt2016,StollengaBLS15,Yu17}, N4 bias correction \cite{Pandian116,Zhao16,BirenbaumG16,StollengaBLS15} and histogram equalization \cite{Chang16}. Again, post-processing involving morphological operations \cite{Pandian116,Zhao16}, conditional random fiels (CRFs) \cite{KamnitsasLNSKMR16,Zhao16} and interpolation \cite{Andermatt2016} are often used to refine segmentation maps.

\textbf{Computed Tomography (CT)}. Finally, CT scans are widely used by clinicians to detect a large variety of lesions in different organs. Patch-based CNNs have been used in the literature to segment pathological kidneys \cite{Zheng16,Thong16}, liver tumors \cite{VivantiEJLKS15}, pancreas \cite{RothLFSLTS15} and urinary bladder \cite{Cha16}. FCN8 and UNet variants have also been tried on CT scans, e.g. to segment liver tumors \cite{Ben-CohenDKAG16,ChristEETBBRAHD16}. In many cases, CT scans are pre-processed by standardizing \cite{Thong16,Li15,Cha16}, clipping \cite{ChristEETBBRAHD16}, applying Gaussian smoothing \cite{Li15,Cha16} or histogram equalization \cite{ChristEETBBRAHD16} to the input. Gaussian noise injection has also been explored as part of data augmentation to account for noise level variability in the CT scans \cite{ChristEETBBRAHD16}. As in the other modalities, the most frequently used post-processing methods include morphological operations \cite{Thong16,VivantiEJLKS15,RothLFSLTS15}, CRFs \cite{ChristEETBBRAHD16} and level sets \cite{Cha16} to refine the segmentation proposals.

\section{Method}
\label{sec:Method}

In this section we explain our segmentation pipeline. As explained in Section \ref{sec:Intro}, our approach combines a FCN model with a FC-ResNet model (see Figure \ref{fig:02}). The goal of FCN in our pipeline is to pre-process the image to a format that can be iteratively refined by FC-ResNet. In the remainder of this section, we describe the architecture of our FCN-based pre-processor (see Subsection \ref{sec:FCN_pre_processor}), our FC-ResNet (see Subsection \ref{sec:FC-ResNet}) as well as the loss function used to train our pipeline (see Subsection \ref{sec:loss}).

\subsection{Fully Convolutional pre-processor}
\label{sec:FCN_pre_processor}
Our FCN takes as input a raw image of size $N \times N \times 1$ (e.g. CT scan slice or EM image) without applying any pre-processing and outputs a processed $N \times N \times 1$ feature map. The FCN pre-processor architecture is described as a variation of the UNet model from \cite{RonnebergerFB15} (see Table \ref{tab:FCN} for details). The contracting path is built by alternating convolutions and max pooling operations, whereas the expanding path is built by alternating convolutions and repeat operations. The expanding path recovers spatial information, lost in pooling operations, by concatenating the corresponding feature maps from the contracting path. In total, the model has 4 pooling operations and 4 repeat operations. All $3 \times 3$ convolutions are followed by ReLU non-linearity, while no non-linearity follows the $1 \times 1$ and $2 \times 2$ convolutions. In our experiments, we reduce the number of feature maps by a factor of 4 when compared to the original UNet (e. g. our first layer has 16 feature maps instead of 64 as in the original model). This step significantly reduces the memory foot-print of the FCN-based pre-processor. Thus, our UNet-like pre-processor has $1.8$ million trainable parameters (vs 33 million in the original implementation of \cite{RonnebergerFB15}). Note that the UNet could potentially be replaced by other FCN models (e.g. FCN8 \cite{long_shelhamer_fcn}).

\subsection{Iterative Estimation with FC-ResNets}
\label{sec:FC-ResNet}

ResNets \cite{HeZRS15} introduce a residual block that sums the identity mapping of the input to the output of a layer allowing for the reuse of features and permitting the gradient to flow directly to earlier layers. The resulting output $x_{\ell}$ of the $\ell^{th}$ block becomes
\begin{equation}
x_\ell = H_\ell(x_{\ell-1}) + x_{\ell-1}, 
\end{equation}
where $H$ is defined as the repetition (2 or 3 times) of a block composed of Batch Normalization (BN) \cite{IoffeS15} followed by ReLU and a convolution.

The FC-ResNet model extends ResNets to be fully convolutional by adding an expanding (upsampling) path (Figure \ref{fig:1}). Spatial reduction is performed along the contracting path (left) and expansion is performed along the expanding path (right). As in \cite{long_shelhamer_fcn} and \cite{RonnebergerFB15}, spatial information lost along the contracting path is recovered in the expanding path by skipping equal resolution features from the former to the latter. Similarly to the identity connections in ResNets, the skipped features coming from the contracting path are summed with the ones in the expanding path. 

Following the spirit of ResNets, FC-ResNets are composed of two different types of blocks: simple blocks and bottleneck blocks, each composed of at least one batch normalization followed by a non-linearity and one convolution (see Figure \ref{fig:2}-\ref{fig:4}). These blocks can maintain the spatial resolution of their input (marked white in Figure \ref{fig:2}-\ref{fig:4}), perform spatial downsampling (marked blue in Figure \ref{fig:2}-\ref{fig:4}) or spatial upsampling (marked yellow in Figure \ref{fig:2}-\ref{fig:4}). As in ResNets, bottleneck blocks are characterized by their $1 \times 1$ convolutions, which are responsible for reducing and restoring the number of feature maps and thus, aim to mitigate the number of parameters of the model.

The detailed description of FC-ResNet architecture used in our pipeline is shown in Table \ref{tab:FC-ResNet}. The contracting path contains 5 downsampling operations, one $3\times3$ convolution, one simple block and 21 bottleneck blocks. The contracting path is followed by 3 bottleneck blocks, which precede the expanding path. The expanding path contains 5 upsampling operations, 21 bottleneck blocks, one simple block and one last $3\times3$ convolution. This FC-ResNet has a total of $11$ millions trainable parameters.

To sum up, our model is composed of a UNet-like model followed by a FC-ResNet. The UNet-like model is composed of 23 convolutional layers and $1.8$ million trainable parameters. Our FC-ResNet has 140 convolutional layers and $11$ millions of trainable parameters. Thus, our full segmentation pipeline has $12.8$ millions of trainable parameters.

\begin{table}[t!]
\begin{center}
\begin{tabular}{||c |c |c |c |c||} 
\hline
\rotatebox{0}{\thead{Layer \\ Name}} & \rotatebox{0}{\thead{Block \\ Type}} & \rotatebox{0}{\thead{Output \\ Resolution}} & \rotatebox{0}{\thead{Output \\ Width}} & \rotatebox{0}{\thead{Repetition \\ Number}} \\ [0.5ex] 
\hline\hline
Input & - & $512 \times 512$ & 1 & -\\
\hline
Down 1 & conv $3 \times 3$ & $512 \times 512$ & 16 & 2\\
\hline
Pooling 1 & maxpooling & $256 \times 256$ & 16 & 1 \\ 
\hline
Down 2 & conv $3 \times 3$ & $256 \times 256$ & 32 & 2\\
\hline
Pooling 2 & maxpooling & $128 \times 128$ & 32 & 1 \\
\hline
Down 3 & conv $3 \times 3$ & $128 \times 128$ & 64 & 2 \\
\hline
Pooling 3 & maxpooling & $64\times 64$ & 64 & 1 \\
\hline
Down 4 & conv $3 \times 3$ & $64\times 64$ & 128 & 2 \\
\hline
Pooling 4 & maxpooling & $32\times 32$ & 128 & 1 \\
\hline
Across & conv $3 \times 3$ & $32\times 32$ & 256 & 2 \\
\hline
Up 1 & upsampling & $64 \times 64$ & 256 & 1 \\
\hline
Merge 1 & concatenate & $64 \times 64$ & 384 & 1 \\
\hline
Up 2 & conv $2 \times 2$ & $64 \times 64$ & 128 & 1\\
\hline
Up 3 & conv $3 \times 3$ & $64 \times 64$ & 128 & 2 \\
\hline
Up 4 & upsampling & $128 \times 128$ & 128 & 1 \\
\hline
Merge 2 & concatenate & $128 \times 128$ & 192 & 1 \\
\hline
Up 5 & conv $2 \times 2$ & $128 \times 128$ & 64 & 1\\
\hline
Up 6 & conv $3 \times 3$ & $128 \times 128$ & 64 & 2 \\
\hline
Up 7 & upsampling & $256 \times 256$ & 64 & 1 \\
\hline
Merge 3 & concatenate & $256 \times 256$ & 96 & 1 \\
\hline
Up 8 & conv $2 \times 2$ & $256 \times 256$ & 32 & 1\\
\hline
Up 9 & conv $3 \times 3$ & $256 \times 256$ & 32 & 2 \\
\hline
Up 10 & upsampling & $512 \times 512$ & 32 & 1 \\
\hline
Merge 4 & concatenate & $512 \times 512$ & 48 & 1 \\
\hline
Up 11 & conv $2 \times 2$ & $512 \times 512$ & 16 & 1\\
\hline
Up 12 & conv $3 \times 3$ & $512 \times 512$ & 16 & 2 \\
\hline
Output & conv $3 \times 3$ & $512 \times 512$ & 1 & 1\\
\hline
\end{tabular}
\end{center}
\caption{Detailed FCN architecture used as a pre-processor in the experiments. Output resolution indicates the spatial resolution of feature maps for an input of size $512\times512$ and output width represents the feature map dimensionality. Repetition number indicates the number of times the layer is repeated.}
\label{tab:FCN}
\vspace{-.5cm}
\end{table}


\begin{table}[t!]
\begin{center}
\begin{tabular}{||c |c |c |c |c||} 
\hline
\rotatebox{0}{\thead{Layer \\ Name}} & \rotatebox{0}{\thead{Block \\ Type}} & \rotatebox{0}{\thead{Output \\ Resolution}} & \rotatebox{0}{\thead{Output \\ Width}} & \rotatebox{0}{\thead{Repetition \\ Number}} \\ [0.5ex] 
\hline\hline
Down 1 & conv $3 \times 3$ & $512 \times 512$ & 32 & 1\\
\hline
Down 2 & simple block & $256 \times 256$ & 32 & 1 \\ 
\hline
Down 3 & bottleneck & $128 \times 128$ & 128 & 3\\
\hline
Down 4 & bottleneck & $64 \times 64$ & 256 & 8 \\
\hline
Down 5 & bottleneck & $32 \times 32$ & 512 & 10 \\
\hline
Across & bottleneck & $32\times 32$ & 1024 & 3 \\
\hline
Up 1 & bottleneck & $64 \times 64$ & 512 & 10 \\
\hline
Up 2 & bottleneck & $128 \times 128$ & 256 & 8\\
\hline
Up 3 & bottleneck & $256 \times 256$ & 128 & 3 \\
\hline
Up 4 & simple block & $512 \times 512$ & 32 & 1\\
\hline
Up 5 & conv 3x3 & $512 \times 512$ & 32 & 1 \\
\hline
Classifier & conv $1 \times 1$ & $512 \times 512$ & 1 & 1\\ %[1ex]
\hline
\end{tabular}
\end{center}
\caption{Detailed FC-ResNet architecture used in the experiments. Output resolution indicates the spatial resolution of feature maps for an input of size $512\times512$ and output width represents the feature map dimensionality. Repetition number indicates the number of times the block is repeated. For the definition of block types, please refer to Figures \ref{fig:2} and \ref{fig:4}.}
\label{tab:FC-ResNet}
\vspace{-.5cm}
\end{table}

\subsection{Dice Loss}
\label{sec:loss}
We train our model using the Dice loss ($L_{Dice}$) computed per batch:
\begin{equation}
L_{Dice} = -\frac{2 \sum_i o_i y_i}{\sum_i o_i + \sum_i y_i},
\end{equation}
where $o_i \in [0, 1]$ represents the $i^{th}$ output of the last network layer (sigmoid output) and $y_i \in \{0, 1\}$ represents the corresponding ground truth label. Note that the minimum value of  the Dice loss is $-1$.

The reason for using the Dice loss over traditional binary crossentropy is two-fold. First, the Dice coefficient is of the common metrics to assess medical image segmentation accuracy, thus, it is natural to optimize it during training. Second, as pointed out in \cite{MilletariNA16}, Dice loss is well adapted to the problems with high imbalance between foreground and background classes as it does not require any class frequency balancing.

\section{Experiments}
\label{sec:Results}
In this section, we present experimental results of the proposed pipeline for image segmentation. First, we show that our method achieves state-of-the-art results among all published 2D methods on challenging EM benchmark \cite{EM_data}. Second, we compare our pipeline with standard FCNs \cite{long_shelhamer_fcn,RonnebergerFB15} and with FC-ResNets \cite{DrozdzalVCKP16} on a dataset of CT scans of liver lesions, with 135 manually annotated scans. Finally, we show the normalization effect of the FCN-based pre-processing module on both datasets.

In our experiments, we train with early stopping at the highest Dice value on the validation set with patience of $50$ epochs. We implemented our model in the Keras framework \cite{chollet2015keras} using the Theano backend \cite{theano}.


\subsection{Electron Microscopy dataset}
\label{sec:EM_data}

The EM training dataset consists of $30$ images ($512 \times 512$ pixels) assembled from serial section transmission electron microscopy of the Drosophila first instar larva ventral nerve cord. The test set is a separate set of $30$ images, for which labels are not provided. 

The official metrics used in this dataset are: Maximal foreground-restricted Rand score after thinning ($V_{rand}$) and maximal foreground-restricted information theoretic score after thinning ($V_{info}$) with ($V_{rand}$), being used to order the entries in the leader board. For a detailed description of the metrics, please refer to \cite{EM_data}.

During training, we augmented the dataset using random flipping (horizontal and vertical), sheering (with maximal range of $0.41$), rotations (with maximal range of $25$), random cropping ($256\times256$) and spline warping. We used the same spline warping strategy as \cite{RonnebergerFB15}. Thus, our data augmentation is similar to the one published in \cite{DrozdzalVCKP16}. We trained the model with RMSprop \cite{Tieleman2012} with an initial learning rate of $0.001$, a learning rate decay of $0.001$ and a batch size of $8$. We used weight decay of $0.0001$. For each training, the model with the best validation Dice was stored. In total, we trained 10 models and averaged their outputs at the test time. Each time the model was trained, we randomly split $30$ images into $24$ training images and $6$ validation images. 

\begin{figure*}[t!]
\centering
\hspace*{\fill}%
\subfigure[Input image]{\includegraphics[width=0.24\textwidth]{Images/EM_image.png}\label{fig:EM1}}\hfill
\subfigure[FC-ResNet with dropout at test time \cite{DrozdzalVCKP16}]{\includegraphics[width=0.24\textwidth]{Images/EM_fc_resnet.png}\label{fig:EM2}}\hfill
\subfigure[Segmentation result of our pipeline]{\includegraphics[width=0.24\textwidth]{Images/EM_ours.png}\label{fig:EM3}}
%\hfill
\hspace*{\fill}%
\caption{Qualitative segmentation results for EM data. (a) An image from the test set, (b) prediction of FC-ResNet with standarization as a pre-porcessor and dropout at test time and (c) prediction obtained by our pipeline with FCN-based processor followed by FC-ResNet.}
\label{fig:EM}
\end{figure*}

\begin{table*}[t!]
\begin{center}
\begin{tabular}{||c |c |c ||c |c |c |c ||} 
\hline
\thead{Method} & \thead{\boldmath$V_{rand}$} & \thead{\boldmath$V_{info}$}  & \thead{post-processing} & \thead{pre-processing} & \thead{average over} & \thead{parameters [M]}\\ [0.5ex] 
\hline\hline
FusionNet \cite{Quan2016} & 0.978 & \textbf{0.990} & YES & YES & 8 &  31 \\
\hline
CUMedVision \cite{0011QCH16} & 0.977 & 0.989 & YES & NO & 6  &  8 \\
\hline
Unet \cite{RonnebergerFB15} & 0.973 & 0.987 & NO &  YES & 7 & 33  \\ 
\hline
FC-ResNet \cite{DrozdzalVCKP16} & 0.969 & 0.986 & NO & YES & Dropout & 11 \\
\hline
\hline
Ours & \textbf{0.981} & 0.988 & NO & NO & 10 & 13 \\%
\hline
\end{tabular}
\end{center}
\caption{Comparison to published FCN results for EM dataset. $V_{rand}$ is the measure used to rank the submissions. For full ranking of all submitted methods please refer to the challenge website: \protect\url{http://brainiac2.mit.edu/isbi_challenge/leaders-board-new}.}
\label{tab:score_FCN}
\vspace{-.5cm}
\end{table*}

\begin{table}[t!]
\begin{center}
\begin{tabular}{||c |c |c |c ||} 
\hline
\thead{Method} & \thead{\boldmath$V_{rand}$} & \thead{\boldmath$V_{info}$} & \thead{2D/3D}\\ [0.5ex] 
\hline\hline
IAL \cite{Beier2016} & 0.980 & 0.988 & 2D\\
\hline
FusionNet \cite{Quan2016} & 0.978 & \textbf{0.990} & 2D\\
\hline
CUMedVision \cite{0011QCH16} & 0.977 & 0.989 & 2D\\
\hline
Unet \cite{RonnebergerFB15} & 0.973 & 0.987 & 2D\\ 
\hline
IDSIA \cite{Ciresan}  & 0.970 & 0.985 & 2D\\
\hline
motif \cite{Wu15f}  & 0.972 & 0.985 & 2D\\
\hline
SCI \cite{Liu201488}  & 0.971 & 0.982 & 3D\\
\hline
optree-idsia\cite{Uzunba2014}  & 0.970 & 0.985 & 2D \\
\hline
FC-ResNet \cite{DrozdzalVCKP16} & 0.969 & 0.986 & 2D\\
\hline
PyraMiD-LSTM\cite{StollengaBLS15}  & 0.968 & 0.983 & 3D\\
\hline
\hline	
Ours & \textbf{0.981} & 0.988 & 2D\\%
\hline
\end{tabular}
\end{center}
\caption{Comparison to published entries for EM dataset. $V_{rand}$ is the measure used to order the submissions. For full ranking of all submitted methods please refer to the challenge website: \protect\url{http://brainiac2.mit.edu/isbi_challenge/leaders-board-new}.}
\label{tab:score}
\vspace{-.5cm}
\end{table}

The comparison of our method to other FCNs is shown in Table \ref{tab:score_FCN}. Our pipeline outperforms all other fully convolutional approaches when looking at the primary metric ($V_{rand}$ score), improving it by 0.003 over the second best fully convolutional approach (FusionNet \cite{Quan2016}). When comparing our pipeline (FCN pre-processing followed by FC-ResNet) to the pipeline of \cite{DrozdzalVCKP16} (standardization pre-processing followed by FC-ResNet), we observe a significant increase in performance. The same happens when comparing our pipeline to FusionNet \cite{Quan2016} (rescaling pre-processing followed by FC-ResNet). It is worth noting that FusionNet applies intensity shifts with Gaussian noise as data augmentation to account for input variability. All these results suggest that FC-ResNet is better complemented by an FCN pre-processor than by traditional pre-processors such as standardization or rescaling. It is worth noting that our pipeline has only slightly more parameters than \cite{DrozdzalVCKP16} and significantly fewer parameters than \cite{Quan2016}. There are two additional fully convolutional approaches submitted for this dataset: UNet \cite{RonnebergerFB15} and CUMedVision \cite{0011QCH16}, the latter being based on FCN8 \cite{long_shelhamer_fcn}.  The pre-processing used on this dataset varies from range normalization to values from 0 to 1 \cite{Quan2016} to data standardization \cite{DrozdzalVCKP16}. For post-processing, median filter \cite{Quan2016} and watershed algorithm \cite{0011QCH16} are used. All methods use either prediction or model averaging at test time.

Table \ref{tab:score} compares our method to other published entries for EM dataset. As shown in the table, we report the highest $V_{rand}$ score (at the moment of writing the paper), when compared to all published 2D methods. The second best entry is IAL \cite{Beier2016}, which introduces a graph-cut post-processing method to refine FCN predictions. In \cite{Beier2016}, a 2D approach was tested resulting in $0.980$ $V_{rand}$ score and $0.988$ $V_{info}$ score. However, IAL leaderboard entrance incorporates 3D context information and achieves an improvement of $0.003$ in $V_{rand}$ score and $0.001$ in $V_{info}$ score\footnote{Personal communication with the authors.} w.r.t. IAL 2D entry. 

Finally, we show some qualitative analysis of our method in Figure \ref{fig:EM}, where we display a prediction of a single test frame for two different pipelines: FC-ResNet from \cite{DrozdzalVCKP16} in Figure \ref{fig:EM2} and of our pipeline in Figure \ref{fig:EM3}. The white color in the prediction images correspond to the cell class and the black color correspond to the cell membrane class. The different degrees of gray correspond to regions in which the model is (more or less) uncertain about the class assignment. The difference is especially visible when comparing our model to FC-ResNet, we can see that the predictions are sharper and clearer, suggesting that the FCN-pre-processor has properly prepared the image for FC-ResNet.

\subsection{Liver lesion dataset}
\label{sec:liver_data}

Our in-house dataset consists of abdominal contrast-enhanced CT-scans from patients diagnosed with colorectal metastases (CRM). All images are $512 \times 512$ with a pixel size varying from $0.53$ to $1.25$ mm and a slice thickness varying from $0.5$ to $5.01$ mm. The pixel intensities vary between $-3000$ and $1500$. For each volume, CRM were segmented manually using MITK Workbench \cite{MITK} by medical students and reviewed by professional image analysts, resulting in 135 CT scans with manually segmented CRM. In addition to that, manual liver segmentation was provided for 58 of the 135 CT scans. This data was collected with the specific goal of segmenting lesions \emph{only} within the liver.

We split the dataset to have $77$ training images with CRM segmented, $28$ validation images with liver and CRM segmented and 30 test images with liver and CRM segmented. Note that for the training set we do not have liver segmentations available. The liver segmentations of both the validation and the test sets are used to limit the lesion segmentation evaluation only to the liver, treating the rest of the image as a void class.

During the training, we randomly crop a 2D $128 \times 128$ pixel patch containing CRM from a CT scan. We train the model with RMSprop \cite{Tieleman2012} with an initial learning rate of $0.001$ and a learning rate decay of $0.001$. We use weight decay of $0.0001$ in the pre-processor and $0.0005$ in FC-ResNet. 

\begin{table*}[t!]
\begin{center}
\begin{tabular}{||c |c ||c |c |c ||c |c |c ||} 
\hline
\multirow{2}{*}{\thead{Method}} &
\multirow{2}{*}{\thead{parameters [M]}} &
      \multicolumn{3}{c||}{\thead{Validation}} &
      \multicolumn{3}{c||}{\thead{Test}} \\
    & & loss & $Dice_{lesion}$ & $Dice_{liver}$ & loss & $Dice_{lesion}$ & $Dice_{liver}$ \\
\hline\hline
FCN8 \cite{long_shelhamer_fcn} & 128 & -0.419 & 0.589 & 0.994 & -0.437 & 0.535 & 0.989 \\
\hline
Unet \cite{RonnebergerFB15} & 33 & -0.451 & 0.553 & 0.994 & -0.396 & 0.570 & 0.990 \\ 
\hline
FC-ResNet \cite{DrozdzalVCKP16} & 11 & -0.223 & 0.551 & 0.993 & -0.224 & 0.617 & 0.990 \\
\hline
\hline
Ours & 13 & \textbf{-0.795} & \textbf{0.771} & \textbf{0.997} & \textbf{-0.796} & \textbf{0.711} & \textbf{0.993} \\%
\hline
\end{tabular}
\end{center}
\caption{Results on the liver lesion dataset for both validation and test sets.}
\label{tab:score_liver}
\vspace{-.5cm}
\end{table*}

\iffalse
\begin{figure}[t!]
\centering
%\hspace*{\fill}%
\subfigure[FCN8]{\includegraphics[width=0.32\textwidth]{Images/learning_curves_FCN8.png}\label{fig:opt_1}}
\hfill
\subfigure[UNet]{\includegraphics[width=0.32\textwidth]{Images/learning_curves_UNet.png}\label{fig:opt_2}}
\hfill
%\hspace*{\fill}%
\subfigure[FC-ResNet]{\includegraphics[width=0.32\textwidth]{Images/learning_curves_FC-ResNet.png}\label{fig:opt_3}}
\hfill
\subfigure[Ours]{\includegraphics[width=0.32\textwidth]{Images/learning_curves_ours.png}\label{fig:opt_4}}
\hfill
\caption{Learning curves for liver lesion dataset. The subplots represent training and validation losses together with Dice on the validation set for: (a) FCN8 model, (b) UNet model, (c) FC-ResNet and (d) Our model. The Dice plot represents the lesion class.}
\label{fig:Optimization}
\end{figure}
\fi

We follow the same training procedure for all FCN methods: FCN8 \cite{long_shelhamer_fcn}, UNet \cite{RonnebergerFB15} and FC-ResNets \cite{DrozdzalVCKP16}. Results are reported in Table \ref{tab:score_liver}. All models were trained with the Dice loss with batch size of $20$ at training time and $1$ at validation and test time. Our approach outperforms other methods on this challenging dataset, achieving the best validation loss of $-0.795$ and lesion validation Dice of $0.771$. The second best validation loss was obtained by the UNet model ($-0.451$) and the second best lesion validation Dice was obtained by the FCN8 model ($0.589$). Moreover, our model generalizes well on test set reaching a loss of $-0.796$ and a Dice of $0.711$.

% In order to better understand the optimization process behind each model, we plot the learning curves for all tested models in Figure \ref{fig:Optimization}. In these plots, we display training and validation losses together with validation Dice. The training of FCN8 (Figure \ref{fig:opt_1}) and UNet (Figure \ref{fig:opt_2}) converge to similar values, with validation Dice slightly more stable in UNet. FC-ResNet (Figure \ref{fig:opt_3}) converges to validation and training losses that are above the ones presented by FCN8 and UNet, suggesting that FC-ResNets are more dependent on proper data preparation than traditional FCNs. Moreover, the difference between training and validation losses suggests that the model is overfitting. However, combining UNet and FC-ResNets (Figure \ref{fig:opt_4}) leads to the best values, with the convergence process being quite smooth. Moreover, the validation loss closely follows the training loss throughout the epochs, suggesting less susceptibility to overfitting in spite of the slight increase in the the number of parameters w.r.t. FC-ResNet.

Finally, we show some qualitative results for liver lesion segmentation in Figure \ref{fig:liver}, obtained without any user interaction or manual intialization. Figure \ref{fig:liver41} displays sample input CT images to segment, Figure \ref{fig:liver42} shows ground truth annotations and Figures \ref{fig:liver43} to \ref{fig:liver46} present predictions of FCN8, UNet, FC-ResNet and of our approach respectively. Our approach performs better for all types of lesions: small lesions (see third row in Figure \ref{fig:liver}), medium size lesions (see first and second rows in Figure \ref{fig:liver}) and large lesions (see forth row in Figure \ref{fig:liver}). Furthermore, the lesion segmentation is better adjusted to ground truth annotation, has less false positives and does not have unsegmented holes or gaps within the lesions.

\begin{figure*}[t!]
\centering
\subfigure{\includegraphics[width=0.2\textwidth]{Images/Ex_Image_1-3-2.png}\label{fig:liver1}}\hfill
\subfigure{\includegraphics[width=0.2\textwidth]{Images/Ex_Image_1-4.png}\label{fig:liver2}}\hfill
\subfigure{\includegraphics[width=0.132\textwidth]{Images/Ex_Image_1-2.png}\label{fig:liver3}}\hfill
\subfigure{\includegraphics[width=0.132\textwidth]{Images/Ex_Image_1-5.png}\label{fig:liver4}}\hfill
\subfigure{\includegraphics[width=0.132\textwidth]{Images/Ex_Image_1-1.png}\label{fig:liver5}}\hfill
\subfigure{\fcolorbox{white}{red}{\includegraphics[width=0.132\textwidth]{Images/Ex_Image_1-6.png}\label{fig:liver6}}}\hfill
%%%
\subfigure{\includegraphics[width=0.2\textwidth]{Images/Ex_Image_2-3-2.png}\label{fig:liver21}}\hfill
\subfigure{\includegraphics[width=0.2\textwidth]{Images/Ex_Image_2-4.png}\label{fig:liver22}}\hfill
\subfigure{\includegraphics[width=0.132\textwidth]{Images/Ex_Image_2-1.png}\label{fig:liver23}}\hfill
\subfigure{\includegraphics[width=0.132\textwidth]{Images/Ex_Image_2-6.png}\label{fig:liver24}}\hfill
\subfigure{\includegraphics[width=0.132\textwidth]{Images/Ex_Image_2-5.png}\label{fig:liver25}}\hfill
\subfigure{\fcolorbox{white}{red}{\includegraphics[width=0.132\textwidth]{Images/Ex_Image_2-2.png}\label{fig:liver26}}}\hfill
%%%

\subfigure{\includegraphics[width=0.2\textwidth]{Images/Ex_Image_3-3-2.png}\label{fig:liver31}}\hfill
\subfigure{\includegraphics[width=0.2\textwidth]{Images/Ex_Image_3-4.png}\label{fig:liver32}}\hfill
\subfigure{\includegraphics[width=0.132\textwidth]{Images/Ex_Image_3-2.png}\label{fig:liver33}}\hfill
\subfigure{\includegraphics[width=0.132\textwidth]{Images/Ex_Image_3-6.png}\label{fig:liver34}}\hfill
\subfigure{\includegraphics[width=0.132\textwidth]{Images/Ex_Image_3-1.png}\label{fig:liver35}}\hfill
\subfigure{\fcolorbox{white}{red}{\includegraphics[width=0.132\textwidth]{Images/Ex_Image_3-5.png}\label{fig:liver36}}}\hfill

\addtocounter{subfigure}{-18}
\subfigure[Input CT image]{\includegraphics[width=0.2\textwidth]{Images/Ex_Image_4-6-2.png}\label{fig:liver41}}\hfill
\subfigure[Ground Truth]{\includegraphics[width=0.2\textwidth]{Images/Ex_Image_4-3.png}\label{fig:liver42}}\hfill
\subfigure[FCN8]{\includegraphics[width=0.132\textwidth]{Images/Ex_Image_4-2.png}\label{fig:liver43}}\hfill
\subfigure[Unet]{\includegraphics[width=0.132\textwidth]{Images/Ex_Image_4-4.png}\label{fig:liver44}}\hfill
\subfigure[FC-ResNet]{\includegraphics[width=0.132\textwidth]{Images/Ex_Image_4-1.png}\label{fig:liver45}}\hfill
\subfigure[Ours]{\fcolorbox{white}{red}{\includegraphics[width=0.132\textwidth]{Images/Ex_Image_4-5.png}\label{fig:liver46}}}\hfill

\caption{Qualitative results on test set for the liver lesion dataset. Each line displays an example form the test set. From left to right: (a) represents an image, (b) displays the expert annotation of liver (red) and lesion (green), (c) displays a prediction for FCN8 model, (d) displays a prediction for UNet model, (e) displays a prediction for FC-ResNet model and (f) displays a prediction of our method.}
\label{fig:liver}
\vspace{-.5cm}
\end{figure*}

\subsection{Prostate dataset}
\label{sec:prostate_data}
The final experiment tested the segmentation framework on T2-w MR images of the prostate provided by the PROMISE12 challenge\footnote{https://grand-challenge.org/site/promise12/home/}. The training dataset contains 50 T2-w MR images of the prostate together with segmentation masks. The test set consists of 30 MR images for which the ground truth is held out by the organizer for independent evaluation. These MRIs are acquired in different hospitals, using different equipments, different acquisition protocols and include both patients with benign disease (e.g. benign prostatic hyperplasia) as well as with prostate cancer. Thus, the dataset variations include: voxel size, dynamic range, position, field of view and anatomic appearance. Contrary to all previously published methods we did not apply any pre-processing step nor volume resizing at training or testing time.

During training, we augmented the dataset using random sheering (with maximal range of $0.1$), rotations (with maximal range of $10$), random cropping ($256\times256$) and spline warping. We trained the model with RMSprop \cite{Tieleman2012} with an initial learning rate of $0.0004$, a learning rate decay of $0.001$ and a batch size of $24$. We used weight decay of $0.00001$. For each training, the model with the best validation Dice was stored. In total, we trained 10 models and averaged their outputs at the test time. Each time the model was trained, we randomly split $50$ training volumes into $40$ training images and $10$ validation images. Because the method is still based on 2D images, a connected component method was applied on the output to select the largest structure on each volume. 

%\iffalse
\begin{table}[t!]
\begin{center}
\begin{tabular}{||c |c |c |c |c ||} 
\hline
\thead{Method} &
\specialcell{\thead{Score}\\{[-]}} &
\specialcell{\thead{Dice}\\{[\%]}} &
\specialcell{\thead{Avg. Dist.}\\{[mm]}} &
\specialcell{\thead{Vol. Diff.}\\{[\%]}}
\\
\hline
\multicolumn{5}{l}{\thead{2D FCNs}} \\
\hline
\textbf{Ours} & \textbf{83.02} & 87.4 & 2.17 & 12.37 \\%
\hline
SITUS & 79.92 & 84.13 & 2.96 & 23.00\\%
\hline
\multicolumn{5}{l}{\thead{3D FCNs}} \\
\hline
CUMED \cite{Yu17} & 86.65 & 89.43 & 1.95 & 6.95\\%
\hline
CAMP-TUM2 \cite{MilletariNA16} & 82.39 & 86.91 & 2.23 & 14.98 \\%
\hline
SRIBHME & 74.17 & 74.46 & 2.83 & 34.89\\%
\hline
\end{tabular}
\end{center}
\caption{Comparison to the automatic entries based on FCNs for the prostate dataset. For full ranking of all submitted methods please refer to the challenge website: \protect\url{https://grand-challenge.org/site/promise12/results/}.}
\label{tab:score_prostate}
\vspace{-.5cm}
\end{table}
%\fi

Overall, for our method the Dice coefficient is of $87.4$ on the entire gland, with an average boundary distance of $2.17$mm and a volume difference of $12.37\%$. The comparison to other FCNs on the prostate data is shown in Table \ref{tab:score_prostate}. In comparison, we use the score provided by the challenge organizer. As it can be seen, our pipeline outperforms other method based on 2D FCNs and is competitive with methods based on 3D FCNs.

Figure \ref{fig:prostate} shows some qualitative results. The prostate segmentations are well adjusted to ground truth annotations and do not have unsegmented holes or gaps within the prostate. Due to the lack of 3D context information, our method, in some cases, is over-segmenting the base of the apex of the prostate (e.g. see Figure \ref{fig:prostate_3}). These results rank amongst the best automated approaches for prostate segmentation and without tedious and application specific pre-processing steps\footnote{For full ranking of all submitted methods, please refer to the challenge website: \protect\url{https://grand-challenge.org/site/promise12/results/}}.

 

\begin{figure}[t!]
\centering
\subfigure[]{\includegraphics[width=0.16\textwidth]{Images/case_12_slice_11.png}\label{fig:prostate_1}}\hfill
\subfigure[]{\includegraphics[width=0.16\textwidth]{Images/case_16_slice_11.png}\label{fig:prostate_2}}\hfill
\subfigure[]{\includegraphics[width=0.16\textwidth]{Images/case_22_slice_15.png}\label{fig:prostate_3}}\hfill
\caption{Qualitative results of our pipeline on the test set. The yellow line shows ground truth annotations while the red line displays our prediction. For more examples of qualitative results please refer to \protect\url{https://grand-challenge.org/site/promise12/resultpro/?id=UdeM2D&folder=20170201000157_2703_UdeM2D_Result}}
\label{fig:prostate}
\vspace{-.5cm}
\end{figure}

\subsection{Data normalization}
\label{sec:normalizatoin}

In this subsection, we provide a more detailed analysis on the trained models. In particular, we investigate the effect of an FCN-based pre-processor on the data distribution. The distribution of validation set pixel intensities at the input of our pipeline (input to FCN-based pre-processor) and at the input of the FC-ResNet are shown in Figure \ref{fig:distribution}. Together with intensity histograms, we plot the normal distribution fitting the validation data (dashed red line). 

Figure \ref{fig:distribution1} shows the plots for prostate data, where class 0 represent background and class 1 represent prostate. We observe that intensities are shifted from $[0, 2000]$ at the FCN input to $[-5, 10]$ at the FC-ResNet input. For liver lesion dataset, we only plot the distributions for liver (referred to as class 0) and lesion (referred to as class 1), ignoring the distribution of regions outside of the liver. The liver distributions are shown in Figure \ref{fig:distribution2}. We observe similar behavior as for the prostate data: intensities are shifted from $[0, 200]$ at the FCN input to $[-2, 3]$ at the FC-ResNet input.

The qualitative evaluation of the FCN-based pre-processor is displayed in Figures \ref{fig:liver_preprocesor_vis} and \ref{fig:EM_preprocesor_vis} for liver and EM data, respectively. Figure \ref{fig:liver_preprocesor_vis} shows visualizations of how a liver image is transformed from the input of our pre-processor (Figure \ref{fig:liver_preprocesor_vis_1}) to its output (Figure \ref{fig:liver_preprocesor_vis_2}). Analogously, Figures \ref{fig:liver_preprocesor_vis_3} and \ref{fig:liver_preprocesor_vis_4} emphasize how intensities within the liver change, by removing the void pixels of the image. Note that the FCN-based pre-processor does not perform any pre-segmentation of the lesion, i.e. the lesion is not more visible on the pre-processed image (Figure \ref{fig:liver_preprocesor_vis_3} than at the input (Figure \ref{fig:liver_preprocesor_vis_4})). This suggests that the FCN is only normalizing the data to values that are adequate for iterative refinement that happens within the FC-ResNet model. Similar observations can be made for EM data, which is displayed in Figure \ref{fig:EM_preprocesor_vis}. 

\begin{figure*}[t!]
\centering
\subfigure[Prostate dataset]{\includegraphics[width=0.49\textwidth]{Images/MRI_distributions.png}\label{fig:distribution1}}\hfill
\subfigure[Liver lesion dataset]{\includegraphics[width=0.49\textwidth]{Images/distributions.png}\label{fig:distribution2}}\hfill
\caption{Intensity distribution histograms for  (a) Prostate dataset and (b) liver lesion dataset. The plots represent the following information: (left-up subfigure) the 0-class distribution of input pixels, (left-bottom subfigure) the 1-class distribution for input pixels, (right-up subfigure) the 0-class distribution after FCN pre-processing, (right-bottom subfigure) the 1-class distribution after pre-processing. The red dashed-line represents normal distribution fitted to the data.}
\label{fig:distribution}
\vspace{-.5cm}
\end{figure*}

\begin{figure*}[t!]
\centering
\subfigure[Input image]{\includegraphics[width=0.22\textwidth]{Images/image_1743-2.png}\label{fig:liver_preprocesor_vis_1}}\hfill
\subfigure[Pre-procesor output]{\includegraphics[width=0.22\textwidth]{Images/procesed_image_1743.png}\label{fig:liver_preprocesor_vis_2}}\hfill
\subfigure[Image cropped to liver]{\includegraphics[width=0.22\textwidth]{Images/liver_1743.png}\label{fig:liver_preprocesor_vis_3}}\hfill
\subfigure[Pre-procesor output cropped to liver]{\includegraphics[width=0.22\textwidth]{Images/processed_liver_1743.png}\label{fig:liver_preprocesor_vis_4}}\hfill

\caption{Visualization of the output of the pre-processor module for liver lesion dataset.}
\label{fig:liver_preprocesor_vis}
\end{figure*}

\begin{figure}[t!]
\centering
\subfigure[Input image]{\includegraphics[width=0.22\textwidth]{Images/image_3.png}\label{fig:EM_preprocesor_vis_1}}\hfill
\subfigure[Pre-procesor output]{\includegraphics[width=0.225\textwidth]{Images/procesed_image_3.png}\label{fig:EM_preprocesor_vis_2}}\hfill
\caption{Visualization of the output of the pre-processor module for the EM dataset.}
\label{fig:EM_preprocesor_vis}
\vspace{-.5cm}
\end{figure}


\section{Discussion \& Conclusion}
\label{sec:Conclusion}

In this paper, we have introduced a simple, yet powerful segmentation pipeline for medical images that combines fully convolutional networks with fully convolutional ResNets. Our pipeline is built from a low-capacity FCN model followed by a very deep FC-ResNet (more than 100 layers). We have highlighted the importance of pre-processing when using FC-ResNets and shown that a low-capacity FCN model can serve as a pre-processor to normalize raw medical image data. We argued that FC-ResNets are better complemented by the proposed FCN pre-processor than by traditional pre-processors, given the normalization they achieve. Finally, we have shown that using this pipeline we exhibit state-of-the-art performance on the challenging EM benchmark, improve segmentation results on our in-house liver lesion dataset, when compared to standard FCN methods and yield competitive results on challenging 3D MRI prostate segmentation task. These results illustrate the strong potential and versatility of the framework by achieving highly accurate results on multi-modality images from different anatomical regions and organs. 

The fact that FC-ResNets require some kind of data pre-processing is not very surprising, given the model construction. The identity path forwards the input data right to the output, while applying small transformations along the residual path. These small transformations are controlled by batch normalization. This input-forwarding especially affects medical imaging data, where the pixel intensities range is much broader than in standard RGB images. Therefore, adequate pre-processing becomes crucial for FC-ResNets to achieve significant improvement over standard FCN approaches, especially for some imaging modalities with greater variability in acquisition protocols. Standard FCNs do not contain the identity path and, thus, are more robust to the input data distribution. However, the lack of this identity path affects the optimization process and limits the depth of the tested models. 

In light of recent advancements in understanding both CNNs and ResNets, our pipeline can have two possible interpretations. On one hand, the pipeline could be explained as having the FCN model working as a pre-processor followed by the FC-ResNet model performing the task of an ensemble of relatively shallow models, leading to a robust classifier. On the other hand, the interpretation of our pipeline could revolve around the iterative inference point of view of ResNets \cite{Greff16}. In this scenario, the role of the FCN would be to produce an input proposal that would be iteratively refined by the FC-ResNet to generate the proper segmentation map. It is worth noting that, in both interpretations, FC-ResNets should be relatively deep (hundreds of layers) in order to take full advantage of the ensemble of shallow networks or iterative refinement of the initial proposal. FC-ResNets might not have been deep enough in many medical image segmentation pipelines to achieve these effects.

Potential future direction might involve experimentation with different variants of architectures that could serve as a pre-processor. This architecture exploration should not be limited to FCN-like models. From medical image segmentation perspective, the model could potentially benefit by expanding it to 3D FCN.

% use section* for acknowledgment
\section*{Acknowledgment}
\footnotesize
We would like to thank all the developers of Theano and Keras. We gratefully acknowledge NVIDIA for GPU donation. The authors would like to thank Mohammad Havaei and Nicolas Chapados for insightful discussions. The authors would like to thank Drs Simon Turcotte, R\'eal Lapointe, Franck Vandenbroucke-Menu and Ms. Louise Rousseau from the CHUM Colorectal, Hepato-Pancreato-Biliary Cancer Biobank and Database, supported by the Universit\'e de Montr\'eal Roger DesGroseillers Hepato-Pancreato-Biliary Surgical Oncology Research Chair, for enabling the selection of consenting patients with diagnostic imaging available for this study. This work was partially funded by Imagia Inc., MITACS (grant number IT05356) and MEDTEQ. An Tang was supported by a research scholarship from the Fonds de Recherche du Qu\'ebec en Sant\'e and Fondation de l'association des radiologistes du Qu\'ebec (FRQS-ARQ \#26993).


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
% \bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
\bibliographystyle{ieee}
\bibliography{bib2}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
% \begin{thebibliography}{1}

% \bibitem{IEEEhowto:kopka}
% H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
%  0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.

% \end{thebibliography}




% that's all folks
\end{document}





\documentclass{article} % For LaTeX2e
\usepackage{nips10submit_e,times}
%\documentstyle[nips10submit_09,times,art10]{article} % For LaTeX 2.09
\usepackage{natbib}
%\usepackage{bbold}
\usepackage{graphicx,subfigure}
\usepackage{amsmath}
\usepackage{fancyvrb}
\usepackage{amssymb}
\usepackage{wrapfig}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algorithmic}
\title{Revisiting Natural Gradient for Deep Networks}
\author{
Razvan Pascanu and Yoshua Bengio\\
Dept. IRO\\
University of Montreal\\
Montreal, QC}

\nipsfinalcopy % Uncomment for camera-ready version
\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\todo}[1]{{\Large\textcolor{red}{#1}}}


% shortcuts for math
\newcommand{\hess}{\mathbf{H}}
\newcommand{\jacob}{\mathbf{J}}
\newcommand{\params}{\mathbf{\theta}}
\newcommand{\cost}{\mathcal{L}}
\newcommand{\lout}{\mathbf{r}}
\newcommand{\louti}{r}
\newcommand{\outi}{y}
\newcommand{\out}{\mathbf{y}}
\newcommand{\gauss}{\mathbf{G_N}}
\newcommand{\eye}{\mathbf{I}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softmax}{\phi}
\newcommand{\targ}{\mathbf{t}}
\newcommand{\diag}{\text{diag}}
\newcommand{\metric}{\mathbf{G}}
\newcommand{\sample}{\mathbf{z}}

\begin{document}
\maketitle
\begin{abstract}
    The aim of this paper is three-fold. First we show that 
    Hessian-Free~\citep{Martens10} and Krylov Subspace 
    Descent~\citep{VinyalsP12} can be described as implementations of 
    natural gradient descent due to their use of the extended Gauss-Newton 
    approximation of the Hessian. Secondly we re-derive natural gradient 
    from basic principles, contrasting the difference between two 
    versions of the algorithm found in the neural network literature, 
    as well as highlighting a few differences between natural gradient and typical 
    second order methods. 
    Lastly we show empirically that natural gradient can be robust to overfitting
    and particularly it can be robust to the order in which the training data is presented 
    to the model.
\end{abstract}

\vspace*{-2mm}
\section{Introduction}
\vspace*{-2mm}

Several recent papers tried to address the issue of using better optimization 
techniques for machine learning, especially for training deep architectures
or neural networks of various kinds. 
Hessian-Free optimization~\citep{Martens10,SutskeverMH11,chapelle11}, 
Krylov Subspace Descent \citep{VinyalsP12}, natural gradient descent
\citep{Amari97,Park00, LeRoux+al-tonga-2008-small,LeRoux-chapter-2011-small} 
are just a 
few of such recently proposed algorithms. They usually can be split in two 
different categories: those which
make use of second order information and those which use the geometry of the 
underlying parameter manifold (natural gradient). 

One particularly interesting pipeline to scale up such algorithms 
was originally proposed in~\citet{Pearlmutter94}, finetuned in 
\citet{Schraudolph01} and represents the backbone behind both 
Hessian-Free optimization~\citep{Martens10} and Krylov Subspace 
Descent~\citep{VinyalsP12}. The core idea behind it is to make use of 
the forward (renamed to {\it{R-operator}} in \citet{Pearlmutter94})
and backward pass of automatic differentiation to compute efficient
products between Jacobian or Hessian matrices and vectors. These 
products are used within a  truncated-Newton approach
\citep{nocedal99} which considers the exact Hessian and only inverts
it approximately without the need for explicitly storing the matrix in memory, 
as opposed to other approaches which perform a more crude approximation of 
the Hessian (or Fisher) matrix (either diagonal or block-diagonal).

The contributions of this paper to the study of the natural gradient are as follows.
We provide a detailed derivation of the natural gradient, avoiding 
elements of information geometry. 
We distinguish natural gradient descent from TONGA
and provide arguments suggesting that natural gradient may also
benefits from a form of robustness that should yield better generalization.
The arguments for this robustness are different from those invoked for TONGA.
We show experimentally the effects of this robustness when we increase
the accuracy of the metric using extra {\em unlabeled data}.
We also provide evidence that the natural gradient is robust to 
the order of training examples, resulting in lower variance as we 
change the order.
%may be less prone to {\it
%early-overfitting}, a phenomenon first reported in \cite{Erhan-aistats-2010-small}, whereby early
%training examples play a disproportionate role in the end model being learnt.
The final contribution of the paper is to show that Martens'
Hessian-Free approach of~\citet{Martens10}
%~\footnote{We use the 
%wording ``Hessian Free`` to refer to the particular algorithm 
%proposed by \citet{Martens10}, and this should not be confused with 
%the more generic truncated gradient method~\citep{nocedal99}.} 
(and implicitly
Krylov Subspace Descent (KSD) algorithm) can be cast
into the framework of the natural 
gradient, showing how these methods can be seen as doing natural 
gradient rather then second order optimization.
%We contrast the differences natural gradient versus typical second order methods, 
%suggesting that is better to think of natural gradient as a first order method.

\vspace*{-2mm}
\section{Natural Gradient}
\vspace*{-2mm}
\label{sec:natgrad}

Natural gradient can be traced back to Amari's work on information
geometry~\citep{Amari-1985} and its application to various neural
networks~\citep{Amari-1992,Amari97}, though a more in depth introduction
can be found in ~\citet{Amari98,Park00,Arnold11}. The algorithm has also been 
successfully applied  
in the reinforcement learning community~\citep{Kakade01, Peters_N_2008} and 
for stochastic search~\citep{YiWSS09}.
\citet{RouxMB07} introduces 
a different formulation of the algorithm for deep models.
Although similar in name,
the algorithm is motivated differently and is not equivalent 
to Amari's version, as will be shown in section
\ref{sec:nat_vs_nat}.

%The main insight of natural gradient is that the model describes a family 
%of density functions parametrized by $\theta$, each value of $\theta$ 
%corresponding to a particular member of this family. In the neighborhood 
%of $\theta$ the KL-divergence between $p_{\theta}$ and 
%$p_{\theta+\Delta \theta}$ behaves like a distance measure,
%and hence can be used to define a Riemannian manifold that can be 
%traced out by changing $\theta$. 
%\begin{wrapfigure}{r}{.2\textwidth}
%    \centering
%    \includegraphics[width=.19\textwidth]{moving_on_manifold.pdf}
%    \caption{Depiction of the gradient along an embedded manifold 
%    $\mathcal{M}$ in the space defined by $\theta$. Note that the arrow 
%is following the curve of $\mathcal{M}$}
%    \label{fig:moving_on_manifold}
%\end{wrapfigure}
%There exists values of $\theta$ that result in the same density function. 
%For example, in the case of MLPs,  we can have a hidden unit whose weights connecting it 
%to the output layer are 0. Any values of the weights from the input to this ``dead`` hidden unit 
%will result in exactly the same probability density function $p_{\theta}$. 
%This defines an equivalence relation and implicitly a quatient space with respect to it. 
%The quotient space, 
%which is the set of all realizable MLPs,
%can be thought of  a lower dimensional manifold \footnote{We are only attempting 
%to improve the intuitions the reader might have about the functional manifold of all possible 
%neural models. In the strict mathematical sense, if we consider the 
%quotient space with respect to all possible equivalence relations in a deep model,
%the result is not a manifold anymore \citep{Amari00} as it is a union of lower-dimensional 
%manifolds of different dimensions.}
%(think of collapsing all equivalent 
%values of $\theta$ to a single point). 


%Natural gradient aims at moving along the functional manifold, 
%i.e. in the functional space, instead of moving in the embedding space $\mathbb{R}^P$
%(which is what normal gradient descent does). 

Let us consider a 
family of density functions $\mathcal{F}:\mathbb{R}^P \to (\mathbb{B} \to [0,1])$,
where for every $\theta \in \mathbb{R}^P$, $\mathcal{F}(\theta)$ defines a density 
function from $\mathbb{B} \to [0,1]$ 
over the random variable $\sample \in \mathbb{B}$, where 
$\mathbb{B}$ is some suitable numeric set of values, for e.g. $\mathbb{B}=\mathbb{R}^N$.
We also define a loss function that we want
to minimize $\mathcal{L}:\mathbb{R}^P \to \mathbb{R}$. 
Any choice of $\theta \in \mathbb{R}^P$
defines a particular density function $p_{\theta}(\sample) = \mathcal{F}(\theta)$ and by considering all possible 
$\theta$ values, we explore the set $\mathcal{F}$, which is our functional
manifold.
%\footnote{Note that there are values of $\theta$ that result in the same density 
%functions.
%For example, in the case of MLPs,  we can have a hidden unit whose weights connecting it 
%to the output layer are 0. Any values of the weights from the input to this ``dead`` hidden unit 
%will result in exactly the same probability density function $p_{\theta}$. 
%This defines an equivalence relation and implicitly a quotient space with respect to it. 
%The quotient space is a lower dimensional manifold embedded in $\mathbb{R}^P$.
%However there are several 
%such equivalence relations and the quotient space with respect to all of them (i.e. the set of 
%all realizable MLPs)
%is a union of lower-dimensional 
%manifolds and hence not a manifold in the strict mathematical sense \citep{Amari00}. 
%}
Because we can define a similarity measures between nearby density functions,
given by the KL-divergence which in its infinitesimal form behaves like a distance measure,
we are dealing with a Riemannian manifold whose metric is given by the Fisher Information
matrix. 
%Natural gradient attempts to move along the manifold (see figure \ref{fig:moving_on_manifold}).
Natural gradient attempts to move along the manifold by correcting the gradient of 
$\mathcal{L}$ according to the local curvature of the KL-divergence surface \footnote{ 
Throughout this paper
we use the mathematical convention that a partial derivative $\frac{\partial \log p_{\theta}}{\partial 
\theta}$ is a row-vector}:

\begin{equation}
\label{eq:nat_grad}
\nabla_N \mathcal{L}(\theta) = 
\frac{\partial \mathcal{L}(\theta)}{\partial \theta}
\mathbb{E}_{\sample} \left[ 
    \left(
    \frac{\partial \log p_{\theta}(\sample)}{\partial \theta}
    \right)^T 
    \left(
    \frac{\partial \log p_{\theta}(\sample)}{\partial \theta} 
    \right)
\right]^{-1} = 
\frac{\partial \mathcal{L}(\theta)}{\partial \theta} \metric^{-1}
\end{equation}

We can derive this result \emph{without relying on information geometry}. We
consider the natural gradient to be defined as the algorithm which, at each step,
picks a descent direction such that the KL-divergence between $p_{\theta}$ 
and $p_{\theta + \Delta \theta}$ is constant. At each step, we need to 
find $\Delta \theta$ such that: 

\begin{equation}
\label{eq:nat_grad_obj}
\begin{array}{l}
\arg\min_{\Delta \theta} \mathcal{L}(\theta + \Delta \theta) \\
\text{s. t. } KL(p_{\theta}||p_{\theta + \Delta \theta}) = constant
\end{array}
\end{equation}

%This is normal gradient descent, where we constrain the KL divergence between 
%$p_{\theta}(\mathbf{z})$ and $p_{\theta + \Delta \theta}(\mathbf{z})$ to be constant. 
Using this constraint we ensure that we move 
along the functional manifold with constant speed, without being slowed down by its curvature.
This also makes learning robust to re-parametrizations of the model, as the 
functional behaviour of $p$ does not depend on how it is parametrized.
%Note that by moving 
%along the functional manifold, natural gradient is invariant to the 
%parametrization of $p_{\theta}(\mathbf{z})$ as 
%the change is constant regardless of how $p_{\theta}$ is parametrized. 

Assuming $\Delta \theta \to 0$, we can approximate the KL divergence by its Taylor series:

\begin{eqnarray}
    \label{eq:KL_derived} 
    KL(p_{\theta}({\sample}) \parallel p_{\theta+\Delta \theta}({\sample})) & \approx &
    \left(\mathbb{E}_{\sample}\left[\log p_{\theta}\right] - 
    \mathbb{E}_{\sample}\left[\log p_{\theta}\right] \right)-
    \mathbb{E}_{\sample}\left[\frac{\partial \log p_{\theta}}{\partial \theta}\right]\Delta \theta -
    \frac{1}{2}\Delta \theta^T \mathbb{E}_{\sample}\left[\frac{\partial^2 \log p_{\theta}}{\partial \theta^2}\right]
    \Delta \theta \nonumber \\
    \label{eq:expected_hessian}
    & = & \frac{1}{2}\Delta\theta^T
    \mathbb{E}_{\sample}
        \left[  
            -\frac{\partial^{2}\log p_{\theta}(\mathbf{z})}{\partial\theta^2}
        \right]
        \Delta\theta \\
        \label{eq:fim}
     & = & \frac{1}{2}\Delta\theta^T
\mathbb{E}_{\sample} \left[ 
     \left( 
    \frac{\partial \log p_{\theta}(\sample)}{\partial \theta}
    \right)^T 
    \left(
    \frac{\partial \log p_{\theta}(\sample)}{\partial \theta} 
    \right)
\right]\Delta \theta
\end{eqnarray}

The first term cancels out and because 
$\mathbb{E}_{\mathbf{z}}\left[\frac{\partial \log p_{\theta}(\sample)}{\partial \theta}\right] = 0$,
\footnote{Proof:
$\mathbb{E}_{\mathbf{z}}\left[\frac{\partial \log p_{\theta}(\sample)}{\partial \theta}\right] = 
\sum_{\sample} \left(p_{\theta}(\sample) \frac{1}{p_{\theta}(\sample)}
\frac{\partial p_{\theta}(\sample)}{\partial \theta}\right) =
\frac{\partial}{\partial \theta} \left(\sum_{\theta} p_{\theta}(\sample)\right) =
\frac{\partial 1}{\partial \theta} =0$. The proof 
holds for the continuous case as well, replacing sums for integrals.}
%\frac{\partial}{\partial \theta} \left(\sum_\sample p_{\theta}(\sample)\right) = 0$
we are left with only the last term. 
 The Fisher Information Matrix form can be obtain from the 
expected value of the Hessian through algebraic manipulations (see the Appendix). 

We now express equation \eqref{eq:nat_grad_obj} as a Lagrangian, where the KL divergence
is approximated by \eqref{eq:fim} and 
$\mathcal{L}(\theta + \Delta \theta)$ by its first order Taylor series 
$\mathcal{L}(\theta) + \frac{\partial \mathcal{L}(\theta)}{\partial \theta} \Delta \theta$: 

\begin{equation}
\label{eq:lagrange}
\mathcal{L}(\theta) + \frac{\partial \mathcal{L}(\theta)}{\partial \theta}\Delta \theta + 
\frac{1}{2}\lambda \Delta \theta^T \mathbb{E}_{\mathbf{z}}\left[\left(\frac{\partial \log p_{\theta}(\mathbf{z})}{\partial \theta}\right)^T
  \left(\frac{\partial \log p_{\theta}(\mathbf{z})}{\partial \theta}\right) \right] \Delta \theta = 0
\end{equation}

Solving equation \eqref{eq:lagrange} for $\Delta \theta$
gives us the natural gradient formula \eqref{eq:nat_grad}. Note that we get a scalar factor 
of $2\frac{1}{\lambda}$ times the natural gradient. We fold this scalar into the learning rate, 
and hence the learning rate also controls the difference between $p_{\theta}$ and $p_{\theta + \Delta \theta}$ 
that we impose at each step. 
Also the approximations we make are meaningful only around $\theta$. 
\citet{Schaul2012proof} suggests that using a large step size might be harmful for convergence. We 
deal with such issues both by using damping (i.e. setting a trust region around $\theta$) 
and by properly selecting a learning rate. 



\vspace*{-2mm}
\section{Natural Gradient for Neural Networks}
\vspace*{-2mm}
\label{sec:nat_grad}
The natural gradient for neural networks relies on their 
probabilistic interpretation (which induces a similarity measure between different 
parametrization of the model) given in the form of conditional probabilities 
$p_{\theta}(\mathbf{t}|\mathbf{x})$, with $\mathbf{x}$ representing the input and 
$\mathbf{t}$ the target.

We make use of the following notation.
$q(\mathbf{x})$ describes the data generating 
distribution of $\mathbf{x}$ and $q(\mathbf{t}|\mathbf{x})$ 
is the distribution we want to learn. $\out$ is the output of the model, and by an 
abuse of notation, it will refer to either the function mapping inputs to outputs,
or the vector of output activations.
$\lout$ is the output of the model before 
applying the output activation function $\sigma$. 
$\mathbf{t}^{(i)}$ and $\mathbf{x}^{(i)}$ are the $i$-th target and input 
samples of the training set. $\mathbf{J}_{\mathbf{y}}$ stands for the Jacobian matrix 
$\mathbf{J}_y=\left[\begin{array}{ccc}
\frac{\partial y_1}{\partial \theta_1} & .. & \frac{\partial y_1}{\partial \theta_P} \\
.. & .. & .. \\
\frac{\partial y_o}{\partial \theta_1} & .. & \frac{\partial y_0}{\partial \theta_P} \end{array}\right]$. 
Finally, lower indices such as $y_i$ denote the $i$-th element of a vector.

We define the neural network loss as follows:
\begin{equation}
\label{eq:deriv}
\cost(\mathbf{\theta}) = \frac{1}{n} \sum_i^n \left[
\log p_{\theta}(\mathbf{t}^{(i)} |\out(\mathbf{x}^{(i)}))\right] = 
\frac{1}{n} \sum_i^n \left[\log p_{\theta}(\mathbf{t}^{(i)}|\sigma(\lout(\mathbf{x}^{(i)})))\right]
\end{equation}

Because we have a conditional density function 
$p_{\theta}(\mathbf{t}|\mathbf{x})$ the formulation for the natural gradient changes slightly.
%to equation \eqref{eq:cond_nat_grad_obj}. 
Each value of $\mathbf{x}$ now defines a different family of 
density functions $p_{\theta}(\mathbf{t}|\mathbf{x})$, and hence a different manifold. 
In order to measure the functional behaviour of $p_{\theta}(\mathbf{t}|\mathbf{x})$ 
for different values of $\mathbf{x}$, we use the expected value (with respect to 
$\mathbf{x} \sim \tilde{q}(\mathbf{x})$) of the
KL-divergence between $p_{\theta}(\mathbf{t}|\mathbf{x})$ and $p_{\theta+\Delta \theta}(\mathbf{t}|\mathbf{x})$. 

\begin{equation}
\label{eq:cond_nat_grad_obj}
\begin{array}{l}
\ \arg\min_{\Delta \theta} \mathcal{L}(\theta + \Delta \theta) \\
\text{s. t. } \mathbb{E}_{\mathbf{x} \sim \tilde{q}(\mathbf{x})}\left[KL(p_{\theta}(\mathbf{t}|\mathbf{x})||p_{\theta + \Delta \theta}(\mathbf{t}|\mathbf{x}))\right] = constant
\end{array}
\end{equation}

The metric $\metric$
is now an expectation over $\tilde{q}(\mathbf{x})$ of an expectation over $p(\mathbf{t}|\mathbf{x})$.
%(equation \eqref{eq:cond_nat_grad}). 
The former averages over possible 
manifolds generated by different choices of $\mathbf{x}$, 
while the latter comes from the definition of the Fisher Information Matrix.

\begin{equation}
\label{eq:cond_nat_grad}
\nabla_N \mathcal{L}(\theta) = 
\frac{\partial \mathcal{L}(\theta)}{\partial \theta}
\mathbb{E}_{\mathbf{x} \sim \tilde{q}(\mathbf{x})} \left[ 
\mathbb{E}_{\mathbf{t} \sim p(\mathbf{t}|\mathbf{x})} \left[
    \left( 
        \frac{\partial \log p_{\theta}(\mathbf{t}|\mathbf{x})}{\partial \theta}
    \right)^T 
    \left(
        \frac{\partial \log p_{\theta}(\mathbf{t}|\mathbf{x})}{\partial \theta} 
    \right)
    \right]
\right]^{-1} = 
\frac{\partial \mathcal{L}(\theta)}{\partial \theta} \metric^{-1}
\end{equation}

Note that we use the distribution $\tilde{q}$ instead of the empirical $q$. This 
is done in order to emphesis that the theory does not force us to use the empirical 
distribution. However, in practice, we do want $\tilde{q}$ to be as close as 
possible to $q$ such that the curvature of the KL-divergence matches (in some sense)
the curvature of the error surface. 
%Because, during a classical training scenario, 
%we want the curvature of the KL-divergence to match (in some sense) 
%the curvature of the error surface, we do want $\tilde{q}$ to be $q$. 
%This comes from the usual choice of error function $\mathcal{L}$ 
%in equation \eqref{eq:deriv} and family of distributions $p_{\theta}$. 
%Note that there is no constraint on 
%$\tilde{q}$. Ideally we want $\tilde{q}(\mathbf{x})$ to be $q(\mathbf{x})$, 
%the input data distribution,
%case in which we look 
%at the complete functional behaviour of $p(\mathbf{t}|\mathbf{x})$ (since we cover 
%all possible values of $\mathbf{x}$). Alternatively one can set $\tilde{q}$ to be 
%the empirical distribution over a minibatch. In this case we care about the functional 
%behaviour of $p(\mathbf{t}|\mathbf{x})$ over the given minibatch. 
To clarify the effects of $\tilde{q}$ let us consider an example. Assume that $\tilde{q}$ 
is unbalanced with respect to $q$. Namely it contains twice the amount of elements of a class
$\mathbf{A}$ versus the other $\mathbf{B}$. This means that a change in $\theta$ that affects
elements of class $\mathbf{A}$ is seen as having a larger impact on $p$ (in the KL sense) 
than a change that affects the prediction of elements in $\mathbf{B}$. Due to the formulation of natural gradient, we 
will move slower along any direction that affects $\mathbf{A}$ at the expense of $\mathbf{B}$ 
landing with higher probability on solutions $\theta^*$ that favour predicting $\mathbf{A}$. 
In practice we approximate this expectation over $\tilde{q}$ by a sample average over minibatches.

In what follows we consider
typical output activation functions and the metrics $\metric$ they induce. 
In the Appendix we provide a detailed description of how these matrices were obtained starting 
from equation \eqref{eq:cond_nat_grad}. Similar derivations were done in \citet{Park00}, 
which we repeat for convenience. The formulas we get for the linear, 
sigmoid and softmax activation functions are:
%in an attempt to clarify some misconceptions regarding the form natural gradient
%metric which 
%is often assumed to be the uncentered covariance of the gradients. 
%In case of linear activation function we assume that each entry of the vector $t$,
%$t_i$ comes from a Gaussian distribution centered around $\out_i(\mathbf{x})$ 
%with some standard deviation $\beta$. From this it follows that :

\begin{align}
\label{eq:deriv_FIM_linear}
\metric_{linear} & = \beta^2 \mathbb{E}_{\mathbf{x} \sim \tilde{q}} \left[ \frac{\partial \out}{\partial \theta}^T \frac{\partial \out}{\partial \theta} \right] 
  = \beta^2 \mathbb{E}_{\mathbf{x} \sim \tilde{q}} \left[ \jacob_{\out}^T \jacob_{\out} \right] \\
\label{eq:sigmoid_FIM}
\metric_{sigmoid} & = \mathbb{E}_{\mathbf{x} \sim \tilde{q}} \left[
\jacob_{\out}^T \diag(\frac{1}{\out (1-\out)}) \jacob_{\out} \right] \\
\metric_{softmax} &= \mathbf{E}_{\mathbf{x} \sim \tilde{q}} \left[
\sum_i^o \frac{1}{\out_i} 
\left( \frac{\partial \outi_i}{\partial \theta}\right)^T \frac{\partial \outi_i}{\partial \theta}
\right] 
\end{align}
%
%\begin{equation}
%\label{eq:deriv_FIM_linear}
%\begin{array}{lll}
%\metric & = & \beta^2 \mathbb{E}_{\mathbf{x} \sim \tilde{q}} \left[ \frac{\partial \out}{\partial \theta}^T \frac{\partial \out}{\partial \theta} \right] 
%  = \beta^2 \mathbb{E}_{\mathbf{x} \sim \tilde{q}} \left[ \jacob_{\out}^T \jacob_{\out} \right] \\
%\end{array}
%\end{equation}
%In the case of the sigmoid units, i,e, $\out = \text{sigmoid}(\lout)$,  we assume a binomial distribution which gives us:
%
%\begin{equation}
%p(\mathbf{t}|\mathbf{x}) = \prod_i \out_i^{t_i}(1-\out_i)^{1-t_i}
%\end{equation}
%$\log p$ gives us the usual cross-entropy objective used with sigmoid units. We can compute the 
%Fisher information matrix as follows:
%\begin{equation}
%\label{eq:sigmoid_FIM}
%\begin{array}{lll}
%\metric & = & \mathbb{E}_{\mathbf{x} \sim \tilde{q}} \left[
%\jacob_{\out}^T \diag(\frac{1}{\out (1-\out)}) \jacob_{\out} \right]
%\end{array}
%\end{equation}
%
%For the softmax activation function, $\out=\text{softmax}(\lout)$, $p(\mathbf{t}|\mathbf{x})$ takes the form 
%of a multinomial:
%
%\begin{equation}
%p(\mathbf{t}|\mathbf{x}) = \prod_i^o \out_i^{t_i}
%\end{equation}
%
%\begin{equation}
%\metric  =  \mathbf{E}_{\mathbf{x} \sim \tilde{q}} \left[
%\sum_i^o \frac{1}{\out_i} 
%\left( \frac{\partial \outi_i}{\partial \theta}\right)^T \frac{\partial \outi_i}{\partial \theta}
%\right] 
%\end{equation}

To efficiently implement the natural gradient, we use a truncated Newton approach 
following the same pipeline 
as Hessian-Free~\citep{Martens10} (more details are provided in the Appendix). We 
rely on Theano \citep{bergstra+al:2010-scipy-short} for both flexibility 
and in order to use GPUs to speed up.
The advantages of this pipeline are two-fold: (1) it uses the full-rank matrix,
without the need for explicitely storing it in memory and (2)
it {\em does not rely on a smoothness assumption} of the metric.
Unlike other
algorithms such as nonlinear conjugate gradient or BFGS, it does \emph{not}
assume that the curvature changes slowly as we change $\theta$.
This seems to be important for recurrent neural 
networks (as well as probably for deep models) where the curvature can change quickly
\citep{Pascanu13}.

\vspace*{-2mm}
\section{Insights into natural gradient}
\vspace*{-2mm}

Figure \ref{fig:movement} considers a one hidden unit auto-encoder,
where we minimize the error $\left(x - w\cdot \text{sigmoid}(wx + b) + b\right)^2$ and shows the 
path taken by Newton method (blue), natural gradient (gray), Le Roux's version 
of natural gradient (orange)  and gradient descent (purple). 
On the left (larger plot) we show the error surface as a contour plot,
where the x-axis represents $b$ and y-axis is $w$. 
We consider two different 
starting points ([1] and [3]) and draw the first 100 steps taken by each algorithm 
towards  a local minima. The length of every other step is depicted by a different shade of color.
Hyper-parameters,
like learning rate and damping constant, were chosen such to improve convergence speed 
while maintaining stability (i.e. we looked for a smooth path). Values are provided in the Appendix. 
At every point $\theta$ during optimization, natural gradient considers a different KL divergence 
surface, $KL(p_{\theta}||p_{\theta+\Delta \theta})$ parametrized by $\Delta \theta$,
which has a minima at origin. 
On the right we have contour plots of four different KL surfaces. They correspond 
to locations indicated 
by black arrows on the path of natural gradient. The x-axis is $\Delta b$ and y-axis is $\Delta w$
for the KL surfaces subplots. 
 On top of these contour plots we show the direction and length 
of the steps proposed by each of the four considered algorithms.

The point of this plot is to illustrate that each algorithm can take a different path 
in the parameter space towards local minima. In a regime where we have a non-convex problem, 
with limited resources, these path can result in qualitatively different kinds of minima. 
We can \emph{not} draw any general conclusions about what kind of minima each algorithm 
finds based on this toy example, however we make two observations. 
First, as showed on the KL-surface plot for [3] 
the step taken by natural gradient can be smaller than gradient descent (i.e. the KL curvature 
is high) even though the error surface curvature is not high (i.e. Newton's method step is 
larger than gradient descent step). Secondly, the direction chosen by 
natural gradient can be quite different from that of gradient descent (see for example in [3] 
and [4]), which can result in finding a different local minima than gradient descent (for e.g. 
when the model starts at [3]). 

\begin{figure}[ht]
\begin{center}
\centerline{\includegraphics[width=1.\columnwidth]{optimization_path.png}}
\caption{Path taken by four different learning algorithms towards a local minima.
    Newton method (blue), natural gradient (gray), Le Roux's natural gradient (orange) 
and gradient descent (purple). See text for details. }
\label{fig:movement}
\end{center}
\end{figure}


\subsection{A comparison between Amari's and Le Roux's natural gradient}
\vspace*{-2mm}
\label{sec:nat_vs_nat}

In \citet{RouxMB07} a different approach is taken to derive natural gradient. Specifically one 
assumes that the gradients computed over different minibatches are distributed according to 
a Gaussian centered around the true gradient with some covariance matrix $\mathbf{C}$.  By using 
the uncertainty provided by $\mathbf{C}$ we can correct the step that we are taking to
maximize the probability of a downward move in generalization error (expected negative log-likelihood),
resulting in a formula similar to that of 
natural gradient. If $\mathbf{g} = \frac{\partial \mathcal{L}}{\partial \theta}$ is the gradient, 
then \citet{RouxMB07} proposes following the direction $
\tilde{g} = 
\frac{\partial \mathcal{L}(\theta)}{\partial \theta} \mathbf{C}^{-1}$ where $\mathbf{C}$ is:

\begin{equation}
\label{eq:leroux_nat_grad}
\begin{array}{l}
\mathbf{C} =  \frac{1}{n} 
\sum_i \left(
\mathbf{g}
 - \left\langle 
 \mathbf{g} \right\rangle
 \right)^T
 \left(\mathbf{g}
 - \left\langle
 \mathbf{g}
 \right\rangle
\right)
\end{array}
\end{equation}

While the probabilistic derivation requires the use of the centered covariance, 
equation \eqref{eq:leroux_nat_grad}, 
 in \citet{RouxMB07} it is argued that using the uncentered covariance
$\mathbf{U}$ is equivalent up to a constant resulting in a simplified formula 
 which is sometimes confused with the metric 
derived by Amari.

\begin{equation}
\label{eq:uncentered_leroux}
\begin{array}{lll}
\mathbf{U} &=&  \frac{1}{n} 
\sum_i 
\mathbf{g}^T
\mathbf{g}
\approx
\mathbb{E}_{(\mathbf{x}, \mathbf{t}) \sim q} \left[
\left(
\frac{\partial \log p(\mathbf{t}|\mathbf{x})}{\partial \theta}
 \right)^T
\left(\frac{ \partial \log p(\mathbf{t}|\mathbf{x})}{\partial \theta}
\right)\right] \\
\end{array}
\end{equation}

The misunderstanding comes from the fact that the equation has the 
form of an expectation, though the expectation is over the
empirical distribution $q(\mathbf{x}, \mathbf{t})$.
It is therefore not clear if $\mathbf{U}$ tells us how $p_\theta$ would change,
whereas it is clear that $\metric$ does.
The two methods are just different, and one can not straightforwardly borrow 
the interpretation of one for the other. However, we believe that there is
an argument strongly suggesting that the protection against drops in generalization error 
afforded by Le Roux's $\mathbf{U}$ is also a property shared by 
the natural gradient's $\metric$. 



If $KL(p \parallel q)$ is small, than $\mathbf{U}$ can be seen as an 
approximation to $\metric$. Specifically we approximate the second expectation 
from equation \eqref{eq:cond_nat_grad}, i.e. the expectation 
over $\mathbf{t}  \sim p_{\theta}(\mathbf{t}|\mathbf{x})$, 
by a single point, the corresponding $\mathbf{t}^{(i)}$. 
This approximation makes sense when $\mathbf{t}^{(i)}$ is a highly probable 
sample under $p$ which happens when we converge. Note that at convergence,
$\mathbf{U}, \mathbf{G}$ and the Hessian are very similar, hence both versions 
of natural gradient and most second order methods would behave similarly. 

An interesting question is if these different paths taken by each algorithm 
represent 
 qualitatively different kinds of solutions. 
We will address this question indirectly by enumerating what 
implications each choice has.

The first observation has to do with numerical stability. One can express $\metric$ as a sum of 
$n \times o$ outer products (where $n$ is the size of the minibatch over which we estimate the 
matrix and $o$ is the number of output units) while $\mathbf{U}$ is a sum of only $n$ outer products.
Since the number of terms in these sums 
 provides an upper bound on the rank of each matrix, it follows that one 
could expect that $\mathbf{U}$ will be lower rank than $\metric$ for the same size of the minibatch $n$. 
This is also pointed out by \citet{Schraudolph02} to motivate the extended Gauss-Newton matrix
as middle ground between natural gradient and the true Hessian\footnote{Note that
\citet{Schraudolph02}  assumes a form of natural gradient that uses $\mathbf{U}$ as a metric,
similar to the \citet{RouxMB07} proposed, an assumption made in \citet{Martens10} as well.}

A second difference regards plateaus of the error surface. Given the formulation of our 
error function in equation \eqref{eq:deriv} (which sums the log of $p_{\theta}(t|x)$ for 
specific values of $t$ and $x$), flat regions of the objective function are intrinsically 
flat regions of the functional manifold\footnote{One could argue that $p$ might be such that 
    $\cost$ has a low curvature while the curvature of KL is much larger. This would happen 
    for example if $p$ is not very sensitive to $\theta$ for the values $x^{(i)}, t^{(i)}$ 
    provided in the training set, but it is for other pairings of $x$ and $t$. However
    we believe that in such a scenario is more useful to move slowly, as the other parings 
of $x$ and $t$ might be relevant for the generalization error}.
Moving at constant speed in the 
functional space means we should not get stalled near such plateaus.
%$\metric$ can be expressed as the expectation over $\mathbf{x}$ of 
%and expectation over $\mathbf{t}$ of the Hessian of $\log p(t|x)$. 
%As a Hessian it measures how a ch
%equation \eqref{eq:expected_hessian}, 
In \citet{Park00} such plateaus are found near singularities of the 
functional manifold, providing a nice 
framework to study them (as is done for example 
in \citet{MagnusSA_98} where they hypothesize that such singularities behave like repellors for 
the dynamics of natural gradient descent). 
An argument can also be made in favour of $\mathbf{U}$ at plateaus.
If a plateau at $\theta$ 
exists for most possible inputs $\mathbf{x}$, than the covariance matrix will have 
a small norm (because the vectors in each outer product will be small in value). 
The inverse of $\mathbf{U}$ consequentially will be large, meaning that we will take a large 
step, possibly out of the plateau region. This suggest both methods should be able 
to escape from some plateaus, though the reasoning behind the functional manifold approach more
clearly motivates this advantage.

Another observation that is usually made regarding the functional manifold interpretation is that 
it is parametrization-independent. That means that regardless of how we parametrize our 
model we should move at the same speed, property assured by the 
constraint on the KL-divergence between $p_{\theta}$ and $p_{\theta + \Delta \theta}$. 
In \cite{Dickstein12}, following this idea, a link is made 
between natural gradient and whitening in parameter space.  
This property does not transfer directly to the covariance matrix. 

On the other hand Le Roux's method is designed to obtain better generalization 
errors by moving mostly in the directions agreed upon by the gradients on most examples.
We will argue that {\em the functional manifold approach can also provide a similar property}.

One argument relies on large detrimental changes of the expected 
log-likelihood, which is what Le Roux's
natural gradient step protects us from with higher probability.
The metric of Amari's natural gradient measures the expected (over $\mathbf{x}$) 
KL-divergence curvature. We argue that if $\Delta \theta$ 
induces a large change in log-likelihood 
computed over $\mathbf{x} \in \mathbb{D}$, where $\mathbb{D}$ corresponds to 
some minibatch, then it produces a large change in
$p_{\theta}$ (in the KL sense), i.e. it results in a high KL-curvature. 
%This comes from 
%the correspondence between the chosen $p_{\theta}$ and the error 
%function $\cost$. 
Because we move at constant speed on the manifold, 
we slow down in these high KL-curvature regions, and hence we do
not allow large detrimental changes to happen. This intuition becomes even 
more suitable when $\mathbb{D}$ is larger than the training set, 
for example by incorporating unlabeled data, 
and hence providing a more accurate measure of how $p_{\theta}$ changes 
(in the KL sense). This increase in accuracy should allow {\em for better predictions of large changes in 
the generalization error as opposed to only the training error}.

A second argument comes from looking at the Fisher Information matrix which has the
form of an uncentered weighted covariance matrix of gradients,
$\mathbb{E}_{\mathbf{x}} \left[ \sum_\mathbf{t} p_{\theta}(\mathbf{t}|\mathbf{x}) 
\left(\frac{\partial \log p_{\theta}(\mathbf{t}|\mathbf{x})}{\partial \theta} \right)^T
\left(\frac{\partial \log p_{\theta}(\mathbf{t}|\mathbf{x})}{\partial \theta} \right)\right]$.
Note that these are {
\em not the gradients $\frac{\partial \cost}{\partial \theta}$ that we follow 
towards a local minima}.
By using this matrix natural gradient moves in the 
expected direction of low variance for $p_\theta$. As the cost $\cost$ just evaluates 
$p_\theta$ at certain points $\mathbf{t}^{(i)}$ for a given $\mathbf{x}^{(i)}$, we argue that with high probability 
expected directions of low variance for $p_\theta$ 
correspond to directions of low variance for $\cost$. Note that directions of 
high variance for $\cost$ indicates direction in which $p_{\theta}$ changes quickly 
which should be reflected in large changes of the KL. 
Therefore, in the same sense as TONGA, natural gradient avoids directions of high 
variance that can lead to drops in generalization error.

\vspace*{-2mm}
\subsection{Natural gradient descent versus second order methods}
\vspace*{-2mm}
\label{sec:second}

Even though natural gradient is usually assumed to be a second order method it is more useful, and 
arguably more correct to think of it as a first order method. While it makes use of 
curvature, it is the curvature of 
the functional manifold and not that of the error function we are trying to minimize.
The two quantities are different. For example the manifold curvature matrix is positive 
semi-definite by construction while for the Hessian we can have negative curvature. 

To make this distinction clear
we can try to see what information carries the metric that we invert (as it was done in 
\cite{RouxF10} for Newton's and Le Roux's methods). 

%The Hessian answers the question: \emph{If I change my parameters $\theta$ by a bit, how does the 
%gradient change ?} The matrix is used to decide how the gradients change when one moves in the
%\emph{parameter space}.
%
%The centered or uncentered covariance of the gradients answers the question: \emph{If I change my
%input $\mathbf{x}$ by a bit, how does the gradient change?} The matrix provides information of how
%the gradient changes when we move in the \emph{input space}.


The functional manifold metric can be written as either the expectation of the Hessian 
$\frac{\partial^2 p_{\theta}}{\partial \theta^2}$ or the expectation of the Fisher 
Information Matrix 
$\left[ \left(\frac{\partial p_{\theta}}{\partial \theta}\right)^T 
\frac{\partial p_{\theta}}{\partial \theta}\right]$ (see \eqref{eq:expected_hessian} and 
\eqref{eq:fim}). 
The first form tells us the that the matrix measures how a change in $\theta$ affects 
the gradients $\frac{\partial p_{\theta}}{\partial \theta}$ of $p_{\theta}$ (as the 
Hessian would do for the error). The second form tells us how the change in 
the input affects the gradients $\frac{\partial p_{\theta}}{\partial \theta}$, 
as the covariance matrix would do for Le Roux's TONGA. However, while the matrix 
measures both the effects of a change in the input and $\theta$ it does so on the 
functional behaviour of $p_{\theta}$ who acts as a surrogate for the training 
error. As a consequence we need to look for 
%probability 
density functions $p_{\theta}$ 
which are correlated with the training error, as we do in the examples discussed 
here. 
%answers the question \emph{If I change my parameters $\theta$ by a
%bit, how does my probability density function $p$ changes (in the KL sense) ?} The change in the KL
%of $p_{\theta}$ (averaged over $q$) measures how both the change in input and parameters affects
%the $p$. The reason is provided in equations \eqref{eq:expected_hessian} and \eqref{eq:fim}, which
%shows that the Fisher Information Matrix can be written as the expectation of the Hessian matrix
%$\frac{\partial^2 p}{\partial \theta^2}$. From the Hessain form, we can see the metric is 
%containing information of how a change in $\theta$ affects $p$. From the Fisher Information
%matrix form, we can see how a change in the input affects $p$. However the metric is different
%from both the Hessian and the covariance matrix by the fact that it uses the functional behaviour
%of $p$ (i.e. we compute the expectation over the random variable $\sample$ to get how $p$ behaves
%over its entire domain) as a surrogate for the error function.

Lastly, compared to second order methods, 
natural gradient lends itself very well to the online optimization regime.
In principle, in order to apply natural gradient we need an estimate of the gradient, {\em which
can be the stochastic gradient over a single sample} and some reliable measure of how our model,
through $p_{\theta}$,
changes with $\theta$ (in the KL sense), which is given by the metric. 
For e.g. in case of probabilistic models like DBMs, the metric relies only on negative 
samples obtained
from $p_{\theta}$ and does not depend on the empirical distribution $q$ at al \cite{DesjardinsPascanu13},
while for a second order method the Hessian would depend on $q$. 
For conditional distributions (as is the case for neural networks), one good choice 
is to compute the metric on a held
out subset of input samples, offering this way an unbiased estimate of how $p(\mathbf{t}|\mathbf{x})$
changes with $\theta$. This can easily be done in an online regime.
%Theoretically there is no reason to compute the metric over the same data
%we compute the gradients (which is what is typically done for second order methods) and 
%it is completely justified to not do so. 
Given that we do not even need to have targets for the data over which we compute the metric,
as $\metric$ integrates out the random variable $t$, we could even {\em use unlabeled data} to improve
the accuracy as long as it comes from the same distribution $q$, which can not be done for second order methods. 

\vspace*{-2mm}
\section{Natural gradient robustness to overfitting}
\vspace*{-2mm}
\label{sec:exp}

\begin{figure}%{r}{.8\textwidth}
\begin{center}
\centerline{\includegraphics[width=.8\textwidth]{tfd.png}}
\caption{(left) train error (cross entropy over the entire training set) 
    on a log scale and (right) test error 
(percentage of misclassified examples)
as a function of number of updates for the Toronto Faces Dataset. 
`kl, unlabeled`
stands for the functional manifold version of natural gradient, where the metric is computed 
over unlabeled data. for 'KL, different training minibatch' we compute the metric on a different
minibatch from the training set, while 'KL, same minibatch' we compute the metric over the same
minibatch we computed the gradient hence matching the standard use of hessian-free. 
'covariance' stands for tonga that
uses the covariance matrix as a metric, while msgd is minibatch stochastic gradient descent. note
that the x axis was interrupted, in order to improve the visibility of how the natural gradient
methods behave.} 
%\caption{(top) Train error (cross entropy averaged over the entire training set) 
%    on a log scale and (bottom) test error (percentage of misclassified examples)
%    as a function of number of updates for the Toronto Faces Dataset. 
%The legend is similar to figure \ref{fig:mnist_small}.}
%where `KL, unlabeled` implies the metric was computed over the training 
%batch plus unlabeled data, while `KL, same minibatch` means that the same minibatch was used to compute both the gradient and 
%the metric. } 
\label{fig:tfd}
\end{center}
\end{figure}
We explore the robustness hypothesis from section \ref{sec:nat_vs_nat} empirically. 
The results of all experiments carried out are summarized in table 
\ref{table:results} present in the Appendix. Firstly we consider 
the effects of {\em using extra unlabeled data to improve the accuracy of the metric}.
A similar idea was proposed in \citet{YiWSS09}.
The idea is that for $\metric$ to do a good job in this robustness sense, it has to 
accurately predict the change in KL divergence in every direction. 
If $\metric$ is estimated from too little data (e.g., a small
labeled set) and that data happens to be the training set, then it might ``overfit'' 
and underestimate the effect of a change in some directions where the training
data would tend to push us. To protect us against this, what we propose here
is the use a {\em large unlabeled set} to obtain a more generalization-friendly
metric $\metric$.

%In figure \ref{fig:mnist_small} we explore this idea on MNIST, where we limit the training 
%dataset to the first 10000 samples (and use the remaining 40000 as unlabeled data, which is guaranteed 
%to be from the same distribution as $q$). Full experimental details are in the Appendix (though hyper-parameters 
%were validated using a grid-search). 

Figure \ref{fig:tfd} describes the results on the Toronto Face Dataset (TFD),
where using unlabeled data 
results in 83.04\% accuracy vs 81.13\% without. State of the art is 85\%\cite{Rifai2012},
though this result is obtained by a larger model that is pre-trained. Hyper-parameters were  
validated using a grid-search (more details in the Appendix). 

%We train a one hidden layer MLP of 1500 hidden units,
%using both the manifold inspired natural gradient as well as the covariance interpretation (and
%minibatch SGD). We compute the metric over only 2500 examples, reason for which we use damping
%(starting with a value of 2 and using the Levenberg-Marquardt heuristic to change it over time).
%In expectation, the metric we compute using unlabeled data can more precisely predict the change
%in KL divergence of $p$ (as it has 4 times more data available) and hence results in better
%generalization error. The covariance matrix based natural gradient is implemented with exactly the 
%same pipeline (we just replace the metric).

%\begin{figure}[ht]
%\begin{center}
%\centerline{\includegraphics[width=.8\columnwidth]{mnist1000.png}}
%\caption{(top) train error (cross entropy over the entire training set) 
%    on a log scale and (bottom) test error 
%(percentage of misclassified examples)
%as a function of number of updates for the restricted mnist dataset. 
%`kl, unlabeled`
%stands for the functional manifold version of natural gradient, where the metric is computed 
%over unlabeled data. for 'kl, different training minibatch' we compute the metric on a different
%minibatch from the training set, while 'kl, same minibatch' we compute the metric over the same
%minibatch we computed the gradient hence matching the standard use of hessian-free. 
%'covariance' stands for tonga that
%uses the covariance matrix as a metric, while msgd is minibatch stochastic gradient descent. note
%that the x axis was interrupted, in order to improve the visibility of how the natural gradient
%methods behave.} 
%\label{fig:mnist_small}
%\end{center}
%\end{figure}



%We additionally do a similar experiment on the Toronto Face Dataset (TFD), where 
%we have a large amount of unlabeled data of poorer quality than the training set. To ensure that the
%noise in the unlabeled data does not affect the metric, we compute the metric over the training
%batch plus unlabeled samples. We used a three hidden layer model, where the first layer is a
%convolutional layer of 300 filters of size 12x12. The second two layers from a 2 hidden layer
%MLP of 2048 and 1024 hidden units respectively. A similar plot to figure \ref{fig:mnist_small} for
%the TFD dataset is provided in the Appendix, figure \ref{fig:tfd}, where using unlabeled data 
%results in 16.96\% error vs 18.87\% without.

As you can see from the plot, it suggests that using unlabeled data helps to obtain better testing error,
as predicted by our argument in Sec.~\ref{sec:nat_vs_nat}. This comes at a price. Convergence (on the
training error) is slower than when we use the same training batch. 

Additionally we explore the effect of using different batches of training data to compute the metric. 
The full results as well as experimental setup are provided in the Appendix. It shows that, 
as most second order methods, natural gradient has a tendency to overfit
the current minibatch if both the metric and the gradient are computed on it.
%(and the metric is not damped too much). 
%This results in both worst training error and test error, as the training error is
%computed over the entire set.
However, as suggested in \cite{VinyalsP12} using different minibatches
for the metric helps as we tend not to ignore directions relevant for other minibatches. 
%We provide a figure for this case in the Appendix, figure \ref{fig:mnist}. 
%In the second experiment, we train a 3 layer model on MNIST, where the first two are convolutional
%layers both with filters of size 5x5. We used 32 filters on the first layer and 64 on the second.
%The last layer forms an MLP with 750 hidden units. In this experiment we explore how helpful is to
%use different samples to compute the metric instead of the ones over which the gradient was computed,
%as means to avoid overfitting the current minibatch. To this effect we used large minibatches (of
%10000 samples). 

\vspace*{-2mm}
\section{Natural gradient is robust to the order of the training set}
\vspace*{-2mm}

We explore the regularization effects of natural gradient descent by looking at the 
variance of the trained model as a function of training samples that it sees. 
To achieve this we repeat the experiment
%early 
%overfitting effect 
described in \cite{Erhan-aistats-2010-small} which looks at how resampling different 
fraction of the training set affects the variance of the model and focuses specifically 
to the relative higher variance of the early examples.
%according to which stochastic
%gradient descent tends to overfits the first few examples that it sees during training. 
Our intuition is that by forbidding large jumps in the KL divergence of $p_{\theta}$ and following 
the direction of low variance natural
gradient will try to limit the amount of overfitting that occurs {\em at any stage of learning}.

\begin{figure}%{r}{.8\textwidth}
\begin{center}
\centerline{\includegraphics[width=.5\textwidth]{early_overfitting.png}}
\caption{The plot describes how much the model is influenced by different parts of an online
training set, for the two learning strategies compared (minibatch stochastic gradient descent
and natural gradient descent). The x-axis indicates which part (1st 10th, 2nd 10th, etc.) of the 
first half of the data
was randomly resampled, while the y-axis measures the resulting variance of the output due to the
change in training data. 
%The plot on the right measures relative change in variance compared to the variance of the 10th segment.
%Not only does NGD yield less variance (which is an important component in
%generalization), but we see that it is less overly influenced by the early examples (which would
%prevent the model from fully taking advantage of large training sets). 
} 
\label{fig:early_variation}
\end{center}
\end{figure}

We repeat the experiment from \cite{Erhan-aistats-2010-small}, using the NISTP dataset introduced
in \cite{Bengio+al-AI-2011-small} (which is just the NIST dataset plus deformations) and use 32.7M
samples of this data. We divide the first 16.3M data into 10 equal size segments. For each data point in the
figure, we fix 9 of the 10 data segments,  and over 5 different runs we replace the 10th with
5 different random sets of samples. This is repeated for each of the 10 segments to
produce the down curves. By looking at the variance of the model outputs on a held out dataset
(of 100K samples) after the whole 32.7M online training samples, we visualize the influence of
each of the 10 segments on the function learnt (i.e., at the end of online training). The 
curves can be seen in figure \ref{fig:early_variation}.

There are two observation to be made regarding this plot. Firstly, it seems that early examples
have a relative larger effect on the behaviour of the function than latter ones (phenomena sometimes 
called early-overfitting). This happens for both methods, natural gradient and stochastic gradient 
descent.
The second observation regards the overall variance of the learnt model.

Note that the variance at each point on the curve depends on the speed with which we move 
in functional space. For a fixed number of examples one can artificially tweak the curves 
for e.g. by decreasing the learning rate. With a smaller learning rate we move slower,
and since the model, from a functional point of view,  does not change by much,
the variance is lower. In the limit, with a learning rate of 0, the model always stays the same. 
If we increase the 
number of steps we take (i.e. measure the variance after $k$ times more 
samples) the curve recovers some of its shape.This is because we allow the model to move further 
away from the starting point. 

In order to be fair to the two algorithms, we use the validation error as a measure of how 
much we moved in the functional space. This helps us to 
chose hyper-parameters
such that after 32.7M samples both methods achieve the same validation error of 49.8\% (see Appendix for 
hyper-parameters). 

%Ideally, what we are after, is that the model converges towards the same 
%place regardless of how we order the data.
%
%We used the validation error as a measure of how 
%far (in functional space) the model moved from the starting point. In figure \ref{fig:early_variation}
%we chose hyper-parameters such that after 32.7M samples both learning algorithm arrive at the
%validation error of 49.8\%. For comparison we show a curve for MSGD that has roughly the same 
%kind of variance as natural gradient, but highlight that this model \emph{has an error of 58.3\%}. 

The results are consistent
with our hypothesis that natural gradient avoids making large steps in function space during training, 
staying on the path that induces least variance.
Such large steps may be present with SGD, possibly yielding the model 
to overfit (e.g. getting forced into some quadrant of parameter space based only
on a few examples) resulting in different models at the end.
%and unduly large influence of the early examples (at the expense of the later
%examples, i.e., at the expense of the ability to learn from large datasets).
By reducing the variance overall the natural gradient becomes more invariant to the order
in which examples are presented. Note that the relative variance of early examples 
to the last re-sampled fraction is about the same for both natural gradient and stochastic 
gradient descent. However, the amount of variance induced in the learnt model by the 
early examples for natural gradient is on the same magnitude as the variance induce by the 
last fraction of examples for MSGD (i.e. in a global sense natural gradient is less sensitive
the order of samples it sees). 

%The rising variance
%on the right hand side of the figure is normal and only due to the exponentially stronger influence
%of the recently seen examples, with online updates. This is not as troublesome as the higher influence
%of early examples because this late examples effect moves on as new data are added to the training set,
%and can be fixed by using a $1/t$ learning rate schedule at the end.

\vspace*{-2mm}
\section{The relationship between Hessian-Free and natural gradient}
\vspace*{-2mm}
\label{sec:gauss_newton}

Hessian-Free as well as  Krylov Subspace Descent rely on the extended Gauss-Newton approximation 
of the Hessian, $\gauss$
%\footnote{Note that we assume the Jacobian to be of the form  
%$\mathbf{J}_y=\left[\begin{array}{ccc}
%\frac{\partial y_1}{\partial \theta_1} & .. & \frac{\partial y_1}{\partial \theta_P} \\
%.. & .. & .. \\
%\frac{\partial y_o}{\partial \theta_1} & .. & \frac{\partial y_0}{\partial \theta_P} \end{array}\right]$
%}, 
%defined in \eqref{eq:gn_general_formula}, 
instead of the actual Hessian (see  \cite{Schraudolph02}).
\begin{equation}
 \gauss = \frac{1}{n} \sum_i \left[
 \left(\frac{\partial \lout}{\partial \params}\right)^T 
         \frac{\partial^2 \log p(\mathbf{t}^{(i)}|\mathbf{x}^{(i)})}{\partial \mathbf{r}^2} 
        \left(\frac{\partial \lout}{\partial \params} \right)\right] =
        \mathbb{E}_{\mathbf{x} \sim \tilde{q}} \left[
        \jacob_{\lout}^T \left(\mathbb{E}_{\mathbf{t} \sim \tilde{q}(\mathbf{t}|\mathbf{x})} \left[\hess_{\cost \circ \lout}\right]\right) \jacob_{\lout} \right]
\label{eq:gn_general_formula}
\end{equation}
The reason is not computational, as computing 
both can be done equally fast, but rather better behaviour during learning. This is usually 
assumed to be caused by the fact that the Gauss-Newton is positive semi-definite by construction,
so one needs not worry about negative curvature issues.
%\footnote{However one still uses damping 
%with Hessian-Free}. 

In this section we show that in fact the extended Gauss-Newton approximation matches perfectly the 
natural gradient metric, and hence by choosing this specific approximation, one can view both 
algorithms as being implementations of natural gradient rather than typical second order methods. 



The last step of equation \eqref{eq:gn_general_formula} is obtained by using the normal assumption that 
$(\mathbf{x}^{(i)}, \mathbf{t}^{(i)})$ are i.i.d samples. We will consider the three 
activation functions and corresponding errors for which the extended Gauss-Newton is defined 
and show it matches perfectly the natural gradient metric for the same activation.


For the linear output 
units with square errors we can derive the matrix $\hess_{\cost \circ \lout}$ as follows:

\begin{equation}
    \label{eq:deriv_linear_gn}
    \begin{array}{lll}
        \hess_{{\cost \circ \lout}_{ij, i \neq j}} & = & \frac{\partial^2 \sum_k (\lout_k - \targ_k)^2}{\partial \lout_i \partial \lout_j} %\\
                                                    =  \frac{\partial 2(\lout_i - \targ_i)}{\partial \lout_j} %\\
                                                    =  0 \\
                  \hess_{{\cost \circ \lout}_{ii}} & = & \frac{\partial^2 \sum_k (\lout_k - \targ_k)^2}{\partial \lout_i \partial \lout_i} %\\
                                                    =  \frac{\partial 2(\lout_i - \targ_i)}{\partial \lout_i} %\\
                                                    =  2 \\
    \end{array}
\end{equation}

\begin{equation}
\label{eq:gn_linear}
\gauss = \frac{1}{n} \sum_{\mathbf{x}^{(i)}, \mathbf{t}^{(i)}} \jacob_{\lout}^T \hess_{\cost \circ \lout} \jacob_{\lout} 
       = \frac{1}{n} \sum_{\mathbf{x}^{(i)}, \mathbf{t}^{(i)}} \jacob_{\out}^T \hess_{\cost \circ \out} \jacob_{\out}
       = \frac{1}{n} \sum_{\mathbf{x}^{(i)}} \jacob_{\out}^T \left(2 \eye\right) \jacob_{\out}
       = 2 \mathbb{E}_{\mathbf{x}\in q(\mathbf{x})} \left[\jacob_{\out}^T \jacob_{\out}\right]
\end{equation}
%following the steps in equation \eqref{eq:deriv_linear_gn}. 
%Note that we drop the expectation 
%over $q(x)$ to simplify notation, but we will come back to this point in section 
%\ref{sec:learning}. 
The result is summarized in equation 
\ref{eq:gn_linear}, where we make use of the fact that $\lout = \out$. It matches the 
corresponding natural gradient metric, equation \eqref{eq:deriv_FIM_linear} from 
section \ref{sec:nat_grad}, up to a constant.


In the case of sigmoid units with cross-entropy objective ($\sigmoid$ is the sigmoid function),
$\hess_{\cost \circ \lout}$ is 

\begin{equation}
    \label{eq:deriv_sigmoid_gn}
    \begin{array}{lll}
        \hess_{{\cost \circ \lout}_{ij, i \neq j}} 
        & = & \frac{\partial^2 \sum_k \left(-t_k \log(\sigmoid(\louti_k)) - (1-t_k)\log(1 - \sigmoid(\louti_k)) \right)}{\partial \louti_i \partial \louti_j} \\
       % & = & \frac{\partial \left(- t_i \frac{1}{\sigmoid(\louti_i)} \frac{\partial \sigmoid(\louti_i)}{\partial \louti_j} 
    %- (1-t_i) \frac{1}{1 - \sigmoid(\louti_i)} \frac{\partial 1 - \sigmoid(\louti_i)}{\partial \louti_i} \right)}{\partial \louti_j} \\
        & = & \frac{\partial \left(- t_i \frac{1}{\sigmoid(\louti_i)} \sigmoid(\louti_i) (1 - \sigmoid(\louti_i)) 
    + (1-t_i) \frac{1}{1 - \sigmoid(\louti_i)} \sigmoid(\louti_i) (1-\sigmoid(\louti_i))\right)}{\partial \louti_j} %\\
     %   & = & \frac{\partial \left(-t_i + t_i \sigmoid(\louti_i) + \sigmoid(\louti_i)) - t_i \sigmoid(\louti_i)\right)}{\partial \louti_j} \\ 
         =  \frac{\partial \sigmoid(\louti_i) - t_i}{\partial \louti_j} %\\
         =  0 \\
        \hess_{{\cost \circ \lout}_{ii}} 
        %& = & \frac{\partial^2 \sum_k \left(-t_k \log(\sigmoid(\louti_k)) - (1-t_k)\log(1 - \sigmoid(\louti_k)) \right)}{\partial \louti_i \partial \louti_i} \\
        & = & ... %\\
         =  \frac{\partial \sigmoid(\louti_i) - t_i}{\partial \louti_i} %\\
         =  \sigmoid(\louti_i) (1-\sigmoid(\louti_i))
    \end{array}
\end{equation}


If we insert this back into the Gauss-Newton 
approximation of the Hessian and re-write the equation in terms of $\jacob_{\out}$ instead of 
$\jacob_{\lout}$, we get, again, the corresponding natural gradient metric, equation \eqref{eq:sigmoid_FIM}. 

\begin{equation}
    \label{eq:sigmoid_gn}
    \begin{array}{lll}
        \gauss &=& \frac{1}{n} \sum_{\mathbf{x}^{(i)}, \mathbf{t}^{(i)}} \jacob_{\lout}^T \hess_{\cost \circ \lout} \jacob_{\lout} 
             %  &=& \frac{1}{n} \sum_{\mathbf{x}^{(i)}} \jacob_{\lout}^T \diag\left(\out (1 - \out)\right) \jacob_{\lout} \\
               =  \frac{1}{n} \sum_{\mathbf{x}^{(i)}} \jacob_{\lout}^T \diag\left(\out (1 - \out)\right)\diag\left(\frac{1}{\out (1 - \out)}\right) \diag\left(\out (1 - \out)\right) \jacob_{\lout} \\
              % &=&  \frac{1}{n} \sum_{\mathbf{x}^{(i)}} \jacob_{\lout}^T \jacob_{\out \circ \lout}^T \diag\left(\frac{1}{\out (1 - \out)}\right) \jacob_{\out \circ \lout} \jacob_{\lout} \\
               &=&\mathbb{E}_{\mathbf{x} \sim \tilde{q}}\left[ \jacob_{\out}^T \diag\left(\frac{1}{\out (1 - \out)}\right) \jacob_{\out}\right] \\
    \end{array}
\end{equation}

The last matching activation and error function that we consider is the softmax with cross-entropy. 
%The derivation of the Gauss-Newton approximation is given in equation \eqref{eq:deriv_softmax_gn}. 

\begin{equation}
    \label{eq:deriv_softmax_gn}
    \begin{array}{lll}
        \hess_{{\cost \circ \lout}_{ij, i \neq j}} 
        & = & \frac{\partial^2 \sum_k \left(-t_k \log(\softmax(\louti_k))\right)}{\partial \louti_i \partial \louti_j}  
        %& = & \frac{\partial \sum_k - \frac{t_k}{\softmax({\louti_k})} \frac{\partial \softmax\louti_k)}{\partial \louti_i}}{\partial \louti_j} \\
         =  \frac{\partial \sum_k \left(t_k \softmax(\louti_i)\right) - t_i }{\partial \louti_j} 
        %& = & \frac{\partial \softmax(\louti_i) - t_i}{\partial \louti_j} \\
         =  -\softmax(\louti_i) \softmax(\louti_j) \\
        \hess_{{\cost \circ \lout}_{ii}} 
        %& = & \frac{\partial^2 \sum_k \left(-t_k \log(\softmax(\louti_k))\right)}{\partial \louti_i \partial \louti_i}  \\
        & = & ... %\\
         =  \frac{\partial \softmax(\louti_i) - t_i}{\partial \louti_i} %\\
         =  \softmax(\louti_i) - \softmax(\louti_i) \softmax(\louti_i) \\
    \end{array}
\end{equation}


Equation \eqref{eq:deriv_softmax_natgrad} starts from the natural gradient metric and singles out a matrix $\mathbf{M}$ in the formula such that the metric 
can be re-written as the product $\jacob_{\lout}^T \mathbf{M} \jacob_{\lout}$ (similar to the formula for the Gauss-Newton approximation). In 
\eqref{eq:deriv_M} we show that indeed $\mathbf{M}$ equals $\hess_{\cost \circ \lout}$  and hence the natural gradient metric is the same as the 
extended Gauss-Newton matrix for this case as well. Note that $\delta$ is the Kronecker delta, where $\delta_{ij, i \neq j} = 0$ and $\delta_{ii} = 1$.

\begin{equation}
    \label{eq:deriv_softmax_natgrad}
    \begin{array}{lll}
    \metric &=& \mathbb{E}_{\mathbf{x} \sim \tilde{q}}\left[ \sum_{k=1}^o \frac{1}{y_k} \left(\frac{\partial y_k}{\partial \theta}\right)^T\frac{\partial y_k}{\partial \theta}\right] 
     = \mathbb{E}_{\mathbf{x} \sim \tilde{q}}\left[ \jacob_{\lout}^T \left( \sum_{k=1}^o \frac{1}{y_k} \left( \frac{\partial y_k}{\partial \lout}\right)^T \left(\frac{\partial y_k}{\partial \lout}\right) \right) 
    \jacob_{\lout} \right] \\
    %\metric &=& \mathbb{E}_{\mathbf{x} \sim \tilde{q}}\left[ \jacob_{\lout}^T \mathbf{M} \jacob_{\lout}^T \right] \\
     &=& \frac{1}{N} \sum_{\mathbf{x}^{(i)}} \left( \jacob_{\lout}^T \mathbf{M} \jacob_{\lout} \right)
    \end{array}
\end{equation}

\begin{equation}
    \label{eq:deriv_M}
    \begin{array}{lll}
   \mathbf{M}_{ij, i \neq j} & = & \sum_{k=1}^o \frac{1}{y_k}\frac{\partial y_k}{\partial \louti_i} \frac{\partial y_k}{\partial \louti_j} 
                            % & = & \sum_{k=1}^o \frac{1}{y_k}y_k(\delta_{ki} - y_i)y_k(\delta_{kj} - y_j) \\
                              =  \sum_{k=1}^o (\delta_{ki} - y_i)y_k(\delta_{kj} - y_j) 
                             %& = & y_iy_j\left(\sum_{k=1}^o y_k\right) - y_i y_j - y_i y_j \\
                              =  y_iy_j - y_iy_j - y_i y_j 
                             %& = & -y_i y_j \\
                              =  - \softmax(\louti_i) \softmax(\louti_j) \\
  \mathbf{M}_{ii} & = & \sum_{k=1}^o \frac{1}{y_k}\frac{\partial y_k}{\partial y_i} \frac{\partial y_k}{\partial \louti_j} 
                  %& = & \sum_{k=1}^o (\delta_{ki} - y_i)y_k(\delta_{ki} - y_i) \\
                   =  y_i^2 \left(\sum_{k=1}^o y_k \right) + y_i -2y_i^2  
                  %& = & y_i - y_i^2 \\
                   =  \softmax(\louti_i) - \softmax(\louti_i) \softmax(\louti_i) \\
    \end{array}
\end{equation} 


\vspace*{-2mm}
\section{Conclusion}
\vspace*{-2mm}
\label{sec:conclusions}

In this paper we re-derive natural gradient, by imposing that at each step we follow the 
direction that minimizes the error function while resulting in a constant change in the 
KL-divergence of the probability density function that represents the model. 
This approach minimizes the amount of differential geometry needed, making the algorithm
more accessible. 

We show that natural gradient, as proposed by Amari, is {\em not the same} as the algorithm proposed 
by Le Roux et al, even though it has the same name. We highlight a few differences of each 
algorithm and hypothesis that Amari's natural gradient should exhibit the same robustness 
against overfitting that Le Roux's algorithm has, but for different reasons. 

We explore empirically this robustness hypothesis, by proving better test errors when 
{\em unlabeled data} is used to improve the accuracy of the metric.
We also show that natural gradient  may reduce the worrisome
early specialization effect previously observed with online stochastic gradient descent applied
to deep neural nets, and reducing the variance of the resulting learnt function (with respect to
the sampled training data).

By computing the specific metrics needed for standard output activation functions we showed that
the extended Gauss-Newton approximation of the Hessian coincides with the natural gradient metric
(provided that the metric is estimated over the same batch of data as the gradient). Given this
identity one can re-interpret the recently proposed Hessian-Free and 
Krylov Subspace Descent as natural gradient.

Finally we point out a few differences between typical second order methods and natural gradient.
The latter seems more suitable for online or probabilistic models, and relies on a surrogate 
probability density function $p_{\theta}$ in place of the error function 
in case of deterministic models. 


\vspace*{-1mm}
\subsection*{Acknowledgements}
\vspace*{-1mm}

We would like to thank Guillaume Desjardens, Aaron Courville, Li Yao, David Warde-Farley and
Ian Goodfellow for the interesting discussion on the topic, or for any help provided during the
development of this work. Reviewers at ICLR were particularly helpful, and we want to thank them, 
especially one of the reviewers that suggested several links with work from the reinforcement learning community. 
Also special thanks goes to the Theano development team as well (particularly
to Frederic Bastien, Pascal Lamblin and James Bergstra) for their help. 

We acknowledge NSERC, FQRNT, CIFAR, RQCHP and Compute Canada for the resources they provided.


{\small
\bibliography{strings,strings-short,strings-shorter,aigaion-shorter,ml,myrefs}
%\bibliographystyle{plain}
\bibliographystyle{natbib}
}
%\newpage
\section*{Appendix}
\subsection{Expected Hessian to Fisher Information Matrix}

 The Fisher Information Matrix form can be obtain from the 
expected value of the Hessian :  
\begin{eqnarray}
    \label{eq:relation1}
\mathbb{E}_{\mathbf{z}}\left[-\frac{\partial^2 \log p_{\theta}}{\partial \theta}\right] &= &
  \mathbb{E}_{\mathbf{z}}\left[-\frac{\partial \frac{1}{p_{\theta}}\frac{\partial p_{\theta}}{\partial \theta}}{\partial \theta} \right] =
  \mathbb{E}_{\mathbf{z}}
  \left[-\frac{1}{p_{\theta}(\sample)}\frac{\partial^2 p_{\theta}}{\partial \theta^2}+
  \left(\frac{1}{p_{\theta}} \frac{\partial p_{\theta}}{\partial \theta} \right)^T 
  \left(\frac{1}{p_{\theta}} \frac{\partial p_{\theta}}{\partial \theta} \right)\right] \nonumber \\
  &=& -\frac{\partial^2}{\partial \theta^2}\left(\sum_{\mathbf{z}} p_{\theta}(\mathbf{z})\right) + 
  \mathbb{E}_{\mathbf{z}}\left[\left(\frac{\partial \log p_{\theta}(\mathbf{z})}{\partial \theta}\right)^T
  \left(\frac{\partial \log p_{\theta}(\mathbf{z})}{\partial \theta}\right) \right] \nonumber \\
  &=&   \mathbb{E}_{\mathbf{z}}\left[\left(\frac{\partial \log p_{\theta}(\mathbf{z})}{\partial \theta}\right)^T
  \left(\frac{\partial \log p_{\theta}(\mathbf{z})}{\partial \theta}\right) \right]
\end{eqnarray}

\subsection{Derivation of the natural gradient metrics}

\subsubsection{Linear activation function}

In the case of linear outputs we assume that each entry of the vector $t$, 
$t_i$ comes from a Gaussian distribution centered around $\out_i(\mathbf{x})$
with some standard deviation $\beta$. From this it follows that:

\begin{equation}
\label{eq:prob_linear}
p_{\theta}(\mathbf{t} | \mathbf{x}) = \prod_{i=1}^o \mathcal{N}(t_i|\out(\mathbf{x}, \theta)_i, \beta^2)
\end{equation}

\begin{equation}
\label{eq:deriv_FIM_linear}
\begin{array}{lll}
\metric & = & \mathbb{E}_{\mathbf{x} \sim \tilde{q}} \left[
  \mathbb{E}_{\mathbf{t} \sim \mathcal{N}(\mathbf{t}|\out(\mathbf{x}, \theta), \beta^2\mathbf{I})}
   \left[
       \sum_{i=1}^o \left(\frac{\partial \log _{\theta}p(t_i| \out(\mathbf{x})_i}{\partial \theta}\right)^T
       \left(\frac{\partial \log p_{\theta}(t_i | \out(\mathbf{x})_i}{\partial \theta}\right) 
\right] \right] \\
& = & \mathbb{E}_{\mathbf{x} \sim \tilde{q}} \left[
  \sum_{i=1}^o \left[
   \mathbb{E}_{\mathbf{t} \sim \mathcal{N}(\mathbf{t}|\out(\mathbf{x}, \theta), \beta^2\mathbf{I})}
\left[
    \left(\frac{\partial (t_i - y_i)^2}{\partial \theta}\right)^T
    \left(\frac{\partial (t_i - y_i)^2}{\partial \theta}\right) \right] \right] \right] \\
& = & \mathbb{E}_{\mathbf{x} \sim \tilde{q}} \left[
    \sum_{i=1}^o \left[
    \mathbb{E}_{\mathbf{t} \sim \mathcal{N}(\mathbf{t}|\out(\mathbf{x}, \theta), \beta^2\mathbf{I})} \left[
      (t_i - y_i)^2 \left(\frac{\partial y_i}{\partial \theta}\right)^T
                    \left(\frac{\partial y_i}{\partial \theta}\right) \right] \right] \right] \\
& = & \mathbb{E}_{\mathbf{x} \sim \tilde{q}} \left[
    \sum_{i=1}^o \left[
    \mathbb{E}_{\mathbf{t} \sim \mathcal{N}(\mathbf{t}|\out(\mathbf{x}, \theta), \beta^\mathbf{I})}
\left[
      (t_i - y_i)^2\right] \left(\frac{\partial y_i}{\partial \theta}\right)^T
                    \left(\frac{\partial y_i}{\partial \theta}\right) \right] \right] \\
& = & \mathbb{E}_{\mathbf{x} \sim \tilde{q}} \left[
    \sum_{i=1}^o \left[
    \mathbb{E}_{\mathbf{t} \sim \mathcal{N}(\mathbf{t}|\out(\mathbf{x}, \theta), \beta^2\mathbf{I})}
\left[
      (t_i - y_i)^2\right] \left(\frac{\partial y_i}{\partial \theta}\right)^T
                    \left(\frac{\partial y_i}{\partial \theta}\right) \right] \right] \\
& = & \mathbb{E}_{\mathbf{x} \sim \tilde{q}} \left[
    \sum_{i=1}^o \left[
    \mathbb{E}_{\mathbf{t} \sim \mathcal{N}(\mathbf{t}|\out(\mathbf{x}, \theta), \beta^2\mathbf{I})}
 \left[
      (t_i - y_i)^2\right] \left(\frac{\partial y_i}{\partial \theta}\right)^T
                    \left(\frac{\partial y_i}{\partial \theta}\right) \right] \right] \\
& = & \beta^2 \mathbb{E}_{\mathbf{x} \sim \tilde{q}} \left[
    \sum_{i=1}^o \left[
     \left(\frac{\partial y_i}{\partial \theta}\right)^T
                    \left(\frac{\partial y_i}{\partial \theta}\right) \right] \right] \\
& = & \beta^2 \mathbb{E}_{\mathbf{x} \sim \tilde{q}} \left[ \jacob_{\out}^T \jacob_{\out} \right] \\
\end{array}
\end{equation}



\subsubsection{Sigmoid activation function}

In the case of the sigmoid units, i,e, $\out = \text{sigmoid}(\lout)$,  we assume a binomial distribution which gives us:

\begin{equation}
p(\mathbf{t}|\mathbf{x}) = \prod_i \out_i^{t_i}(1-\out_i)^{1-t_i}
\end{equation}

$\log p$ gives us the usual cross-entropy error used with sigmoid units. We can compute the 
Fisher information matrix as follows:

\begin{equation}
\begin{array}{lll}
\metric & = & \mathbb{E}_{\mathbf{x} \sim \tilde{q}} \left[
\mathbb{E}_{\mathbf{t} \sim p(\mathbf{t}|\mathbf{x})} \left[
\sum_{i=1}^{o} \frac{(t_i-\outi_i)^2}{\outi_i^2 (1-\outi_i)^2} 
\left( \frac{\partial \outi_i}{\partial \theta}\right)^T \frac{\partial \outi_i}{\partial \theta}
\right] \right] \\
& = & \mathbb{E}_{\mathbf{x} \sim \tilde{q}} \left[
\sum_{i=1}^o \frac{1}{\outi_i(1-\outi_i)}
\left( \frac{\partial \outi_i}{\partial \theta}\right)^T \frac{\partial \outi_i}{\partial \theta}
\right] \\
& = & \mathbb{E}_{\mathbf{x} \sim \tilde{q}} \left[
\jacob_{\out}^T \diag(\frac{1}{\out (1-\out)}) \jacob_{\out} \right]
\end{array}
\end{equation}

\subsubsection{Softmax activation function}

For the softmax activation function, $\out=\text{softmax}(\lout)$, $p(\mathbf{t}|\mathbf{x})$ takes the form 
of a multinomial:

\begin{equation}
p(\mathbf{t}|\mathbf{x}) = \prod_i^o \out_i^{t_i}
\end{equation}


\begin{equation}
\metric  =  \mathbf{E}_{\mathbf{x} \sim \tilde{q}} \left[
\sum_i^o \frac{1}{\out_i} 
\left( \frac{\partial \outi_i}{\partial \theta}\right)^T \frac{\partial \outi_i}{\partial \theta}
\right] 
\end{equation}


\subsection{Implementation Details}


We have implemented natural gradient descent using a truncated Newton approach similar to the 
pipeline proposed by \cite{Pearlmutter94} and used by \cite{Martens10}. 
In order to better deal with singular and ill-conditioned matrices we use the MinRes-QLP
algorithm \citep{CPS11} instead of linear conjugate gradient.
Both Minres-QLP as well as linear conjugate gradient can be found implemented in Theano 
at \href{https://github.com/pascanur/theano_optimize}{https://github.com/pascanur/theano\_optimize}. 
We used the Theano library \citep{bergstra+al:2010-scipy-short}
which allows for a flexible implementation of the pipeline, that can automatically 
generate the computational graph of the metric times some vector for different 
models:
\newcommand\codeComment[1]{\textcolor[rgb]{.3,.3,.3}{\textbf{#1}}}
\begin{Verbatim}[commandchars=\\\{\}]
import theano.tensor as TT
\codeComment{# `params` is the list of Theano variables containing the parameters}
\codeComment{# `vs` is the list of Theano variable representing the vector `v`}
\codeComment{# with whom we want to multiply the metric}
\codeComment{# `Gvs` is the list of Theano expressions representing the product} 
\codeComment{# between the metric and `vs`}

\codeComment{# `out_smx` is the output of the model with softmax units}
Gvs = TT.Lop(out_smx,params, 
             TT.Rop(out_smx,params,vs)/(out_smx*out_smx.shape[0]))

\codeComment{# `out_sig` is the output of the model with sigmoid units}
Gvs = TT.Lop(out_sig,params, 
             TT.Rop(out_sig,params,vs)/(out_sig*
                                        (1-out_sig)*
                                        out_sig.shape[0]))

\codeComment{# `out` is the output of the model with linear units}
Gvs = TT.Lop(out,params,TT.Rop(out,params,vs)/out.shape[0])
\end{Verbatim}

The full pseudo-code of the algorithm (which is very similar to the one for Hessian-Free) is 
given below. The full Theano implementation can be retrieved from 
\href{https://github.com/pascanur/natgrad}{https://github.com/pascanur/natgrad}.

\begin{algorithm}
\caption{Pseudocode for natural gradient algorithm}
\begin{algorithmic}

    %\Comment{$gfn$ is a function that computes the metric times a vector}
    \STATE \codeComment{\# `gfn` is a function that computes the metric times some vector}
    \STATE gfn $\gets \left(\text{lambda } v \to \mathbf{G}v\right)$
    \WHILE{not \text{early\_stopping\_condition}}
    \STATE g $\gets \frac{\partial \mathcal{L}}{\partial \theta}$
    \STATE \codeComment{\# linear\_cg solves the linear system $G x = \frac{\partial \mathcal{L}}{\partial \theta}$}
    \STATE ng$ \gets$ linear\_cg(gfn, g, max\_iters = 20, rtol=1e-4)
    %\COMMENT{$\gamme$ is the learning rate}
    \STATE \codeComment{\# $\gamma$ is the learning rate}
    \STATE $\theta \gets \theta - \gamma$ ng
    \ENDWHILE
\end{algorithmic}
\end{algorithm}


Even though we are ensured that $\metric$ is positive semi-definite by construction, and 
MinRes-QLP is able to find a suitable solutions in case of singular matrices, we still use a damping 
strategy for two reasons. The first one is that we want to take in consideration the
inaccuracy of the metric (which is approximated only over a small minibatch). 
The second reason is that natural gradient makes sense only in the 
vicinity of $\theta$ as it is obtained by using a Taylor series approximation, hence (as for 
ordinary second order methods) it is appropriate to enforce a trust region for the gradient.
See \citet{Schaul2012proof}, where the convergence properties of natural gradient (in a specific case) 
are studied. 

Following the functional manifold interpretation of the algorithm, we can recover the Levenberg-Marquardt 
heuristic used in \citet{Martens10} by considering a first order Taylor approximation, where 
for any function $f$,

\begin{equation}
f\left(\theta_t - \eta \mathbf{G}^{-1}\frac{\partial f(\theta_t)}{\partial \theta_t}^T\right) 
\approx f(\theta_t) - \eta \frac{\partial f(\theta_t)}{\partial \theta_t}
 \mathbf{G}^{-1} \frac{\partial f(\theta_t)}{\partial \theta_t}^T
\end{equation}
This gives as the reduction ratio given by equation \eqref{eq:our_rho} which can be shown to behave identically with the one in \citet{Martens10}.
\begin{equation}
\label{eq:our_rho}
\rho = \frac{f\left(\theta_t - \eta \mathbf{G}^{-1} \frac{\partial f(\theta_t)}{\partial \theta_t}^T\right) -  f(\theta_t)}
{- \eta \frac{\partial f(\theta_t)}{\partial \theta_t} \mathbf{G}^{-1} \frac{\partial f(\theta_t)}{\partial \theta_t}^T}
\end{equation}
%Let us massage the formula of $\rho$ coming from the second order approximation (Levenberg-Marquardt) 
%used by~\citet{Martens10}, 
%considering the step size $\Delta \theta$ to be in his case the inverse Hessian times the gradient,
%replacing the Hessian in this step by $\mathbf{G}$:
%\begin{equation}
%\label{eq:martens}
%\rho = \frac{f\left(\theta_t - \mathbf{G}^{-1} \frac{\partial f(\theta_t)}{\partial \theta_t}^T\right) -  f(\theta_t)}
%{ -\frac{\partial f(\theta_t)}{\partial \theta_t} \mathbf{G}^{-1} \frac{\partial f(\theta_t)}
%{\partial \theta_t}^T - \frac{1}{2}\frac{\partial f(\theta_t)}{\partial \theta_t} \mathbf{G}^{-1} \frac{\partial f(\theta_t)}
%{\partial \theta_t}^T} 
%\end{equation}
%which is just a constant away from equation \ref{eq:our_eq}, implying that in principle using 
%the same heuristic for damping the metric matrix is well motivated from a theoretical point of view.

\subsection{Additional experimental results}

For the one hidden unit auto-encoder we selected hyper-parameters such to ensure stability of training,
while converging as fast as possible to a minima.
We compute the inverse of the metric or Hessian exactly (as it is just a 2 by 2 matrix). The learning 
rate for SGD is set to .1, for Amari's natural gradient .5 and for the covariance of gradience 1. (Newton's
method usually does not use a learning rate). We damped the Hessian and the covariance 
of gradients by adding $\mathbf{I}$ and Amari's metric using $0.01\cdot \mathbf{I}$. 

\subsection{Restricted MNIST experiment}
For the restricted MNIST, we train a one hidden layer MLP of 1500 hidden units.
The hyper-parameters where chosen based on a grid search over learning rate, damping factor and damping strategy. 
Note that beside using unlabeled data, the regularization effect of natural gradient is strongly connected to the damping 
factor which accounts for the uncertainty in the metric (in a similar way to how it does in the uncentered covariance 
version of natural gradient). 
The minibatch size was kept constant to 2500 samples for natural gradient methods and 250 for MSGD. We used a constant 
learning rate and used a budged of 2000 iterations for natural gradient and 40000 iterations for MSGD.

We used a learning rate of 1.0 for MSGD and 5.0 for the functional manifold NGD using unlabeled
data or the covariance based 
natural gradient. For the functional manifold NGD using either the same training 
minibatch or a different batch 
from the training set for computing the  metric we set the learning rate to 0.1. 
We use a Levenberg-Marquardt heuristic only when using unlabeled data, otherwise
the damping factor was kept constant. Its initial value was 2.0 for when using unlabeled data, 
and 0.01 for every case except when using the covariance of the gradients as the metric, 
when is set to 0.1. 

\begin{figure}[ht]
\begin{center}
\centerline{\includegraphics[width=.8\columnwidth]{mnist1000.png}}
\caption{(left) train error (cross entropy over the entire training set) 
    on a log scale in order to improve visibility and (right) test error 
(percentage of misclassified examples)
as a function of number of updates for the restricted mnist dataset. 
%`kl, unlabeled`
%stands for the functional manifold version of natural gradient, where the metric is computed 
%over unlabeled data. for 'kl, different training minibatch' we compute the metric on a different
%minibatch from the training set, while 'kl, same minibatch' we compute the metric over the same
%minibatch we computed the gradient hence matching the standard use of hessian-free. 
%'covariance' stands for tonga that
%uses the covariance matrix as a metric, while msgd is minibatch stochastic gradient descent. note
%that the x axis was interrupted, in order to improve the visibility of how the natural gradient
%methods behave.
} 
\label{fig:mnist_small}
\end{center}
\end{figure}

%These parameters had been validated using a grid search. 
%The configuration for best performing are summarized in table \ref{table:params_restricted_MNIST}

%\begin{table}[t]
%\vspace*{-2mm}
%\caption{Selected hyper-parameters for the restricted MNIST experiment. 
%    The algorithms name are the same as in the legend of figure \ref{fig:mnist_small}
%\emph{MSGD} stands for minibatch stochastic gradient descent. 
%\emph{NGD (KL based), unlabeled data} stands for natural gradient Descent
%(the functional manifold interpretation) where the metric was 
%computed over unlabeled data. \emph{NGD(KL based), different batch} is the functional
%manifold interpretation of natural gradient where 
%we compute the metric over a different training minibatch than the gradient.
%\emph{NGD (KL based), same batch} is again 
%natural gradient, where we compute the metric over the same batch as the gradient.
%\emph{NGD (covariance based} is TONGA version 
%of natural gradient, where the metric is the covariance of the gradient computed over
%the same batch as the gradient.
%}
%    \label{table:params_restricted_MNIST}
%     \vskip 0.15in
%\vspace*{-3mm}
%    \begin{center}
%        \begin{small}
%            \begin{sc}
%                \begin{tabular}{c|c|c|c|}
%                    \hline
%                    %\abovespace\belowspace
%                    Algorithm & Learning rate & Initial damping value & Damping strategy \\
%                    \hline
%                    \hline
%                    MSGD & 1. & N/A & N/A  \\\hline
%                    NGD (KL based), & 5. & 2. & adapted using \\
%                         unlabeled data                      &    &     & Levenberg-Marquardt \\
%                                               &    &     & heuristic \\ \hline
%               NGD (KL based), different batch & 0.1 & 0.01 & constant \\ \hline
%                     NGD(KL based), same batch & 0.1 & 0.01 & constant \\ \hline
%                        NGD (covariance based) & 5. & 0.1 & constant \\ 
%                    \hline
%                \end{tabular}
%            \end{sc}
%        \end{small}
%    \end{center}
%    \vskip -0.1in
%\vspace*{-5mm}
%\end{table}


\subsection{MNIST experiment}
\label{sec:MNIST_params}
The model used has 3 layers, where the first two are convolutional layers 
both with filters of size 5x5. We used 32 filters on the first layer and 64
on the second. The last layer forms an MLP with 750 hidden units. 
We used minibatches of 10000 examples (for both the gradient and the metric), 
and a $\frac{1}{t}$ decaying learning rate strategy.
The learning rate was kept constant for the first 200 updates and then it was
computed based on the formula $\frac{l_0}{1+\frac{t-200}{20}}$,
where $t$ is the number of the current update. We used a budged ot 2000 update. 

The learning rate was set to 0.5 for the functional manifold approach when using 
a different batch for computing the metric and 1.0 when using the same 
batch for computing the metric, or for using the covariance of gradients as metric.
We use a Levenberg-Marquardt heuristic to adapt the damping factor which initially 
is 5.0 for the functional manifold approach, and a constant damping factor of 0.1
for using the covariance as metric. These values were validated by a grid search.

%\begin{table}[t]
%\vspace*{-2mm}
%\caption{Selected hyper-parameters for the MNIST experiment.
%    The algorithms name are the same as in the legend of figure \ref{fig:mnist_small}
%\emph{NGD(KL based), different batch} is the functional manifold interpretation of 
%natural gradient where we compute the metric over a different training minibatch than 
%the gradient. \emph{NGD (KL based), same batch} is again 
%natural gradient, where we compute the metric over the same batch as the gradient.
%\emph{NGD (covariance based} is TONGA version 
%of natural gradient, where the metric is the covariance of the gradient computed over the same batch as the gradient.
%}
%    \label{table:params_MNIST}
%     \vskip 0.15in
%\vspace*{-3mm} 
%    \begin{center}
%        \begin{small}
%            \begin{sc}
%                \begin{tabular}{c|c|c|c|}
%                    \hline
%                    %\abovespace\belowspace
%                    Algorithm & Learning rate ($l_0$) & Initial damping value & Damping strategy \\
%                    \hline
%                    \hline
%               NGD (KL based),  & 0.5 & 5. & adapted using \\
%                  different batch                             &    &     & Levenberg-Marquardt \\
%                                               &    &     & heuristic \\ \hline
%                      NGD(KL based), & 1. & 5. & adapted using  \\
%                          same batch                     &    &     & Levenberg-Marquardt  \\ 
%                                               &    &     & heuristic \\ \hline
%                        NGD (covariance based) & 1. & 0.1 & constant \\
%                    \hline
%                \end{tabular}
%            \end{sc}
%        \end{small}
%    \end{center}
%    \vskip -0.1in
%\vspace*{-5mm}
%\end{table}

\subsection{TFD experiment}
The Toronto Face Dataset (TFD), has a large amount of unlabeled data of poorer quality than the 
training set. To ensure that the
noise in the unlabeled data does not affect the metric, we compute the metric over the training
batch plus unlabeled samples. We used a three hidden layer model, where the first layer is a
convolutional layer of 300 filters of size 12x12. The second two layers from a 2 hidden layer
MLP of 2048 and 1024 hidden units respectively. 


For the TFD experiment we used the same decaying learning rate strategy introduced above,
in subsection \ref{sec:MNIST_params}, where we computed gradients over the minibatch of
960 examples. When using the unlabeled data, we added 480 unlabeled examples to the 960 
used to compute the gradient (therefore the metric was computed over 1440 examples)
otherwise we used the same 960 examples for the metric. In both cases we used an initial
damping factor of 8, and the Levenberg-Marquardt heuristic to adapt 
this damping value. Initial learning rate $l_0$ was set to 1 in both cases. 

Note that we get only 83.04\% accuracy on this dataset, when the state of the art
is 85.0\% \cite{Rifai2012}, but our first layer is roughly 3 times smaller (300 filters versus 1024). 

\subsection{NISTP exepriment (robustness to the order of training samples)}

The model we experimented with was an MLP of only 500 hidden units. We compute the gradients for both MSGD and natural gradient 
over minibatches of 512 examples. In case of natural gradient we compute the metric over the same input batch of 512 examples.
Additionally we use a constant damping factor of 3 to account for the noise in the metric (and ill-conditioning
since we only use batches of 512 samples). The learning rates were kept constant, and we use .2 for the natural gradient and .1 for 
MSGD. 


\begin{table}[t]
\vspace*{-2mm}
    \caption{Results on the three datasets considered (restricted MNIST, MNIST and TFD). Note 
    that different models are used for different datasets. The training error is given as 
    cross-entropy error, while the test error is percentage of miss-classified examples.
    The algorithms name are the same as in the legend of figure \ref{fig:tfd}
    %The algorithms considered are MSGD (minibatch stochastic gradient descent), 
    % `KL, unlabeled` stands for natural gradient using the functional manifold interpretation, 
    % where the metric is computed over unlabeled data, `KL, different batch` states that the 
    % metric is computed over a different batch than the gradient, `KL, same batch` the metric is 
    % computed over the same batch as the gradient and `covariance` refers to Le Roux approach to 
    % natural gradient.
}
    \label{table:results}
    \vskip 0.15in
\vspace*{-3mm}
    \begin{center}
        \begin{small}
            \begin{sc}
                \begin{tabular}{c|c|c|c|c|c|c}
                    \hline
                    %\abovespace\belowspace
                    Data set & Data fold & MSGD & KL, unlabeled & KL, different & KL, same & covariance \\
                             & & & & batch & batch & \\
                    \hline
                    \hline
                    Restricted & train & 0.0523 & 0.0017 &0.0012 &0.0023  &0.0006 \\\cline{2-7}
                    MNIST      & test  & 5.22\% & 4.63\% &4.89\% & 4.91\% &4.74\% \\ 
                    \hline \hline
                    MNIST & train & & &0.00010 &0.0011 &0.024  \\ \cline{2-7}
                          & test  & & &0.78\% &0.82\%& 1.07\% \\ \hline \hline
                    TFD   & train & & 0.054 & 0.098 & & \\ \cline{2-7}
                          & test  & & 16.96\%&18.87\% & & \\
                    \hline
                \end{tabular}
            \end{sc}
        \end{small}
    \end{center}
    \vskip -0.1in
\vspace*{-5mm}
\end{table}


\begin{figure}[ht]
\begin{center}
\centerline{\includegraphics[width=.8\columnwidth]{mnist.png}}
\caption{Train and test error (cross entropy) on a log scale as a function of number of updates for the MNIST dataset.
The legend is similar to figure \ref{fig:tfd}
%, where 'KL, different training minibatch' implies that natural 
%gradient uses a different training batch to compute the metric, 'KL, same minibatch' 
%the metric is computed over the 
%same batch as the gradient, and 'covariance' stands for Le Roux version of natural gradient. 
} 
\label{fig:mnist}
\end{center}
\end{figure}

\end{document}



\documentclass{article} % For LaTeX2e
\usepackage{iclr2015,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{xspace}
\usepackage{tabularx}
\usepackage{arydshln}
\usepackage{wrapfig}
\usepackage[utf8]{inputenc}

\usepackage[normalem]{ulem}
\newcounter{quotecount}
\newcommand{\MyQuote}[1]{\vspace{0.4cm}\addtocounter{quotecount}{1}%
     (\arabic{quotecount})\hspace*{1cm}\parbox{12cm}{\em #1}\\[0.4cm]}


\title{Embedding Word Similarity with \\ Neural Machine Translation}

% TODO: Author list - tentative
\author{
Felix Hill \\
University of Cambridge \\
\texttt{felix.hill@cl.cam.ac.uk} 
\And
KyungHyun Cho \\
Universit\'{e} de Montr\'{e}al
\And
Sbastien Jean \\
Universit\'{e} de Montr\'{e}al
\And
Coline Devin \\
Harvey Mudd College
\And
Yoshua Bengio \\
Universit\'{e} de Montr\'{e}al, CIFAR Senior Fellow
}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version

% \iclrconference % Uncomment if submitted as conference paper instead of workshop

\begin{document}


\maketitle

\begin{abstract}
Neural language models learn word representations, or embeddings, that capture rich linguistic and conceptual information. Here we investigate the embeddings learned by \emph{neural machine translation models}, a recently-developed class of neural language model. We show that embeddings from translation models outperform those learned by monolingual models at tasks that require knowledge of both conceptual similarity and lexical-syntactic role. We further show that these effects hold when translating from both English to French and English to German, and argue that the desirable properties of translation embeddings should emerge largely independently of the source and target languages. Finally, we apply a new method for training neural translation models with very large vocabularies, and show that this vocabulary expansion algorithm results in minimal degradation of embedding quality. Our embedding spaces can be queried in an online demo and downloaded from our web page. Overall, our analyses indicate that translation-based embeddings should be used in applications that require concepts to be organised according to similarity and/or lexical function, while monolingual embeddings are better suited to modelling (nonspecific) inter-word relatedness.
\end{abstract}

\section{Introduction}

It is well known that word representations can be learned from the distributional patterns in corpora. Originally, such representations were constructed by counting word co-occurrences, so that the features in one word's representation corresponded to other words~\citep{landauer1997solution,turney2010frequency}. Neural language models, an alternative method for learning word representations, use language data to optimise (latent) features with respect to a language modelling objective. The objective can be to predict either the next word given the initial words of a sentence~\citep{Bengio2003lm,mnih2009scalable,collobert2008unified}, or simply a nearby word given a single cue word~\citep{mikolov2013distributed,Pennington2014}. The representations learned by neural models (sometimes called \emph{embeddings}) perform very effectively when applied as pre-trained features in a range of NLP applications and tasks~\citep{baroni2014don}. 

Despite these clear results, it is not well understood how the architecture of neural models affects the information encoded in their embeddings. Here we contribute to this understanding by considering the embeddings learned by architectures with a very different objective function: \emph{neural machine translation} (NMT) \emph{models}. NMT models have recently emerged as an alternative to statistical, phrase-based translation models, and are beginning to achieve impressive translation performance~\citep{kalchbrenner13emnlp,devlin2014fast,Sutskever2014sequence}.

We show that NMT models are not only a potential new direction for machine translation, but are also an effective means of learning word embeddings. Specifically, translation-based embeddings encode information relating to conceptual similarity (rather than non-specific relatedness or association) and lexical syntactic role more effectively than embeddings from monolingual neural language models. We demonstrate that these properties persist when translating between different language pairs (English-French and English-German). Further, based on the observation of subtle language-specific effects in the embedding spaces, we conjecture as to why similarity dominates over other semantic relations in translation embedding spaces. Finally, we discuss a potential limitation of the application of NMT models for embedding learning - the computational cost of training large vocabularies of embeddings - and show that a novel method for overcoming this issue preserves the aforementioned properties of translation-based embeddings. 

% We show that the precise information encoFinally, we consider ways to combine the distinct information encoded in language-model embeddings and those from translation embeddings. And we consider ways to overcome two important limitations of neural translation models as embedding-learning architectures: the computational difficulty in learning large vocabularies of embeddings, and the requirement for sentence-aligned bilingual training corpora.   

\section{Learning Embeddings with Neural Language Models}

All neural language models, including NMT models, learn real-valued embeddings (of specified dimension) for words in some pre-specified vocabulary, \(V\), covering many or all words in their training corpus. At each training step, a `score' for the current training example (or batch) is computed based on the embeddings in their current state. This score is compared to the model's objective function, and the error is backpropagated to update both the model weights (affecting how the score is computed from the embeddings) and the embedding features themselves. At the end of this process, the embeddings should encode information that enables the model to optimally satisfy its objective.     

\subsection{Monolingual Models}

In the original neural language model~\citep{Bengio2003lm} and subsequent
variants \citep{collobert2008unified}, training examples consist of an ordered sequence of $n$ words,
with the model trained to predict the $n$-th word given the first $n-1$
words. The model first represents the input as an ordered sequence of embeddings,
which it transforms into a single fixed length `hidden' representation, generally by concatenation and non-linear projection.
Based on this representation, a probability distribution is
computed over the vocabulary, from which the model can sample a guess for the
next word. The model weights and embeddings are updated to maximise the
probability of correct guesses for all sentences in the
training corpus. 
 
More recent work has shown that high quality word embeddings can be learned via simpler models
with no nonlinear hidden layer ~\citep{mikolov2013distributed,Pennington2014}. Given a single word or unordered window of words in the corpus, these models predict which words will occur nearby. For each word \( w\) in \(V\), a list of training cases \({(w,c) : c \in V }\) is extracted from the training corpus according to some algorithm.  For instance, in the \emph{skipgram}
approach ~\citep{mikolov2013distributed}, for each `cue word' \(w\) the `context words' \(c\) are sampled
from windows either side of tokens of \(w\) in the corpus (with \(c\) more
likely to be sampled if it occurs closer to \(w\)).\footnote{
    Subsequent variants use different algorithms for selecting the
    \((w,c)\) from the training corpus \citep{Hill2014EMNLP,levy2014dependency}
} For each \(w\) in \( V\), the model initialises both a
cue-embedding, representing the \(w\) when it occurs as a cue-word, and a
context-embedding, used when \(w\) occurs as a context-word. For a cue word
\(w\), the model uses the corresponding cue-embedding and all
context-embeddings to compute a probability distribution over \(V\) that
reflects the probability of a word occurring in the context of \(w\). When a
training example \((w,c)\) is observed, the model updates both the cue-word
embedding of \(w\) and the context-word embeddings in order to increase the
conditional probability of \(c\). 

\subsection{Bilingual Representation-learning Models}
Various studies have demonstrated that word representations can also be effectively learned from bilingual corpora, aligned at the document, paragraph or word level~\citep{haghighi2008learning,vulic2011identifying,mikolov2013exploiting,Hermann:2014:ICLR,Chandar}. These approaches aim to represent the words from two (or more) languages in a common vector space so that words in one language are close to words with similar or related meanings in the other. The resulting multilingual embedding spaces have been effectively applied to bilingual lexicon extraction~\citep{haghighi2008learning,vulic2011identifying,mikolov2013exploiting} and document classification~\citep{klementiev2012inducing,Hermann:2014:ICLR,Chandar,Kocisky:2014}.

We focus our analysis on two representatives of this class of (non-NMT) bilingual model. The first is that of \cite{Hermann:2014:ICLR}, whose embeddings improve on the performance of~\cite{Klementiev} in document classification applications. As with the NMT models introduced in the next section, this model can be trained directly on bitexts aligned only at the sentence rather than word level. When training, for aligned sentences \(S_E\) and \(S_F\) in different languages, the model computes representations \(R_E\) and \(R_F\) by summing the embeddings of the words in \(S_E\) and \(S_F\) respectively. The embeddings are then updated to minimise the divergence between \(R_E\) and \(R_F\) (since they convey a common meaning). A noise-contrastive loss function ensures that the model does not arrive at trivial (e.g. all zero) solutions to this objective.~\cite{Hermann:2014:ICLR} show that, despite the lack of prespecified word alignments, words in the two languages with similar meanings converge in the bilingual embedding space.\footnote{The models of \cite{Chandar} and \cite{Hermann:2014:ICLR} both aim to minimise the divergence between source and target language sentences represented as sums of word embeddings. Because of these similarities, we do not compare with both in this paper.}

The second model we examine is that of \cite{faruqui2014improving}. Unlike the models described above, \cite{faruqui2014improving} showed explicitly that projecting word embeddings from two languages (learned independently) into a common vector space can favourably influence the orientation of word embeddings when considered in their monolingual subspace; i.e relative to other words in their own language.  In contrast to the other models considered in this paper, the approach of \cite{faruqui2014improving} requires bilingual data to be aligned at the word level.

\subsection{Neural Machine Translation Models}

The objective of NMT is to generate an appropriate sentence in a target
language \(S_t\)  given a sentence \(S_s\) in the source language~\citep[see,
e.g.,][]{kalchbrenner13emnlp,Sutskever2014sequence}. As a by-product of learning to meet this objective, NMT models learn distinct sets of embeddings for the vocabularies \(V_ s\) and \(V_t\) in the source and target languages respectively.

Observing a training case \((S_s, S_t)\), these models represent \(S_s\) as an ordered sequence of embeddings of words from \(V_s\). The sequence for \(S_s\) is then encoded into a single representation \(R_S\).\footnote{Alternatively, subsequences (phrases) of \(S_s\) may be encoded at this stage in place of the whole sentence~\citep{Bahdanau2014}.} Finally, by referencing the embeddings in \(V_t\), \(R_S\) and a representation of what has been generated thus far, the model decodes a sentence in the target language word by word. If at any stage the decoded word does not match the corresponding word in the training target \(S_t\), the error is recorded. The weights and embeddings in the model, which together parameterise the encoding and decoding process, are updated based on the accumulated error once the sentence decoding is complete. 

Although NMT models can differ in their low-level architecture~\citep{kalchbrenner13emnlp,Cho2014,Bahdanau2014}, the translation objective exerts similar pressure on the embeddings in all cases. The source language embeddings must be such that the model can combine them to form single representations for ordered sequences of multiple words (which in turn must enable the decoding process). The target language embeddings must facilitate the process of decoding these representations into correct target-language sentences.    

\section{Experiments}

To learn translation-based embeddings, we trained two different NMT models. The first is the RNN encoder-decoder~\citep[\emph{RNNenc},][]{Cho2014}, which uses a recurrent-neural-network to encode all of the source sentence into a single vector on which the decoding process is conditioned. The second is the \emph{RNN Search} architecture~\citep{Bahdanau2014}, which was designed to overcome limitations exhibited by the RNN encoder-decoder when translating very long sentences. RNN Search includes a \emph{attention} mechanism, an additional feed-forward network that learns to attend to different parts of the source sentence when decoding each word in the target sentence.\footnote{Access to source code and limited GPU time prevent us from training and evaluating the embeddings from other NMT models such as that of~\citep{kalchbrenner13emnlp},~\citep{devlin2014fast} and \cite{Sutskever2014sequence}. The underlying principles of encoding-decoding also apply to these models, and we expect the embeddings would exhibit similar properties to those analysed here.} Both models were trained on a 348m word corpus of English-French sentence pairs or a 91m word corpus of English-German sentence pairs.\footnote{These corpora were produced from the WMT 14 parallel data after conducting the data-selection procedure described by~\cite{Cho2014}. } 

To explore the properties of bilingual embeddings learned via objectives other than direct translation, we trained the \emph{BiCVM} model of~\cite{Hermann:2014:ICLR} on the same data, and also downloaded the projected embeddings of~\cite{faruqui2014improving}, \emph{FD}, trained on a bilingual corpus of comparable size (\(\approx 300\) million words per language).\footnote{Available from \url{http://www.cs.cmu.edu/~mfaruqui/soft.html}. The available embeddings were trained on English-German aligned data, but the authors report similar to for English-French.} Finally, for an initial comparison with monolingual models, we trained a conventional skipgram model~\citep{mikolov2013distributed} and its \emph{Glove} variant~\citep{Pennington2014} for the same number of epochs on the English half of the bilingual corpus. 

To analyse the effect on embedding quality of increasing the quantity of training data, we then trained the monolingual models on increasingly large random subsamples of Wikipedia text (up to a total of 1.1bn words). Lastly, we extracted embeddings from a full-sentence language model~\citep[\emph{CW},][]{collobert2008unified}, which was trained for several months on the same Wikipedia 1bn word corpus. Note that increasing the volume of training data for the bilingual (and NMT) models was not possible because of the limited size of available sentence-aligned bitexts. 
 
\subsection{Similarity and relatedness modelling}

As in previous studies~\citep{Agirre2009,Bruni2014,baroni2014don}, our initial evaluations involved calculating pairwise (cosine) distances between embeddings and correlating these distances with (gold-standard) human judgements of the strength of relationships between concepts. For this we used three different gold standards: WordSim-353~\citep{Agirre2009}, MEN~\citep{Bruni2014} and SimLex-999~\citep{hill2014simlex}. Importantly, there is a clear distinction between WordSim-353 and MEN, on the one hand, and SimLex-999, on the other, in terms of the semantic relationship that they quantify. For both WordSim-353 and MEN, annotators were asked to rate how \emph{related} or \emph{associated} two concepts are. Consequently, pairs such as [\emph{clothes}-\emph{closet}], which are clearly related but ontologically dissimilar, have high ratings in WordSim-353 and MEN. In contrast, such pairs receive a low rating in SimLex-999, where only genuinely \emph{similar} concepts, such as [\emph{coast}- \emph{shore}], receive high ratings. 

To reproduce the scores in SimLex-999, models must thus distinguish pairs that are similar from those that are merely related. In particular, this requires models to develop sensitivity to the distinction between synonyms (similar) and antonyms (often strongly related, but highly dissimilar).\footnote{For a more detailed discussion of the similarity/relatedness distinction, see~\citep{hill2014simlex}.}

Table~\ref{table:perf} shows the correlations of NMT (English-French) embeddings, other bilingually-trained embeddings and monolingual embeddings with these three lexical gold-standards. NMT outperform monolingual embeddings, and, to a lesser extent, the other bilingually trained embeddings, on SimLex-999. However, this clear advantage is not observed on MEN and WordSim-353, where the projected embeddings of~\cite{faruqui2014improving}, which were tuned for high performance on WordSim-353, perform best. Given the aforementioned differences between the evaluations, this suggests that bilingually-trained embeddings, and NMT based embeddings in particular, better capture similarity, whereas monolingual embedding spaces are orientated more towards relatedness. 

\begin{table}[t]
\begin{center}
\begin{tabular}{r c | r  r  r  | r r | r r |}
\multicolumn{2}{c|}{~} &\multicolumn{3}{c|}{Monolingual models}  & \multicolumn{2}{c|}{\small Biling. models} & \multicolumn{2}{c|}{NMT models}\\ 
    \multicolumn{2}{c|}{~} &\bf Skipgram &\bf Glove &\bf CW & \bf FD & \bf BiCVM & \bf RNNenc &\bf RNNsearch \\ 
\hline
WordSim-353   & \(\rho\) & 0.52 & 0.55 & 0.51 &{\bf  0.69} & 0.50 &   0.57 &  0.58 \\
MEN & \(\rho\) & 0.44 & 0.71 & 0.60 & {\bf 0.78} & 0.45 &  0.63 &  0.62  \\
\hdashline
SimLex-999 & \(\rho\) & 0.29 & 0.32 & 0.28 & 0.39 & 0.36 &   {\bf 0.52} &  0.49 \\
SimLex-333 & \(\rho\) &  018&0.18  &0.07  & 0.24  &  0.34 & { \bf 0.49}   & 0.45   \\
TOEFL & \(\%\) & 0.75 & 0.78 & 0.64 & 0.84 & 0.87 &  {\bf 0.93} &  {\bf 0.93} \\
Syn/antonym & \(\%\) & 0.69  & 0.72  &  0.75 & 0.76 & 0.70 &  {\bf 0.79} & 0.74 \\
\end{tabular}
\caption{ NMT embeddings (RNNenc and RNNsearch) clearly outperform alternative embedding-learning architectures on tasks that require modelling similarity (below the dashed line), but not on tasks that reflect relatedness. Bilingual embedding spaces learned without the translation objective are somewhere between these two extremes.}
\label{table:perf}
\end{center}
\vspace{-5mm}
\end{table}


\begin{table}[t]
\begin{center}
\begin{tabular}{r | r  r  r | r r | r r}
&\bf Skipgram &\bf Glove &\bf CW&\bf FD &\bf BiCVM  &\bf RNNenc &\bf RNNsearch \\ 
\hline
\emph{teacher}  & {\small \emph{vocational}} &  {\small \emph{student}} 
& {\small \emph{student}} &{\small \emph{elementary}} & {\small  \emph{faculty}} & {\small \emph{professor}}  & {\small \emph{instructor}} \\ 
 & {\small \emph{in-service}} &  {\small \emph{pupil}} 
& {\small \emph{tutor}} & {\small \emph{school}}& {\small  \emph{professors}} & {\small \emph{instructor}}  & {\small \emph{professor}} \\ 
 & {\small \emph{college}} &  {\small \emph{university}} 
& {\small \emph{mentor}} & {\small \emph{classroom}}& {\small \emph{teach}}& {\small \emph{trainer}}  & {\small \emph{educator}} \\ 
\hdashline
\emph{eaten}  & {\small \emph{spoiled}} &  {\small \emph{cooked}} 
&  {\small \emph{baked}} &{\small \emph{ate}}& {\small  \emph{eating}}& {\small \emph{ate}} & {\small \emph{ate}} \\ 
  & {\small \emph{squeezed}} &  {\small \emph{eat}} 
&  {\small \emph{peeled}} &{\small \emph{meal}}& {\small \emph{eat}}& {\small \emph{consumed}} & {\small \emph{consumed}} \\ 
  & {\small \emph{cooked}} &  {\small \emph{eating}} 
&  {\small \emph{cooked}} &{\small \emph{salads}}& {\small \emph{baking}}& {\small \emph{tasted}} & {\small \emph{eat}} \\ 
\hdashline
\emph{Britain}  & {\small \emph{Northern}} &  {\small \emph{Ireland}} 
& {\small \emph{Luxembourg}} &{\small \emph{UK}}& {\small \emph{UK}} & {\small  \emph{UK}} & {\small \emph{England}} \\ 
& {\small \emph{Great}} &  {\small \emph{Kingdom}} 
& {\small \emph{Belgium}} &{\small \emph{British}}& {\small \emph{British}} & {\small  \emph{British}} & {\small \emph{UK}} \\ 
 & {\small \emph{Ireland}} &  {\small \emph{Great}} 
& {\small \emph{Madrid}} &{\small \emph{London}}& {\small \emph{England}} & {\small  \emph{America}} & {\small \emph{Syria}} \\ 


\end{tabular}
\caption{Nearest neighbours (excluding plurals) in the embedding spaces of different models. All models were trained for 6 epochs on the translation corpus except CW and FD (as noted previously). NMT embedding spaces are oriented according to similarity, whereas embeddings learned by monolingual models are organized according to relatedness. The other bilingual model BiCVM also exhibits a notable focus on similarity.}
\label{table:neigh}
\end{center}
\vspace{-5mm}
\end{table}

To test this hypothesis further, we ran three more evaluations designed to probe the sensitivity of models to similarity as distinct from relatedness or association. In the first, we measured performance on SimLex-Assoc-333 ~\citep{hill2014simlex}. This evaluation comprises the 333 most related pairs in SimLex-999, according to an independent empirical measure of relatedness (free associate generation~\citep{nelson2004university}). Importantly, the pairs in SimLex-Assoc-333, while all strongly related, still span the full range of similarity scores.\footnote{The most dissimilar pair in SimLex-Assoc-333 is [\emph{shrink,grow}] with a score of 0.23. The highest is [\emph{vanish},\emph{disappear}] with 9.80.} Therefore, the extent to which embeddings can model this data reflects their sensitivity to the similarity (or dissimilarity) of two concepts, even in the face of a strong signal in the training data that those concepts are related.    

The TOEFL synonym test is another similarity-focused evaluation of embedding spaces. This test contains 80 cue words, each with four possible answers, of which one is a correct synonym~\citep{landauer1997solution}. We computed the proportion of questions answered correctly by each model, where a model's answer was the nearest (cosine) neighbour to the cue word in its vocabulary.\footnote{To control for different vocabularies, we restricted the effective vocabulary of each model to the intersection of all model vocabularies, and excluded all questions that contained an answer outside of this intersection.} Note that, since TOEFL is a test of synonym recognition, it necessarily requires models to recognise similarity as opposed to relatedness.  

Finally, we tested how well different embeddings enabled a supervised classifier to distinguish between synonyms and antonyms, since synonyms are necessarily similar and people often find antonyms, which are necessarily dissimilar, to be strongly associated. For 744 word pairs hand-selected as either synonyms or antonyms,\footnote{Available online at \url{http://www.cl.cam.ac.uk/~fh295/}.} we presented a Gaussian SVM with the concatenation of the two word embeddings. We evaluated accuracy using 10-fold cross-validation. 

As shown in Table~\ref{table:perf}, with these three additional similarity-focused tasks we again see the same pattern of results. NMT embeddings outperform other bilingually-trained embeddings which in turn outperform monolingual models. The difference is particularly striking on SimLex-Assoc-333, which suggests that the ability to discern similarity from relatedness (when relatedness is high) is perhaps the most clear distinction between the bilingual spaces and those of monolingual models. 

These conclusions are also supported by qualitative analysis of the various embedding spaces. As shown in Table~\ref{table:neigh}, in the NMT embedding spaces the nearest neighbours (by cosine distance) to concepts such as \emph{teacher} are genuine synonyms such as \emph{professor} or \emph{instructor}. The bilingual objective also seems to orientate the non-NMT embeddings towards semantic similarity, although some purely related neighbours are also oberved. In contrast, in the monolingual embedding spaces the neighbours of \emph{teacher} include  highly related but dissimilar concepts such as \emph{student} or \emph{college}. 

%\footnote{Readers can inspect nearest neighbours in each embedding space using our web demo.} 
 
\subsection{Importance of training data quantity}

In previous work, monolingual models were trained on corpora many times larger than the English half of our parallel translation corpus. Indeed, the ability to scale to large quantities of training data was one of the principal motivations behind the skipgram architecture~\citep{mikolov2013distributed}. To check if monolingual models simply need more training data to capture similarity as effectively as bilingual models, we therefore trained them on increasingly large subsets of Wikipedia.\footnote{We did not do the same for our translation models because sentence-aligned bilingual corpora of comparable size do not exist.} As shown in Figure~\ref{fig:size}, this is not in fact the case. The performance of monolingual embeddings on similarity tasks remains well below the level of the NMT embeddings and somewhat lower than the non-MT bilingual embeddings as the amount of training data increases. 

\begin{figure*}[h]
\includegraphics[width = \textwidth,clip=True,trim=0 10 0 10]{Figure_1}
\vspace{-4mm}
\caption{The effect of increasing the amount of training data on the quality of monolingual embeddings, based on similarity-based evaluations (SimLex-999) and two relatedness-based evaluations (MEN and WordSim-353). \emph{ET} in the legend indicates models trained on the English half of the translation corpus. \emph{Wiki} indicates models trained on Wikipedia.}
\label{fig:size}
\end{figure*}


\subsection{Analogy Resolution}

Lexical analogy questions have been used as an alternative way of evaluating word representations. In this task, models must identify the correct answer (\emph{girl}) when presented with analogy questions such as `\emph{man} is to \emph{boy} as \emph{woman} is to ?'. It has been shown that Skipgram-style models are surprisingly effective at answering such questions~\citep{mikolov2013distributed}. This is because, if \( \bf m, b \) and \( \bf w\) are skigram-style embeddings for \emph{man}, \emph{boy} and \emph{woman} respectively, the correct answer is often the nearest neighbour in the vocabulary (by cosine distance) to the vector \( \bf v = w + b - m \). 

We evaluated embeddings on analogy questions using the same vector-algebra method as~\citet{mikolov2013distributed}. As in the previous section, for fair comparison we excluded questions containing a word outside the intersection of all model vocabularies, and restricted all answer searches to this reduced vocabulary. This left 11,166 analogies. Of these, 7219 are classed as `syntactic', in that they exemplify mappings between parts-of-speech or syntactic roles (e.g. \emph{fast} is to \emph{fastest} as \emph{heavy} is to \emph{heaviest}), and 3947 are classed as `semantic` (\emph{Ottawa} is to \emph{Canada} as \emph{Paris} is to \emph{France}), since successful answering seems to rely on some (world) knowledge of the concepts themselves. 

As shown in Fig.~\ref{fig:analogy}, NMT embeddings yield relatively poor answers to semantic analogy questions compared with monolingual embeddings and the bilingual embeddings \emph{FD} (which are projections of similar monolingual embeddings).\footnote{The performance of the FD embeddings on this task is higher than that reported by~\cite{faruqui2014improving} because we search for answers over a smaller total candidate vocabulary.} It appears that the translation objective prevents the embedding space from developing the same linear, geometric regularities as skipgram-style models with respect to semantic organisation. This also seems to be true of the embeddings from the full-sentence language model \emph{CW}. Further, in the case of the Glove and FD models this advantage seems to be independent of both the domain and size of the training data, since embeddings from these models trained on only the English half of the translation corpus still outperform the translation embeddings. 

On the other hand, NMT embeddings are effective for answering syntactic analogies using the vector algebra method. They perform comparably to or even better than monolingual embeddings when trained on less data (albeit bilingual data). It is perhaps unsurprising that the translation objective incentivises the encoding of a high degree of lexical syntactic information, since coherent target-language sentences could not be generated without knowledge of the parts-of-speech, tense or case of its vocabulary items. The connection between the translation objective and the embedding of lexical syntactic information is further supported by the fact that embeddings learned by the bilingual model BiCVM do not perform comparably on the syntactic analogy task. In this model, sentential semantics is transferred via a bag-of-words representation, presumably rendering the precise syntactic information less important.

When considering the two properties of NMT embeddings highlighted by these experiments, namely the encoding of semantic similarity and lexical syntax, it is worth noting that items in the similarity-focused evaluations of the previous section (SimLex-999 and TOEFL) consist of word groups or pairs that have identical syntactic role. Thus, even though lexical semantic information is in general pertinent to conceptual similarity~\citep{levy2014dependency}, the lexical syntactic and conceptual properties of translation embeddings are in some sense independent of one another.  


\begin{figure*}[ht]

\includegraphics[width = \textwidth,clip=True,trim=0 10 0 10]{Figure_2}

\vspace{-4mm}
\caption{Translation-based embeddings perform best on syntactic analogies (\emph{run,ran: hide, hid}). Monolingual skipgram/Glove models are better at semantic analogies (\emph{father, man; mother, woman})}
\label{fig:analogy}

\end{figure*}

\section{Effect of Target Language}

To better understand why a translation objective yields embedding spaces with particular properties, we trained the RNN Search architecture to translate from English to German. 

\begin{table}[ht]
\begin{center}
\begin{tabular}{r c | m{0.9cm}  m{0.9cm}  r c c c }
    \multicolumn{2}{c|}{~} &\bf  \small EN-FR &\bf  \small EN-DE &  &  `earned' & `castle' & `money'\\ 
\cline{1-4} \cline{6-8}
WordSim-353   & \(\rho\) & 0.60 & \bf 0.61 &   & {\small \emph{gained}} & {\small \emph{chateau}} & {\small \bf \emph{ silver}} \\
MEN & \(\rho\) & 0.61 & \bf 0.62 \bf & \bf \small  EN-FR  & {\small \bf \emph{won}} & {\small \emph{palace}} & {\small \emph{funds}} \\
SimLex-999 & \(\rho\) & 0.49 &  \bf 0.50 &  & {\small \emph{acquired}}  & {\small \emph{fortress}}  & {\small \emph{cash}} \\
\cline{6-8}
SimLex-Assoc-333 & \(\rho\) & 0.45  & \bf 0.47   &   &  &   \\
TOEFL & \(\%\) & 0.90 & \bf 0.93  & &  {\small \emph{gained}}&  {\small \emph{chateau}} &  {\small \emph{funds}} \\ 
Syn/antonym & \(\%\) & \bf 0.72 &  0.70  &  \bf \small EN-DE   &  {\small \emph{deserved}}    &  {\small \emph{palace}}    &  {\small \emph{cash}} \\ 
Syntactic analogies & \(\%\) & \bf 0.73 & 0.62   &  &  {\small \emph{accummulated}}   &  {\small \bf \emph{ padlock}}   &  {\small \emph{resources}} \\  
Semantic analogies & \(\%\) & 0.10 & \bf  0.11  \\
\end{tabular}
\caption{Comparison of embeddings learned by RNN Search models translating between English-French (EN-FR) and English-German (EN-DE) on all semantic evaluations (left) and nearest neighbours of selected cue words (right). Bold italics indicate target-language-specific effects. Evaluation items and vocabulary searches were restricted to words common to both models. }
\label{table:de}
\end{center}
\vspace{-5mm}
\end{table}

As shown in Table~\ref{table:de} (left side), the performance of the source (English) embeddings learned by this model was comparable to that of those learned by the English-to-French model on all evaluations, even though the English-German training corpus (91 million words) was notably smaller than the English-French corpus (348m words). This evidence shows that the desirable properties of translation embeddings highlighted thus far are not particular to English-French translation, and can also emerge when translating to a different language family, with different word ordering conventions.     

\section{Overcoming the Vocabulary Size Problem}

A potential drawback to using NMT models for learning word embeddings is the computational cost of training such a model on large vocabularies. To generate a target language sentence, NMT models repeatedly compute a softmax distribution over the target vocabulary. This computation scales with vocabulary size and must be repeated for each word in the output sentence, so that training models with large output vocabularies is challenging. Moreover, while the same computational bottleneck does not apply to the encoding process or source vocabulary, there is no way in which a translation model could learn a high quality source embedding for a word if the plausible translations were outside its vocabulary. Thus, limitations on the size of the target vocabulary effectively limit the scope of NMT models as representation-learning tools. This contrasts with the shallower monolingual and bilingual representation-learning models considered in this paper,  which efficiently compute a distribution over a large target vocabulary using either a hierarchical softmax~\citep{morin2005hierarchical} or approximate methods such as negative sampling~\citep{mikolov2013distributed,Hermann:2014:ICLR}, and thus can learn large vocabularies of both source and target embeddings.

A recently proposed solution to this problem enables NMT models to be trained with larger target vocabularies (and hence larger meaningful source vocabularies) at comparable computational cost to training with a small target vocabulary~\citep{Jean}. The algorithm uses (biased) importance sampling~\citep{Bengio+Senecal-2003-small} to approximate the probability distribution of words over a large target vocabulary with a finite set of distributions over subsets of that vocabulary. Despite this element of approximation in the decoder, extending the effective target vocabulary in this way significantly improves translation performance, since the model can make sense of more sentences in the training data and encounters fewer unknown words at test time. In terms of representation learning, the method provides a means to scale up the NMT approach to vocabularies as large as those learned by monolingual models. However, given that the method replaces an exact calculation with an approximate one, we tested how the quality of source embeddings is affected by scaling up the target language vocabulary in this way. 

\begin{table}[t]
\begin{center}
\begin{tabular}{r c | c c c c }
    \multicolumn{2}{c|}{~} &\bf RNN Search &\bf RNN Search & \bf RNN Search-LV  & \bf RNN Search-LV  \\ 
 \multicolumn{2}{c|}{~} &\bf \small EN-FR &\bf \small  EN-DE & \bf  \small EN-FR & \bf \small EN-DE \\ 
\hline
WordSim-353   & \(\rho\) & 0.60 & \bf 0.61 & 0.59 & 0.57  \\
MEN & \(\rho\) & 0.61 & \bf 0.62 & \bf 0.62 & 0.61 \\
SimLex-999 & \(\rho\) & 0.49 & 0.50 & \bf  0.51 & 0.50  \\
SimLex-Assoc-333 & \(\rho\) & 0.45  & \bf 0.47  & \bf 0.47  & 0.46   \\
TOEFL & \(\%\) & 0.90 & 0.93 & 0.93 & \bf 0.98  \\
Syn/antonym & \(\%\) & 0.72 &  0.70 & \bf 0.74 & 0.71 \\
Syntactic analogies & \(\%\) & \bf 0.73 &  0.62 & 0.71 & 0.62\\
Semantic analogies & \(\%\) & 0.10 &  0.11 & 0.08 & \bf 0.13\
\end{tabular}
\caption{Comparison of embeddings learned by the original (RNN Search - 30k French words, 50k German words) and extended-vocabulary (RNN Search-LV -500k words) models translating from English to French (EN-FR) and from English to German (EN-DE). For fair comparisons, all evaluations were restricted to the intersection of all model vocabularies.}
\label{table:ex}
\end{center}
\vspace{-5mm}
\end{table}

As shown in Table~\ref{table:ex}, there is no significant degradation of embedding quality when scaling to large vocabularies with using the approximate decoder. Note that for a fair comparison we filtered these evaluations to only include items that are present in the smaller vocabulary. Thus, the numbers do not directly reflect the quality of the additional 470k embeddings learned by the extended vocabulary models, which one would expect to be lower since they are words of lower frequency. All embeddings can be downloaded from \url{http://www.cl.cam.ac.uk/~fh295/}, and the embeddings from the smaller vocabulary models can be interrogated at \url{http://lisa.iro.umontreal.ca/mt-demo/embs/}.\footnote{A different solution to the rare-word problem was proposed by~\citep{luong2014addressing}. We do not evaluate the effects on the resulting embeddings of this method because we lack access to the source code.} 


\section{How Similarity Emerges}
\label{section:exp}

Although NMT models appear to encode both conceptual similarity and syntactic information for any source and target languages, it is not the case that embedding spaces will always be identical. Interrogating the nearest neighbours of the source embedding spaces of the English-French and English-German models reveals occasional language-specific effects. As shown in Table~\ref{table:de} (right side), the neighbours for the word \emph{earned} in the English-German model are as one might expect, whereas the neighbours from the English-French model contain the somewhat unlikely candidate \emph{won}. In a similar vein, while the neighbours of the word \emph{castle} from the English-French model are unarguably similar, the neighbours from the English-German model contain the word \emph{padlock}.
 
These infrequent but striking differences between the English-German and English-French source embedding spaces indicate how similarity might emerge effectively in NMT models. Tokens of the French verb \emph{gagner} have (at least) two possible English translations (\emph{win} and \emph{earn}). Since the translation model, which has limited encoding capacity, is trained to map tokens of \emph{win} and \emph{earn} to the same place in the target embedding space, it is efficient to move these concepts closer in the source space. Since \emph{win} and \emph{earn} map directly to two different verbs in German, this effect is not observed. On the other hand, the English nouns \emph{castle} and \emph{padlock} translate to a single noun (\emph{Schloss}) in German, but different nouns in French. Thus, \emph{padlock} and \emph{castle} are only close in the source embeddings from the English-German model. 

Based on these considerations, we can conjecture that the following condition on the semantic configuration between two language is crucial to the effective induction of lexical similarity. 

\MyQuote{\emph{For \(s_1\) and \(s_2\) in the source language, there is some \(t\) in the target language such that there are sentences in the training data in which \(s_1\) translates to \(t\) and sentences in which \(s_2\) translates to \(t\).}}

{\centering \emph{if and only if} \\}

\MyQuote{\emph{\(s_1\) and \(s_2\) are semantically similar.}}

Of course, this condition is not true in general. However, we propose that the extent to which it holds over all possible word pairs corresponds to the quality of similarity induction in the translation embedding space. Note that strong polysemy in the target language, such as \emph{gagner = win, earn}, can lead to cases in which \(1\) is satisfied but \(2\) is not. The conjecture claims that these cases are detrimental to the quality of the embedding space (at least with regards to similarity). In practice, qualitative analyses of the embedding spaces and native speaker intuitions suggest that such cases are comparatively rare. Moreover, when such cases are observed, \(s_1\) and \(s_2\), while perhaps not similar, are not strongly dissimilar. This could explain why related but strongly dissimilar concepts such as antonym pairs do not converge in the translation embedding space. This is also consistent with qualitative evidence presented by ~\citep{faruqui2014improving} that projecting monolingual embeddings into a bilingual space orientates them to better reflect the synonymy/antonymy distinction.
    

\section{Conclusion}

In this work, we have shown that the embedding spaces from neural machine translation models are orientated more towards conceptual similarity than those of monolingual models, and that translation embedding spaces also reflect richer lexical syntactic information. To perform well on similarity evaluations such as SimLex-999, embeddings must distinguish information pertinent to what concepts \emph{are} (their function or ontology) from information reflecting other non-specific inter-concept relationships. Concepts that are strongly related but dissimilar, such as antonyms, are particularly challenging in this regard~\citep{hill2014simlex}. Consistent with the qualitative observation made by ~\cite{faruqui2014improving}, we suggested how the nature of the semantic correspondence between the words in languages enables NMT embeddings to distinguish synonyms and antonyms and, more generally, to encode the information needed to reflect human intuitions of similarity.   

The language-specific effects we observed in Section 4 suggest a potential avenue for improving translation and multi-lingual embeddings in future work. First, as the availability of fast GPUs for training grows, we would like to explore the embeddings learned by NMT models that translate between much more distant language pairs such as English-Chinese or English-Arabic. For these language pairs, the word alignment will less monotonic and may result in even more important semantic and syntactic information being encoded in the lexical representation. Further,  as observed by both~\cite{Hermann:2014:ICLR} and~\cite{faruqui2014improving}, the bilingual representation learning paradigm can be naturally extended to update representations based on correspondences between multiple languages (for instance by interleaving English-French and English-German training examples). Such an approach should smooth out language-specific effects, leaving embeddings that encode only language-agnostic conceptual semantics and are thus more generally applicable. Another related challenge is to develop smaller or less complex representation-learning tools that encode similarity with as much fidelity as NMT models but without the computational overhead. One promising approach for this is to learn word alignments and word embeddings jointly~\citep{Kocisky:2014}. This approach is effective for cross-lingual document classification, although the authors do evaluate the monolingual subspace induced by the model.\footnote{These embeddings are not publicly available and we were unable to re-train them using the source code.}

Not all word embeddings learned from text are born equal. Depending on the application, those learned by NMT models may have particularly desirable properties. For decades, distributional semantic models have aimed to exploit Firth's famous \emph{distributional hypothesis} to induce word meanings from (monolingual) text. However, the hypothesis also betrays the weakness of the monolingual distributional approach when it comes to learning humah-quality concept representations. For while it is undeniable that ``words which are similar in meaning appear in similar distributional contexts"~\citep{dist}, the converse assertion, which is what really matters, is only sometimes true. 





\subsubsection*{Acknowledgments}
The authors would like to thank the developers of
Theano~\cite{bergstra+al:2010-scipy,Bastien-Theano-2012}.  We acknowledge the
support of the following agencies for research funding and computing support:
St John's College Cambridge, NSERC, Calcul Qu\'{e}bec, Compute Canada, the Canada Research Chairs and CIFAR.


\bibliography{iclr2015}
\bibliographystyle{iclr2015}

\end{document}



%% testflow.tex 
%% V1.1
%% 2007/01/10
%% Copyright (c) 2002-2007 by Michael Shell and The LaTeX3 Project
%% See: http://www.michaelshell.org/
%% for current contact information.
%% 
%% A test page based on Rainer Schoepf's testpage.tex which is part of the
%% LaTeX2e system with added modifications to add tests to verify PS/PDF
%% output by Michael Shell.
%%
%% For complete usage documentation, see the file testflow_doc.pdf
%%
%% Support sites:
%% http://www.michaelshell.org/tex/testflow/
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/testflow/
%%
%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%
%% File list of work: testflow.tex, testflow_ctl_A4.ps, testflow_ctl_A4.pdf,
%%                    testflow_ctl_LTR.ps, testflow_ctl_LTR.pdf, 
%%                    testflow_doc.pdf
%%*************************************************************************
%
% Changelog:
%
% 1.1 (2007/01/10)
%  1. new duplex test
%  2. tests for Palladio hinting
%
% 1.0 (2002/04/08) Initial release




% ***** To prevent testflow from prompting user on the console, *****
% ***** uncomment one each of the two pairs below:              *****
%
% *** paper size (choose one) ***
%\def\papertype{letterpaper}
%\def\papertype{a4paper}
%
% *** duplex page test (choose one) ***
%\def\makeduplexpage{yes}
%\def\makeduplexpage{no}




\def\docversion{1.1}
\def\docbuildcode{0000}

\newif\ifmakeduplextest
\makeduplextestfalse
\newif\ifneeduserresponse
\needuserresponsetrue

\def\useryesstring{yes}
\def\usernostring{no}
\def\userystring{y}
\def\usernstring{n}

\ifx\papertype\TESTFLOWundefined
\typeout{}
\typeout{ This is the testflow.tex (V\docversion) test page generator.}
\typeout{ See the testflow_doc.pdf file for full documentation.}

\typeout{}
\typeout{**********************************************************}
\typeout{*  Enter paper type in form of document class option,%
\space\space\space\space\space*}
\typeout{*  e.g., `a4paper' or `letterpaper' (without the quotes).\space*}
\typein[\papertype]{******************************************%
****************}
\fi

\ifx\makeduplexpage\TESTFLOWundefined
\loop
\typeout{}
\typeout{**************************************************************}
\typeout{*  Do you wish to make a second page to test duplex printing?%
\space*}
\typeout{*  Enter `yes' or `no' (without the quotes).%
\space\space\space\space\space\space\space\space\space\space\space\space\space
\space\space\space\space\space*}
\typein[\makeduplexpage]{******************************************%
********************}
\ifx\makeduplexpage\useryesstring\makeduplextesttrue\needuserresponsefalse\fi
\ifx\makeduplexpage\usernostring\makeduplextestfalse\needuserresponsefalse\fi
\ifx\makeduplexpage\userystring\makeduplextesttrue\needuserresponsefalse\fi
\ifx\makeduplexpage\usernstring\makeduplextestfalse\needuserresponsefalse\fi
\ifneeduserresponse
\repeat
\else
\ifx\makeduplexpage\useryesstring\makeduplextesttrue\fi
\ifx\makeduplexpage\userystring\makeduplextesttrue\fi
\fi



% declare symbols for lasy test
\DeclareSymbolFont{lasy}{U}{lasy}{m}{n}
\SetSymbolFont{lasy}{bold}{U}{lasy}{b}{n}
\DeclareMathSymbol\lasymho     {\mathord}{lasy}{"30}
\DeclareMathSymbol\lasyJoin    {\mathrel}{lasy}{"31}
\DeclareMathSymbol\lasyBox     {\mathord}{lasy}{"32}
\DeclareMathSymbol\lasyDiamond {\mathord}{lasy}{"33}
\DeclareMathSymbol\lasyleadsto {\mathrel}{lasy}{"3B}
\DeclareMathSymbol\lasysqsubset{\mathrel}{lasy}{"3C}
\DeclareMathSymbol\lasysqsupset{\mathrel}{lasy}{"3D}
\DeclareMathSymbol\lasylhd     {\mathbin}{lasy}{"01}
\DeclareMathSymbol\lasyunlhd   {\mathbin}{lasy}{"02}
\DeclareMathSymbol\lasyrhd     {\mathbin}{lasy}{"03}
\DeclareMathSymbol\lasyunrhd   {\mathbin}{lasy}{"04}



\documentclass[\papertype, 10pt]{article}

\typeout{-- Option summary:}
\typeout{-- Paper size: \papertype}
\ifmakeduplextest
\typeout{-- With duplex test page}
\else
\typeout{-- No duplex test page}
\fi


% enable Times fonts
\renewcommand{\sfdefault}{phv}
\renewcommand{\rmdefault}{ptm}
\renewcommand{\ttdefault}{pcr}

\def\fontsubfuzz{.9pt}

\def\tenptsize{\fontsize{10}{12pt}\selectfont}
\def\nineptsize{\fontsize{9}{11pt}\selectfont}
\def\eightptsize{\fontsize{8}{10pt}\selectfont}
\def\sevenptsize{\fontsize{7}{9pt}\selectfont}
\def\sixptsize{\fontsize{6}{8pt}\selectfont}
\def\fiveptsize{\fontsize{5}{7pt}\selectfont}


\pagestyle{empty}
\thispagestyle{empty}

\nofiles

\setlength{\oddsidemargin}{0pt}
\setlength{\evensidemargin}{0pt}
\setlength{\marginparwidth}{1in}
\setlength{\marginparsep}{0pt}

\setlength{\topmargin}{0pt}
\setlength{\headheight}{0pt}
\setlength{\headsep}{0pt}
\setlength{\topskip}{0pt}

\setlength{\footskip}{0pt}

\setlength{\textwidth}{\paperwidth}
\addtolength{\textwidth}{-2in}
\setlength{\textheight}{\paperheight}
\addtolength{\textheight}{-2in}

\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt}

\setlength{\unitlength}{1sp}


\newcounter{textheight}
\newcounter{textwidth}

\setcounter{textheight}{\textheight}
\setcounter{textwidth}{\textwidth}



\newlength{\help}

\newcounter{help}

\newcounter{in}
\newcounter{halfin}
\newcounter{fifthin}
\newcounter{tenthin}
\newcounter{twtin}

\newcounter{mm}
\newcounter{tmm}
\newcounter{frmm}
\newcounter{fvmm}
\newcounter{tenmm}

\newcounter{foo}

\newcounter{x}
\newcounter{y}

\newcommand{\addtox}{\addtocounter{x}}
\newcommand{\addtoy}{\addtocounter{y}}

\newcommand{\putxy}{\put(\value{x},\value{y})}
\newcommand{\multiputxy}{\multiput(\value{x},\value{y})}

\newcommand{\sethelpcounter}[2]{%
   \setlength{\help}{#2}\setcounter{#1}{\help}}


\newenvironment{testflowpage}{\setlength{\help}{1in}
\setcounter{in}{\help}

\setlength{\help}{0.5in}
\setcounter{halfin}{\help}

\setlength{\help}{0.2in}
\setcounter{fifthin}{\help}

\setlength{\help}{0.1in}
\setcounter{tenthin}{\help}

\setlength{\help}{0.05in}
\setcounter{twtin}{\help}


\setlength{\help}{1mm}
\setcounter{mm}{\help}

\setlength{\help}{2mm}
\setcounter{tmm}{\help}

\setlength{\help}{4mm}
\setcounter{frmm}{\help}

\setlength{\help}{5mm}
\setcounter{fvmm}{\help}

\setlength{\help}{10mm}
\setcounter{tenmm}{\help}


\begin{picture}(0,0)
\scriptsize

\put(0,-\value{textheight}){%
     \framebox(\value{textwidth},\value{textheight}){}}

% left mm ruler
\setcounter{x}{0}
\sethelpcounter{y}{-0.45\textheight}
\putxy{\line(-1,0){\value{in}}}


\addtox{-\value{fvmm}}
\addtoy{-\value{tmm}}
\multiputxy(-\value{fvmm},0){5}{\line(0,1){\value{frmm}}}


\addtoy{\value{mm}}
\multiput(-\value{mm},\value{y})(-\value{mm},0){25}%
          {\line(0,1){\value{tmm}}}

\addtoy{\value{frmm}}
\setcounter{foo}{5}
\multiput(-\value{fvmm},\value{y})(-\value{fvmm},0){4}{%
    \makebox(0,0){\arabic{foo}}\addtocounter{foo}{5}}


% left in ruler
\setcounter{x}{0}
\sethelpcounter{y}{-0.55\textheight}
\putxy{\line(-1,0){\value{in}}}

\addtox{-\value{tenthin}}
\addtoy{-\value{tenthin}}
\multiputxy(-\value{tenthin},0){10}{\line(0,1){\value{fifthin}}}

\addtox{\value{twtin}}
\addtoy{\value{twtin}}
\multiputxy(-\value{tenthin},0){10}{\line(0,1){\value{tenthin}}}

\setcounter{foo}{1}

\addtox{-\value{twtin}}
\addtoy{\value{tenthin}}
\addtoy{\value{tenthin}}
\multiputxy(-\value{tenthin},0){9}{%
    \makebox(0,0){\arabic{foo}}\addtocounter{foo}{1}}

% right mm ruler
\sethelpcounter{x}{\textwidth}
\sethelpcounter{y}{-0.45\textheight}
\putxy{\line(1,0){\value{in}}}

\addtox{\value{mm}}
\addtoy{-\value{mm}}
\multiputxy(\value{mm},0){25}{\line(0,1){\value{tmm}}}

\addtox{\value{frmm}}
\addtoy{-\value{mm}}
\multiputxy(\value{fvmm},0){5}{\line(0,1){\value{frmm}}}

\addtoy{\value{fvmm}}
\setcounter{foo}{5}
\multiputxy(\value{fvmm},0){4}{%
    \makebox(0,0){\arabic{foo}}\addtocounter{foo}{5}}

% right in ruler
\sethelpcounter{x}{\textwidth}
\sethelpcounter{y}{-0.55\textheight}
\putxy{\line(1,0){\value{in}}}

\addtox{\value{tenthin}}
\addtoy{-\value{tenthin}}
\multiputxy(\value{tenthin},0){10}{%
   \line(0,1){\value{fifthin}}}

\addtox{-\value{twtin}}
\addtoy{\value{twtin}}
\multiputxy(\value{tenthin},0){10}{%
   \line(0,1){\value{tenthin}}}

\setcounter{foo}{1}
\addtox{\value{twtin}}
\addtoy{\value{tenthin}}
\addtoy{\value{tenthin}}
\multiputxy(\value{tenthin},0){9}{%
    \makebox(0,0){\arabic{foo}}\addtocounter{foo}{1}}


% top mm ruler
\sethelpcounter{x}{0.45\textwidth}
\setcounter{y}{0}
\putxy{\line(0,1){\value{in}}}

\addtox{-\value{tmm}}
\addtoy{\value{fvmm}}
\multiputxy(0,\value{fvmm}){5}{\line(1,0){\value{frmm}}}

\addtox{\value{mm}}
\addtoy{-\value{frmm}}
\multiputxy(0,\value{mm}){25}{\line(1,0){\value{tmm}}}

\setcounter{foo}{5}
\addtox{-\value{tmm}}
\addtoy{-\value{mm}}
\addtoy{\value{fvmm}}
\multiputxy(0,\value{fvmm}){4}{%
  \makebox(0,0){\arabic{foo}\rule{5pt}{0pt}}\addtocounter{foo}{5}}

% top in ruler
\sethelpcounter{x}{0.55\textwidth}
\setcounter{y}{0}
\putxy{\line(0,1){\value{in}}}

\addtox{-\value{tenthin}}
\addtoy{\value{tenthin}}
\multiputxy(0,\value{tenthin}){10}{\line(1,0){\value{fifthin}}}

\addtox{\value{twtin}}
\addtoy{-\value{twtin}}
\multiputxy(0,\value{tenthin}){10}{\line(1,0){\value{tenthin}}}

\setcounter{foo}{1}
\addtox{-\value{tenthin}}
\addtoy{\value{twtin}}
\multiputxy(0,\value{tenthin}){9}{%
   \makebox(0,0){\arabic{foo}}\addtocounter{foo}{1}}

% bottom mm ruler
\sethelpcounter{x}{0.45\textwidth}
\setcounter{y}{-\textheight}
\putxy{\line(0,-1){\value{in}}}

\addtox{-\value{tmm}}
\addtoy{-\value{fvmm}}
\multiputxy(0,-\value{fvmm}){5}{\line(1,0){\value{frmm}}}

\addtox{\value{mm}}
\addtoy{\value{frmm}}
\multiputxy(0,-\value{mm}){25}{\line(1,0){\value{tmm}}}

\setcounter{foo}{5}
\addtox{-\value{tmm}}
\addtoy{\value{mm}}
\addtoy{-\value{fvmm}}
\multiputxy(0,-\value{fvmm}){4}{%
   \makebox(0,0){\arabic{foo}\rule{5pt}{0pt}}\addtocounter{foo}{5}}


% bottom in ruler
\sethelpcounter{x}{0.55\textwidth}
\setcounter{y}{-\textheight}
\putxy{\line(0,-1){\value{in}}}

\addtox{-\value{tenthin}}
\addtoy{-\value{tenthin}}
\multiputxy(0,-\value{tenthin}){10}{\line(1,0){\value{fifthin}}}

\addtox{\value{twtin}}
\addtoy{\value{twtin}}
\multiputxy(0,-\value{tenthin}){10}{\line(1,0){\value{tenthin}}}

\setcounter{foo}{1}
\addtox{-\value{tenthin}}
\addtoy{-\value{twtin}}
\multiputxy(0,-\value{tenthin}){9}{%
   \makebox(0,0){\arabic{foo}}\addtocounter{foo}{1}}


\end{picture}

\setlength{\help}{\textwidth}
\addtolength{\help}{-1.25in}

\vspace*{\baselineskip}
\mbox{}\hfill
\begin{minipage}{\help}\relax}
% end of testpage environment
{\end{minipage}\hfill\mbox{}\vfill\mbox{}}


\begin{document}


\begin{testflowpage}

\centerline{\Huge A Test for \LaTeX\ PS/PDF Printing}
\vspace{0.75\baselineskip}
\centerline{\large Version \docversion\hspace{0.2in} Build Code: \docbuildcode}
\vspace{0.75\baselineskip}
\centerline{\large See the testflow home page for the latest news and FAQ:}
\centerline{\texttt{\footnotesize http://www.michaelshell.org/tex/testflow/}} 
\vspace{1.5\baselineskip}
\centerline{\large\bfseries\scshape Notes}
\vspace{0.5\baselineskip}

Document paper type selected under \LaTeX: \mbox{\ttfamily\papertype}\\
Depends only on the base article.cls --- no other external packages are loaded.\\
The main text font is Times Roman, the math font is Computer Modern.\\
Imperial (0.1in) and metric (mm) rulers are provided to measure centering.\\
The frame on this page should be centered on the paper and 1in (25.4mm) from the edges.\\
To maintain accurate dimensions, do not scale page when printing.\\
(i.e., deselect any ``fit to page" or ``shrink/expand page" options.)\\
For complete usage information, read the \texttt{testflow\_doc.pdf} file.

\vspace{1.0\baselineskip}
\centerline{\large\bfseries\scshape Palladio Font Hinting Test}
\vspace{0.5\baselineskip}

\centerline{\renewcommand{\rmdefault}{ppl}\rmfamily\bfseries This is in bold Palatino/Palladio.}


\vspace{1.0\baselineskip}
\centerline{\large\bfseries\scshape Ligature Test}
\vspace{0.5\baselineskip}

\makebox[0.85in][l]{\textbf{Ligatures}} ---~~~The office was affected by the five flawed mufflers.

\makebox[0.85in][l]{\textbf{No Ligatures}} ---~~~The of\/f\/ice was a\/f\/fected by the f\/ive f\/lawed muf\/f\/lers.


\vspace{1.0\baselineskip}
\centerline{\large\bfseries\scshape Math Tests}
\vspace{0.5\baselineskip}

\centerline{\normalsize\bfseries Large Delimiter and Operator Test}
\begin{center}
\begin{math}
I = \left[ 
    \begin{array}{cc}
    1 & 0\\
    0 & 1
   \end{array}
   \right]\quad \mbox{and}   
\quad\Bigg(\sum\limits_{i=0}^{2} 2^i = 7 \Bigg)
\end{math}
\end{center}

\vspace{0.25\baselineskip}

\centerline{\normalsize\bfseries Minus Sign Test}
\begin{center}
\begin{math}
\mbox{If } a = 4\mbox{, then: } 2^{-a + 7} - 2^{a-3} = 2^{3} - 2^{1} = 6
\end{math}
\end{center}

\vspace{0.1\baselineskip}
\centerline{\normalsize\bfseries Problem Character, Times Roman and Font Kerning Tests}
\vspace{0.5\baselineskip}
% here is how we would directly access them within cmmi10
%{\fontencoding{OML}\fontfamily{cmm}\fontshape{it}\selectfont
%\symbol{0}\hspace{1ex}\symbol{9}\hspace{1ex}\symbol{10}\hspace{1ex}\symbol{13}
%\hspace{1ex}\symbol{32}\hspace{1ex}\symbol{127}}
\centerline{\normalfont Math italic glyphs: $\mathnormal{\Gamma,~\Psi,~\Omega,~\gamma,~\psi,~\mbox{\t{\mbox{}\mbox{}}}}$}
\vspace{0.25\baselineskip}
\centerline{Large Times Roman italic: {\LARGE \textit{z}}\qquad GS kerning test: {\footnotesize The ``Problematic" little quotes.}} 

\vspace{0.25\baselineskip}
{\footnotesize\textbf{Note:} The math italic glyphs are in the control character positions 
0, 9, 10, 13, 32 and 127.\hfill\\
i.e., \texttt{\string\Gamma}, \texttt{\string\Psi}, \texttt{\string\Omega}, 
\texttt{\string\gamma}, \texttt{\string\psi}, and the \texttt{\string\t\{xx\}} tie-after accent, respectively.} 


\vspace{1.0\baselineskip}
\centerline{\large\bfseries\scshape Picture and Lasy Fonts Test}
\vspace{0.5\baselineskip}

\def\lasyglyphs{\lasymho, \lasyJoin, \lasyBox,
\lasyDiamond, \lasyleadsto, \lasysqsubset, \lasysqsupset, 
\lasylhd, \lasyunlhd, \lasyrhd, \lasyunrhd}

\setlength{\unitlength}{1cm}

\begin{center}

\begin{picture}(12,2.4)
\qbezier(0,2.4)(2.25,2.8)(4.5,2.4)
\put(1,1.8){\oval(1,1)[tl]}
\put(3.5,1.8){\oval(1,1)[tr]}
\put(2.25,1.9){\oval(4,1)[t]}
\put(.2,1.5){\oval(1,2.2)[l]}
\put(1.5,1.5){\circle*{0.1}}
\put(1.5,1.5){\circle{1.2}}
\put(1.5,1.5){\vector(0,1){0.6}}
\put(1.5,1.5){\vector(1,0){0.6}}
\put(1.5,1.5){\vector(0,-1){0.6}}
\put(1.5,1.5){\vector(-1,0){0.6}}
\thicklines
\qbezier(0,0.6)(2.25,0.2)(4.5,0.6)
\put(0,0){\vector(1,0){11.5}}
\put(3,1.5){\circle*{0.1}}
\put(3,1.5){\circle{1.2}}
\put(3,1.5){\vector(0,1){0.6}}
\put(3,1.5){\vector(1,0){0.6}}
\put(3,1.5){\vector(0,-1){0.6}}
\put(3,1.5){\vector(-1,0){0.6}}
\put(1,1.2){\oval(1,1)[bl]}
\put(3.5,1.2){\oval(1,1)[br]}
\put(2.25,1.1){\oval(4,1)[b]}
\put(4.3,1.5){\oval(1,2.2)[r]}
\put(4.9,2.4){\makebox[0.75in][r]{10pt bold:\hspace{0.1in}}\tenptsize\boldmath$\lasyglyphs$}
\put(4.9,2.0){\makebox[0.75in][r]{10pt:\hspace{0.1in}}\tenptsize$\lasyglyphs$}
\put(4.9,1.65){\makebox[0.75in][r]{\nineptsize9pt:\hspace{0.1in}}\nineptsize$\lasyglyphs$}
\put(4.9,1.35){\makebox[0.75in][r]{\eightptsize8pt:\hspace{0.1in}}\eightptsize$\lasyglyphs$}
\put(4.9,1.1){\makebox[0.75in][r]{\sevenptsize7pt:\hspace{0.1in}}\sevenptsize$\lasyglyphs$}
\put(4.9,0.875){\makebox[0.75in][r]{\sixptsize6pt:\hspace{0.1in}}\sixptsize$\lasyglyphs$}
\put(4.9,0.675){\makebox[0.75in][r]{\fiveptsize5pt:\hspace{0.1in}}\fiveptsize$\lasyglyphs$}
\end{picture}
\end{center}
\end{testflowpage}


% Optional duplex test page
\ifmakeduplextest
\newpage
\begin{testflowpage}
\centerline{\Huge A Test for \LaTeX\ PS/PDF Printing}
\vspace{0.75\baselineskip}
\centerline{\large Version \docversion\hspace{0.2in} Build Code: \docbuildcode}
\vspace{1.0\baselineskip}
\centerline{\large\bfseries\scshape Duplex Alignment Test Side}
\mbox{}\vfill\mbox{}
\end{testflowpage}
\fi

\end{document}




\documentclass[a4paper]{article}

%cognitive dissonance

\usepackage{INTERSPEECH2016}

\usepackage{graphicx}
\usepackage{amssymb,amsmath,bm}
\usepackage{textcomp}
\usepackage{amsfonts}
\usepackage{booktabs}
\usepackage{siunitx}
\usepackage{subcaption}
\usepackage{afterpage}
\usepackage{hyperref}
\usepackage [english]{babel}
\usepackage [autostyle, english = american]{csquotes}
\MakeOuterQuote{"}

%dashed line
\usepackage{array}
\usepackage{arydshln}
\setlength\dashlinedash{0.2pt}
\setlength\dashlinegap{1.5pt}
\setlength\arrayrulewidth{0.3pt}

\def\vec#1{\ensuremath{\bm{{#1}}}}
\def\mat#1{\vec{#1}}
\newcommand{\etal}{\textit{et al.}}

\sloppy % better line breaks
\ninept

\title{Learning Visual Reasoning Without Strong Priors}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% If multiple authors, uncomment and edit the lines shown below.       %%
%% Note that each line must be emphasized {\em } by itself.             %%
%% (by Stephen Martucci, author of spconf.sty).                         %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\makeatletter
%\def\name#1{\gdef\@name{#1\\}}
%\makeatother
%\name{{\em Firstname1 Lastname1, Firstname2 Lastname2, Firstname3 Lastname3,}\\
%      {\em Firstname4 Lastname4, Firstname5 Lastname5, Firstname6 Lastname6,
%      Firstname7 Lastname7}}
%%%%%%%%%%%%%%% End of required multiple authors changes %%%%%%%%%%%%%%%%%

\makeatletter
\def\name#1{\gdef\@name{#1\\}}
\makeatother
\name{{\em Ethan~Perez$^1$$^2$, Harm~de~Vries$^1$, Florian~Strub$^{3}$,}\\ 
     {\em Vincent~Dumoulin$^1$, Aaron~Courville$^1$$^4$}}

\address{$^1$MILA, Universit\'e of Montr\'eal, Canada;
  $^2$Rice University, U.S.A. \\
  $^3$Univ. Lille, CNRS, Centrale Lille, Inria, UMR 9189 CRIStAL France \\
  $^4$CIFAR Fellow, Canada \\
  {\small \tt ethanperez@rice.edu, mail@harmdevries.com, florian.strub@inria.fr}\\
  {\small \tt dumouliv@iro.umontreal.ca, courvila@iro.umontreal.ca}\\
}

% \lhead{Presented as a workshop paper at ICML 2017}

\begin{document}

  \maketitle

  \begin{abstract}
	Achieving artificial visual reasoning --- the ability to answer image-related questions which require a multi-step, high-level process --- is an important step towards artificial general intelligence. This multi-modal task requires learning a question-dependent, structured reasoning process over images from language. Standard deep learning approaches tend to exploit biases in the data rather than learn this underlying structure, while leading methods learn to visually reason successfully but are hand-crafted for reasoning. We show that a general-purpose, Conditional Batch Normalization approach achieves state-of-the-art results on the CLEVR Visual Reasoning benchmark with a 2.4\% error rate. We outperform the next best end-to-end method (4.5\%) and even methods that use extra supervision (3.1\%). We probe our model to shed light on how it reasons, showing it has learned a question-dependent, multi-step process. Previous work has operated under the assumption that visual reasoning calls for a specialized architecture, but we show that a general architecture with proper conditioning can learn to visually reason effectively.
\end{abstract}
\noindent{\bf Index Terms}: Deep Learning, Language and Vision

\vspace{1em}
\noindent{\bf Note}: \textit{A full paper extending this study is available at \url{http://arxiv.org/abs/1709.07871}, with additional references, experiments, and analysis.}

\section{Introduction} \label{introduction}
    The ability to use language to reason about every-day visual input is a fundamental building block of human intelligence. Achieving this capacity to visually reason is thus a meaningful step towards artificial agents that truly understand the world. Advances in both image-based learning and language-based learning using deep neural networks have made huge strides in difficult tasks such as object recognition \cite{NIPS2012_4824,DBLP:journals/corr/HeZRS15} and machine translation \cite{DBLP:journals/corr/ChoMGBSB14,DBLP:journals/corr/SutskeverVL14}. These advances have in turn fueled research on the intersection of visual and linguistic learning \cite{malinowski2014multi,geman2015visual,antol2015,hvries2016,DBLP:journals/corr/JohnsonHMFZG16}.
    
    To this end, \cite{DBLP:journals/corr/JohnsonHMFZG16} recently proposed the CLEVR dataset to test multi-step reasoning from language about images, as traditional visual question-answering datasets such as~\cite{malinowski2014multi,antol2015} ask simpler questions on images that can often be answered in a single glance. Examples from CLEVR are shown in Figure~\ref{fig:CLEVR}. Structured, multi-step reasoning is quite difficult for standard deep learning approaches~\cite{DBLP:journals/corr/JohnsonHMHLZG17,DBLP:journals/corr/SantoroRBMPBL17}, including those successful on traditional visual question answering datasets. Previous work highlights that standard deep learning approaches tend to exploit biases in the data rather than reason~\cite{DBLP:journals/corr/JohnsonHMFZG16,goyal2016making}. To overcome this, recent efforts have built new learning architectures that explicitly model reasoning or relational associations~\cite{DBLP:journals/corr/JohnsonHMHLZG17,DBLP:journals/corr/SantoroRBMPBL17,DBLP:journals/corr/HuARDS17}, some of which even outperform humans~\cite{DBLP:journals/corr/JohnsonHMHLZG17,DBLP:journals/corr/SantoroRBMPBL17}.

    In this paper, we show that a general model can achieve strong visual reasoning from language. We use Conditional Batch Normalization \cite{DBLP:journals/corr/DumoulinSK16,modulating_vision,DBLP:journals/corr/GhiasiLKDS17} with a Recurrent Neural Network (RNN) and a Convolutional Neural Network (CNN) to show that deep learning architectures built without strong priors can learn underlying structure behind visual reasoning, directly from language and images. We demonstrate this by achieving state-of-the-art visual reasoning on CLEVR and finding structured patterns while exploring the internals of our model.

\begin{figure}[t]
	\centering
    \begin{subfigure}[t]{.2\textwidth}
        \centering
        \includegraphics[height=0.8in]{CLEVR_val_000008.png}
        \caption{\it{What number of cylinders are small purple things or yellow rubber things?\\
        \bf{Predicted: 2}}}
    \end{subfigure}
    ~ 
    \begin{subfigure}[t]{.2\textwidth}
		\centering
        \includegraphics[height=0.8in]{CLEVR_val_000007.png}
        \caption{\it{What color is the other object that is the same shape as the large brown matte thing?\\
        \bf{Predicted: Brown}}}
    \end{subfigure}
    \caption{Examples from CLEVR and our model's answer.}
    \label{fig:CLEVR}
    \vskip -1em
\end{figure}

\section{Method} \label{method}

	Our model processes the multi-modal question-image input using a RNN and CNN combined via Conditional Batch Normalization (CBN). CBN has proven highly effective for image stylization \cite{DBLP:journals/corr/DumoulinSK16,DBLP:journals/corr/GhiasiLKDS17}, speech recognition \cite{DynamicLayerNorm}, and traditional visual question answering tasks \cite{modulating_vision}. We start by explaining CBN in Section~\ref{cbn} and then describe our model in Section~\ref{model}.

	\subsection{Conditional batch normalization}
		\label{cbn}

	Batch normalization (BN) is a widely used technique to improve neural network training by normalizing activations throughout the network with respect to each mini-batch. BN has been shown to accelerate training and improve generalization by reducing covariate shift throughout the network~\cite{DBLP:journals/corr/IoffeS15}. To explain BN, we define $\mathcal{B} = \{\bm{F}_{i,.,.,.}\}_{i=1}^N$ as a mini-batch of $N$ samples, where $\bm{F}$ corresponds to input feature maps whose subscripts $c,h,w$ refers to the $c^{th}$ feature map at the spatial location $(h,w)$. We also define $\gamma_c$ and $\beta_c$ as per-channel, trainable scalars and $\epsilon$ as a constant damping factor for numerical stability. BN is defined at training time as follows:
\begin{equation}
BN(\bm{F}_{i,c,h,w} | \gamma_c, \beta_c) = \gamma_c\frac{\bm{F}_{i, c, w, h} - \mathbb{E}_{\mathcal{B}}[\bm{F}_{\cdot, c, \cdot, \cdot}]}{\sqrt{\text{Var}_{\mathcal{B}}[\bm{F}_{\cdot, c, \cdot, \cdot}]+\epsilon}} + \beta_c.
\end{equation}

Conditional Batch Normalization (CBN)~\cite{DBLP:journals/corr/DumoulinSK16,modulating_vision,DBLP:journals/corr/GhiasiLKDS17} instead learns to output new BN parameters $\hat{\gamma}_{i,c}$ and $\hat{\beta}_{i,c}$ as a function of some input $\bm{x_i}$:
    \begin{align}
		\hat{\gamma}_{i,c} = f_c(\bm{x}_i) \qquad ~ \qquad 
		\hat{\beta}_{i,c} = h_c(\bm{x}_i),
	\end{align}
where $f$ and $h$ are arbitrary functions such as neural networks. Thus, $f$ and $h$ can learn to control the distribution of CNN activations based on $\bm{x_i}$.

	Combined with ReLU non-linearities, CBN empowers a conditioning model to manipulate feature maps of a target CNN by scaling them up or down, negating them, shutting them off, selectively thresholding them, and more. Each feature map is modulated independently, giving the conditioning model an exponential (in the number of feature maps) number of ways to affect the feature representation.
    
    Rather than output $\hat{\gamma}_{i,c}$ directly, we output $\Delta\hat{\gamma}_{i,c}$, where:
\begin{equation}
	\hat{\gamma}_{i,c}=1+\Delta\hat{\gamma}_{i,c},
\end{equation}
since initially zero-centered $\hat{\gamma}_{i,c}$ can zero out CNN feature map activations and thus gradients. In our implementation, we opt to output $\Delta\hat{\gamma}_{i,c}$ rather than $\hat{\gamma}_{i,c}$, but for simplicity, in the rest of this paper, we will explain our method using $\hat{\gamma}_{i,c}$.

	\subsection{Model}
		\label{model}

        \begin{figure}[t]
            \centering
            \includegraphics[width=0.9\linewidth]{updated_model.png}
	            \caption{\it{The linguistic pipeline (left), visual pipeline (middle), and CBN residual block architecture (right) of our model.}}
            \label{fig:model}
            \vskip -1em
        \end{figure}

		Our model consists of a linguistic pipeline and a visual pipeline as depicted in Figure~\ref{fig:model}. The linguistic pipeline processes a question $q$ using a Gated Recurrent Unit (GRU)~\cite{DBLP:journals/corr/ChungGCB14} with 4096 hidden units that takes in learned, 200-dimensional word embeddings. The final GRU hidden state is a question embedding $\bm{e}_q$. From this embedding, the model predicts the CBN parameters $(\bm{\gamma}^{m,n}_{i,\cdot}, \bm{\beta}^{m,n}_{i,\cdot})$ for the $n^{th}$ CBN layer of the $m^{th}$ residual block via linear projection with a trainable weight matrix $\bm{W}$ and bias vector $\bm{b}$:
        \begin{equation}
        (\bm{\gamma}^{m,n}_{i,\cdot}, \bm{\beta}^{m,n}_{i,\cdot}) = \bm{W}^{m,n}\bm{e}_q + \bm{b}^{m,n}
        \end{equation}

        The visual pipeline extracts $14\times14$ image features using the \textit{conv4} layer of a ResNet-101~\cite{DBLP:journals/corr/HeZRS15} pre-trained on ImageNet \cite{DBLP:journals/corr/RussakovskyDSKSMHKKBBF14}, as done in \cite{DBLP:journals/corr/JohnsonHMHLZG17} for CLEVR. Image features are processed by a $3\times3$ convolution followed by several --- 3 for our model --- CBN residual blocks with 128 feature maps, and a final classifier. The classifier consists of a $1\times1$ convolution to 512 feature maps, global max-pooling, and a two-layer MLP with 1024 hidden units that outputs a distribution over final answers.

        Each CBN residual block starts with a $1\times1$ convolution followed by two $3\times3$ convolutions with CBN as depicted in Figure~\ref{fig:model}. Drawing from~\cite{DBLP:journals/corr/SantoroRBMPBL17,DBLP:journals/corr/WattersTWPBZ17}, we concatenate coordinate feature maps indicating relative spatial position (scaled from $-1$ to $1$) to the image features, each residual block's input, and the classifier's input. We train our model end-to-end from scratch with Adam (learning rate $3e^{-4}$) \cite{DBLP:journals/corr/KingmaB14}, early stopping on the validation set, weight decay ($1e^{-5}$), batch size 64, and BN and ReLU throughout the visual pipeline, using only image-question-answer triplets from the training set.

\section{Experiments} \label{experiments}

	\subsection{CLEVR dataset}
    	CLEVR is a generated dataset of 700K (image, question, answer, program) tuples. Images contain 3D-rendered objects of various shapes, materials, colors, and sizes. Questions are multi-step and compositional in nature, as shown in Figure~\ref{fig:CLEVR}. They range from counting questions (\emph{"How many green objects have the same size as the green metallic block?"}) to comparison questions (\emph{"Are there fewer tiny yellow cylinders than yellow metal cubes?"}) and can be 40+ words long. Answers are each one word from a set of $28$ possible answers. Programs are an additional supervisory signal consisting of step-by-step instructions, such as \texttt{filter\_shape[cube]}, \texttt{relate[right]}, and \texttt{count}, on how to answer the question. Program labels are difficult to generate or come by for real world datasets. Our model avoids using this extra supervision, learning to reason effectively directly from linguistic and visual input.

	\subsection{Results}
        \begin{table*}[ht!]
        \centering
        
        \vspace{-1em}
        {\small
        \begin{tabular}{l|c|cccccc}
        \toprule
        {Model} & {\textbf{Overall}} & {Count} & {Exist} & \begin{tabular}{@{}c@{}}Compare \\ Numbers\end{tabular} & \begin{tabular}{@{}c@{}}Query \\ Attribute\end{tabular} & \begin{tabular}{@{}c@{}}Compare \\ Attribute\end{tabular}\\
        \midrule
        Human \cite{DBLP:journals/corr/JohnsonHMHLZG17}                   & 92.6 &86.7 &96.6 &86.5 &95.0 &96.0\\
        \midrule
        Q-type baseline \cite{DBLP:journals/corr/JohnsonHMHLZG17}         &41.8 &34.6 &50.2 &51.0 &36.0 &51.3\\
        LSTM \cite{DBLP:journals/corr/JohnsonHMHLZG17}                    &46.8 &41.7 &61.1 &69.8 &36.8 &51.8\\
        CNN+LSTM \cite{DBLP:journals/corr/JohnsonHMHLZG17}                &52.3 &43.7 &65.2 &67.1 &49.3 &53.0\\
        CNN+LSTM+SA \cite{DBLP:journals/corr/SantoroRBMPBL17}             &76.6 &64.4 &82.7 &77.4 &82.6 &75.4\\
        N2NMN* \cite{DBLP:journals/corr/HuARDS17}                         &83.7 &68.5 &85.7 &84.9 &90.0 &88.7\\
        PG+EE (9K prog.)* \cite{DBLP:journals/corr/JohnsonHMHLZG17}       &88.6 &79.7 &89.7 &79.1 &92.6 &96.0\\
        PG+EE (700K prog.)* \cite{DBLP:journals/corr/JohnsonHMHLZG17}     &96.9 &92.7 &97.1 &\bf{98.7} &98.1 &98.9\\
        CNN+LSTM+RN\textdagger \cite{DBLP:journals/corr/SantoroRBMPBL17}  &95.5 &90.1 &97.8 & 93.6 &97.9 &97.1\\
        \midrule
%         Ours &\bf{97.0} &\bf{93.7} &\bf{99.1} &90.7 &\bf{99.1} &\bf{98.9}\\
        CNN+GRU+CBN &\bf{97.6} &\bf{94.5} &\bf{99.2} &93.8 &\bf{99.2} &\bf{99.0}\\
        \bottomrule
        \end{tabular}
        }
        \caption{\it{\label{tab:results}
        CLEVR accuracy by baseline methods, competing methods, and our method (CBN). Methods denoted with (*) use extra supervisory information through program labels. Methods denoted with (\textdagger) use data augmentation and no pre-trained CNN.}}
        \end{table*}
		Our results on CLEVR are shown in Table~\ref{tab:results}. Our model achieves a new overall state-of-the-art, outperforming humans and previous, leading models, which often use additional program supervision. Notably, CBN outperforms Stacked Attention networks (CNN+LSTM+SA in \ref{tab:results}) by 21.0\%. Stacked Attention networks are highly effective for visual question answering with simpler questions~\cite{DBLP:journals/corr/YangHGDS15} and are the previously leading model for visual reasoning that does not build in reasoning, making them a relevant baseline for CBN. We note also that our model's pattern of performance more closely resembles that of humans than other models do. Strong performance ($<1\%$ error) in \texttt{exist} and \texttt{query\_attribute} categories is perhaps explained by our model's close resemblance to standard CNNs, which traditionally excel at these classification-type tasks. Our model also demonstrates strong performance on more complex categories such as \texttt{count} and \texttt{compare\_attribute}.
        
        Comparing numbers of objects gives our model more difficulty, understandably so; this question type requires more high-level reasoning steps --- querying attributes, counting, and comparing --- than other question type. The best model from \cite{DBLP:journals/corr/JohnsonHMHLZG17} beats our model here but is trained with extra supervision via 700K program labels. As shown in Table~\ref{tab:results}, the equivalent, more comparable model from \cite{DBLP:journals/corr/JohnsonHMHLZG17} which uses 9K program labels significantly underperforms our method in this category.

	\subsection{What does conditional batch norm learn?}

    	To understand what our model learns, we use t-SNE~\cite{maaten2008visualizing} to visualize the CBN parameter vectors $(\bm{\gamma}, \bm{\beta})$, of 2,000 random validation points, modulating first and last CBN layers in our model, as shown in Figure~\ref{fig:tsne}. The $(\bm{\gamma}, \bm{\beta})$ parameters of the first and last CBN layers are grouped by the low-level and high-level reasoning functions necessary to answer CLEVR questions, respectively. For example, the CBN parameters for \texttt{equal\_color} and \texttt{query\_color} are close for the first layer but apart for the last layer, and the same is true for \texttt{equal\_shape} and \texttt{query\_shape}, \texttt{equal\_size} and \texttt{query\_size}, and \texttt{equal\_material} and \texttt{query\_material}. Conversely, \texttt{equal\_shape}, \texttt{equal\_size}, and \texttt{equal\_material} CBN parameters are grouped in the last layer but split in the first layer. Similar patterns emerge when visualizing residual block activations. Thus, we see that CBN learns a sort of function-based modularity, directly from language and image inputs and without an architectural prior on modularity. Simply with end-to-end training, our model learns to handle not only different types of questions differently, but also different types of question sub-parts differently, working from low-level to high-level processes as is the proper approach to answer CLEVR questions.

        Additionally, we observe that many points that break the previously mentioned clustering patterns do so in meaningful ways. For example, Figure~\ref{fig:tsne} shows that some \texttt{count} questions have last layer CBN parameters far from those of other \texttt{count} questions but close to those of \texttt{exist} questions. Closer examination reveals that these \texttt{count} questions have answers of either 0 or 1, making them similar to \texttt{exist} questions.
    
	\subsection{Error analysis}
		An analysis of our model's errors reveals that 94\% of its counting mistakes are off-by-one errors, indicating our model has learned underlying concepts behind counting, such as close relationships between close numbers.
        
        As shown in Figure~\ref{fig:errorrate}, our CBN model struggles more on questions that require more steps, as indicated by the length of the corresponding CLEVR programs; error rates for questions requiring 10 or fewer steps are around $1.5\%$, while error rates for questions requiring 17 or more steps are around $5.5\%$, more than three times higher.

    	Furthermore, the model sometimes makes curious reasoning mistakes a human would not. In Figure~\ref{fig:failure}, we show an example where our model correctly counts two cyan objects and two yellow objects but simultaneously does not answer that there are the same number of cyan and yellow objects. In fact, it does not answer that the number of cyan blocks is more, less, \textit{or} equal to the number of yellow blocks. These errors could be prevented by directly minimizing logical inconsistency, which is an interesting avenue for future work orthogonal to our approach.
        
        These types of mistakes in a state-of-the-art visual reasoning model suggest that more work is needed to truly achieve human-like reasoning and logical consistency. We view CLEVR as a curriculum of tasks and believe that the key to the most meaningful and advanced reasoning lies in tackling these last few percentage points of error.

		\begin{figure}[t]
            \centering
            \includegraphics[width=1\linewidth]{program_failure.png}
            \caption{\it{Validation error rate by program length.}}
             \label{fig:errorrate}
            \vskip -1em
        \end{figure}

        \begin{figure*}[th]
        \centering
        \includegraphics[width=0.9\linewidth]{250_tsne_block.png}
        \caption{\it{t-SNE plots of  $\bm{\gamma}$, $\bm{\beta}$  of the first BN layer of the first residual block (left) and the last BN layer of the last residual block (right). CBN parameters are grouped by low-level reasoning functions for the first layer and by high-level reasoning functions for the last layer.}
        }
        \label{fig:tsne}
        \end{figure*}

        \begin{figure}[ht]
        \center
            \includegraphics[width=0.7\linewidth]{CLEVR_val_000001.png}

        \vskip 1em
        \scriptsize
        \begin{tabular}{|l|l|}
        \hline
        Question & Answer\\
        \hline
        How many yellow things are there? & 2\\
        How many cyan things are there? & 2\\
        Are there as many yellow things as cyan things? & \bf{No}\\
        Are there more yellow things than cyan things? & No\\
        Are there fewer yellow things than cyan things? & No\\
        \hline
        \end{tabular}
        \caption{\it{An interesting failure example where our model counts correctly but compares counts erroneously. Its third answer is incorrect and inconsistent with its other answers.}}
        \label{fig:failure}
        \vskip -1em
        \end{figure}

\section{Related Work} \label{relatedwork}

	One leading approach for visual reasoning is the Program Generator + Execution Engine model from \cite{DBLP:journals/corr/JohnsonHMHLZG17}. This approach consists of a sequence-to-sequence Program Generator (PG), which takes in a question and outputs a sequence corresponding to a tree of composable Neural Modules, each of which is a two-layer residual block similar to ours. This tree of Neural Modules is assembled to form the Execution Engine (EE) that then predicts an answer from the image. The PG+EE model uses a strong prior by training with program labels and explicitly modeling the compositional nature of reasoning. Our approach learns to reason directly from textual input without using additional cues or a specialized architecture.

	This modular approach is part of a recent line of work in Neural Module Networks \cite{DBLP:journals/corr/HuARDS17,DBLP:journals/corr/AndreasRDK15,DBLP:journals/corr/AndreasRDK16}. Of these, End-to-End Module Networks (N2NMN) \cite{DBLP:journals/corr/HuARDS17} also tackle visual reasoning but do not perform as well as other approaches. These methods also use strong priors by modeling the compositionality of reasoning, using program-level supervision, and building per-module, hand-crafted neural architectures for specific functions.\\

	Relation Networks (RNs) from \cite{DBLP:journals/corr/SantoroRBMPBL17} are another leading approach for visual reasoning. RNs use an MLP to carry out pairwise comparisons over each location of extracted convolutional features over an image, including LSTM-extracted question features as input to this MLP. RNs then element-wise sum over the resulting comparison vectors to form another vector from which a final classifier predicts the answer. This approach is end-to-end differentiable and trainable from scratch to high performance, as we show in Table~\ref{tab:results}. Our approach lifts the explicitly relational aspect of this model, freeing our approach from the use of a comparison-based prior, as well as the scaling difficulties of pairwise comparisons over spatial locations. 

	CBN itself has its own line of work. The results of \cite{DBLP:journals/corr/DumoulinSK16,DBLP:journals/corr/GhiasiLKDS17} show that the closely related Conditional Instance Normalization is able to successfully modulate a convolutional style-transfer network to quickly and scalably render an image in a huge variety of different styles, simply by learning to output a different set of BN parameters based on target style. For visual question answering, answering general questions often of natural images, de Vries \etal~\cite{modulating_vision} show that CBN performs highly on real-world VQA and GuessWhat?! datasets, demonstrating CBN's effectiveness beyond the simpler CLEVR images. Their architecture conditions 50 BN layers of a pre-trained ResNet. We show that a few layers of CBN after a ResNet can also be highly effective, even for complex problems. We also show how CBN models can learn to carry out multi-step processes and reason in a structured way --- from low-level to high-level.

	Additionally, CBN is essentially a post-BN, feature-wise affine conditioning, with BN's trainable scalars turned off. Thus, there are many interesting connections with other conditioning methods. A common approach, used for example in Conditional DCGANs~\cite{DBLP:journals/corr/RadfordMC15}, is to concatenate constant feature maps of conditioning information to the input of convolutional layers, which amounts to adding a post-convolutional, feature-wise conditional bias. Other approaches, such as LSTMs~\cite{LSTM} and Hierarchical Mixtures of Experts~\cite{HME}, gate an input's features as a function of that same input (rather than a separate, conditioning input), which amounts to a feature-wise, conditional scaling, restricted to between 0 and 1. CBN consists of both scaling and shifting, each unrestricted, giving it more capacity than many of these related approaches. We leave exploring these connections more in-depth for future work.

\section{Conclusion} \label{conclusions}

	With a simple and general model based on CBN, we show it is possible to achieve state-of-the-art visual reasoning on CLEVR without explicitly incorporating reasoning priors. We show that our model learns an underlying structure required to answer CLEVR questions by finding clusters in the CBN parameters of our model; earlier parameters are grouped by low-level reasoning functions while later parameters are grouped by high-level reasoning functions. Simply by manipulating feature maps with CBN, a RNN can effectively use language to influence a CNN to carry out diverse and multi-step reasoning tasks over an image. It is unclear whether CBN is the most effective general way to use conditioning information for visual reasoning or other tasks, as well as what precisely about CBN is so effective. Other approaches \cite{DBLP:journals/corr/RadfordMC15, LSTM, HME, DBLP:journals/corr/OordDZSVGKSK16,van2016conditional,DBLP:journals/corr/ReedOKCWBF17,reed2016generating} employ a similar, repetitive conditioning, so perhaps there is an underlying principle that explains the success of these approaches. Regardless, we believe that CBN is a general and powerful technique for multi-modal and conditional tasks, especially where more complex structure is involved.

  \section{Acknowledgements}

    We would like to thank the developers of PyTorch (\url{http://pytorch.org/}) for their elegant deep learning framework. Also, our implementation was based off the open-source code from \cite{DBLP:journals/corr/JohnsonHMHLZG17}. We thank Mohammad Pezeshki, Dzmitry Bahdanau, Yoshua Bengio, Nando de Freitas, Joelle Pineau, Olivier Pietquin, J\'er\'emie Mary, Chin-Wei Huang, Layla Asri, and Max Smith for helpful feedback and discussions, as well as Justin Johnson for CLEVR test set evaluations. We thank NVIDIA for donating a DGX-1 computer used in this work. We also acknowledge FRQNT through the CHIST-ERA IGLU project and CPER Nord-Pas de Calais and FEDER DATA Advanced data science and technologies 2015-2020 for funding our research.
    
%   \newpage
 
  \eightpt
  \bibliographystyle{IEEEtran}
  \bibliography{mybib}

\end{document}



\documentclass[10pt,twocolumn,letterpaper]{article}
\pdfoutput=1

\usepackage{cvpr}
%

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{soul}

%
\usepackage{url}
\usepackage{amsfonts,bm}
\usepackage{epstopdf}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{array}
\usepackage{booktabs,siunitx}
\usepackage{footmisc}

\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks]{hyperref}
%
\usepackage{multibib} %
\newcites{sup}{Supplementary References}

%

\newcommand{\Ctext}{C_o} %

\usepackage{algorithmic,algorithm}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

%
\newcommand{\new}[1]{\textcolor{red}{#1}}
\newcommand{\newer}[1]{\textcolor{cyan}{#1}}
%
%
%
%


\newcommand{\jb}[1]{\textcolor{red}{JB: #1}}

%
\hypersetup{
  plainpages=false,
  pdfpagemode=UseNone, %
  %
  colorlinks=true,   %
  linkcolor=blue,    %
  anchorcolor=blue,  %
  citecolor=blue,    %
  filecolor=blue,    %
  pagecolor=blue,    %
  urlcolor=blue,     %
  pdfview=FitH,               %
  pdfstartview=FitH,          %
  pdfpagelayout=SinglePage    %
}
\newcommand{\SLJ}[1]{\textcolor{red}{SLJ: #1}}

\renewcommand{\baselinestretch}{0.96}

\cvprfinalcopy %

\def\cvprPaperID{1383} %
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

%
\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

%
\title{Unsupervised Learning from Narrated Instruction Videos}

\author{
Jean-Baptiste Alayrac\thanks{WILLOW project-team, D\'{e}partement d'Informatique de l'Ecole Normale Sup\'{e}rieure, ENS/INRIA/CNRS UMR 8548, Paris, France.} \ \thanks{SIERRA project-team, D\'epartement d'Informatique de l'Ecole Normale Sup\'{e}rieure, ENS/INRIA/CNRS UMR 8548, Paris, France.}
\and
Piotr Bojanowski\footnotemark[1]
\and
Nishant Agrawal \footnotemark[1] \ \thanks{IIIT Hyderabad} 
\and
Josef Sivic\footnotemark[1]
\and
Ivan Laptev\footnotemark[1] 
\and
Simon Lacoste-Julien\footnotemark[2] 
}

\maketitle

%
\begin{abstract}
   We address the problem of automatically learning the main steps to complete a certain task, such as changing a car tire, 
from a set of narrated instruction videos. 
%
The contributions of this paper are three-fold.
First, we develop a new unsupervised learning approach that takes advantage of the complementary nature of the input video and the associated narration.   
The method solves two clustering problems, one in text and one in video, applied one after each other and linked by joint constraints to obtain a single coherent sequence of steps in both modalities.  
Second, we collect and annotate a new challenging dataset of real-world instruction videos from the Internet. The dataset contains about 800,000 frames for five different tasks\footnote{How to : change a car tire, perform CardioPulmonary resuscitation (CPR), jump a car, repot a plant and make coffee} that include complex interactions between people and objects, and are captured in a variety of indoor and outdoor settings.	
Third, we experimentally demonstrate that the proposed method can automatically discover, in an \emph{unsupervised manner}, the main steps to achieve the task and locate the steps in the input videos. 
%
%
%
%
%
\end{abstract}

%
%
\section{Introduction}
Millions of people watch narrated instruction videos\footnote{Some instruction videos on YouTube have tens of millions of views, 
e.g.~\url{www.youtube.com/watch?v=J4-GRH2nDvw}.} to learn new tasks such as assembling IKEA furniture or changing a flat car tire. 
Many of such tasks have large amounts of videos available on-line. For example, querying for ``how to change a tire'' results in more than 300,000 hits on YouTube.
Most of these videos, however, are made with the intention to teach other people to perform the task and do not provide direct supervisory signal for automatic learning algorithms. Developing unsupervised methods that could learn tasks from myriads of instruction videos on the Internet is therefore a key challenge.
%
Such automatic cognitive ability would enable constructing virtual assistants and smart robots that learn new skills from the Internet to, for example, help people achieve new tasks in unfamiliar situations. 

%
%
%
%
%




%

In this work, we consider instruction videos and develop a method that learns a sequence of steps, as well as their textual and visual representations, required to achieve a certain task. 
For example, given a set of narrated instruction videos demonstrating how to change a car tire, our method automatically discovers consecutive steps for this task such as {\em loosen the nuts of the wheel}, {\em jack up the car}, {\em remove the spare tire} and so on as illustrated in Figure~\ref{fig:mainpaper}. 
In addition, the method learns the visual and linguistic variability of these steps from natural videos.
%

 \begin{figure*}[t]
       \centering
       \includegraphics[width=\linewidth]{figs/teaserfigv2.pdf}
     \caption{\small Given a set of narrated instruction videos demonstrating a particular task, we wish to automatically discover the main steps to achieve the task and associate each step with its corresponding narration and appearance in each video. Here frames from two videos demonstrating changing the car tire are shown, together with excerpts of the corresponding narrations.  Note the large variations in both the narration and appearance of the different steps highlighted by the same colors in both videos (here only three steps are shown).}
     \vspace{-4mm}
     %
     \label{fig:mainpaper}
 \end{figure*}


Discovering key steps from instruction videos is a highly challenging task.
%
%
%
First, linguistic expressions for the same step can have high variability across videos, for example: ``...Loosen up the wheel nut just a little before you start jacking the car...'' and ``...Start to loosen the lug nuts just enough to make them easy to turn by hand...''.
Second, the visual appearance of each step varies greatly between videos as the people and objects are different, the action is captured from a different viewpoint, and the way people perform actions also vary.
Finally, there is also a variability of the overall structure of the sequence of steps achieving the task. For example, 
some videos may omit some steps or change slightly their order.

To address these challenges, in this paper we develop an unsupervised learning approach that takes advantage of the complementarity of the 
visual signal in the video and the corresponding natural language narration to resolve their ambiguities. 
%
We assume that the same ordered sequence of steps (also called script in the NLP literature~\cite{Regneri10learning}) is common to all input videos of the same task, but the actual sequence and the individual steps are unknown and are learnt directly from data. 
This is in contrast to other existing methods for modeling instruction videos~\cite{Malmaud15what} that assume a script (recipe) is known and fixed in advance. 
We address the problem by first performing temporal clustering of text followed by clustering in video, where the two clustering tasks are linked by joint constraints. 
The complementary nature of the two clustering problems helps to resolve ambiguities in the two individual modalities. For example, two video segments with very different appearance but depicting the same step can be grouped together  because they are narrated in a similar language.
Conversely, two video segments described with very different expressions, for example, ``jack up the car'' and ``raise the vehicle'' can be identified as belonging to the same instruction step because they have similar visual appearance.
The output of our method is the script listing the discovered steps of the task as well as the temporal location of each step in the input videos. 
We validate our method on a new dataset of instruction videos composed of five different tasks with a total of 150 videos and about 800,000 frames.

%
%
%
%
%
%
%

%

\section{Related work}

This work relates to unsupervised and weakly-supervised learning methods in computer vision and natural language processing.
Particularly related to ours is the work on learning script-like knowledge from natural language descriptions~\cite{Chambers08,Frermann14,Regneri10learning}. These methods aim to discover typical events (steps) and their order for particular scenarios (tasks)\footnote{We here assign the same meaning to terms ``event'' and ``step'' as well as to terms ``script'' and ``task''.} such as ``cooking scrambled egg'', ``taking a bus'' or ``making coffee''. While~\cite{Chambers08} uses large-scale news copora,~\cite{Regneri10learning} argues that many events are implicit and are not described in such general-purpose text data. Instead,~\cite{Frermann14,Regneri10learning} use event sequence descriptions collected for particular scenarios. Differently to this work, we learn sequences of events from narrated instruction videos on the Internet. Such data contains detailed event descriptions but is not structured and contains more noise compared to the input of~\cite{Frermann14,Regneri10learning}. 
%
%
%

Interpretation of narrated instruction videos has been recently addressed in~\cite{Malmaud15what}. While this work analyses cooking videos at a great scale, it relies on readily-available recipes which may not be available for more general scenarios. Differently from~\cite{Malmaud15what}, we here aim to learn the steps of instruction videos using a discriminative clustering approach.
A similar task to ours is addressed in~\cite{Naim15discriminative} using latent variable structured perceptron algorithm to align nouns in instruction sentences with objects touched by hands in instruction videos.
%
However, similarly to~\cite{Malmaud15what}, \cite{Naim15discriminative} uses laboratory experimental protocols as textual input, whereas here we consider a weaker signal in the form of the real transcribed narration of the video.


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

In computer vision, unsupervised action recognition has been explored in simple videos~\cite{Niebles08}. 
More recently, weakly supervised learning of actions in video using video scripts or event order has been addressed in~\cite{Bojanowski13finding,Bojanowski14weakly,Bojanowski15weakly,Duchenne2009automatic,Laptev08a}. 
Particularly related to ours is the work~\cite{Bojanowski14weakly} which explores the known order of events to localize and learn actions in training data. 
While~\cite{Bojanowski14weakly} uses manually annotated sequences of events, we here discover the sequences of main events by clustering  transcribed narrations of the videos. Related is also the work of
\cite{Bojanowski15weakly} that aligns natural text descriptions to video but in contrast to our approach does not discover automatically the common sequence of main steps. 
Methods in~\cite{Niebles10a,Raptis13} learn in an unsupervised manner the temporal structure of actions from video but do not discover textual expressions for actions as we do in this work.
The recent concurrent work~\cite{Sener15unsupervised} is addressing, independently of our work, a similar problem but with a different approach based on a probabilistic generative model and considering a different set of tasks mainly focussed on cooking activities.
%
%

%

Our work is also related to video summarization and in particular to the recent work on category-specific video summarization \cite{Potapov14category,Sun14ranking}. While summarization is a subjective task, we here aim to extract the key steps required to achieve a concrete task that  consistently appear in the same sequence in the input set of videos. In addition, unlike video summarization~\cite{Potapov14category,Sun14ranking} we jointly exploit visual and linguistic modalities in our approach.

%
%
%

%
%
%
%
%

%


\vspace{3mm}
%
\section{New dataset of instruction videos}
\label{sec:dataset}
%
%
We have collected a dataset of narrated instruction videos for five tasks: \textit{Making a coffee}, \textit{Changing car tire}, \textit{Performing cardiopulmonary resuscitation (CPR)}, \textit{Jumping a car} and \textit{Repotting a plant}. 
The videos were obtained by searching YouTube with relevant keywords. 
The five tasks were chosen so that they have a large number of available videos with English transcripts while trying to cover a wide range of activities that include complex interactions of people with objects and other people.
For each task, we took the top 30 videos with English ASR returned by YouTube.
We also quickly verified that each video contains a person actually performing the task (as opposed to just talking about it).
The result is a total of 150 videos, 30 videos for each task.
%
The average length of our videos is about 4,000 frames (or 2 minutes) and the entire dataset contains about 800,000 frames.

%
%
The selected videos have English transcripts obtained from YouTube's automatic speech recognition (ASR) system.
To remove the dependence of results on errors of the particular ASR method, we have manually corrected misspellings and punctuations in the output transcriptions.
We believe this step will soon become obsolete given rapid improvements of ASR methods. 
As we do not modify the content of the spoken language in videos, the transcribed verbal instructions still represent an extremely challenging example of natural language with large variability in the used expressions and terminology.
%
Each word of the transcript is associated with a time interval in the video (usually less than 5 seconds) obtained from the closed caption timings.

%
%
%
For the purpose of evaluation, we have manually annotated the temporal location in each video of the main steps necessary to achieve the given task.
For all tasks, we have defined the ordered sequence of ground truth steps before running our algorithm.
The choice of steps was made by an agreement of 2-3 annotators who have watched the input videos and verified the steps on instruction video websites such as \url{http://www.howdini.com}.
 While some steps can be occasionally left out in some videos or the ordering slightly modified, overall we have observed a good consistency 
 in the given sequence of instructions among the input videos.
We measured that only 6\% of the step annotations did not fit the global order, while a step was missing from the video 27\% of the time.\footnote{We describe these measurements in more details in the supplementary material given in Appendix~\ref{subsec:score_dataset}.}
%
%
We hypothesize that this could be attributed to the fact that all videos are made with the same goal of giving other humans clear, concise and comprehensible verbal and visual instructions on how to achieve the given task.  
Given the list of steps for each task, we have manually annotated each time interval in each input video to one of the ground truth steps (or no step).
%
%
%
%
%
%
%
%
%
%
The actions of the individual steps are typically separated by hundreds of frames where the narrator transitions between the steps or explains verbally what is going to happen.  
Furthermore, some steps could be missing in some videos, or could be present but not described in the narration.  Finally, the temporal alignment between the narration and the actual actions in video is only coarse as the action is often described before it is performed. 

%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%

\begin{figure*}[t]
\includegraphics[width=1.02\linewidth]{figs/reduced_fig2update.pdf}
   \caption{\small {\bf Clustering transcribed verbal instructions.} {\bf Left:} The input raw text for each video is converted into a sequence of direct object relations. Here, an illustration of four sequences from four different videos is shown.  {\bf Middle:} Multiple sequence alignment is used to align all sequences together. Note that different direct object relations are aligned together as long as they have the same sense, e.g. ``loosen nut" and ``undo bolt".  {\bf Right:} The main instruction steps are extracted as the $K=3$ most common steps in all the sequences. \vspace{-.3cm}
   }
   \label{tab:MSAillustration}
   \vspace*{-0.1cm}
\end{figure*}

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\section{Modelling narrated instruction videos}
\label{subsec:model_var}

We are given a set of $N$ instruction videos all depicting the same task (such as ``changing a tire'').
The $n$-th input video is composed of a video stream of $T_n$ segments of frames $(x^n_t)_{t=1}^{T_n}$ and an audio stream containing a detailed verbal description of the depicted task. 
We suppose that the audio description was transcribed to raw text and then processed to a sequence of $S_n$ text tokens $(d^n_s)_{s=1}^{S_n}$. 
Given this data, we want to automatically recover the sequence of $K$ main steps that compose the given task and locate each step within each input video and text transcription. 

%
 We formulate the problem as two clustering tasks, one in text and one in video, applied one after each other and linked by joint constraints linking the two modalities.
  This two-stage approach is based on the intuition that the variation in natural language describing each task is easier to capture than the visual variability of the input videos. 
 In the first stage, we cluster the text transcripts into a sequence of $K$ main steps to complete the given task.  Empirically, we have found (see results in Sec.~\ref{sec:step_discovery}) that it is possible to discover the sequence of the $K$ main steps for each task with high precision. However, the text itself gives only a poor localization of each step in each video. 
 Therefore, in the second stage we accurately localize each step in each video by clustering the input videos using the sequence of $K$ steps extracted from text as constraints on the video clustering. 
 %
%
%
%
%
%
To achieve this, we use two types of constraints between video and text.  First, we assume that both the video and the text narration follow the same sequence of steps. This results in a global ordering constraint on the recovered clustering. Second, we assume that people perform the action approximately at the same time that they talk about it. This constraint temporally links the recovered clusters in text and video. %
%
%
The important outcome of the video clustering stage is that the $K$ extracted steps get propagated by visual similarity to videos where the text descriptions are missing or ambiguous. 

We first describe the text clustering in Sec.~\ref{subsec:model_text} and then introduce the video clustering with constraints in Sec.~\ref{sec:model-video}.
%
%
%
%

%
%
%


%
%
%



%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\subsection{Clustering transcribed verbal instructions}
\label{subsec:model_text}
\label{sec:text}


The goal here is to cluster the transcribed verbal descriptions of each video into a sequence of \emph{main steps} necessary to achieve the task.  
This stage is important as the resulting clusters will be used as constraints for jointly learning and localizing the main steps in video. 
We assume that the important steps are common to many of the transcripts and that the sequence of steps is (roughly) preserved in all transcripts.  
Hence, following~\cite{Regneri10learning}, we formulate the problem of clustering the input transcripts as a multiple sequence alignment problem.
However, in contrast to~\cite{Regneri10learning} who cluster manually provided descriptions of each step, we wish to cluster transcribed verbal instructions.
Hence our main challenge is to deal with the variability in spoken natural language.  To overcome this challenge, we take advantage of the fact that completing a certain task usually involves interactions with objects or people and hence we can extract a more structured representation from the input text stream.      

More specifically, we represent the textual data as a sequence of {\em direct object relations}.
A direct object relation~$d$  is a pair composed of a verb and its direct object complement, such as ``remove tire".
Such a direct object relation can be extracted from the dependency parser of the input transcribed narration~\cite{Marneffe06generating}.
%
We denote the set of all different direct object relations extracted from all narrations as $\mathcal{D}$,
with cardinality~$D$. 
%
%
For the $n$-th video, we thus represent the text signal as a sequence of direct object relation tokens: $d^n = (d_1^n, \dots, d_{S_n}^n)$, where the length $S_n$ of the sequence varies from one video clip to another. 
This step is key to the success of our method as it allows us to convert the problem of clustering raw transcribed text into an easier problem of clustering sequences of direct object relations.    
%
%
The goal is now to extract from the narrations the most common sequence of  $K$ main steps to achieve the given task. 
%
To achieve this, we first find a globally consistent alignment of the direct object relations that compose all text sequences by solving a multiple sequence alignment problem.
Second, we pick from this alignment the $K$ most globally consistent clusters across videos. %
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\textbf{Multiple sequence alignment model.} We formulate the first stage of finding the common alignment between the input sequences of direct object relations as a multiple sequence alignment problem with the \emph{sum-of-pairs score}~\cite{wang1994msaNPhard}. 
In details, a global alignment can be defined by re-mapping each input sequence $d^n$ of tokens to a global common template of $L$ slots, for $L$ large enough. We let $(\phi(d^n))_{1\leq l \leq L}$ represent
the (increasing) re-mapping for sequence $d^n$ at the new locations indexed by $l$:  $\phi(d^n)_l$ represents the direct object relation put at location $l$, with $\phi(d^n)_l = \varnothing$ if a slot is left empty (denoting the insertion of a gap in the original sequence of tokens). See the middle of Figure~\ref{tab:MSAillustration} for an example of re-mapping.
The goal is then to find a global alignment that minimizes the following sum-of-pairs cost function:
\vspace{-3mm}
\begin{equation}
\sum_{(n,m)} \sum_{l=1}^L c(\phi(d^n)_l,\phi(d^{m})_{l}),
\label{eq:msa_cost}
\end{equation}
where $c(d_1,d_2)$ denotes the cost of aligning the direct object relations $d_1$ and $d_2$ at the same common slot~$l$ in the global template. The above cost thus denotes the sum of all pairwise alignments of the individual sequences (the outer sum), where the quality of each alignment is measured by summing the cost $c$ of matches of individual direct object relations mapped into the common template sequence. We use a negative cost when $d_1$ and $d_2$ 
%
%
%
%
%
%
are similar according to the distance in the WordNet tree~\cite{Fellbaum98Wordnet,Miller95Wordnet} of their verb and direct object constituents, and positive if they are dissimilar (details are given in 
Sec.~\ref{sec:experiments}).
As the verbal narrations can talk about many other things than the main steps of a task,
we set $c(d,d')=0$ if either $d$ or $d'$ is $\varnothing$.
An illustration of clustering the transcribed verbal instructions into a sequence of $K$ steps is shown in Figure~\ref{tab:MSAillustration}.
%
%


\textbf{Optimization using Frank-Wolfe.} 
Optimizing the cost~\eqref{eq:msa_cost} is NP-hard~\cite{wang1994msaNPhard} 
because of the combinatorial nature of the problem. The standard
solution from computational biology is to apply a heuristic algorithm that proceeds by incremental pairwise alignment using dynamic programming~\cite{Lee01poa}.
In contrast, we show in Appendix~\ref{subsec:details_msa} that the multiple sequence alignment problem given by~\eqref{eq:msa_cost} can be reformulated as an integer quadratic program with combinatorial constraints, for which the Frank-Wolfe optimization algorithm has been used recently with increasing success~\cite{Bojanowski14weakly,Jaggi2013,Joulin14efficient,Lacoste15GlobalLinearFW}. 
Interestingly, we have observed empirically (see Appendix~\ref{subsec:comparison_msa}) that
the Frank-Wolfe algorithm was giving better solutions (in terms of objective~\eqref{eq:msa_cost}) than the state-of-the-art heuristic procedures for this task~\cite{Higgins88clustal,Lee01poa}. Our Frank-Wolfe based solvers also offer us greater flexibility in defining the alignment cost and scale better with the length of input sequences and the vocabulary of direct object relations. 
%

\textbf{Extracting the main steps.}
After a global alignment is obtained, we sort the global template $l$
by the number of direct object relations aligned to each slot. Given
$K$ as input, the top $K$ slots give the main instruction steps for
the task, unless there are multiple steps with the same support, which
go beyond $K$. In this case, we pick the next smaller number below $K$
which excludes these ties, allowing the choice of an \emph{adaptive}
number of main instruction steps when there is not enough saliency for
the last steps.  This strategy essentially selects $k\leq K$ salient
steps, while refusing to make a choice among steps with equal support
that would increase the total number of steps beyond $K$.  As we will
see in our results in Sec.~\ref{subsec:exp0}, our algorithm
sometimes returns a much smaller number than $K$ for the main
instruction steps, giving more robustness to the exact choice of
parameter $K$.

\textbf{Encoding of the output.}
We post-process the output of multiple sequence alignment into an assignment matrix $R_n \in \{0, 1\}^{S_n \times K}$ for each input video $n$, 
where $(R_n)_{sk} = 1$ means that the direct object token $d^n_s$ has been assigned to step $k$. 
If a direct object has not been assigned to any step, the corresponding row of the matrix $R_n$ will be zero.

\subsection{Discriminative clustering of videos under text constraints}
\label{sec:model-video}

Given the output of the text clustering that identified the important $K$ steps forming a task,
we now want to find their temporal location in the video signal. We formalize this problem
as looking for an assignment matrix $Z_n\in \{0, 1\}^{T_n \times K}$ for each input video $n$,
where $(Z_n)_{tk} = 1$ indicates the visual presence of step $k$ at time interval~$t$ in video~$n$, and $T_n$ is the length of video~$n$.
Similarly to $R_n$, we allow the possibility that a whole row of $Z_n$ is zero, indicating that no step 
is visually present for the corresponding time interval.

We propose to tackle this problem using a discriminative clustering approach with global
ordering constraints, as was successfully used in the past
for the temporal localization of actions in videos~\cite{Bojanowski14weakly}, 
but with additional \emph{weak temporal constraints}.
In contrast to~\cite{Bojanowski14weakly} where the order of actions was manually
given for each video, our multiple sequence alignment approach automatically
discovers the main steps. More importantly, we also use the \emph{text caption timing}
to provide a fine-grained weak temporal supervision for the visual 
appearance of steps, which is described next.

\setlength{\tabcolsep}{2pt}
\begin{table*}[t]\centering
\resizebox{\textwidth}{!}{
    \footnotesize
    \begin{tabular}{lr >{\centering\hspace{0.5pt}}m{0cm} lr >{\centering\hspace{0.5pt}}m{0cm} lr >{\centering\hspace{0.5pt}}m{0cm} lr >{\centering\hspace{0.5pt}}m{0cm} lr}
    \toprule
         \multicolumn{2}{c}{Changing a tire}  & \phantom{abc} & \multicolumn{2}{c}{Performing CPR} & \phantom{abc} & \multicolumn{2}{c}{Repot a plant} & \phantom{abc} & \multicolumn{2}{c}{Make coffee} & \phantom{abc} & \multicolumn{2}{c}{Jump car} \\
    
\cmidrule{1-2} \cmidrule{4-5} \cmidrule{7-8} \cmidrule{10-11} \cmidrule{13-14} 
GT (\textbf{11}) & $K\leq 10$ && GT (\textbf{7}) & $K\leq 10$  && GT (\textbf{7}) & $K\leq 10$  && GT (\textbf{10)} & $K\leq 10$ && GT (\textbf{12}) & $K\leq 10$  \\
\cmidrule{1-2} \cmidrule{4-5} \cmidrule{7-8} \cmidrule{10-11} \cmidrule{13-14} 
\textit{get tools out}  & \textbf{get tire}     &&  \textit{open airway}   & \textbf{open airway}      &&  \textit{take plant}   & \textbf{remove plant}    && \textit{add coffee}    & \textbf{put coffee}    && \textit{connect red A} & \textbf{connect cable}    \\ 

     
\textit{start loose}  & \textbf{loosen nut}     &&  \textit{check pulse}   & \textbf{put hand}     &&  \textit{put soil}  & \textbf{use soil}    &&   & \textbf{fill chamber}      &&   & \textbf{charge battery}    \\  


\textit{}          & \textbf{put jack}     &&        & \textbf{tilt head}     &&  \textit{loosen roots}    & \textbf{loosen soil}   && \textit{fill water}     & \textbf{fill water}   && \textit{connect red B}  & \textbf{connect end}    \\  


\textit{jack car}  & \textbf{jack car}     &&        & \textbf{lift chin}     &&  \textit{place plant}    &\textbf{place plant}   && \textit{screw filter}  & \textbf{put filter}   && \textit{start car A}  & \textbf{start car}    \\  


\textit{unscrew wheel}  & \textbf{remove nut}     &&  \textit{give breath}   & \textbf{give breath}     &&  \textit{add top}   & \textbf{add soil}    && \textit{}     & \textbf{see steam } &&  \textit{remove cable A}  &\textbf{remove cable}    \\  


\textit{remove wheel}  & \textbf{take wheel}     &&  \textit{do compressions}   & \textbf{do compression}    &&  \textit{water plant}   & \textbf{water plant}    && \textit{put stove}     & \textbf{take minutes }                  &&  \textit{remove cable B}  &\textbf{disconnect cable}   \\  


\textit{put wheel}  & \textbf{take tire}     &&  \textit{}   & \textbf{open airway}      &&      &    && \textit{}    & \textbf{make coffee}   &&    &  \\  


\textit{screw wheel}  & \textbf{put nut}     &&  \textit{}   & \textbf{start compression}     &&      &     &&   \textit{see coffee}  &  \textbf{see coffee}   &&    &     \\  


\textit{lower car}  & \textbf{lower jack}     &&  \textit{}   & \textbf{do compression}     &&      &    &&    \textit{pour coffee}  &  \textbf{make cup}    &&    &     \\  


\textit{tight wheel}  & \textbf{tighten nut}     &&  \textit{}   & \textbf{give breath}      &&      &     &&      &    &&   &     \\  
\midrule
Precision  & 0.9    &&  Precision &   0.4   &&  Precision &   1 && Precision &   0.67  &&  Precision &   0.83   \\
Recall     & 0.9    &&  Recall    &   0.57  &&  Recall &   0.86  && Recall &   0.6 && Recall &   0.42   \\

        \bottomrule
    \end{tabular}
    
}

    \vspace{-2mm}

    \caption{\small 
        Automatically recovered sequences of steps for the five tasks.
        Each recovered step is represented by one of the aligned direct object relations  (shown in bold). 
        Note that most of the recovered steps correspond well to the ground truth steps (shown in italic).
        The results are shown for the maximum number of discovered steps $K$ set to $10$. Note how our method automatically selects less than 10 steps in some cases. These are the automatically chosen $k\leq K$ steps that are the most salient in the aligned narrations as described in Sec.~\ref{subsec:model_text}.  For {\em CPR}, our method recovers fine-grained steps e.g.~{\em tilt head}, {\em lift chin}, which are not included in the main ground truth steps, but nevertheless could be helpful in some situations, as well as repetitions that were not annotated but were indeed present.
%
%
    }
    \label{exp:MSARes}

    \vspace{-4mm}

\end{table*}

\setlength{\tabcolsep}{6pt}

%
\textbf{Temporal weak supervision from text.} 
From the output of the multiple sequence alignment (encoded in the matrix $R_n \in \{0,1\}^{S_n \times K}$),
each direct object token $d_s^n$ has been assigned to one of the possible $K$ steps,
or to no step at all. 
We use the tokens that have been assigned to a step as 
a constraint on the visual appearance of the same step in the video (using
the assumption that people do what they say approximately when they say it).
We encode the closed caption timing alignment by a binary matrix 
$A_n \in \{0,1\}^{S_n \times T_n}$ for each video, 
where $(A_n)_{st}$ is $1$ if the $s$-th direct object is mentioned in a closed caption 
that overlaps with the time interval $t$ in video.
Note that this alignment is only approximate as people 
usually do not perform the action exactly at the same time that they talk about it, 
but instead with a varying delay.
Second, the alignment is noisy as people typically perform the action only 
once, but often talk about it multiple times (e.g. in a summary at the beginning of the video).
We address these issues by the following two \emph{weak supervision} constraints.
First, we consider a larger set of possible time intervals $[t-\Delta_b, t+\Delta_a]$ in the matrix $A$ rather than the exact time interval $t$ given by the timing of the closed caption.
$\Delta_b$ and $\Delta_a$ are global parameters fixed either qualitatively, or by cross-validation
if labeled data is provided. 
Second, we put as a constraint that the action happens at least once in the set of all possible video time intervals where the action is mentioned in the transcript (rather than every time it is mentioned).
These constraints can be encoded as the following linear inequality constraint on $Z_n$: $A_n Z_n \geq R_n$
(see Appendix~\ref{subsec:fw_dp} for the detailed derivation). 

%
\textbf{Ordering constraint.} In addition, we also enforce that the temporal order of the steps appearing
visually is consistent with the discovered script from the text, encoding
our assumption that there is a common ordered script for the task across videos.
We encode these sequence constraints on $Z_n$ in a similar manner to~\cite{Bojanowski15weakly},
which was shown to work better than the encoding used in~\cite{Bojanowski14weakly}.
In particular, we only predict the \emph{most salient} time interval in the video that describes a given step.
This means that a particular step is assigned to \emph{exactly one} time interval in each video.
We denote by $\mathcal{Z}_n$ this sequence ordering constraint set.

%
\textbf{Discriminative clustering.}
The main motivation behind discriminative clustering is to find a \emph{clustering} of the data that can be easily recovered by a \emph{linear classifier} through the minimization of an appropriate \emph{cost function} over the assignment matrix $Z_n$.
The approach introduced in \cite{Bach07diffrac} allows to easily add prior information on the expected clustering.
Such priors have been recently introduced in the context of aligning video and text~\cite{Bojanowski14weakly, Bojanowski15weakly} in the form of ordering constraints over the latent label variables.
Here we use a similar approach to cluster the $N$ input video streams $(x_t)$ into a sequence of $K$ steps, as follows.
We represent each time interval by a $d$-dimensional feature vector. 
The feature vectors for the $n$-th video are stacked in a $T_n\times d$ design matrix denoted by $X_n$. 
We denote by $X$ the $T \times d$ matrix obtained by the concatenation of all $X_n$ matrices
(and similarly, by $Z$, $R$ and $A$ the appropriate concatenation of the $Z_n$, $R_n$ and $A_n$ matrices over $n$). 
%
In order to obtain the temporal localization into $K$ steps, we learn a linear classifier represented by a $d \times K$ matrix denoted by $W$.
This model is shared among all videos. 
%
%

The target assignment~$\hat{Z}$ is found by minimizing the clustering cost function $h$ under 
both the consistent script ordering constraints $\mathcal{Z}$ and
our weak supervision constraints:
%
\begin{equation}
\underset{Z}{\text{minimize}} \quad h(Z) \quad  \text{ s.t. }  \underbrace{Z \in \mathcal{Z}}_{\text{ordered script}}, \quad
\underbrace{AZ \geq R}_{\substack{\text{weak textual}\\ \text{constraints}} }.
\label{eq:videocost}
\end{equation}
The clustering cost $h(Z)$ is given as in DIFFRAC~\cite{Bach07diffrac} as:
\begin{align}
%
    h(Z) = \min_{W \in \mathbb{R}^{K \times d}} \ \underbrace{\frac{1}{2T} \|Z - X W\|_F^2}_\text{Discriminative loss on data} + \underbrace{\frac{\lambda}{2} \|W\|_F^2}_\text{Regularizer}. 
    \label{eq:discriminative}
\end{align}
The first term in~(\ref{eq:discriminative}) is the discriminative loss on the data that measures how easy the input data $X$ is separable by the linear classifier $W$ when the target classes are given by the assignments $Z$.  
For the squared loss considered in eq.~(\ref{eq:discriminative}), the optimal weights $W^*$ minimizing~(\ref{eq:discriminative}) can be found in closed form, 
%
which significantly simplifies the computation. 
However, to solve~\eqref{eq:videocost}, we need to optimize over assignment matrices $Z$ that encode sequences of events and incorporate constraints given by clusters obtained from transcribed textual narrations (Sec.~\ref{sec:text}). 
This is again done by using the Frank-Wolfe algorithm, which allows the use of \emph{efficient dynamic programs} 
to handle the combinatorial constraints on $Z$.
More details are given in Appendix~\ref{sec:details_diffrac}.
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%

%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
\begin{figure*}[t!]
    \centering
    \begin{subfigure}[t]{0.19\linewidth}
        \centering
        \includegraphics[width=\linewidth]{rebuttal/figs/f1_changing_tire_withdobj_rebuttal_with_18_singleK_opti_CR_less_test.pdf}
        \caption{Change tire (\textbf{11})}
    \end{subfigure}%
    ~ 
    \begin{subfigure}[t]{0.19\linewidth}
        \centering
        \includegraphics[width=\linewidth]{rebuttal/figs/f1_cpr_withdobj_rebuttal_with_18_singleK_opti_CR_less_test.pdf}
        \caption{Perform CPR (\textbf{7})}
    \end{subfigure}%
	~
	\begin{subfigure}[t]{0.19\linewidth}
        \centering
        \includegraphics[width=\linewidth]{rebuttal/figs/f1_repot_withdobj_rebuttal_with_18_singleK_opti_CR_less_test.pdf}
        \caption{Repot plant (\textbf{7})}
    \end{subfigure}%
    ~
    \begin{subfigure}[t]{0.19\linewidth}
        \centering
        \includegraphics[width=\linewidth]{rebuttal/figs/f1_coffee_withdobj_rebuttal_with_18_singleK_opti_CR_less_test.pdf}
        \caption{Make coffee (\textbf{10})}
    \end{subfigure}%
    ~
    \begin{subfigure}[t]{0.19\linewidth}
        \centering
        \includegraphics[width=\linewidth]{rebuttal/figs/f1_jump_car_withdobj_rebuttal_with_18_singleK_opti_CR_less_test.pdf}
        \caption{ Jump car (\textbf{12})}
    \end{subfigure}%
    \vspace{-1mm}
    \caption{\small Results for temporally localizing recovered steps in the input videos.
    We give in \textbf{bold} the number of ground truth steps.   
%
    }
    \vspace{-3mm}
    \label{tab:exp-localization}
\end{figure*}


\section{Experimental evaluation}
\setlength{\tabcolsep}{6pt}
\label{sec:experiments}
%
In this section, we first describe the details of the text and video features. 
Then we present the results divided into two experiments:
(i) in Sec.~\ref{sec:step_discovery}, we evaluate the quality of steps extracted from video narrations, and
(ii) in Sec.~\ref{sec:localization}, we evaluate the temporal localization of the recovered steps in video using constraints derived from text. 
All the data and code are available at our project webpage~\cite{Alayrac15UnsupervisedWeb}.


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


\textbf{Video and text features.} 
We represent the transcribed narrations as sequences of direct object relations. 
For this purpose, we run a dependency parser~\cite{Marneffe06generating} on each transcript. 
We lemmatize all direct object relations and keep the ones for which the direct object corresponds to nouns.
%
%
%
To represent a video, we use motion descriptors in order to capture actions (loosening, jacking-up, giving compressions) and frame appearance descriptors to capture the depicted objects (tire, jack, car). 
We split each video into 10-frame time intervals and represent each interval by its motion and appearance descriptors aggregated over a longer block of 30 frames. The motion representation is a histogram of local optical flow (HOF) descriptors aggregated into a single bag-of-visual-word vector of 2,000 dimensions~\cite{Wang13action}.  The visual vocabulary is generated by k-means on a separate large set of training descriptors.
To capture the depicted objects in the video, we apply the VGG-verydeep-16 CNN~\cite{Simonyan14c} over each frame in a sliding window manner over multiple scales. This can be done efficiently in a fully convolutional manner. The resulting 512-dimensional feature maps of conv5 responses are then aggregated into a single bag-of-visual-word vector of 1,000 dimensions, which aims to capture the presence/absence of different objects within each video block. A similar representation (aggregated into compact VLAD descriptor) was shown to work well recently for a variety of recognition tasks~\cite{Cimpoi15}. The bag-of-visual-word vectors representing the motion and the appearance are normalized using the Hellinger normalization and then concatenated into a single 3,000 dimensional vector representing each time interval.

%
%
%
%
\textbf{WordNet distance.}
For the multiple sequence alignment presented in Sec.~\ref{subsec:model_text}, we set $c(d_1,d_2)=-1$ if $d_1$ and $d_2$ have both their verbs and direct objects that match exactly in the Wordnet tree (distance equal to 0).
Otherwise we set $c(d_1,d_2)$ to be 100.
This is to ensure a high precision for the resulting alignment. %


%
\subsection{Results of step discovery from text narrations}
\label{sec:step_discovery}
%
\label{subsec:exp0}

Results of discovering the main steps for each task from text narrations are presented in Table~\ref{exp:MSARes}.
We report results of the multiple sequence alignment described in Sec.~\ref{subsec:model_text} when the maximum number of recoverable steps is $K=10$. Additional results for different choices of $K$ are given in the Appendix~\ref{subsec:script_disc}. %
%
%
With increasing $K$, we tend to recover more complete sequences at the cost of occasional repetitions, e.g.~{\em position jack} and {\em jack car} that refer to the same step. 
%
To quantify the performance, we measure precision as the proportion of correctly recovered steps appearing in the correct order. 
We also measure recall as the proportion of the recovered ground truth steps. 
The values of precision and recall are given at the bottom of Table~\ref{exp:MSARes}.
%
%
%


%
\subsection{Results of localizing instruction steps in video}
\label{sec:localization}
%

In the previous section, we have evaluated the quality of the sequences of steps recovered from the transcribed narrations. 
In this section, we evaluate how well we localize the individual instruction steps in the video by running our two-stage approach from Sec.~\ref{subsec:model_var}.


\textbf{Evaluation metric.}
To evaluate the temporal localization, we need to have a one-to-one mapping between the discovered steps in the videos and the ground truth steps.  Following~\cite{Liao05Clustering}, we look for a one-to-one global matching (shared across all videos of a given task) that maximizes the evaluation score for a given method (using the Hungarian algorithm).
Note that this mapping is used only for evaluation, the algorithm does not have access to the ground truth annotations for learning.


The goal is to evaluate whether each ground truth step has been correctly localized in all instruction videos. We thus use the \emph{F1 score} that combines precision and recall into a single score as our evaluation measure. 
%
For a given video and a given recovered step, our video clustering method predicts exactly one video time interval $t$. 
This detection is considered correct if the time interval falls inside any of the corresponding ground truth intervals, and incorrect otherwise (resulting in a false positive for this video). 
We compute the recall across all steps and videos, defined as the ratio of the number of correct predictions over the total number of possible ground truth steps across videos. A recall of 1 indicates that every ground truth step has been correctly detected across all videos. 
%
The recall decreases towards 0 when we miss some ground truth steps (missed detections). 
This happens either because this step was not recovered globally, or because it was detected in the video at an incorrect location. This is because the algorithm predicts exactly one occurrence of each step in each video. %
Similarly, precision measures the proportion of correct predictions among all  $N \! \cdot \!K_{\mathrm{pred}}$  possible predictions, where $N$ is the number of videos and $K_{\mathrm{pred}}$
is the number of main steps used by the method. The F1 score is the harmonic mean of precision and recall, giving a score that ranges between 0 and 1, with the perfect score of 1 when all the steps are predicted at their correct locations in all videos.  

\textbf{Hyperparameters.} 
We set the values of parameters $\Delta_b$ and $\Delta_a$ to 0 and 10 seconds. %
The setting is the same for all five tasks.
This models the fact that typically each step is first described verbally and then performed on the camera.
%
We set $\lambda = 1/(N K_{\mathrm{pred}})$ for all methods that use~\eqref{eq:discriminative}.

\textbf{Baselines.} 
We compare results to four baselines. 
To demonstrate the difficulty of our dataset, we first evaluate a ``Uniform" baseline, which simply distributes instructions steps uniformly over the entire instruction video.
The second baseline ``Video only"~\cite{Bojanowski14weakly} does not use the narration and performs only discriminative clustering on visual features with a global order constraint.\footnote{We use here the improved model from~\cite{Bojanowski15weakly} which does not require a ``background class'' and yields a stronger baseline equivalent to our model~\eqref{eq:videocost} \emph{without} the weak textual constraints.}
%
The third baseline ``Video + BOW dobj'' basically adds text-based features to the ``Video only'' baseline (by concatenating the text and video features in the discriminative clustering approach).
Here the goal is to evaluate the benefits of our two-stage clustering approach, in contrast to this single-stage clustering baseline. 
The text features are bag-of-words histograms over a fixed vocabulary of direct object relations.\footnote{Alternative features of bag-of-words histograms treating separately nouns and verbs also give similar results.}
The fourth baseline is our own implementation of the alignment method of~\cite{Malmaud15what} (without the supervised vision refinement procedure that requires a set of pre-trained visual classifiers that are not available a-priori in our case).
%
%
We use~\cite{Malmaud15what} to re-align the speech transcripts to the sequence of steps discovered by our method of Sec.~\ref{subsec:model_text} (as a proxy for the recipe assumed to be known in~\cite{Malmaud15what}).\footnote{Note that our method finds at the same time the sequence of steps (a recipe in ~\cite{Malmaud15what}) and the alignment of the transcripts.}
To assess the difficulty of the task and dataset, we also compare results with a ``Supervised" approach.
The classifiers $W$ for the visual steps are trained by running the discriminative clustering of Sec.~\ref{sec:model-video} with only ground truth annotations as constraints on the training set.
At test time, these classifiers are used to make predictions under the global ordering constraint on unseen videos.
%
%
We report results using 5-fold cross validation for the supervised approach, with the variation 
across folds giving the error bars. 
For the unsupervised discriminative clustering methods, the error bars represent the variation of performance obtained from different rounded solutions collected during the Frank-Wolfe optimization.
%

%
%
%
%


\begin{figure}
%
\centering\includegraphics[width=0.98\linewidth]{figs/qual_res_v3_v1}
\vspace{-1mm}
\caption{\footnotesize {\bf Examples of three recovered instruction steps for each of the five tasks in our dataset.} %
For each step, we first show clustered direct object relations, followed by representative example frames localizing the step in the videos. Correct localizations are shown in green. Some steps are incorrectly localized in some videos (red), but often look visually very similar.  
%
{\bf See Appendix~\ref{subsec:action_loc} for additional results.}
}
\label{fig:qualitative_results} 
\end{figure}


\textbf{Results.} 
%
Results for localizing the discovered instruction steps are shown
in Figure~\ref{tab:exp-localization}. 
In order to perform a fair comparison to the baseline methods that require a known number of steps $K$, we report results for a range of $K$ values. Note that in our case the actual number of automatically recovered steps can be (and often is) smaller than $K$.
%
%
For {\em Change tire} and {\em Perform CPR}, our method consistently outperforms all baselines for all values of $K$ demonstrating the benefits of our approach. 
For {\em Repot}, our method is comparable to text-based baselines, underlying the importance of the text signal for this problem.
For {\em Jump car}, our method delivers the best result (for $K=15$) but struggles for lower values of $K$, which we found was due to visually similar repeating steps (e.g. start car A and start car B) which are mixed-up for lower values of $K$.
For the {\em Make coffee} task, the video only baseline is comparable to our method, which by inspecting the output could be attributed to large variability of narrations for this task. 
Qualitative results of the recovered steps are illustrated in Figure~\ref{fig:qualitative_results}.

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


\section{Conclusion and future work}
We have described a method to automatically discover the main steps of a task from a set of narrated instruction videos in an unsupervised manner.
%
The proposed approach has been tested on a new annotated dataset of challenging real-world instruction videos containing complex person-object interactions in a variety of indoor and outdoor scenes. Our work opens up the possibility for large scale learning from instruction videos on the Internet.
Our model currently assumes the existence of a common script with a fixed ordering of the main steps.
While this assumption is often true, e.g.~one cannot remove the wheel before jacking up the car, or make coffee before filling the water, some tasks can be performed while swapping (or even leaving out) some of the steps. Recovering more complex temporal structures is an interesting direction for future work. 
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


\small{
\paragraph{Acknowledgments}
This research was supported in part by a Google Research Award, and the ERC grants VideoWorld (no. 267907), Activia (no. 307574) and LEAP (no. 336845).
}
%
%
%
%

%

{\small
\bibliographystyle{ieee}
\bibliography{biblio}
}

%
%
%
%
%

%
%
%
%
%
%





%

\clearpage

\appendix

%
\section*{Outline of Supplementary Material}
%
\label{app:intro}
This supplementary material provides additional details for our method and presents a more complete set of results.
%
Section~\ref{app:dataset} gives detailed statistics and an illustration of the newly collected dataset of instruction videos.
Section~\ref{app:text_msa} gives details about our new formulation of the multiple sequence alignment problem (Section~\ref{subsec:model_text} of the main paper) as a quadratic program and presents empirical results showing that our Frank-Wolfe optimization approach obtains solutions with lower objective values than the state-of-the-art heuristic algorithms for multiple sequence alignment. 
Section~\ref{sec:details_diffrac} provides the details for the discriminative clustering of videos with text constraints that was briefly described in Section~\ref{sec:model-video} of the main paper.
Section~\ref{subsec:details_experiments} gives additional details about the experimental protocol used in Section~\ref{sec:localization} in the main paper.
Finally, in Section~\ref{sec:qual_res}, we give a more complete set of qualitative results for both the clustering of transcribed verbal instructions (see~\ref{subsec:script_disc}) and localizing instruction steps in video (see~\ref{subsec:action_loc}).

%
%
%

%
%

%
\section{New challenging dataset of instruction videos}
\label{app:dataset}
%

\subsection{Dataset statistics}
\label{subsec:score_dataset}

In this section, we introduce three different scores which aim to illustrate different properties of our dataset.
The scores characterize (i) the step ordering consistency, (ii) the missing steps and (iii) the possible step repetitions.
 
Let $N$ be the number of videos for a given task and $K$ the number of steps defined in the ground truth.
We assume that the ground truth steps are given in an ordered fashion, meaning the global order is defined as the sequence $\left\lbrace 1,\ldots, K \right\rbrace $. 
For the $n$-th video, we denote by $g_n$ the total number of annotated steps, by $u_n$ the number of unique annotated steps and finally by $l_n$ the length of the longest common subsequence between the annotated sequence of steps and the ground truth sequence $\left\lbrace 1,\ldots, K \right\rbrace$.

\paragraph{Order consistency error.}
The \emph{order error} score $O$ is defined as the proportion of non repeated annotated steps that are not consistent with the global ordering.
In other words, it is defined as the number of steps that do not fit the global ordering defined in the ground truth divided by the total number of unique annotated steps.
More formally, $O$ is defined as follows:
\begin{equation}
O := 1-\frac{\sum_{n=1}^N l_n}{\sum_{n=1}^N u_n}.
\label{order_score}
\end{equation}

\paragraph{Missing steps.} 
We define the \emph{missing steps} score $M$ as the proportion of steps that are visually missing in the videos when compared to the ground truth.
Formally, 
\begin{equation}
M := 1-\frac{\sum_{n=1}^N u_n}{KN}.
\label{missing_score}
\end{equation}

\paragraph{Repeated steps.}
The \emph{repetition score} $R$ is defined as the proportion of steps that are repeated:
\begin{equation}
R := 1-\frac{\sum_{n=1}^N u_n}{\sum_{n=1}^N g_n}.
\label{repet_score}
\end{equation}

\paragraph{Results.}
In Table~\ref{tab:dataset_stat}, we give the previously defined statistics for the five tasks of the instruction videos dataset.
Interestingly, we observed that globally the order is consistent for the five tasks with a total order error of only $6\%$.
Steps are missing in $27\%$ of the cases. 
This illustrates the difficulty of defining the right granularity of the ground truth for this task. 
Indeed, some steps might be optional and thus not visually demonstrated in all videos.
Finally the global repetition score is $14\%$.
Looking more closely, we observe that the \emph{Performing CPR} task is the main contributor to this score.
This is obviously a good example where one needs to repeat several times the same steps (here alternating between compressions and giving breath).
Even if our model is not explicitly handling this case, we observed that our multiple sequence alignment technique for clustering the text inputs discovered these repetitions (see Table~\ref{tab:script_res}).
Finally, these statistics show that the problem introduced in this paper is very challenging and that designing models which are able to capture more complex structure in the organization of the steps is a promising direction for future work.

\begin{table*}[ht]
\centering
\begin{tabular}{@{}lrrrrrr@{}}
\toprule
Task             & \multicolumn{1}{l}{Changing tire} & \multicolumn{1}{l}{Performing CPR} & \multicolumn{1}{l}{Repoting plant} & Making coffee & \multicolumn{1}{c}{Jumping cars} & \multicolumn{1}{c}{\textbf{Average}} \\ \midrule
Order error      & 0.7\%                             & 11\%                               & 6\%                                & 3\%           & 8\%                              & \textbf{6\%}                       \\
Missing steps    & 16\%                              & 32\%                               & 30\%                               & 28\%          & 27\%                             & \textbf{27\%}                      \\
Repetition score & 4\%                               & 50\%                               & 7\%                                & 11\%          & 0.4\%                            & \textbf{14\%}                      \\ \bottomrule
\end{tabular}
\caption{Statistics of the instruction video dataset.}
\label{tab:dataset_stat}
\end{table*}

\subsection{Complete illustration of the dataset}

%
Figure~\ref{fig:datasetApp} illustrates all five tasks in our newly collected dataset.
For each task, we show a subset of 3 events that compose the task.
Each event is represented by several sample frames and extracted verbal narrations.
Note the large variability of verbal expressions and the terminology in the transcribed narrations as well as the large variability of visual appearance  due to viewpoint, used objects, and actions performed in different manner.
At the same time, note the the consistency of the actions between the different videos and the underlying script of each task.
%


%
%
\section{Clustering transcribed verbal instructions}
%
\label{app:text_msa}
In this section, we review in details the way we model the text clustering.
In particular, we give details on how we can reformulate multiple sequence alignment as a quadratic program.
Recall that we are given $N$ narrated instruction videos.
For the $n$-th video, the text signal is represented as a sequence of direct object relation tokens : $d^n=(d_1^n,\dots,d^n_{S_n})$, where the length $S_n$ of the sequences varies from one video clip to another.
The number of possible direct object relations in our dictionary is denoted $D$.
The multiple sequence alignment (MSA) problem was formulated
as mapping each input sequence $d^n$ of tokens to a global common
template of $L$ slots, while minimizing the sum-of-pairs score given in~\eqref{eq:msa_cost}.
For each input sequence $d^n$, we used the notation $(\phi(d^n))_{1\leq l\leq L}$
to denote the re-mapped sequence of tokens into $L$ slots: $\phi(d^n)_l$
represents the direct object relation put at location $l$, with $\phi(d^n)_l = \varnothing$
denoting that a gap was inserted in the original sequence and
the slot $l$ is left empty.
We also have defined a cost $c(d_1, d_2)$ of aligning two direct object 
relations together, with the possibility that $d_1$ or $d_2$
is~$\varnothing$, in which case we defined the cost to be $0$
by default.
In the following, we summarize the cost of aligning non-empty
direct object relations
by the matrix $\Ctext\in\mathbb{R}^{D\times D}$.
$(\Ctext)_{ij}$ is equal to the cost of aligning the $i$-th and the 
$j$-th direct object relation from the dictionary together.


\subsection{Reformulating multiple sequence alignment as a quadratic program}
\label{subsec:details_msa}

We now present our formalization of the search problem as a quadratic program.
To the best of our knowledge this is a new formulation of the multiple sequence alignment (MSA) problem, 
which in our setting (results shown later) consistently obtains better values of the multiple sequence alignment objective than the current
state-of-the-art MSA heuristic algorithms.
 
We encode the identity of a direct object relation with a $D$-dimensional
indicator vector.
The text sequence $n$ can then be represented by an indicator matrix $Y_n \in \{0,1\}^{S_n\times D}$.
The $j$-th row of $Y_n$ indicates which direct object relations is evoked at the $j$-th position.
Similarly, the token re-mapping  $(\phi(d^n))_{1\leq l\leq L}$ can
be represented as a $L \times D$ indicator matrix; where 
each row $l$ encodes which token is appearing in slot $l$ (and
a whole row of zero is used to indicates an empty $\varnothing$ slot).
This re-mapping can be constructed from two 
pieces of information: first, which token index $s$ of the original sequence
is re-mapped to which global template slot $l$; 
we represent this by the decision matrix  $U_n\in\{0,1\}^{S_n\times L}$,
which satisfies very specific constraints (see below).
The second piece of information is the composition 
of the input sequence encoded by $Y_n$.
We thus have $\phi(d^n) = U_n^T Y_n$ (as a $L \times D$ indicator
matrix). Given this encoding, the cost matrix $\Ctext$, and the fact
that the alignment of empty slots has zero cost, we
can then rewrite the MSA problem that minimizes
the sum-of-pairs objective~\eqref{eq:msa_cost}
as follows:
\begin{equation}
\begin{aligned}
& \underset{U_n, n\in\{1,\ldots,N\}}{\text{minimize}}
& & \sum_{(n,m)} \mathrm{Tr}(U_n^TY_n \Ctext Y_m^TU_m) \\
& \text{subject to}
& & U_n \in \mathcal{U}_n, \; n = 1, \ldots, N.
\end{aligned}
\label{eq:msaeq}
\end{equation}
In the above equation, the trace ($\mathrm{Tr}$) is computing the cost of aligning sequence $m$ with sequence $n$
(the inner sum in~\eqref{eq:msa_cost}).
Moreover, $\mathcal{U}_n$ is a constraint set that encodes the fact that $U_n$ has to be a valid (increasing) re-mapping.\footnote{More formally  $\mathcal{U}_n :=\{U\in\{0,1\}^{S_n \times L}$ s.t.  $U\textbf{1}_L=\textbf{1}_{S_n}$ and $\forall l, \left( U_{sl}=1 \Rightarrow ((\forall s'>s,l'\leq l),  U_{s'l'}=0 \right)\}$.}
As before, we can eliminate the video index $n$ by simply stacking the assignment matrices $U_n$ in one matrix $U$ of size $S\times L$.
Similarly, we denote $Y$ the $S\times D$ matrix which is obtained by the concatenation of all the $Y_n$ matrices. 
We can then rewrite the equation~(\ref{eq:msaeq}) as a quadratic program over the (integer) variable $U$:

\begin{equation}
\underset{U}{\text{minimize}} \ \mathrm{Tr}(U^TBU), \ \text{subject to} \ U \in \mathcal{U}.
\label{eq:msaeqcompact}
\end{equation}
In this equation, the $S \times S$ matrix $B$ is deduced from the input sequences and the cost between different direct object relations by computing $B := Y \Ctext Y^T$.
It represents the pairwise cost at the token level, i.e. the cost of aligning token $s$ in one
sequence to token $s'$ in another sequence.
%
%

\subsection{Comparison of methods}
\label{subsec:comparison_msa}

\begin{table*}[!ht] 
    \centering
    \setlength{\tabcolsep}{.6em} 
    \begin{tabular}{lccccc} 
        \toprule
        Task 			&  Changing tire   &  Performing CPR  & Repotting plant &  Making coffee   & Jumping cars\\ 
        \midrule
%
        Poa~\cite{Lee01poa} 			    &   11.30   &   -3.82   &  1.65    &   -2.99   &   4.55	\\ 
        Ours using Frank-Wolfe 					    &  \textbf{-5.18}  &   \textbf{-4.51}   & \textbf{-3.55} &   \textbf{-3.86}   & \textbf{-4.67} 	\\ 
        \bottomrule
    \end{tabular}  
%
    \caption{Comparison of different optimization approaches for solving problem~\eqref{eq:msaeqcompact}. (Objective value, lower is better).}  
    \label{tab:msaObj}
%
\end{table*} 

The problem~(\ref{eq:msaeqcompact}) is NP-hard~\cite{wang1994msaNPhard} in general, as is typical 
for integer quadratic programs.
However, much work has been done in computational biology
to develop efficient heuristics to solve the MSA problem,
as it is an important problem in their field.
We briefly describe below some of the existing heuristics to solve it,
and then present our Frank-Wolfe optimization approach,
which gave surprisingly good empirical results
for our problem.\footnote{We stress here that we do not claim 
that our formulation of the multiple sequence alignment (MSA) problem as a quadratic program outperforms the state-of-the-art 
computational biology heuristics for their MSA problems \emph{arising in biology}.
We report our observations on application of multiple sequence alignment to our application,
which might have a structure for which these heuristics are not as appropriate.}


\paragraph{Standard methods.}
Here, we compare to a standard state-of-the-art method for multiple sequence alignment~\cite{Lee01poa}.
Similarly to~\cite{Higgins88clustal}, they first align two sequences and merge them in a common template.
Then they align a new sequence to the template and then update the template.
They continue like this until no sequence is left.
%
%
Differently from~\cite{Higgins88clustal}, they use a better representation of the template by using partial order graph instead of simple linear representations.
This gives more accuracy for the final alignment.
For the experiments, we use the author's implementation.\footnote{Code available at \url{http://sourceforge.net/projects/poamsa/}.}

\paragraph{Our solution using Frank-Wolfe optimization.}
We first note that problem~\eqref{eq:msaeqcompact} has a very similar 
structure to an optimization problem that we solve using Frank-Wolfe optimization 
for the discriminative clustering of videos; see Equations~\eqref{eq:appAexph} and~\eqref{eq:hFWproblem} below.
For this, we first perform a continuous relaxation of the set of constraints $\mathcal{U}$ by replacing it with its convex hull $\bar{\mathcal{U}}$.
The Frank-Wolfe optimization algorithm~\cite{Jaggi2013} can solve quadratic program over constraint sets
for which we have access to an efficient linear minimization oracle.
In the case of $\mathcal{U}$,
the linear oracle can be solved exactly with a dynamic program very similar to the one described in Section~\ref{subsec:fw_dp}.
We note here that even with the continuous relaxation over $\bar{\mathcal{U}}$, the resulting problem is still non-convex because $B$ is not positive semidefinite -- this is because of the cost function appearing in the MSA problem.
However, the standard convergence proof for Frank-Wolfe can easily be extended to show
that it converges at a rate of $O(1/\sqrt{k})$ to a stationary point on non-convex objectives~\citesup{lacoste16nonconvexFW}.
Once the algorithm has converged to a (local) stationary point,
we need to round the fractional solution to obtain a valid encoding $U$.
We follow here a similar rounding strategy that was originally proposed by~\citesup{Chari15FWtrack}
and then re-used in~\cite{Joulin14efficient}: we pick the last visited
corner (which is necessarily integer) 
which was given as a solution to the linear minimization oracle
(this is called Frank-Wolfe rounding). 

\paragraph{Results.} In Table~\ref{tab:msaObj}, we give the value of the objective~(\ref{eq:msaeqcompact}) for the rounded solutions obtained by the two different optimization approaches (lower is better), for the MSA problem on our
five tasks.
Interestingly, we observe that the Frank-Wolfe algorithm consistently outperforms the state-of-the-art method of~\cite{Lee01poa} in our setting.


\section{Discriminative clustering of videos under text constraints }
\label{sec:details_diffrac}

We give more details here on the discriminative clustering
framework from~\cite{Bojanowski14weakly,Bojanowski15weakly}
(and our modifications to include the text constraints) that we use
to localize the main actions in the video signal.

\subsection{Explicit form of $h(Z)$}
\label{subsec:explicit_h}

We recall that $h(Z)$ is the cost of clustering all the video streams $\{x^n\}, n=1,\ldots, N$, into a sequence of $K$ steps. 
The design matrix $X$  $\in \mathbb{R}^{T \times d}$ contains the feature describing the time intervals in our videos.
The indicator latent variable $Z\in \mathcal{Z} := \{0,1\}^{T\times K}$ encodes the visual presence of a step $k$ at a time interval $t$. 
Recall also that $X$ and $Z$ contains the information about all videos $n \in \{1,\ldots, N\}$.
Finally, $W\in \mathbb{R}^{d\times K}$ represents a linear classifier for our $K$ steps, that is shared among all videos. We now derive the explicit form of $h(Z)$ as in the DIFFRAC approach~\cite{Bach07diffrac}, though yielding a somewhat simpler expression (as in~\cite{Bojanowski15weakly}) due to our use of a (weakly regularized) bias feature in $X$ instead of a separate (unregularized) bias $b$. 
Consider the following joint cost function $f$ on $Z$ and $W$ defined as
\begin{align}
f(Z,W) =  \frac{1}{2T} \|Z - X W\|_F^2+ \frac{\lambda}{2} \|W\|_F^2.
\label{eq:appAf}
\end{align}
The cost function $f$ simply represents the ridge regression objective with output labels $Z$ and input design matrix $X$. We note that $f$ has the nice property of being \emph{jointly} convex in both $Z$ and $W$, implying that its unrestricted minimization with respect to $W$ yields a \emph{convex} function in $Z$. This minimization defines our clustering cost $h(Z)$; rewriting the definition of $h$ with the joint cost $f$ from~\eqref{eq:appAf}, we have:
\begin{align}
%
    h(Z) = \min_{W \in \mathbb{R}^{d \times K}} \ f(Z,W).
\label{eq:appAh}
\end{align}
As $f$ is strongly convex in $W$ (for any $Z$), we can obtain its unique minimizer $W^*(Z)$ as a function of $Z$ by zeroing its gradient and solving for $W$.
For the case of the square loss in equation~\eqref{eq:appAf}, the optimal classifier $W^*(Z)$ can be computed in closed form:
\begin{align}
%
%
W^*(Z) =  (X^TX+T\lambda I_d)^{-1}X^TZ, 
\label{eq:appAWopt}
\end{align}
where $I_d$ is the $d$-dimensional identity matrix.
We obtain the explicit form for $h(Z)$ by substituting the expression~\eqref{eq:appAWopt} for $W^*(Z)$ in equation~\eqref{eq:appAf} and properly simplifying the expression:
\begin{align}
h(Z) = f(Z,W^*) = \frac{1}{2T}\text{Tr}(ZZ^TB),
\label{eq:appAexph}
\end{align}
where $B := I_T-X(X^TX+T\lambda I_d)^{-1}X^T$ is a strictly positive definite matrix (and so $h$ is actually strongly convex). The clustering cost is a quadratic function in $Z$, encoding how the clustering decisions in one interval $t$ interact with the clustering decisions in another interval $t'$. In the next section, we explain how we can optimize the clustering cost $h(Z)$ subject to the constraints from Section~\ref{sec:model-video} using the Frank-Wolfe algorithm.

\subsection{Frank Wolfe algorithm for minimizing $h(Z)$}
\label{subsec:fw_dp}

The localization of steps in the video stream is done by solving the following optimization problem
(repeated from~\eqref{eq:videocost} here for convenience):
\begin{equation}
\underset{Z}{\text{minimize}} \quad h(Z) \quad  \text{ s.t. }  \underbrace{Z \in \mathcal{Z}}_{\text{ordered script}}, \quad
\underbrace{AZ \geq R}_{\substack{\text{weak textual}\\ \text{constraints}} }.
\label{eq:hFWproblem}
\end{equation}
where $Z$ is the latent assignment matrix of video time intervals to $K$ clusters and $R$ is the matrix of assignments of direct object relations in text to $K$ clusters.
%
Note that $R$ is obtained from the text clustering using multiple sequence alignment as described in Section~\ref{sec:text} and~\ref{subsec:details_msa}, and is fixed before optimizing over $Z$.
$R$ is a $S \times K$ matrix obtaining by picking the $K$ main columns of the $U$ matrix
defined in Section~\ref{subsec:details_msa}. This selection step
was described in the ``extracting the main steps'' paragraph in Section~\ref{sec:text}.

%
The constraint set encodes several concepts.
First, it imposes the temporal consistency between the text stream and the video stream. 
We recall that this constraint was written as $AZ \geq R$,\footnote{
When $R_{sk} = 0$, then this constraint does not do anything. When $R_{sk} = 1$ (i.e. the
text token $s$ was assigned to the main action $k$), then the constraint enforces
that $\sum_{t \in A_{s \cdot}} Z_{tk} \geq 1$, where $A_{s \cdot}$ represents
which video frames are temporally close to the caption time of the text token~$s$.
It thus then enforces that at least one temporally close video frame is assigned
to the main action $k$.
} 
where $A$ encodes the temporal alignment constraints between video and text (type I).
Second, it includes the event ordering constraints within each video input (type II).
Finally, it encodes the fact that each event is assigned to exactly one time interval within each video (type III).
The last two constraints are encoded in the set of constraints $\mathcal{Z}$.
To summarize, let $\mathcal{\tilde{Z}}$ denote the resulting (discrete) feasible space for $Z$ i.e. $\mathcal{\tilde{Z}} := \{Z \in \mathcal{Z} \, | \,AZ \geq R \}$.
%
%

%
%
%
%
%
%

%
%
%

%
We are then left with a problem in $Z$ which is still hard to solve because the set $\tilde{\mathcal{Z}}$ is not convex. 
To approximately optimize $h$ over $\tilde{\mathcal{Z}}$, we follow the strategy of~\cite{Bojanowski14weakly,Bojanowski15weakly}.  First, we optimize $h$ over the relaxed $\text{conv}(\tilde{\mathcal{Z}})$ by using the Frank-Wolfe algorithm to get a fractional solution $Z^* \in \text{conv}(\tilde{\mathcal{Z}})$. We then find a feasible candidate $\hat{Z} \in \tilde{\mathcal{Z}}$ by using a rounding procedure. We now give the details of these steps.  

First we note that the linear oracle of the Frank-Wolfe algorithm can be solved separately for each video $n$.
Indeed, because we solve a linear program, there is no quadratic term that brings dependence between different videos in the objective, and moreover all the constraints are blockwise in $n$.
%
Thus, in the following, we will give details for one video only by adding an index $n$ to $\tilde{\mathcal{Z}}$, to $Z$ and to $T$.

The linear oracle of the Frank-Wolfe algorithm can be solved via an efficient dynamic program.
%
Let us suppose that the linear oracle corresponds to the following problem: 
\begin{equation}
    \min_{Z_n \in \tilde{\mathcal{Z}}_n} \text{Tr}(C_n^\top Z_n),
    \label{eq:linear}
\end{equation}
where $C_n \in \mathbb{R}^{T_n \times K}$ is a cost matrix that arises by computing the gradient of $h$ with respect to $Z_n$ at the current iterate. The goal of the dynamic program is to find which entries of $Z_n$ are equal to 1, recalling that $(Z_n)_{tk}=1$ means that the step $k$ was assigned to time interval $t$. From the constraint of type III (unique prediction per step), we know that each column $k$ of $Z_n$ has exactly one $1$ (to be found). From the ordering constraint (type II), we know that if $(Z_n)_{tk} = 1$, then the only possible locations for a 1 in the $(k+1)$-th column is for $t' > t$ (i.e. the pattern of 1's is going downward when traveling from left to right in $Z_n$). Note that there can be ``jumps'' in between the time assignment for two subsequent steps $k$ and $k+1$. In order to encode this possibility using a continuous path search in a matrix, we insert dummy columns into the cost matrix $C$.
We first subtract the minimum value from $C$ and then insert columns filled with zeros in between every pair of columns of $C$.
In the end, we pad $C$ with an additional row filled with zeros at the bottom.
The resulting cost matrix $\tilde{C}$ is of size $(T_n+1) \times (2K+1)$ and is illustrated (as its transpose) along with the corresponding update rules in Figure~\ref{fig:dp}.

\begin{figure}[tbp]
    \centering
    \includegraphics[width=\linewidth]{figs/reduced_dp.pdf}

    %

    \caption{
        Illustration of the dynamic programming solution to the linear program~\eqref{eq:linear}.
        The drawing shows a possible cost matrix~$\tilde{C}$ and an optimal path in red.
        The gray entries in the matrix $\tilde{C}$ correspond to the values from the matrix \(C\). The white entries have minimal cost and are thus always preferred over any gray entry. 
        Note that we display~$\tilde{C}$ in a transpose manner to better fit on the page.
}
    \label{fig:dp}

    %

\end{figure}

The problem that we are interested in is subject to the additional linear constraints given by the clustering of text transcripts (constraints of type I).
These constraint can be added by constraining the path in the dynamic programming algorithm.
This can be done for instance by setting an infinite alignment cost outside of the constrained region.

At the end of the Frank-Wolfe optimization algorithm, we obtain a continuous solution $Z^*_n$ for each $n$.
By stacking them all together again, we obtain a continuous solution $Z^*$. From the definition of $h$, we can also look at the corresponding model $W^*(Z^*)$ defined by equation~\eqref{eq:appAWopt} which again is shared among all videos.
All $Z_n^*$ have to be rounded in order to obtain a feasible point for the initial, non relaxed problem.
Several rounding options were suggested in~\cite{Bojanowski15weakly}; it turns out that the one which uses $W^*$ gives better results in our case.
%
More precisely, in order to get a good feasible binary matrix $\hat{Z}_n \in \tilde{Z}_n$, we solve the following problem: $\min_{Z_n \in \tilde{\mathcal{Z}}_n} \ \|Z_n - X_n W^*\|_F^2$.
By expanding the norm, we notice that this corresponds to a simple linear program over $\tilde{\mathcal{Z}}_n$ as in equation~\eqref{eq:linear} that can be solved using again the same dynamic program detailed above.
Finally, we stack these rounded matrices $\hat{Z}_n$ to obtain our predicted assignment matrix $\hat{Z} \in \tilde{\mathcal{Z}}$.



\begin{figure*}[ht!]
       \centering
%
%
	 \includegraphics[width=\linewidth]{figs/dataset_all_v2.pdf} %
%
     \caption{\small Illustration of our newly collected dataset of instructions videos. 
     Examples of transcribed narrations together with still frames from the corresponding videos are shown for the 5 tasks of the dataset: 
     %
     {\em Repotting a plant}, {\em Performing CPR}, {\em Jumping cars}, {\em Changing a car tire} and {\em Making coffee}. 
     The dataset contains challenging real-world videos performed by many different people, captured in uncontrolled settings in a variety of outdoor and indoor environments.  
%
     }
     \label{fig:datasetApp}
 \end{figure*}

\section{Experimental protocol}
\label{subsec:details_experiments}

In this section, we give more details about the setting for our experiments on the time localization of events with results given in Figure~\ref{tab:exp-localization}.

\subsection{Supervised experiments.} 
Here, we describe in more details how we obtained the scores for the supervised approach depicted in yellow in Figure~\ref{tab:exp-localization}.
We first divided the $N$ input videos in 5 different folds.
One fold is kept for the test set while the 4 other are used as train/validation dataset.
With the 4 remaining folds, we perform a 4-fold cross validation in order to choose the hyperparameter $\lambda$.
Once the hyper parameter is fixed, we retrain a model on the 4 folds and evaluate it on the test set.
By iterating over the five possible test folds, we report variation in performance with error bars in Figure~\ref{tab:exp-localization}.

\paragraph{Training phase.}
The goal of this phase is to learn classifiers $W$ for the visual steps.
To that end, we minimize the cost defined in~\eqref{eq:videocost} under the ground truth annotations constraints.
This is very close to our setting, and in practice we can use exactly the same framework as in problem~\eqref{eq:hFWproblem} by simply replacing the constraints coming from the text by the constraints coming from the ground truth annotations.
\paragraph{Testing phase.}
At test time, we simply use the classifiers $W$ to perform least-square prediction of $Z_{\text{test}}$ under ordering constraints.
Performance are evaluated with the F1 score.

%

\subsection{Error bars for Frank-Wolfe methods.}
We explain here how we obtained the error bars of Figure~\ref{tab:exp-localization} in the main paper for the unsupervised approaches.
Let us first recall that the Frank-Wolfe algorithm is used to solve a continuous relaxation of problem~\eqref{eq:hFWproblem}.
To obtain back an integer solution, we round the continuous solution using the rounding method described at the end of Section~\ref{subsec:fw_dp}.
This rounding procedure is performed at each iteration of the optimization method.
When the stopping criterion of the Frank-Wolfe scheme is reached (fixed number of iterations or target sub-optimality in practice), we have as many rounded solutions as number of iterations.
Our output integer solution is then the integer point that achieves the lowest objective.
Note that we are only guaranteed to diminish objective in the continuous domain and \emph{not} for the integer points, therefore there are no guarantees that this solution is the last rounded point.
In order to illustrate the variation of the performance with respect to the optimization scheme, we defined our error bars as being the interval with bounds determined by the minimal performance and the maximal performance obtained \emph{after} visiting the best rounded point (the output solution).
This notably explains why the error bars of Figure~\ref{tab:exp-localization} are not necessarily symmetric.
Overall, the observed variation is not very important, thus highlighting the stability of the procedure.

%
%
%
%
%
%
%
%
%
%
%
%


\section{Qualitative results}
\label{sec:qual_res}

In Section~\ref{subsec:script_disc}, we give detailed results of script discovery for the five different tasks.
In Section~\ref{subsec:action_loc}, we present detailed results for the action localization experiment.

\subsection{Script discovery}
\label{subsec:script_disc}
Table~\ref{tab:script_res} shows the automatically recovered sequences of steps for the five tasks considered in this work. 
%
%
        The results are shown for setting the maximum number of discovered steps, $K = \{7,10,12,15\}$. 
        Note how our method automatically selects less than $K$ steps in some cases.
         These are the automatically chosen $k\leq K$ steps that are the most salient in the aligned narrations as described in Section~\ref{subsec:model_text}.  
		 This is notably the case for the {\em Repotting a plant} task. 
		 Even for $K\leq 12$, the algorithm recovers only 6 steps that match very well the seven ground truth steps for this task. 
		 This saliency based task selection is important because it allows for a better precision at high $K$ without lowering much the recall.       

Please note also how the steps and their ordering recovered by our method correspond well to the ground truth steps for each task. 
        For {\em CPR}, our method recovers fine-grained steps e.g.~{\em tilt head}, {\em lift chin}, which are not included in the main ground truth steps, but nevertheless could be helpful in some situations.
	    For {\em Changing tire}, we also recover more detailed actions such as~{\em remove jack} or {\em put jack}.
%
%
	    In some cases, our method recovers repeated steps. %
	    For example, for {\em CPR} our method learns that one has to alternate between {\em giving breath} and {\em performing compressions} even if this alternation was not annotated in the the ground truth.
%
%
	     Or for  {\em Jumping Cars} our method learns that cables need to be connected twice (to both cars).
%
%
	    
	    These results demonstrate that our method is able to automatically discover meaningful scripts describing very different tasks.
	    The results also show that the constraint of a single script providing an ordering of events is a reasonable prior for a variety of different tasks. 
%
 
%
%

\subsection{Action localization}
\label{subsec:action_loc}
Examples of the recovered instruction steps for all five tasks are shown in Figure~\ref{fig:qualitative_results_changing_tire}--\ref{fig:qualitative_results_cpr}.
Each row shows one recovered step. 
For each step, we first show the clustered direct object relations, followed by representative example frames localizing the step in the videos. Correct localizations are shown in green. Some steps are incorrectly localized in some videos (red), but often look visually very similar.  Note how our method correctly recovers the main steps of the task and localizes them in the input videos.
Those results have been obtained by imposing $K\leq 10$ in our method.
The video on the project website illustrates action localization for the five tasks.


%
\bibliographystylesup{ieee}
\bibliographysup{biblio}

%

%
%

\newpage

\setlength{\tabcolsep}{2pt}

\begin{table*}
\footnotesize %

     \begin{subtable}{0.48\textwidth}
     \centering
\resizebox{0.95\textwidth}{!}{	
    \begin{tabular}{lrrrr >{\centering\hspace{0.5pt}}m{0cm}}
    \toprule
GT (\textbf{11}) & $K\leq 7$ &  $K\leq 10$ & $K\leq 12$ & $K\leq 15$\\
\cmidrule{1-5}
\textit{put brake on}  & &      &   &    \\ 
\textit{get tools out}  & & \textbf{get tire}     & \textbf{get tire}  & \textbf{get tire}   \\ 

     
\textit{start loose} & \textbf{loosen nut} & \textbf{loosen nut}     & \textbf{loosen nut}   &   \textbf{loosen nut}   \\ 

\textit{} &  &  & \textbf{} &  \textbf{lift car} \\

\textit{}          & \textbf{put jack} & \textbf{put jack}     &  \textbf{put jack} &   \textbf{put jack} \\  

\textit{}  &   &     &  \textbf{raise vehicle} &  \textbf{raise vehicle}\\  

\textit{jack car}  & \textbf{jack car}  & \textbf{jack car}     &  \textbf{jack car} &  \textbf{jack car}\\  


\textit{unscrew wheel} & \textbf{remove nut}   & \textbf{remove nut}     &  \textbf{remove nut}  & \textbf{remove nut} \\  


\textit{remove wheel} & & \textbf{take wheel}     &  \textbf{take wheel} & \textbf{take wheel} \\  


\textit{put wheel}  & \textbf{take tire}  & \textbf{take tire}     & \textbf{take tire} &  \textbf{take tire} \\  


\textit{screw wheel} & & \textbf{put nut}     & \textbf{put nut} &    \textbf{put nut} \\  


\textit{lower car}  &  \textbf{lower jack} & \textbf{lower jack}     & \textbf{lower jack} &   \textbf{lower jack} \\  

\textit{}  &  & & & \textbf{remove jack}\\  

\textit{tight wheel}  & \textbf{tighten nut}  & \textbf{tighten nut}     &  \textbf{tighten nut}   &    \textbf{tighten nut}   \\  
\textit{put things back} &  &  & \textbf{take tire} &  \textbf{take tire} \\
\midrule
Precision  & 0.85  &  0.9    & 0.83 & 0.71   \\
Recall     & 0.54  &  0.9    & 0.9  & 0.9   \\

        \bottomrule
    \end{tabular}
}
     \caption{Changing a tire}
     \end{subtable}%
     \hspace*{\fill}%
	      \begin{subtable}{0.48\textwidth}
     \centering
\resizebox{0.95\textwidth}{!}{
    \begin{tabular}{lrrrr >{\centering\hspace{0.5pt}}m{0cm}}
    \toprule
GT (\textbf{10}) & $K\leq 7$ &  $K\leq 10$ & $K\leq 12$ & $K\leq 15$\\
\cmidrule{1-5}
\textit{grind coffee} & \textbf{} & \textbf{} & \textbf{} & \textbf{} \\
\textit{put filter} & \textbf{} & \textbf{} & \textbf{} & \textbf{} \\
\textit{add coffee} & \textbf{} & \textbf{put coffee} & \textbf{put coffee} & \textbf{put coffee} \\
\textit{even surface} & \textbf{} & \textbf{} & \textbf{} & \textbf{} \\
\textit{} & \textbf{} & \textbf{fill chamber} & \textbf{fill chamber} & \textbf{fill chamber} \\
\textit{} & \textbf{} & \textbf{} & \textbf{} & \textbf{make noise} \\
\textit{fill water} & \textbf{fill water} & \textbf{fill water} & \textbf{fill water} & \textbf{fill water} \\
\textit{screw top} & \textbf{} & \textbf{put filter} & \textbf{put filter} & \textbf{put filter} \\
\textit{} & \textbf{} & \textbf{} & \textbf{} & \textbf{fill basket} \\
\textit{} & \textbf{} & \textbf{see steam} & \textbf{see steam} & \textbf{see steam} \\
\textit{put stove} & \textbf{take minutes} & \textbf{take minutes} & \textbf{take minutes} & \textbf{take minutes} \\
\textit{} & \textbf{make coffee} & \textbf{make coffee} & \textbf{make coffee} & \textbf{make coffee} \\
\textit{see coffee} & \textbf{see coffee} & \textbf{see coffee} & \textbf{see coffee} & \textbf{see coffee} \\
\textit{withdraw stove} & \textbf{} & \textbf{} & \textbf{} & \textbf{turn heat} \\
\textit{pour coffee} & \textbf{make cup} & \textbf{make cup} & \textbf{make cup} & \textbf{make cup} \\
\textit{} & \textbf{} & \textbf{} & \textbf{} & \textbf{pour coffee} \\
\midrule
Precision  & 0.8 &  0.67   & 0.67 &    0.54\\
Recall     & 0.4  &  0.6 & 0.6  &   0.7\\

        \bottomrule
    \end{tabular}
}
     \caption{Making coffee}
     \end{subtable}%
\vspace{3mm}
\hspace*{\fill}
%
\footnotesize %
 \begin{subtable}{0.48\textwidth}
     \centering
\resizebox{0.95\textwidth}{!}{
    \begin{tabular}{lrrrr >{\centering\hspace{0.5pt}}m{0cm}}
    \toprule
GT (\textbf{7}) & $K\leq 7$ &  $K\leq 10$ & $K\leq 12$ & $K\leq 15$\\
\cmidrule{1-5}
\textit{cover hole}      & \textbf{}&  \textbf{}   & \textbf{}  & \textbf{take piece}   \\ 
    & \textbf{}&  \textbf{}   & \textbf{}  & \textbf{keep soil}   \\ 
        & \textbf{}&  \textbf{}   & \textbf{}  & \textbf{stop soil}   \\ 
        
\textit{take plant}   & \textbf{take plant}&  \textbf{take plant}  & \textbf{take plant}  & \textbf{take plant}  \\ 
\textit{put soil}     & \textbf{use soil}& \textbf{use soil}&\textbf{use soil}&\textbf{use soil}\\
\textit{loosen root}  & \textbf{loosen soil}&  \textbf{loosen soil}&\textbf{loosen soil}&\textbf{loosen soil}\\
\textit{place plant}  & \textbf{place plant}&   \textbf{place plant}&  \textbf{place plant}&  \textbf{place plant}\\
\textit{add top}      & \textbf{add soil}& \textbf{add soil}& \textbf{add soil}& \textbf{add soil}\\
& \textbf{}&  \textbf{}   & \textbf{}  & \textbf{fill pot}   \\
& \textbf{}&  \textbf{}   & \textbf{}  & \textbf{get soil}   \\ 
& \textbf{}&  \textbf{}   & \textbf{}  & \textbf{give drink}   \\ 
\textit{water plant}  & \textbf{water plant}&  \textbf{water plant}&\textbf{water plant}&\textbf{water plant}\\
& \textbf{}&  \textbf{}   & \textbf{}  & \textbf{give watering}   \\ 
\midrule
Precision  & 1    &  1      & 1    &  0.54  \\
Recall     & 0.86 &  0.86   & 0.86 &  1 \\

        \bottomrule
    \end{tabular}
}
     \caption{Repot a plant}
     \end{subtable}        
     \hspace*{\fill}%
     \begin{subtable}{0.48\textwidth}
          \centering
     \resizebox{0.95\textwidth}{!}{
      \footnotesize
    \begin{tabular}{lrrrr >{\centering\hspace{0.5pt}}m{0cm}}
    \toprule
GT (\textbf{7}) & $K\leq 7$ &  $K\leq 10$ & $K\leq 12$ & $K\leq 15$\\
\cmidrule{1-5}
\textit{open airway}  & \textbf{open airway}&    \textbf{open airway} & \textbf{open airway} & \textbf{open airway}   \\ 
\textit{check response}  & \textbf{}&    \textbf{}  &  \textbf{} & \textbf{}   \\ 
\textit{call 911}  & \textbf{}&    \textbf{}  &  \textbf{} & \textbf{}   \\ 
\textit{check breathing}  & \textbf{}&    \textbf{}  &  \textbf{} & \textbf{}   \\ 
\textit{check pulse}  & \textbf{}&    \textbf{put hand}  &  \textbf{put hand}  & \textbf{put hand}   \\ 
\textit{}  & \textbf{tilt head} &    \textbf{tilt head}  &  \textbf{tilt head} & \textbf{tilt head}   \\  
\textit{}  &  \textbf{lift chin} &    \textbf{lift chin}  &  \textbf{lift chin} & \textbf{lift chin}   \\  
\textit{give breath}  & \textbf{give breath}&    \textbf{give breath}  &  \textbf{give breath}& \textbf{give breath}   \\ 
\textit{give compression}  & \textbf{do compr.}&    \textbf{do compr.}  &  \textbf{do compr.} & \textbf{do compr.}   \\ 
\textit{}  & \textbf{open airway}&    \textbf{open airway}  &  \textbf{open airway} & \textbf{open airway}   \\  
\textit{}  & \textbf{}&    \textbf{start compr.}  &  \textbf{start compr.} & \textbf{start compr.}   \\  
\textit{}  & \textbf{}&    \textbf{}  &  \textbf{} & \textbf{continue cpr}   \\ 
\textit{}  & \textbf{}&    \textbf{do compr.}  &  \textbf{do compr.} & \textbf{do compr.}   \\ 
\textit{}  & \textbf{}&    \textbf{}  &  \textbf{} & \textbf{put hand}   \\ 
\textit{}  & \textbf{}&    \textbf{give breath}  &  \textbf{give breath} & \textbf{give breath}   \\ 

\midrule
Precision  &  0.	5 &  0.4      &  0.4 &  0.33  \\
Recall     &  0.43 &  0.57    & 0.57 & 0.57   \\
\bottomrule
    \end{tabular}
   } 
     \caption{Performing CPR}
     \end{subtable}
   
   
%
     
%
%
%
%
%
\centering
\footnotesize %

     \begin{subtable}{0.48\textwidth}
     \centering
\resizebox{0.95\textwidth}{!}{     
    \begin{tabular}{lrrrr >{\centering\hspace{0.5pt}}m{0cm}}
    \toprule
GT (\textbf{12}) & $K\leq 7$ &  $K\leq 10$ & $K\leq 12$ & $K\leq 15$\\
\cmidrule{1-5}
\textit{get cars}  & \textbf{}&    \textbf{}  &  \textbf{} & \textbf{}   \\ 
\textit{open hood}  & \textbf{}&    \textbf{}  &  \textbf{} & \textbf{}   \\ 
\textit{}  & \textbf{}&    \textbf{}  &  \textbf{} & \textbf{have terminal}   \\  
\textit{}  & \textbf{}&    \textbf{}  &  \textbf{attach cab.} & \textbf{attach cab.}   \\ 
\textit{connect red A}  & \textbf{connect cable}&    \textbf{conn. cable}  &  \textbf{conn. cable} & \textbf{conn. cable}   \\
\textit{}  & \textbf{}&    \textbf{}  &  \textbf{} & \textbf{conn. clamp}   \\  
\textit{}  & \textbf{charge battery}&    \textbf{charge batt.}  &  \textbf{charge batt.} & \textbf{charge batt.}   \\ 
\textit{connect red B}  & \textbf{connect end}&    \textbf{conn. end}  &  \textbf{conn. end} & \textbf{conn. end}   \\ 
\textit{connect black A}  & \textbf{}&    \textbf{}  &  \textbf{conn. cab.} & \textbf{conn. cab.}   \\ 
\textit{connect ground}  & \textbf{}&    \textbf{}  &  \textbf{have cab.} & \textbf{have cab.}   \\ 
\textit{start car A}  & \textbf{start car}&    \textbf{start car}  &  \textbf{start car} & \textbf{start car}   \\ 
\textit{start car B}  & \textbf{}&    \textbf{}  &  \textbf{start vehicle} & \textbf{start veh.}   \\ 
\textit{}  & \textbf{}&    \textbf{}  &  \textbf{start engine} & \textbf{start eng.}   \\ 
\textit{remove ground}  & \textbf{remove cable}&    \textbf{rem. cable}  &  \textbf{rem. cable} & \textbf{rem. cable}   \\ 
\textit{remove black A}  & \textbf{disconnect cable}&    \textbf{disc. cable}  &  \textbf{disc. cable} & \textbf{disc. cable}   \\ 
\textit{remove red B}  & \textbf{}&    \textbf{}  &  \textbf{} & \textbf{}   \\ 
\textit{remove red A}  & \textbf{}&    \textbf{}  &  \textbf{} & \textbf{}   \\ 
        
        \midrule
Precision  & 0.83  &  0.83   &  0.72 &  0.69  \\
Recall     & 0.42  &  0.42   & 0.67 &  0.67 \\
\bottomrule
    \end{tabular}
}
     \caption{Jumping cars}
     \end{subtable}%

%
 \caption{\small
  Automatically recovered sequences of steps for the five tasks considered in this work. 
        Each recovered step is represented by one of the aligned direct object relations  (shown in bold). 
        Note that most of the recovered steps correspond well to the ground truth steps (showed in italic).
        The results are shown for setting the maximum number of discovered steps, $K = \{7,10,12,15\}$. 
        Note how our method automatically selects less than $K$ steps in some cases.
         These are the automatically chosen $k\leq K$ steps that are the most salient in the aligned narrations as described in Sec.~\ref{subsec:model_text}.  
%
        }    
    \label{tab:script_res} 
\end{table*}


\setlength{\tabcolsep}{6pt}




\begin{figure*}[!ht]
%
\includegraphics[width=\linewidth]{figs/qual_res_changing_tire_v2}
%
\caption{\footnotesize {\bf Examples of the recovered instruction steps for the task ``Changing the car tire".} 
%
%
}

\label{fig:qualitative_results_changing_tire} 
\end{figure*}

\begin{figure*}[!ht]
%
\includegraphics[width=\linewidth]{figs/qual_res_jump_car_v2}
%
\caption{\footnotesize {\bf Qualitative results for the task ``Jumping cars".}
%
}



\label{fig:qualitative_results_jump_car} 
\end{figure*}


\begin{figure*}[!ht]
%
\includegraphics[width=\linewidth]{figs/qual_res_repot_v2}
%
\caption{\footnotesize {\bf Qualitative results for the task ``Repot a plant".}
%
}

\label{fig:qualitative_results_repot} 
\end{figure*}


\begin{figure*}[!ht]
%
\includegraphics[width=\linewidth]{figs/qual_res_coffee_v2}
%
\caption{\footnotesize {\bf Qualitative results for the task ``Making coffee".}
%
}
\label{fig:qualitative_results_coffee} 
\end{figure*}


\begin{figure*}[!ht]
%
\includegraphics[width=\linewidth]{figs/qual_res_cpr_v2}
%
\caption{\footnotesize {\bf Qualitative results for the task ``Performing CPR".}
%
}

\label{fig:qualitative_results_cpr} 
\end{figure*}





\end{document}



\documentclass{article}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{color}
\usepackage{comment}
\usepackage{fancyvrb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{latexsym}
\usepackage{listings}
\usepackage{multicol}
\usepackage{multirow}
\usepackage[normalem]{ulem}
\usepackage{tabularx}
\usepackage{times}
\usepackage{url}
\usepackage[utf8]{inputenc}
\usepackage{xspace}
\usepackage{wrapfig}
\usepackage[]{units}
\usepackage{subfig}
\usepackage{siunitx}
\usepackage[nonatbib,final]{nips_2017} 
\usepackage{natbib}
\sisetup{output-exponent-marker=\ensuremath{\mathrm{e}}}

%\allowdisplaybreaks

\setlength{\columnsep}{.5in}

\newcommand{\obs}{\text{obs}}
\newcommand{\mis}{\text{mis}}

\newcommand{\qt}[1]{\left<#1\right>}
\newcommand{\ql}[1]{\left[#1\right]}
\newcommand{\hess}{\mathbf{H}}
\newcommand{\jacob}{\mathbf{J}}
\newcommand{\hl}{HL}
\newcommand{\cost}{\mathcal{L}}
\newcommand{\lout}{\mathbf{r}}
\newcommand{\louti}{r}
\newcommand{\outi}{y}
\newcommand{\out}{\mathbf{y}}
\newcommand{\gauss}{\mathbf{G_N}}
\newcommand{\eye}{\mathbf{I}}
\newcommand{\softmax}{\phi}
\newcommand{\targ}{\mathbf{t}}
\newcommand{\metric}{\mathbf{G}}
\newcommand{\sample}{\mathbf{z}}
\newcommand{\f}{\text{f}}

\newcommand{\bmx}[0]{\begin{bmatrix}}
\newcommand{\emx}[0]{\end{bmatrix}}
\newcommand{\qexp}[1]{\left<#1\right>}
\newcommand{\vect}[1]{\mathbf{#1}}
\newcommand{\vects}[1]{\boldsymbol{#1}}
\newcommand{\matr}[1]{\mathbf{#1}}
\newcommand{\var}[0]{\operatorname{Var}}
\newcommand{\std}[0]{\operatorname{std}}
\newcommand{\cov}[0]{\operatorname{Cov}}
\newcommand{\diag}[0]{\operatorname{diag}}
\newcommand{\matrs}[1]{\boldsymbol{#1}}
\newcommand{\va}[0]{\vect{a}}
\newcommand{\vb}[0]{\vect{b}}
\newcommand{\vc}[0]{\vect{c}}
\newcommand{\ve}[0]{\vect{e}}
\newcommand{\vn}[0]{\vect{n}}
\newcommand{\vh}[0]{\vect{h}}
\newcommand{\vv}[0]{\vect{v}}
\newcommand{\vx}[0]{\vect{x}}
\newcommand{\vz}[0]{\vect{z}}
\newcommand{\vw}[0]{\vect{w}}
\newcommand{\vs}[0]{\vect{s}}
\newcommand{\vf}[0]{\vect{f}}
\newcommand{\vi}[0]{\vect{i}}
\newcommand{\vo}[0]{\vect{o}}
\newcommand{\vy}[0]{\vect{y}}
\newcommand{\vg}[0]{\vect{g}}
\newcommand{\vm}[0]{\vect{m}}
\newcommand{\vu}[0]{\vect{u}}
\newcommand{\vL}[0]{\vect{L}}
\newcommand{\vr}[0]{\vect{r}}
\newcommand{\vp}[0]{\vect{p}}

\newcommand{\mW}[0]{\matr{W}}
\newcommand{\mP}[0]{\matr{P}}
\newcommand{\mM}[0]{\matr{M}}
\newcommand{\mE}[0]{\matr{E}}
\newcommand{\mG}[0]{\matr{G}}
\newcommand{\mX}[0]{\matr{X}}
\newcommand{\mQ}[0]{\matr{Q}}
\newcommand{\mU}[0]{\matr{U}}
\newcommand{\mF}[0]{\matr{F}}
\newcommand{\mV}[0]{\matr{V}}
\newcommand{\mA}{\matr{A}}
\newcommand{\mC}{\matr{C}}
\newcommand{\mD}{\matr{D}}
\newcommand{\mS}{\matr{S}}
\newcommand{\mI}{\matr{I}}

\newcommand{\td}[0]{\text{d}}
\newcommand{\TT}[0]{\vects{\theta}}
\newcommand{\PP}[0]{\vects{\phi}}
\newcommand{\PS}[0]{\vects{\psi}}
\newcommand{\vsig}[0]{\vects{\sigma}}
\newcommand{\valpha}[0]{\vects{\alpha}}
\newcommand{\vmu}[0]{\vects{\mu}}
\newcommand{\vsigma}[0]{\vects{\sigma}}
\newcommand{\vepsilon}[0]{\vects{\epsilon}}
\newcommand{\vzero}[0]{\vect{0}}
\newcommand{\tf}[0]{\text{m}}
\newcommand{\tdf}[0]{\text{dm}}
\newcommand{\grad}[0]{\nabla}
\newcommand{\alert}[1]{\textcolor{red}{#1}}
\newcommand{\N}[0]{\mathcal{N}}
\newcommand{\LL}[0]{\mathcal{L}}
\newcommand{\HH}[0]{\mathcal{H}}
\newcommand{\NN}[0]{\mathcal{N}}
\newcommand{\RR}[0]{\mathbb{R}}
\newcommand{\II}[0]{\mathbb{I}}
\newcommand{\Scal}[0]{\mathcal{S}}
\newcommand{\sigmoid}{\text{sigmoid}}
\newcommand{\Bernoulli}{\text{Bernoulli}}
\newcommand{\E}[0]{\mathbb{E}}
\newcommand{\enabla}[0]{\ensuremath{%
    \overset{\raisebox{-0.3ex}[0.5ex][0ex]{%
    \ensuremath{\scriptscriptstyle e}}}{\nabla}}}
\newcommand{\enhnabla}[0]{\nabla_{\hspace{-0.5mm}e}\,}

\renewcommand*{\thefootnote}{\fnsymbol{footnote}}

\newcommand{\todo}[1]{{\Large\textcolor{red}{#1}}}
\newcommand{\done}[1]{{\Large\textcolor{green}{#1}}}
\newcommand{\dd}[1]{\ensuremath{\mbox{d}#1}}

\DeclareMathOperator*{\argmax}{\arg \max}
\DeclareMathOperator*{\argmin}{\arg \min}
\newcommand{\newln}{\\&\quad\quad{}}

\newcommand{\Ax}{\mathcal{A}_x}
\newcommand{\Ay}{\mathcal{A}_y}
\newcommand{\ola}{\overleftarrow}
\newcommand{\ora}{\overrightarrow}
\newcommand{\ov}{\overline}
\newcommand{\ts}{\rule{0pt}{2.6ex}}       % Top strut
\newcommand{\ms}{\rule{0pt}{0ex}}         % Middle strut
\newcommand{\bs}{\rule[-1.2ex]{0pt}{0pt}} % Bottom strut
\newcommand{\specialcell}[2][c]{%
  \begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}

\newcommand{\yb}[2]{\textcolor{red}{#1 {\em (YB: #2)}}}
\newcommand{\rdh}[2]{\textcolor{blue}{#1 {\em (RDH: #2)}}}

\title{Boundary-Seeking Generative Adversarial Networks}

\author{R Devon Hjelm\thanks{Authors contributed equally.}\\
University of Montreal\\
Montreal Institute for Learning Algorithms\\
\texttt{erroneus@gmail.com}
\And
Athul Paul Jacob\footnotemark[1]\\ 
University Of Waterloo\\
Montreal Institute for Learning Algorithms\\
\And
Tong Che\\
University of Montreal\\
Montreal Institute for Learning Algorithms\\
\And
Kyunghyun Cho\\
Courant Institute \& Center for Data Science \\
New York University\\
\And
Yoshua Bengio\\
University of Montreal\\
Montreal Institute for Learning Algorithms
}

\begin{document}

\maketitle

\begin{abstract}
We introduce a novel approach to training generative adversarial networks~\citep[GANs, ][]{goodfellow2014generative}, where we train a generator to match a target distribution that converges to the data distribution at the limit of a perfect discriminator.
This objective can be interpreted as training a generator to produce samples that lie on the decision boundary of the current discriminator in training at each update, and we call a GAN trained using this algorithm a boundary-seeking GAN (BGAN).
This approach can be used to train a generator with discrete output when the generator outputs a parametric conditional distribution. We demonstrate the effectiveness of the proposed algorithm with discrete image and character-based natural language generation. 
Finally, we notice that the proposed boundary-seeking algorithm works even with continuous variables, and demonstrate its effectiveness with various natural image benchmarks.
\end{abstract}

\section{Introduction}
%Generative models provide a means of representing the underlying structure in data. 
%While the objective is to faithfully generate data with representative statistics, the strength of generative models lies partially in representing the data as a hypothetical compressed version of its underlying structure.
Generative adversarial networks~\citep[GAN, ][]{goodfellow2014generative} involve a unique generative learning framework that uses two separate models with opposing, competing, or \emph{adversarial} objectives.
In contrast to those models that generate the data from a parameteric distribution, such as directed graphical models (e.g., Helmholtz machines~\citep{dayan1995helmholtz} and variational autoencoders~\citep[VAEs,][]{kingma2013auto}) or undirected graphical models (e.g., restricted Boltzmann machines~\citep[RBMs,][]{hinton2002training} and deep Boltzmann machines~\citep[DBMs,][]{salakhutdinov2009deep}), GANs do not rely on maximum likelihood estimation~\citep[MLE,][]{dempster1977maximum} to learn.

Rather, training a GAN, which consists of two competing networks--a discriminator and a generator-- only requires back-propagating a learning signal originating from a learned objective function corresponding to the loss of a discriminator trained in an adversarial way.
This framework is powerful, as it trains a generator without relying on an explicit formulation of the probability function which requires marginalizing out latent variables. 
Also, it avoids the issue with the MLE objective which makes an MLE-trained model conservative in probability mass placement. 
The latter issue is known to be a problem behind MLE-trained models generating samples that lack real-world qualities, often evidenced by the lack of sharpness in natural images~\citep{goodfellow2016nips}.
Unlike MLE-trained models, GANs have been shown to generate often-diverse and realistic samples even when trained on high-dimensional large-scale data~\citep{radford2015unsupervised} in the case of continuous variables.

GANs however have a serious limitation on the type of variables they can model, because they require the composition of the generator and discriminator to be fully differentiable.
With discrete variables, this is not true. For instance, consider using a step function at the end of a generator in order to generate a discrete value. 
In this case, back-propagation alone cannot provide the training signal for the generator, because the derivative of a step function is 0 almost everywhere (and so would be the back-propagated signal). 
This is problematic, as many important real-world datasets are discrete, such as character- or word-based representations of language.
The general issue of credit assignment for computational graphs with discrete operations (e.g. discrete stochastic neurons) is difficult, and only approximate solutions have been proposed in the past~\citep{bengio2013estimating,gu2015muprop,gumbel1954statistical,jang2016categorical, maddison2016concrete, tucker2017rebar}.

%This situation is analogous to that in training Helmholtz machines with discrete hidden variables (i.e., sigmoid belief networks~\citep[SBNs,][]{saul1996mean}), where advanced learning algorithms are necessary to train an approximate inference network~\citep{mnih2014neural,bornschein2014reweighted}. When those hidden units are continuous, usual backpropagation with a re-parametrization technique works~\citep{kingma2013auto}. On the other hand, when they are discrete, only recently an approximate re-parametrization technique, called ``Gumbel-Softmax technique'', was introduced~\citep{gumbel1954statistical, jang2016categorical, maddison2016concrete}. The effectiveness of this approach was only demonstrated with simple directed models having a single layer of discrete latent variables, and there is no consensus nor evidence whether it works with GANs.

In this paper, we introduce a novel learning algorithm for GANs. This algorithm estimates the gradient of the discriminator's output w.r.t. the generator as the weighted sum of the gradients of the log-probabilities of the samples generated from the generator. The weights are given by the discriminator's performance on the samples, such that those samples that look similar to true training examples, are weighted more (because they would be scored highly by the discriminator). This lets us interpret the discriminator as computing a target distribution for the discrete stochastic units, allowing us to train a generator to converge toward the hypothetical true distribution of the data as the discriminator is optimized.
%This distribution was discovered independently from us by \citet{che2017maximum} in the context of language modeling.
% I think it's best to omit any mentions of this paper during review.
This objective can further be interpreted as an alternative loss on the discriminator output, which allows us to derive a novel learning algorithm for GANs even with continuous variables. Under this interpretation, the learning objective of a generator is to minimize the difference between the discriminator's log-probabilities for the sample being positive and negative. This objective can be thought of as training the generator to {\em place generated samples at the decision boundary of the discriminator}, and hence we call a GAN trained with the proposed algorithm a boundary-seeking generative adversarial network (BGAN).

We demonstrate the effectiveness of the proposed BGAN in both settings, with discrete as well as continuous variables. We use MNIST and quantized CelebA for the discrete image setting, and street view house numbers~\citep[SVHN,][]{netzer2011reading}, CIFAR-10~\citep{krizhevsky2009learning}, CelebA~\citep{liu2015deep}, and the Caltech-USCB birds dataset~\citep[][]{WelinderEtal2010} for the continuous image setting.
We compare to competing GAN objectives using the CelebA dataset. 
%In the case of discrete variables, we also try the Gumbel-Softmax technique, however, without much success, unlike the proposed BGAN which we found extremely easy to train. % Because of REBAR, we are moving this to the appendix.
We also demonstrate discrete BGAN on a simple character-based model on the 1-billion word dataset~\citep{chelba2013one}, showing successful learning and stability for natural language generation. 
In the case of continuous variables, we observe that the proposed algorithm works qualitatively better than conventional GANs.

\section{Methods}
\subsection{Generative Adversarial Networks}
The generative adversarial networks~\citep[GAN, ][]{goodfellow2014generative} framework combines two models: a generator model pitted against a discriminatory adversary.
The generator is composed of a prior distribution, $p(\vz)$, over latent variable $\vz$ and a generator function, $G(\vz)$, which maps from the space of $\vz$ to the space of data $\vx$.
The objective of the discriminator is to correctly distinguish between data and samples from the generator by maximizing
\begin{align}
\mathbb{E} \left[\log D(\vx) \right]_{\vx \sim p_{\text{data}}(\vx)} + \mathbb{E} \left[ \log(1 - D(G(\vz)) \right]_{\vz \sim p(\vz)},
\label{eq:gan}
\end{align}
where $D(.)$ is a discriminator function with output that spans $[0, 1]$.
The generator is trained to ``fool" the discriminator, that is minimize $\mathbb{E} \left[ \log(1 - D(G(\vz)) \right]_{\vz \sim p(\vz)}$ or to minimize a proxy $\mathbb{E} \left[ -\log(D(G(\vz)) \right]_{\vz \sim p(\vz)}$.
Ignoring the proxy case, the two models play a minimax game with value function:
\begin{align}
\min_G \max_D V(G, D) = \mathbb{E}[\log D(\vx)]_{\vx \sim p(\vx)} + \mathbb{E}[\log (1 - D(G(\vz)))]_{\vz \sim p(\vz)}.
\label{eq:minmax}
\end{align}

%Previous deep generative models like the variational auto-encoder (VAE)~\citep{kingma2013auto} involve a neural network which takes the latent sampled values $\vz$ and  outputs parameters of a distribution (e.g. Bernoulli, Gaussian) from which the samples are drawn. 
%Instead, t
The GAN generator is defined by the composition of sampling the latent values $\vz$ and applying the deterministic function $G$. The generative objective involves one continuous function with parameters from both models: $D(G(.;\PS);\PP)$, but optimized over $\PP$ only. This allows for training a generative model with a relatively simple fitness function and back-propagation, rather than a complex marginalized likelihood function, and which does not suffer from some of the learning difficulties of maximum likelihood
estimators (MLE), e.g., giving rise to blurred samples.

In practice, the generator, rather than being trained to minimize the above value function, can be trained to maximize the proxy $\log(D(G(\vz)))$ or $\log(D(G(\vz))) - \log(1 - D(G(\vz)))$, which can alleviate some learning issues related to the discriminator loss saturating early in learning and improve stability.
In addition, recent advances using mean-squared error~\citep[LSGAN,][]{mao2016least} or the earth-movers distance~\citep[WGAN,][]{arjovsky2017wasserstein} as alternative learning signals for both the discriminator and generator can improve the approach.
%With the basic GAN architecture, both generator and discriminator are feedforward networks, $D(.; \PP)$ and $G(.; \PS)$, while in deep convolutional GANs~\citep[DCGAN,][]{radford2015unsupervised}, the discriminator and generator are convolutional neural networks with strided and fractionally-strided convolutional layers, respectively.

In order for a generating function, $G(.;\PS)$, with real-valued parameters, $\PS$, to generate discrete-valued outputs, $\vx$, $G(.;\PS)$ cannot be fully continuous.
A natural choice for the generator would be a continuous function, $\vy = f(\vz)$ (e.g., a feed forward network), followed by a step function.
However, as the gradients that would otherwise be used to train the generator are zero almost everywhere, it is not possible to train the generator from the discriminator error signal.
Approximations for the back-propagated signal exist~\citep{bengio2013estimating,gu2015muprop,gumbel1954statistical,jang2016categorical,maddison2016concrete,tucker2017rebar}.
We propose a novel approach to dealing with this issue, based on estimating
a proposal target distribution for the generator output.

\subsection{Target distribution for the generator}
First consider the optimal discriminator function~\citep{goodfellow2014generative},
$D(\vx)^{\star}$, derived from 
analytically optimizing the discriminator objective given a fixed generator $G(.)$ and assuming no functional constraints on the discriminator, i.e., in a non-parametric setting:
\begin{align}
D^{\star}_G(\vx) = \frac{p_{\text{data}}(\vx)}{p_{\text{data}}(\vx) + p_g(\vx)},
\label{eq:opt_disc}
\end{align}
where $p_{\text{data}}(\vx)$ is the data distribution, and $p_g(\vx)$ is the distribution as generated by $p(\vz)$ and $G(\vz)$.
Given this optimal discriminator, the density of the data can then be written as:
\begin{align}
p_{\text{data}}(\vx) = p_g(\vx) \frac{D^{\star}_G(\vx)}{1 - D^{\star}_G(\vx)}.
\end{align}
This indicates that, in the limit of a perfect discriminator, the true distribution can be perfectly estimated from a said discriminator and any fixed generator, even if that generator is not
perfectly trained. {\em A sample from that corrected estimator can thus be obtained by reweighing 
samples from the generator according to the ratio} $\frac{D^{\star}_G(\vx)}{1 - D^{\star}_G(\vx)}$.
Unfortunately, it is unlikely that such a perfect discriminator is learnable or even exists.
However, one can posit the following estimator for the true distribution, given an imperfect discriminator, $D(\vx)$:
\begin{align}
\tilde{p}(\vx) = \frac{1}{Z} p_g(\vx) \frac{D(\vx)}{1 - D(\vx)},
\quad \text{where} \quad
Z = \sum_{\vx} p_g(\vx) \frac{D(\vx)}{1 - D(\vx)}
\label{eq:target}
\end{align}
is the normalization constant that guarantees that $\tilde{p}(\vx)$ is a proper probability distribution.
This distribution was discovered independently from us by \citet{che2017maximum}.
Note that the optimum for the generator occurs at $D(\vx) = D^{\star}(\vx) = 1/2$, so that $Z = 1$ and \mbox{$\tilde{p}(\vx) = p_g(\vx) = p(\vx)$}.
$\tilde{p}(\vx)$ is a potentially biased estimator for the true density; however, {\em the bias only depends on the quality of $D(\vx)$}: the closer $D(\vx)$ is to $D^{\star}(\vx)$, the lower the bias. This is an important consideration because 
it is likely that training the discriminator (a standard probabilistic binary classifier)
is much easier than training the generator (whose objective function is a moving
target as the discriminator adapts to the generator and vice-versa).
The optimum for the generator exists 
when generated samples have equal probability of being classified as data or as samples.
This occurs at the decision boundary of the discriminator, so that we call this approach boundary-seeking generative adversarial networks (BGAN).


\subsection{BGAN with discrete variables}
For discrete variables, parameterizing the generating distribution directly reveals a method for training the generator without relying on back-propagation through the generator's 
discrete output.
As with previous work on evaluating GANs~\citep{wu2016quantitative}, we parameterize $p_g(\vx)$ as a the marginalization of a joint density, $p_g(\vx) = \sum_{\vz} g(\vx | \vz) p(\vz)$, rephrasing the generator function, $G(\vz)$, as a conditional distribution, $g(\vx | \vz)$.
To minimize the distance between $\tilde{p}(\vx)$ and $p_g(\vx)$, we can minimize the exclusive KL divergence, so that the gradients w.r.t. the generator parameters, $\PS$, are (the full derivation can be found in the Appendix):
\begin{align}
\nabla_{\PS} D_{KL}(\tilde{p}(\vx) || p_g(\vx)) = -\frac{1}{Z} \mathbb{E}\left[\sum_\vx g(\vx | \vz) \frac{D(\vx)}{1 - D(\vx)} \nabla_{\PS} \log p_g(\vx)\right]_{\vz \sim p(\vz)},
\label{eq:grad-KL}
\end{align}
where $\tilde{p}(\vx)$ is held fixed.
Note that, while $\tilde{p}(\vx)$ depends on the generator parameters, $\PS$, we interpret this distribution as the current estimate of the true density and hence a {\em target} for the generator.
The gradients would require some sort of Monte-Carlo (MC) estimator across the input and output noise, and will have high variance, notably due to the partition function, $Z$.
The intuition here is to note that, as the conditional density, $g(\vx | \vz)$, is unimodal, it can be used to define a low-variance estimator analogous to that of Equation~\ref{eq:target}.
Following this intuition and by inspection of Eq.~\ref{eq:grad-KL}, we propose updating the joint distribution, $p(\vx, \vz) = g(\vx | \vz) p(\vz)$, to fit the joint target distribution:
\begin{align}
\tilde{p}(\vx, \vz) = \tilde{p}(\vx | \vz) p(\vz) = \frac{1}{Z_{|\vz}} g(\vx | \vz) \frac{D(\vx)}{1 - D(\vx)} p(\vz),
\label{eq:target2}
\end{align}
where $\tilde{p}(\vx |\vz)$ is an estimate of the data in the neighborhood of the generated samples defined by $\vz$ and $Z_{|\vz} = \sum_{\vx} g(\vx | \vz) \frac{D(\vx)}{1 - D(\vx)}$ is the partition function that ensures $\tilde{p}(\vx |\vz)$ is a proper probability distribution.
Note we are no longer targeting marginal of Equation~\ref{eq:target}, as the marginal of the joint given by Equation~\ref{eq:target2} over $\vz$,
%\begin{align}
%\tilde{p}(\vx) = \sum_{\vz} \frac{1}{Z_{|\vz}} g(\vx | \vz) \frac{D(\vx)}{1 - D(\vx)} p(\vz),
%\end{align}
which is also normalized, has the same convergence behavior as Equation~\ref{eq:target}.
The gradients then can be estimated as the exclusive KL divergence of the joint distributions:
\begin{align}
&\nabla_{\PS} D_{KL}\left(\tilde{p}(\vx, \vz) || p_g(\vx, \vz)\right) 
\approx -\mathbb{E} \left[\tilde{p}(\vx | \vz) \nabla_{\PS} \log g(\vx | \vz) \right]_{\vz \sim p(\vz)} 
\nonumber \\
&= -\mathbb{E} \left[\frac{1}{Z_{|\vz}} \sum_\vx g(\vx | \vz) \frac{D(\vx)}{1 - D(\vx)} \nabla_{\PS} \log g(\vx | \vz) \right]_{\vz \sim p(\vz)}
\approx -\mathbb{E}\left[ \sum_m \tilde{w}^{(m)} \nabla_{\PS} \log g(\vx^{(m)} | \vz)\right]_{\vz \sim p(\vz)},
\nonumber \\
&\text{where} \quad
\tilde{w}^{(m)} = \frac{w^{(m)}}{\sum_{m'} w^{(m')}} 
\quad
\text{and} \quad
w^{(m)} = \frac{D(\vx^{(m)})}{1 - D(\vx^{(m)})} \nonumber
\end{align}
are the normalized and unnormalized importance weights respectively (note
how $Z_{|\vz}$ cancels in the normalization) and $\vx^{(m)}$
are samples from the generator for the given $\vz$.
The normalization can help generate a relative learning signal, which will help in the case that the samples are all fairly bad from the point of view of $D(\vx)$.
The benefit of using Equation~\ref{eq:target2} over Equation~\ref{eq:target} is clear: the normalized weights, $\tilde{w}^{(m)}$, when $\vx^{(m)}$ is sampled from a unimodal distribution, will be of low variance compared to those estimated by marginalizing over $\vz$.
The gradients can be computed over a mini-batch of samples from $p(\vz)$, % so that the updates to the generator parameters, $\PS$, become:
%\begin{align}
%\Delta \PS \propto \frac{1}{N} \sum_n \sum_m \tilde{w}^{(m)} \nabla_{\PS} \log g(\vx^{(m)} | \vz^{(n)}).
%\end{align}
%This 
which reveals a straightforward procedure for training discrete GANs with a parametric conditional generator without relying on back-propagation.
Compared to normal GANs, this method requires $M$ times more space, 
but computing the $M$ (instead of 1) scalar values of $D(\vx^{(m)})$ can be parallelized,
as samples from $g(\vx|\vz)$ can be drawn independently.

One could have derived a similar learning objective using the {\em inclusive} KL divergence (rather than the exclusive in our formulation), so that the gradients become (see Appendix for details):
\begin{align}
\nabla_{\PS} D_{KL}(p_g(\vx, \vz) || \tilde{p}(\vx, \vz)
\approx - \mathbb{E} \left[\frac{1}{M} \sum_m  (\log{w^{(m)}} - \log{Z_{|\vz}} - 1) \nabla \log{g(\vx^{(m)} | \vz)}\right]_{\vz \sim p(\vz)}.
\label{eq:bgan_reinforce}
\end{align}
This gradient strongly resembles REINFORCE, and in fact we can interpret $\log{Z_{|\vz}}$ as an input-dependent baseline (using a deep neural network trained with MSE to predict $\log{Z_{|\vz}}$, using $\vz$ as input), as in neural variational inference and learning~\citep[NVIL,][]{mnih2014neural}.

\subsection{Continuous variables}
While we originally set out to find a means for training discrete GANs, the insight above can also be used to provide a learning algorithm in the continuous case.
For continuous variables, we could introduce a parametric conditional distribution (such as Gaussian) as we did in the discrete case, defining importance weights, and computing a gradient directly at the generator output using the KL divergence.
However, this would abandon some of the strengths central to GANs relative to other generative models with continuous data~\citep{goodfellow2016nips}.

While a tractable form of $p_g(\vx)$ is unavailable to us, a simple analysis of Equation~\ref{eq:target} shows that the distance (by a number of metrics) between $\tilde{p}(\vx)$ and $p_g(\vx)$ is minimized when $\frac{1}{Z} \frac{D(\vx)}{1 - D(\vx)} = 1$.
This indicates an alternative learning objective to training GANs for a generator of continuous data.
There are a number of loss functions that would be suitable, and we 
have experimented with a simple squared-error loss:
\begin{align}
\label{eq:bs_gan_cont}
\mathcal{L}_g(\vx) = \left( \log D(\vx) - \log (1 - D(\vx)) - \log{Z} \right) ^ 2.
\end{align}
An alternative is to use the cross-entropy with a target of $1 / 2$~\citep[this has been done in the context of domain adaptations, e.g.,][]{tzeng2015simultaneous}, though the above loss works very well in practice. In the case of discriminators with a simple sigmoid output, $D(\vx) = \sigma(r(\vx))$, where $\sigma(r) = \frac{1}{1 + e^{-r}}$, the loss simplifies to:
\begin{align}
\mathcal{L}_g(\vx) = \left( r(\vx) - \log{Z} \right) ^ 2.
\label{eq:bs_gan_cont2}
\end{align}
As the ratio $\frac{D(\vx)}{1 - D(\vx)}$ converges to one, so does the partition function, which can be used to formulate an alternative objective to Equation~\ref{eq:bs_gan_cont2}, but without the $\log{Z}$ term.
Though this also works in practice, in our experiments using an estimate of $\log{Z}$ can improve stability and results.
When used, we estimate $\log{Z}$ using a moving average of Monte-Carlo estimates for each update, $t$:
\begin{align}
\log{\tilde{Z}}_{t+1} = \gamma \log{\tilde{Z}}_{t} + (1 - \gamma) \frac{1}{N}\log{\left(\sum_n \frac{D(\vx^{(n)}_t)}{1 - D(\vx^{(n)}_t)}\right)},
\label{eq:log_Z_est}
\end{align}
where $\gamma$ is hyper-parameter controlling the moving average, and $\vx^{(n)}_t$ are the batch of $N$ generated samples at the $t$-th step.

While the continuous-case loss has a very different form from that of the discrete cases, the optimum is the same and corresponds exactly with the minimum of the KL divergence.
The above loss function can be used as an alternative to the usual GAN generator objective for continuous data, and we demonstrate its effectiveness in our experiments below.

\section{Related Work}

\paragraph{GAN for discrete variables}

Our approach introduces new learning algorithms that allows for training generative adversarial networks (GANs) on discrete data. Discrete data with GANs is an active and unsolved area of research, particularly with language model data involving recurrent neural network (RNN) generators~\citep{yu2016seqgan, li2017adversarial}. Our importance sampling-based solution is analogous to reweighted wake-sleep~\citep[RWS,][]{bornschein2014reweighted}, a successful approach for training Helmholtz machines with discrete variables (but not in a GAN framework).

%While this discrete case of BGAN resembles REINFORCE, the motivations and approach are quite different, relying on normalized importance sampling rather than baselines to reduce variance.
Other REINFORCE-based methods have been proposed for language modeling~\citep{yu2016seqgan, che2017maximum}, but the solution we introduce is derived from the inclusive KL divergence following directly from the intuition of BGAN.
The REINFORCE estimator works, though less well compared to the importance sampling-based BGAN in our experiments of the following section.
This situation is analogous to performance of RWS vs NVIL in sigmoid belief networks (SBMs), where RWS outperforms the input-dependent baseline method.
Alternatively, advanced techniques~\citep{mnih2016variational} may improve the REINFORCE-based method, though we leave this for future work.
%From our framework, we were able to derive a new objective for training a generator in the case of continuous variables.

The Gumbel-Softmax technique is another approach to train Helmholz machines with discrete variables~\citep{jang2016categorical,maddison2016concrete}, which effectively allows for training them as variational autoencoders~\citep[VAEs,][]{kingma2013auto}.
However, this technique has only been shown to work in very limited settings, and has yet to been shown to work well with GANs with discrete data.
In our experiments, we find that this estimator does not help train GANs, and these results can be found in the Appendix.
More recent approaches, such as REBAR~\cite{tucker2017rebar}, may prove to be more successful, but this is left for future work.

\paragraph{Learning algorithms for GAN}

Since its introduction by \citet{goodfellow2014generative}, there have been a number of novel objectives for improving GAN training. Recently, \citet{arjovsky2017wasserstein}, proposed a modification to the learning procedure of GAN based on the earth-mover's distance. This method works very well for stabilizing GAN training, but necessarily limits the capacity of the discriminator to limit its Lipschitz constant. 
\citet{mao2016least} proposed a mean-squared error-based approach that improves mode coverage. 
Our work contributes to this pile of recent works, however, with a distinct goal of building a unified learning framework for both discrete and continuous variables.
In addition, LSGAN shares the same optimal discriminator as BGAN in the form of Equation~\ref{eq:opt_disc}.
This indicates an alternative learning objective for the generator in LSGANs, such that the target lies at the average of the real and fake targets of the discriminator (e.g., $1 / 2$ in a 0-1 coding).
Finally, traditional GANs rely on the generator pushing the generated samples in the direction of highest certainty as ``true" data, or towards infinity in the logit space.
This causes the well-known pathology of GANs sometimes pushing the samples beyond the true samples, thus causing learning to fail.
This pathology is explicitly absent from the GAN objective, which removes one possible source of instability (see Appendix for more details).

\paragraph{Implicit inference}
There is a strong connection to implicit variational inference with GANs~\citep{tran2017deep, huszar2017variational}, where the logit of a binary GAN ($r$ in Equation~\ref{eq:bs_gan_cont2}) can be used as a proxy for the log-ratio in the variational lower bound.
This indicates an alternative approach to encoder / decoder type GANs~\citep{dumoulin2016adversarially}, so that the objective is to ensure that the encoding and decoding distributions match.
This is left for future work.

\section{Discrete variables}

\subsection{Datasets and setup}
We first test our approach to training discrete generative adversarial networks (GANs) using two imaging benchmarks: the common discretized MNIST dataset~\citep{salakhutdinov2008quantitative} and a new quantized version of the CelebA dataset \citep[see ][for the original CelebA dataset]{liu2015deep}.
For CelebA quantization, we first downsampled the images from $64\times64$ to $32\times32$.
We then generated a $16$-color palette using Pillow, a fork of the Python Imaging Project (\href{https://python-pillow.org}{https://python-pillow.org}).
This palette was then used to quantize the RGB values of the CelebA samples to a one-hot representation of $16$ colors.
%The quantized CelebA dataset is available for download from \url{http://tdb.url}.
Next, we test BGAN with natural language generation with the 1-billion word dataset~\citep{chelba2013one} at the character-level, limiting the dataset to sentences of at least and truncating to $32$ characters.
As we found better results with the importance-weighted version of BGAN, we use this version for discrete data unless otherwise noted.
Our implementation for discrete BGAN as well as the continuous case in the next Section can be found at \href{https://github.com/rdevon/BGAN}{https://github.com/rdevon/BGAN}.

For imaging data, our models used deep convolutional GANs~\citep[DCGAN,][]{radford2015unsupervised}.
The generator is fed a vector of $64$ i.i.d. random variables drawn from a uniform distribution, $[0, 1]$.
%For MNIST, the generator was composed of dense layers with $1024$ and $128\times7\times7$ units followed by $2$ fractionally-strided convolutional layers with $64$ filters and $1$ filter, respectively.
%For quantized CelebA, the generator was composed of a dense layer with $512 \times 4 \times 4$ units followed by $3$ fractionally-strided convolutional layers with $256$, $128$, and $16$ filters respectively.
%The fractional stride was chosen to ensure that the filter size doubled at each layer, so that the output was $14$ channels of size $32\times32$.
%All layers other than the output layer of the generator were rectified linear units (ReLU), while all layers other than the output layer of the discriminator were leaky rectifiers.
The output nonlinearity was sigmoid for MNIST to model the Bernoulli centers $g(x_i | \vz)$ for each pixel, while the output was softmax for quantized CelebA.
%The discriminator used $3$ convolutional layers, the structure of which was the transpose of that in the generator, followed by a dense layer with a single-unit output and a sigmoid nonlinearity.
For character-level language generation, we follow recent work with advanced training of WGANs~\citep{gulrajani2017improved}, and use deep CNNs for both the generator and discriminator.
For the input-dependent baseline REINFORCE method in Equation~\ref{eq:bgan_reinforce}, we use a three layer fully-connected neural network with $256$ hidden units, leaky-ReLU units, and batch normalization~\citep[BN,][]{ioffe2015batch}.
We applied BN to the generator, as is consistent in the literature.
However, across our experiments with discrete datasets, we found that BN in the discriminator made for unstable learning and worse results.
We therefore omitted the BN from the discriminator.

The model was trained using the Adam method for stochastic optimization~\citep{kingma2014method} with exponential decay rates of $\beta_1 = 0.95$ and $\beta_2 = 0.5$.
MNIST was trained for $100$ epochs, while quantized CelebA was trained for $50$ epochs.
Unlike what is commonly done in the literature for training GANs~\citep{goodfellow2014generative, arjovsky2017wasserstein}, the discriminator was not updated multiple times for every generator update.
We trained the model with learning rates of \num{1e-4}, \num{1e-5}, and \num{1e-6} to test convergence properties.
The number of samples per generated examples, $M$, was 
set to $20$, which was used to generate the normalized weights and train the input-dependent baseline.

%\begin{figure}
%\begin{minipage}{0.49\textwidth}
%\centering
%\includegraphics[width=0.9\linewidth]{extra_figures/BS-GAN.png}
%\end{minipage}
%\hfill
%\begin{minipage}{0.49\textwidth}
%\caption{The evolution of the generator cost with varying learning rates when training a boundary-seeking GAN (BGAN). We observe a smooth convergence of the generator cost for all the learning rates, demonstrating the stability of learning with the proposed BGAN.}
%\label{fig:BGAN_cost}
%\end{minipage}
%\end{figure}

\begin{figure*}[bt]
\begin{minipage}{0.32\textwidth}
\includegraphics[width=0.99\columnwidth]{extra_figures/rand_gen_trunc}
\end{minipage}
\hfill
\begin{minipage}{0.66\textwidth}
\caption{
Left: Random samples from the generator trained as a boundary-seeking GAN (BGAN) with discrete MNIST data.
Shown are the Bernoulli centers of the generator conditional distribution.
Samples show high variability, with realistic generated hand-drawn digits.}
\label{fig:rand_gen}
\end{minipage}

\begin{minipage}{0.32\textwidth}
\includegraphics[width=0.99\columnwidth]{extra_figures/rand_gen_nvil}
\end{minipage}
\hfill
\begin{minipage}{0.66\textwidth}
\caption{
Left: Random samples from the generator trained using the REINFORCE-like algorithm with an input-dependent baseling.
Samples are of good quality, but the model is missing modes.}
\label{fig:rand_gen_nvil}
\end{minipage}

\begin{minipage}{0.32\textwidth}
\includegraphics[width=0.99\columnwidth]{extra_figures/celeba_gt}
\end{minipage}
\hfill
\begin{minipage}{0.32\textwidth}
\includegraphics[width=0.99\columnwidth]{extra_figures/celeba_gen}
\end{minipage}
\hfill
\begin{minipage}{0.32\textwidth}
\caption{
Left: Ground-truth 16-color (4-bit) quantized CelebA images downsampled to $32\times32$.
Right: Samples produced from the generator trained as a boundary-seeking GAN on the quantized CelebA for $50$ epochs.}
\label{fig:celeb_gt}
\end{minipage}
\vspace{-3mm}
\end{figure*}

\subsection{Generation result}

\begin{table}
\caption{Random samples drawn from a generator trained with the discrete BGAN objective.
The model is able to successfully learn many important character-level English language patterns.}
\small
\begin{tabular}{c | c | c}
And it 's miant a quert could he 
& He weirst placed produces hopesi 
& What 's word your changerg bette\\
" We pait of condels of money wi 
& Sance Jory Chorotic , Sen doesin
& In Lep Edger 's begins of a find", \\
Lankard Avaloma was Mr. Palin , 
& What was like one of the July 2 
& " I stroke like we all call on a\\
Thene says the sounded Sunday in 
& The BBC nothing overton and slea
& With there was a passes ipposing\\
About dose and warthestrinds fro 
& College is out in contesting rev
& And tear he jumped by even a roy
\end{tabular}
\label{tab:lm_res}
\end{table}


Our results showed that training the importance-weighted BGAN on discrete MNIST data is stable and produces realistic and highly variable generated handwritten digits.
Figure~\ref{fig:rand_gen} shows example digits from the model trained with a learning rate of \num{1e-4}, sampled randomly from the generator to demonstrate individual digit quality, respectively.
The model was  relatively robust against learning rate, showing stable learning for the generator across learning rates tested without any noticeable sensitivity to hyperparameters.
For quantized CelebA, the generator trained as a BGAN produced reasonably realistic images, which resemble the original dataset well.
In addition, the faces produced are highly variable, and show the type of diversity we expect from a good generator.

For language data, training with BGAN yielded stable, reliably good character-level generation (Table~\ref{tab:lm_res}), though generation is poor compared to recurrent neural network-based methods~\citep{sutskever2011generating,mikolov2012statistical}.
We are however not aware of any previous work in which a discrete GAN, without any continuous relaxation~\citep{gulrajani2017improved}, was successfully trained from scratch without any pretraining and without any auxiliary supervised loss to generate any sensible text . Despite the low quality of the text relative to the existing supervised recurrent language model, we find it a favourable evidence showing the stability and capability of the proposed boundary-seeking criterion for training discrete GAN.

\section{Continuous Variables}
\subsection{Datasets and setup}
We tested the boundary-seeking objective on street view house numbers~\citep[SVHN][]{netzer2011reading}, CIFAR-10~\citep{krizhevsky2009learning}, CelebA, and the Caltech-USCB birds dataset~\citep[][scaled to 64x64]{WelinderEtal2010}. 
As with our experiments with discrete data, we used the DCGAN architecture.
We used this same parameterization, except with one additional convolutional layer for both the generator and discriminator, as is consistent with the literature.
We also used $100$ i.i.d. random variables for the 64x64 image datasets and $64$ variables for the 32x32 image datasets, each drawn from a uniform distribution.
All models were trained with the same Adam hyper-parameters and a learning rate of \num{1e-3}.

For the 32x32 CIFAR-10 and SVHN datasets, BGAN setting $\log{Z} = 0$ was sufficient.
For the larger 64x64 images from CelebA and Caltech-UCSB birds, we found that estimating $\log{Z}$ from Equation~\ref{eq:log_Z_est} improved stability and achieved better samples.

Next, we used CelebA to compare between various GAN variants: normal GAN, least-squares GAN~\citep[LSGAN][]{mao2016least}, and Wasserstein GAN~\citep[WGAN][]{arjovsky2017wasserstein}.
In addition, we train on a boundary-seeking version of LSGANs, setting the generator target to $1 / 2$ with a 0-1 coding discriminator.
We tested on a wider range learning rates and selected those for which the models performed the best qualitatively.
Although \citet{arjovsky2017wasserstein} recommends otherwise, we trained all models, including WGAN,  with one discriminator update per generator update, as in the context of our initial experiments we found there to be no discernible difference.
All models were trained for a maximum of $30$ epochs, though models were stopped early in the case of collapse.

\subsection{Generation result}
\begin{figure*}[t]
\begin{minipage}{0.45\textwidth}
\centering
\includegraphics[width=0.99\columnwidth]{extra_figures/svhn_clip}

(a) SVHN
\end{minipage}
\hfill
\begin{minipage}{0.45\textwidth}
\centering
\includegraphics[width=0.99\columnwidth]{extra_figures/bgan_new_celeba_clip}

(b) CelebA (Continuous)
\end{minipage}

\begin{minipage}{0.45\textwidth}
\centering
\includegraphics[width=.99\linewidth]{extra_figures/bgan_old_cifar_clip}

(c) CIFAR-10
\end{minipage}
\hfill
\begin{minipage}{0.45\textwidth}
\centering
\includegraphics[width=.99\linewidth]{extra_figures/bgan_new_birds_clip}

(d) Caltech-UCSB Birds
\end{minipage}
\caption{
Samples generated from the continuous BGANs trained on various real-valued benchmark datasets; (a) SVHN (32x32), (b) CelebA (64x64 without quantization), (c) CIFAR-10 (32x32), and (d) rescaled Caltech-USBC Birds dataset (64x6x).
Both SVHN and CIFAR-10 were generated from BGAN trained setting $\log{Z} = 0$, while CelebA and Caltech-UCSB birds were generated using the moving average in Equation~\ref{eq:log_Z_est}.}
\label{fig:svhn}
\vspace{-.2cm}
\end{figure*}

\begin{figure*}[t]
\begin{minipage}{0.24\textwidth}
\centering
\includegraphics[width=0.99\columnwidth]{extra_figures/GAN_cifar}

(a) GAN
\end{minipage}
\hfill
\begin{minipage}{0.24\textwidth}
\centering
\includegraphics[width=0.99\columnwidth]{extra_figures/LSGAN_cifar}

(b) LSGAN
\end{minipage}
\begin{minipage}{0.24\textwidth}
\centering
\includegraphics[width=.99\linewidth]{extra_figures/WGAN_cifar}

(c) WGAN
\end{minipage}
\hfill
\begin{minipage}{0.24\textwidth}
\centering
\includegraphics[width=.99\linewidth]{extra_figures/BLSGAN_cifar}

(d) BGAN-LSGAN
\end{minipage}
\caption{
Examples generated from other GAN objectives with the same architecture; (a) GAN, (b) LSGAN, (c) WGAN, and (d) LSGAN trained with a boundary-seeking objective, setting the generator target to $1 / 2$.}
\label{fig:celeba_comp}
\vspace{-.5cm}
\end{figure*}
Example generations from BGAN are presented in Figure~\ref{fig:svhn} (Full sets can be found in the Appendix).
Of all of the models shown, only normal GANs collapsed within the number of epochs run, so we show the last best samples for normal GANs.
BGAN performs well generating images from the imaging datasets and performs comparably in image quality to that of LSGANs and WGANs (Figure~\ref{fig:celeba_comp}), but better than normal GANs within the constraints of our hyperparameter search and training procedures.
Surprisingly, LSGAN with the $1 / 2$ generator target performs very well, generating samples that are at least of equal quality to WGAN, BGAN, and LSGAN.

Though we found that BGAN with continuous data did not easily collapse, it did so more than WGAN, which seems relatively robust against collapse in our experiments (though WGAN can produce strong artifacts or be slow to train, depending on the hyper-parameters).
Particularly for larger datasets like CelebA, we found that BGAN performed better with higher learning rates (\num{1e-3}), higher ``leaking" on the discriminator rectifier nonlinearities, and at least $2$ discriminator updates per generator update during learning.
However, none of the models we tested were absolutely reliable in image quality across a wide range of learning rates, and we conclude that BGAN can produce high-dimensional images as well as WGAN and LSGAN.

\section{Conclusion}

Reinterpreting the generator objective to match the proposal target distribution reveals a novel learning algorithm for training a generative adversarial network~\citep[GANs, ][]{goodfellow2014generative}. 
%At the core of this algorithm is the boundary seeking objective for a generator that stabilizes learning greatly. 
This proposed approach of boundary-seeking provides us with a unified framework under which learning algorithms for both discrete and continuous variables are derived. Empirically, we showed the effectiveness of training a GAN with the proposed learning algorithm, to which we refer as a boundary-seeking GAN (BGAN), on both discrete and continuous variables. 
%In due course, we observed that the proposed approach is more stable compared to the recently proposed re-parametrization technique for discrete variables, called the Gumbel-Softmax technique. 

%The proposed learning algorithm can be extended for the case of continuous variables by incorporating the idea of importance sampling, which was a core recipe of the version of the proposed algorithm for discrete variables. This can be done by using the normalized weights as a multinomial distribution over a batch of generated samples. This amounts
%to filtering out poor samples generated from a generator according to a discriminator, and can be used both during training and inference. We leave this for the future.

\section*{Acknowledgements}

RDH thanks IVADO, MILA, UdeM, NIH grants R01EB006841 and P20GM103472, and NSF grant 1539067 for support.
APJ thanks UWaterloo, Waterloo AI lab and MILA for their support and Michael Noukhovitch, Pascal Poupart for constructive discussions.
KC thanks TenCent, eBay, Google (Reward Awards 2015, 2016), NVIDIA (NVAIL) and Facebook for their support.
YB thanks CIFAR, NSERC, IBM, Google, Facebook and Microsoft for their support.
Finally, we wish to thank the developers of Theano~\citep{team2016theano}, Lasagne~\url{http://lasagne.readthedocs.io}, and Fuel~\citep{van2015blocks} for their valuable code-base.

\bibliography{main}
\bibliographystyle{icml2017}

\section{Appendix}
\subsection{Derivation of importance-weighted discrete BGAN}
Here we derive the gradient of the exclusive KL divergence between the joint target distribution, $\tilde{p}(\vx, \vz)$, which is kept fixed w.r.t. the generative parameters, $\PS$, and the joint generator distribution, $p_g(\vx, \vz) = g(\vx | \vz) p(\vz)$.
\begin{align}
&\nabla_{\PS} D_{KL}(\tilde{p}(\vx) || p_g(\vx)) 
= \nabla_{\PS} \sum_{\vx} \tilde{p}(\vx) \log \frac{\tilde{p}(\vx)}{p_g(\vx)} 
\nonumber\\
&= -\sum_{\vx} \tilde{p}(\vx) \nabla_{\PS} \log p_g(\vx)
= -\sum_{\vx}  \frac{1}{Z} p_g(\vx) \frac{D(\vx)}{1 - D(\vx)} \nabla_{\PS} \log p_g(\vx)
\nonumber\\
&= -\sum_{\vx} \sum_{\vz}  \frac{1}{Z} g(\vx | \vz) p(\vz) \frac{D(\vx)}{1 - D(\vx)} \nabla_{\PS} \log p_g(\vx),
\end{align}

\subsection{REINFORCE-based algorithm}
Here we derive the REINFORCE-based algorithm from the inclusive KL divergence between the joint target distribution and the joint generation distribution.
\begin{align}
&\nabla_{\PS} D_{KL}(p_g(\vx, \vz) || \tilde{p}(\vx, \vz)) 
= \nabla_{\PS} \sum_{\vx, \vz} p(\vz) g(\vx | \vz) \log{\frac{p_g(\vx, \vz)}{\tilde{p}(\vx, \vz)}} \nonumber\\
&= \sum_{\vx, \vz} p(\vz) \left( \nabla_{\PS} g(\vx | \vz) \log{\frac{p_g(\vx, \vz)}{\tilde{p}(\vx, \vz)}} + g(\vx | \vz) \nabla_{\PS} \log{p_g(\vx, \vz)} \right) \nonumber\\
&= \sum_{\vx, \vz} p(\vz) \left( \nabla_{\PS} g(\vx | \vz) \left( \log {g(\vx | \vz')} - \log{\tilde{p}(\vx | \vz)}\right) + g(\vx | \vz) \nabla_{\PS} \log{p(\vz) g(\vx | \vz)} \right) \nonumber\\
&= \sum_{\vx, \vz} p(\vz) g(\vx | \vz) \left( \nabla_{\PS} \log{g(\vx | \vz)} \left( \log{g(\vx | \vz)} - \log{\tilde{p}(\vx)_{|\vz}}\right) + \nabla_{\PS} \log{ g(\vx | \vz)} \right) \nonumber\\
&= -\sum_{\vx, \vz} p(\vz) g(\vx | \vz) \nabla_{\PS} \log{ g(\vx | \vz)} \left(\log{\frac{D(\vx)}{1 - D(\vx)}} - \log{Z_{\vz|}} + 1 \right) \nonumber\\
&\approx - \mathbb{E}\left[\frac{1}{M} \sum_m \left(\log{w^{(m)}} - \log{Z_{|\vz}} - 1\right) \nabla \log{g(\vx^{(m)} | \vz^{(n)})}\right]_{\vz \sim p(\vz)}.
\end{align}

The log-partition function can be estimated a number of ways.
The simplest is to estimate using a logarithm of the Monte-Carlo estimate:
\begin{align}
\log{Z_{\vz|}} = \log \sum_{\vx} g(\vx | \vz) \frac{D(\vx)}{1 - D(\vx)}
\approx \log \sum_m \frac{D(\vx^{(m)})}{1 - D(\vx)^{(m)}} - \log M,
\end{align}
where $\vx^{(m)} \sim g(\vx | \vz)$ are $M$ samples drawn from the generator given $\vz$.
To reduce variance, we can use a moving average, as in Equation~\ref{eq:log_Z_est}.
However, these estimates do not capture the dependence of $\vz$.
As in \citet{mnih2014neural}, we can use an input-dependent baseline with parameters $\PP$, $C(\vz; \PP)$, using a neural network with input $\vz$.
This network is trained with the simple MSE loss function:
\begin{align}
\mathcal{L}_{\PP} = \mathbb{E}\left[C(\vz; \PP) - \log \sum_m \frac{D(\vx^{(m)})}{1 - D(\vx)^{(m)}} + \log M)^2\right]_{\vz \sim p(\vz)}.
\end{align}

\subsection{Visual comparison: GAN vs. BGAN}

In the case of continuous variables, we can qualitatively analyze the difference between the conventional GAN and the proposed boundary-seeking GAN. We consider an one-dimensional variable, and draw 20 samples as each of real and generated sets of samples from two Gaussian distributions. We consider two cases; (a) early stage of learning, and (b) late stage of learning. We use $-2$ and $2$ in the first case, and $-0.1$ and $0.1$ in the second case, for the centers of those two Gaussians. The variances were set to 0.3 for both distributions. Instead of learning, we simply set our discriminator $D(x)$ to
\begin{align*}
    D(x) = \frac{1}{1 + \exp(-c x)},
\end{align*}
where $c$ is set to $0.8$ and $10$ for the cases (a) and (b), respectively. These values were selected to illustrate sub-optimal discriminators, which is almost always true when training a GAN on a real data.

In Fig.~\ref{fig:example_1d}, we plot both real and generated samples on the x-axis ($y=0$). The solid red curve corresponds to the discriminator $D(x)$ above, and its log-gradient ($\partial \log D(x) / \partial x$) is drawn with a dashed red curve. It is clear that maximizing $\log D(x)$, as conventionally done with GAN, pushes the generator beyond the real samples (orange circles). On the other hand, the proposed criterion from Eq.~\eqref{eq:bs_gan_cont} has its minimum at the decision boundary of the discriminator. Minimizing this criterion has the effect of pushing the generated samples, or correspondingly the generator, toward the real samples, but never beyond the region occupied by them. The issue is much more apparent in Fig.~\ref{fig:example_1d}~(b), where the real and generated samples are extremely close to each other. The proposed BGAN encourages the generator to stay close to the center of real samples, while the conventional objective pushes the generated samples beyond the real samples. We conjecture that this property avoids a well-known failure mode of GANs and improves learning. 

\begin{figure*}[t]
    \centering
    \begin{minipage}{0.48\textwidth}
    \centering
        \includegraphics[width=.99\columnwidth]{./extra_figures/example_1d.pdf}
        
        (a)
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
    \centering
        \includegraphics[width=.99\columnwidth]{./extra_figures/example_1d_2.pdf}
        
        (b)
    \end{minipage}
    
    \caption{
    Qualitative comparison between the conventional GAN and the proposed BGAN in 1-D examples. The discriminator $D$ is fixed to a logistic regression classifier with a coefficient $0.8$. Note that the maximum of the generator objective from the conventional GAN is at the positive infinity (red curve), while the minimum from the proposed BGAN is at the decision boundary (blue curve). Unlike the proposed BGAN, the learning gradient of the conventional GAN (red dashed curve) pushes the generated samples beyond the real samples.
    }
    \label{fig:example_1d}
\vspace{-3mm}
\end{figure*}

\subsection{Training discrete GANs with Gumbel softmax}
\begin{table}[h]
\begin{center}
\caption{
Hyperparameters used in our sweep of discrete GANs trained using the Gumbel-Softmax and straight-through Gumbel-Softmax}
\begin{tabular}{ |c|c| } 
 \hline
 \textbf{Optimizer} & [Adam, RMSProp, SGD] \\ 
 \hline
 \textbf{Learning Rate} & [\num{1e-2}, \num{1e-3}, \num{1e-4}, \num{1e-5}]\\
 \hline
 \textbf{Anneal Rate ($r$)} & [$0.01$, $0.001$, $0.0001$]\\ 
 \hline
 \textbf{Anneal Interval ($N$)} & [$200$, $300$, $500$]\\ 
 \hline
\end{tabular}
\label{table:GUMB-GRID-SEARCH}
\end{center}

\vspace{-4mm}
\end{table}

The ''Gumbel technique``~\citep{gumbel1954statistical, jang2016categorical} has been shown to be successful in some tasks where back-propogation through discrete random variables is not possible.
Using the same model parameterization as our MNIST experiments above, we compare our approach to Gumbel-Softmax~\citep{maddison2016concrete, jang2016categorical} and straight-through Gumbel-Softmax~\citep{maddison2016concrete, jang2016categorical} on the binarized MNIST dataset. 
The temperature was annealed using the schedule $\tau = \max(0.5, \exp(-rt))$ as a function of the global training step $t$, where $\tau$ is updated every $N$ steps. The values for the grid search of $r$ and $N$ are listed in Table~\ref{table:GUMB-GRID-SEARCH}.

We observed that the generator cost of Gumbel-Softmax and straight-through Gumbel diverged across all hyperparameters (Figure~\ref{fig:GUMBEL}).
No set of hyperparameters produced a generator that was able to produce satisfactory representative samples of the original dataset.

\begin{figure}
\begin{minipage}{0.49\textwidth}
\centering
\includegraphics[width=0.9\linewidth,clip=True,trim=0 110 0 20]{extra_figures/gumbel_softmax.png}
(a) Gumbel-Softmax
\end{minipage}
\begin{minipage}{0.49\textwidth}
\centering
\includegraphics[width=0.9\linewidth,clip=True,trim=0 110 0 20]{extra_figures/gumbel_softmax_st.png}
(b) Gumbel-Softmax (Straight-Through)
\end{minipage}

\begin{minipage}{0.49\textwidth}
{
\centering
\scriptsize
\begin{tabular}{c l| c c c c}
& & {\color{blue} \bf b} & {\color{yellow}\bf y} & {\color{red}\bf r} & {\color{green}\bf g} \\
\hline\hline
\multirow{4}{*}{\rotatebox{90}{(a)}} &  & adam & rmsprop & adam & rmsprop \\
& lr & $10^{-4}$ & $10^{-4}$ & $10^{-5}$ & $10^{-5}$ \\
& $r$ & $10^{-3}$ & $10^{-4}$ & $10^{-3}$ & $10^{-3}$ \\
& $N$ & $500$ & $200$ & $200$ & $300$ \\
\hline
\multirow{4}{*}{\rotatebox{90}{(b)}}&  & adam & rmsprop & adam & rmsprop \\
& lr & $10^{-4}$ & $10^{-4}$ & $10^{-5}$ & $10^{-5}$ \\
& $r$ & $10^{-3}$ & $10^{-5}$ & $10^{-3}$ & $10^{-3}$ \\
& $N$ & $300$ & $500$ & $200$ & $300$ 
\end{tabular}
}
\end{minipage}
\begin{minipage}{0.49\textwidth}
\caption{
The diverging behaviour of the generator training when using Gumbel-Softmax technique, both (a) with and (b) without straight-through estimation. The curves are a running average and are sampled from our hyperparameter search:
}
\label{fig:GUMBEL}
\end{minipage}
\end{figure}

\subsection{Generation results}
Provided are the complete set of random samples, which were truncated in the main text.
\begin{figure*}[t]
\begin{minipage}{0.45\textwidth}
\centering
\includegraphics[width=0.99\columnwidth]{extra_figures/BGAN_old_svhn_full}

(a) SVHN
\end{minipage}
\hfill
\begin{minipage}{0.45\textwidth}
\centering
\includegraphics[width=0.99\columnwidth]{extra_figures/BGAN_cifar_full}

(b) CelebA (Continuous)
\end{minipage}

\begin{minipage}{0.45\textwidth}
\centering
\includegraphics[width=.99\linewidth]{extra_figures/BGAN_old_cifar_full}

(c) CIFAR-10
\end{minipage}
\hfill
\begin{minipage}{0.45\textwidth}
\centering
\includegraphics[width=.99\linewidth]{extra_figures/BGAN_new_birds_full}

(d) Caltech-UCSB Birds
\end{minipage}
\caption{
Samples generated from the continuous BGANs trained on various real-valued benchmark datasets; (a) SVHN (32x32), (b) CelebA (64x64 without quantization), (c) CIFAR-10 (32x32), and (d) rescaled Caltech-USBC Birds dataset (64x6x).}
\vspace{-.5cm}
\end{figure*}

\begin{figure*}[t]
\begin{minipage}{0.45\textwidth}
\centering
\includegraphics[width=0.99\columnwidth]{extra_figures/GAN_cifar_full}

(a) GAN
\end{minipage}
\hfill
\begin{minipage}{0.45\textwidth}
\centering
\includegraphics[width=0.99\columnwidth]{extra_figures/LSGAN_cifar_full}

(b) LSGAN
\end{minipage}

\begin{minipage}{0.45\textwidth}
\centering
\includegraphics[width=.99\linewidth]{extra_figures/WGAN_cifar_full}

(c) WGAN
\end{minipage}
\hfill
\begin{minipage}{0.45\textwidth}
\centering
\includegraphics[width=.99\linewidth]{extra_figures/BLSGAN_cifar_full}

(d) BGAN-LSGAN
\end{minipage}
\caption{
Examples generated from other GAN objectives with the same architecture; (a) GAN lr=\num{1e-4}, (b) LSGAN lr=\num{1e-3}, (c) WGAN lr=\num{1e-4}, and (d) LSGAN trained with a boundary-seeking objective, setting the generator target to $1 / 2$, lr=\num{1e-3}.}
\end{figure*}

\end{document}


\documentclass{article}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{color}
\usepackage{comment}
\usepackage{fancyvrb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{latexsym}
\usepackage{listings}
\usepackage{multicol}
\usepackage{multirow}
\usepackage[normalem]{ulem}
\usepackage{tabularx}
\usepackage{times}
\usepackage{url}
\usepackage[utf8]{inputenc}
\usepackage{xspace}
\usepackage{wrapfig}
\usepackage[]{units}
\usepackage{subfig}
\usepackage{siunitx}
\usepackage[nonatbib,final]{nips_2017} 
\usepackage{natbib}
\sisetup{output-exponent-marker=\ensuremath{\mathrm{e}}}

%\allowdisplaybreaks

\setlength{\columnsep}{.5in}

\newcommand{\obs}{\text{obs}}
\newcommand{\mis}{\text{mis}}

\newcommand{\qt}[1]{\left<#1\right>}
\newcommand{\ql}[1]{\left[#1\right]}
\newcommand{\hess}{\mathbf{H}}
\newcommand{\jacob}{\mathbf{J}}
\newcommand{\hl}{HL}
\newcommand{\cost}{\mathcal{L}}
\newcommand{\lout}{\mathbf{r}}
\newcommand{\louti}{r}
\newcommand{\outi}{y}
\newcommand{\out}{\mathbf{y}}
\newcommand{\gauss}{\mathbf{G_N}}
\newcommand{\eye}{\mathbf{I}}
\newcommand{\softmax}{\phi}
\newcommand{\targ}{\mathbf{t}}
\newcommand{\metric}{\mathbf{G}}
\newcommand{\sample}{\mathbf{z}}
\newcommand{\f}{\text{f}}

\newcommand{\bmx}[0]{\begin{bmatrix}}
\newcommand{\emx}[0]{\end{bmatrix}}
\newcommand{\qexp}[1]{\left<#1\right>}
\newcommand{\vect}[1]{\mathbf{#1}}
\newcommand{\vects}[1]{\boldsymbol{#1}}
\newcommand{\matr}[1]{\mathbf{#1}}
\newcommand{\var}[0]{\operatorname{Var}}
\newcommand{\std}[0]{\operatorname{std}}
\newcommand{\cov}[0]{\operatorname{Cov}}
\newcommand{\diag}[0]{\operatorname{diag}}
\newcommand{\matrs}[1]{\boldsymbol{#1}}
\newcommand{\va}[0]{\vect{a}}
\newcommand{\vb}[0]{\vect{b}}
\newcommand{\vc}[0]{\vect{c}}
\newcommand{\ve}[0]{\vect{e}}
\newcommand{\vn}[0]{\vect{n}}
\newcommand{\vh}[0]{\vect{h}}
\newcommand{\vv}[0]{\vect{v}}
\newcommand{\vx}[0]{\vect{x}}
\newcommand{\vz}[0]{\vect{z}}
\newcommand{\vw}[0]{\vect{w}}
\newcommand{\vs}[0]{\vect{s}}
\newcommand{\vf}[0]{\vect{f}}
\newcommand{\vi}[0]{\vect{i}}
\newcommand{\vo}[0]{\vect{o}}
\newcommand{\vy}[0]{\vect{y}}
\newcommand{\vg}[0]{\vect{g}}
\newcommand{\vm}[0]{\vect{m}}
\newcommand{\vu}[0]{\vect{u}}
\newcommand{\vL}[0]{\vect{L}}
\newcommand{\vr}[0]{\vect{r}}
\newcommand{\vp}[0]{\vect{p}}

\newcommand{\mW}[0]{\matr{W}}
\newcommand{\mP}[0]{\matr{P}}
\newcommand{\mM}[0]{\matr{M}}
\newcommand{\mE}[0]{\matr{E}}
\newcommand{\mG}[0]{\matr{G}}
\newcommand{\mX}[0]{\matr{X}}
\newcommand{\mQ}[0]{\matr{Q}}
\newcommand{\mU}[0]{\matr{U}}
\newcommand{\mF}[0]{\matr{F}}
\newcommand{\mV}[0]{\matr{V}}
\newcommand{\mA}{\matr{A}}
\newcommand{\mC}{\matr{C}}
\newcommand{\mD}{\matr{D}}
\newcommand{\mS}{\matr{S}}
\newcommand{\mI}{\matr{I}}

\newcommand{\td}[0]{\text{d}}
\newcommand{\TT}[0]{\vects{\theta}}
\newcommand{\PP}[0]{\vects{\phi}}
\newcommand{\PS}[0]{\vects{\psi}}
\newcommand{\vsig}[0]{\vects{\sigma}}
\newcommand{\valpha}[0]{\vects{\alpha}}
\newcommand{\vmu}[0]{\vects{\mu}}
\newcommand{\vsigma}[0]{\vects{\sigma}}
\newcommand{\vepsilon}[0]{\vects{\epsilon}}
\newcommand{\vzero}[0]{\vect{0}}
\newcommand{\tf}[0]{\text{m}}
\newcommand{\tdf}[0]{\text{dm}}
\newcommand{\grad}[0]{\nabla}
\newcommand{\alert}[1]{\textcolor{red}{#1}}
\newcommand{\N}[0]{\mathcal{N}}
\newcommand{\LL}[0]{\mathcal{L}}
\newcommand{\HH}[0]{\mathcal{H}}
\newcommand{\NN}[0]{\mathcal{N}}
\newcommand{\RR}[0]{\mathbb{R}}
\newcommand{\II}[0]{\mathbb{I}}
\newcommand{\Scal}[0]{\mathcal{S}}
\newcommand{\sigmoid}{\text{sigmoid}}
\newcommand{\Bernoulli}{\text{Bernoulli}}
\newcommand{\E}[0]{\mathbb{E}}
\newcommand{\enabla}[0]{\ensuremath{%
    \overset{\raisebox{-0.3ex}[0.5ex][0ex]{%
    \ensuremath{\scriptscriptstyle e}}}{\nabla}}}
\newcommand{\enhnabla}[0]{\nabla_{\hspace{-0.5mm}e}\,}

\renewcommand*{\thefootnote}{\fnsymbol{footnote}}

\newcommand{\todo}[1]{{\Large\textcolor{red}{#1}}}
\newcommand{\done}[1]{{\Large\textcolor{green}{#1}}}
\newcommand{\dd}[1]{\ensuremath{\mbox{d}#1}}

\DeclareMathOperator*{\argmax}{\arg \max}
\DeclareMathOperator*{\argmin}{\arg \min}
\newcommand{\newln}{\\&\quad\quad{}}

\newcommand{\Ax}{\mathcal{A}_x}
\newcommand{\Ay}{\mathcal{A}_y}
\newcommand{\ola}{\overleftarrow}
\newcommand{\ora}{\overrightarrow}
\newcommand{\ov}{\overline}
\newcommand{\ts}{\rule{0pt}{2.6ex}}       % Top strut
\newcommand{\ms}{\rule{0pt}{0ex}}         % Middle strut
\newcommand{\bs}{\rule[-1.2ex]{0pt}{0pt}} % Bottom strut
\newcommand{\specialcell}[2][c]{%
  \begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}

\newcommand{\yb}[2]{\textcolor{red}{#1 {\em (YB: #2)}}}
\newcommand{\rdh}[2]{\textcolor{blue}{#1 {\em (RDH: #2)}}}

\title{Boundary-Seeking Generative Adversarial Networks}

\author{R Devon Hjelm\thanks{Authors contributed equally.}\\
University of Montreal\\
Montreal Institute for Learning Algorithms\\
\texttt{erroneus@gmail.com}
\And
Athul Paul Jacob\footnotemark[1]\\ 
University Of Waterloo\\
Montreal Institute for Learning Algorithms\\
\And
Tong Che\\
University of Montreal\\
Montreal Institute for Learning Algorithms\\
\And
Kyunghyun Cho\\
Courant Institute \& Center for Data Science \\
New York University\\
\And
Yoshua Bengio\\
University of Montreal\\
Montreal Institute for Learning Algorithms
}

\begin{document}

\maketitle

\begin{abstract}
We introduce a novel approach to training generative adversarial networks~\citep[GANs, ][]{goodfellow2014generative}, where we train a generator to match a target distribution that converges to the data distribution at the limit of a perfect discriminator.
This objective can be interpreted as training a generator to produce samples that lie on the decision boundary of the current discriminator in training at each update, and we call a GAN trained using this algorithm a boundary-seeking GAN (BGAN).
This approach can be used to train a generator with discrete output when the generator outputs a parametric conditional distribution. We demonstrate the effectiveness of the proposed algorithm with discrete image and character-based natural language generation. 
Finally, we notice that the proposed boundary-seeking algorithm works even with continuous variables, and demonstrate its effectiveness with various natural image benchmarks.
\end{abstract}

\section{Introduction}
%Generative models provide a means of representing the underlying structure in data. 
%While the objective is to faithfully generate data with representative statistics, the strength of generative models lies partially in representing the data as a hypothetical compressed version of its underlying structure.
Generative adversarial networks~\citep[GAN, ][]{goodfellow2014generative} involve a unique generative learning framework that uses two separate models with opposing, competing, or \emph{adversarial} objectives.
In contrast to those models that generate the data from a parameteric distribution, such as directed graphical models (e.g., Helmholtz machines~\citep{dayan1995helmholtz} and variational autoencoders~\citep[VAEs,][]{kingma2013auto}) or undirected graphical models (e.g., restricted Boltzmann machines~\citep[RBMs,][]{hinton2002training} and deep Boltzmann machines~\citep[DBMs,][]{salakhutdinov2009deep}), GANs do not rely on maximum likelihood estimation~\citep[MLE,][]{dempster1977maximum} to learn.

Rather, training a GAN, which consists of two competing networks--a discriminator and a generator-- only requires back-propagating a learning signal originating from a learned objective function corresponding to the loss of a discriminator trained in an adversarial way.
This framework is powerful, as it trains a generator without relying on an explicit formulation of the probability function which requires marginalizing out latent variables. 
Also, it avoids the issue with the MLE objective which makes an MLE-trained model conservative in probability mass placement. 
The latter issue is known to be a problem behind MLE-trained models generating samples that lack real-world qualities, often evidenced by the lack of sharpness in natural images~\citep{goodfellow2016nips}.
Unlike MLE-trained models, GANs have been shown to generate often-diverse and realistic samples even when trained on high-dimensional large-scale data~\citep{radford2015unsupervised} in the case of continuous variables.

GANs however have a serious limitation on the type of variables they can model, because they require the composition of the generator and discriminator to be fully differentiable.
With discrete variables, this is not true. For instance, consider using a step function at the end of a generator in order to generate a discrete value. 
In this case, back-propagation alone cannot provide the training signal for the generator, because the derivative of a step function is 0 almost everywhere (and so would be the back-propagated signal). 
This is problematic, as many important real-world datasets are discrete, such as character- or word-based representations of language.
The general issue of credit assignment for computational graphs with discrete operations (e.g. discrete stochastic neurons) is difficult, and only approximate solutions have been proposed in the past~\citep{bengio2013estimating,gu2015muprop,gumbel1954statistical,jang2016categorical, maddison2016concrete, tucker2017rebar}.

%This situation is analogous to that in training Helmholtz machines with discrete hidden variables (i.e., sigmoid belief networks~\citep[SBNs,][]{saul1996mean}), where advanced learning algorithms are necessary to train an approximate inference network~\citep{mnih2014neural,bornschein2014reweighted}. When those hidden units are continuous, usual backpropagation with a re-parametrization technique works~\citep{kingma2013auto}. On the other hand, when they are discrete, only recently an approximate re-parametrization technique, called ``Gumbel-Softmax technique'', was introduced~\citep{gumbel1954statistical, jang2016categorical, maddison2016concrete}. The effectiveness of this approach was only demonstrated with simple directed models having a single layer of discrete latent variables, and there is no consensus nor evidence whether it works with GANs.

In this paper, we introduce a novel learning algorithm for GANs. This algorithm estimates the gradient of the discriminator's output w.r.t. the generator as the weighted sum of the gradients of the log-probabilities of the samples generated from the generator. The weights are given by the discriminator's performance on the samples, such that those samples that look similar to true training examples, are weighted more (because they would be scored highly by the discriminator). This lets us interpret the discriminator as computing a target distribution for the discrete stochastic units, allowing us to train a generator to converge toward the hypothetical true distribution of the data as the discriminator is optimized.
%This distribution was discovered independently from us by \citet{che2017maximum} in the context of language modeling.
% I think it's best to omit any mentions of this paper during review.
This objective can further be interpreted as an alternative loss on the discriminator output, which allows us to derive a novel learning algorithm for GANs even with continuous variables. Under this interpretation, the learning objective of a generator is to minimize the difference between the discriminator's log-probabilities for the sample being positive and negative. This objective can be thought of as training the generator to {\em place generated samples at the decision boundary of the discriminator}, and hence we call a GAN trained with the proposed algorithm a boundary-seeking generative adversarial network (BGAN).

We demonstrate the effectiveness of the proposed BGAN in both settings, with discrete as well as continuous variables. We use MNIST and quantized CelebA for the discrete image setting, and street view house numbers~\citep[SVHN,][]{netzer2011reading}, CIFAR-10~\citep{krizhevsky2009learning}, CelebA~\citep{liu2015deep}, and the Caltech-USCB birds dataset~\citep[][]{WelinderEtal2010} for the continuous image setting.
We compare to competing GAN objectives using the CelebA dataset. 
%In the case of discrete variables, we also try the Gumbel-Softmax technique, however, without much success, unlike the proposed BGAN which we found extremely easy to train. % Because of REBAR, we are moving this to the appendix.
We also demonstrate discrete BGAN on a simple character-based model on the 1-billion word dataset~\citep{chelba2013one}, showing successful learning and stability for natural language generation. 
In the case of continuous variables, we observe that the proposed algorithm works qualitatively better than conventional GANs.

\section{Methods}
\subsection{Generative Adversarial Networks}
The generative adversarial networks~\citep[GAN, ][]{goodfellow2014generative} framework combines two models: a generator model pitted against a discriminatory adversary.
The generator is composed of a prior distribution, $p(\vz)$, over latent variable $\vz$ and a generator function, $G(\vz)$, which maps from the space of $\vz$ to the space of data $\vx$.
The objective of the discriminator is to correctly distinguish between data and samples from the generator by maximizing
\begin{align}
\mathbb{E} \left[\log D(\vx) \right]_{\vx \sim p_{\text{data}}(\vx)} + \mathbb{E} \left[ \log(1 - D(G(\vz)) \right]_{\vz \sim p(\vz)},
\label{eq:gan}
\end{align}
where $D(.)$ is a discriminator function with output that spans $[0, 1]$.
The generator is trained to ``fool" the discriminator, that is minimize $\mathbb{E} \left[ \log(1 - D(G(\vz)) \right]_{\vz \sim p(\vz)}$ or to minimize a proxy $\mathbb{E} \left[ -\log(D(G(\vz)) \right]_{\vz \sim p(\vz)}$.
Ignoring the proxy case, the two models play a minimax game with value function:
\begin{align}
\min_G \max_D V(G, D) = \mathbb{E}[\log D(\vx)]_{\vx \sim p(\vx)} + \mathbb{E}[\log (1 - D(G(\vz)))]_{\vz \sim p(\vz)}.
\label{eq:minmax}
\end{align}

%Previous deep generative models like the variational auto-encoder (VAE)~\citep{kingma2013auto} involve a neural network which takes the latent sampled values $\vz$ and  outputs parameters of a distribution (e.g. Bernoulli, Gaussian) from which the samples are drawn. 
%Instead, t
The GAN generator is defined by the composition of sampling the latent values $\vz$ and applying the deterministic function $G$. The generative objective involves one continuous function with parameters from both models: $D(G(.;\PS);\PP)$, but optimized over $\PP$ only. This allows for training a generative model with a relatively simple fitness function and back-propagation, rather than a complex marginalized likelihood function, and which does not suffer from some of the learning difficulties of maximum likelihood
estimators (MLE), e.g., giving rise to blurred samples.

In practice, the generator, rather than being trained to minimize the above value function, can be trained to maximize the proxy $\log(D(G(\vz)))$ or $\log(D(G(\vz))) - \log(1 - D(G(\vz)))$, which can alleviate some learning issues related to the discriminator loss saturating early in learning and improve stability.
In addition, recent advances using mean-squared error~\citep[LSGAN,][]{mao2016least} or the earth-movers distance~\citep[WGAN,][]{arjovsky2017wasserstein} as alternative learning signals for both the discriminator and generator can improve the approach.
%With the basic GAN architecture, both generator and discriminator are feedforward networks, $D(.; \PP)$ and $G(.; \PS)$, while in deep convolutional GANs~\citep[DCGAN,][]{radford2015unsupervised}, the discriminator and generator are convolutional neural networks with strided and fractionally-strided convolutional layers, respectively.

In order for a generating function, $G(.;\PS)$, with real-valued parameters, $\PS$, to generate discrete-valued outputs, $\vx$, $G(.;\PS)$ cannot be fully continuous.
A natural choice for the generator would be a continuous function, $\vy = f(\vz)$ (e.g., a feed forward network), followed by a step function.
However, as the gradients that would otherwise be used to train the generator are zero almost everywhere, it is not possible to train the generator from the discriminator error signal.
Approximations for the back-propagated signal exist~\citep{bengio2013estimating,gu2015muprop,gumbel1954statistical,jang2016categorical,maddison2016concrete,tucker2017rebar}.
We propose a novel approach to dealing with this issue, based on estimating
a proposal target distribution for the generator output.

\subsection{Target distribution for the generator}
First consider the optimal discriminator function~\citep{goodfellow2014generative},
$D(\vx)^{\star}$, derived from 
analytically optimizing the discriminator objective given a fixed generator $G(.)$ and assuming no functional constraints on the discriminator, i.e., in a non-parametric setting:
\begin{align}
D^{\star}_G(\vx) = \frac{p_{\text{data}}(\vx)}{p_{\text{data}}(\vx) + p_g(\vx)},
\label{eq:opt_disc}
\end{align}
where $p_{\text{data}}(\vx)$ is the data distribution, and $p_g(\vx)$ is the distribution as generated by $p(\vz)$ and $G(\vz)$.
Given this optimal discriminator, the density of the data can then be written as:
\begin{align}
p_{\text{data}}(\vx) = p_g(\vx) \frac{D^{\star}_G(\vx)}{1 - D^{\star}_G(\vx)}.
\end{align}
This indicates that, in the limit of a perfect discriminator, the true distribution can be perfectly estimated from a said discriminator and any fixed generator, even if that generator is not
perfectly trained. {\em A sample from that corrected estimator can thus be obtained by reweighing 
samples from the generator according to the ratio} $\frac{D^{\star}_G(\vx)}{1 - D^{\star}_G(\vx)}$.
Unfortunately, it is unlikely that such a perfect discriminator is learnable or even exists.
However, one can posit the following estimator for the true distribution, given an imperfect discriminator, $D(\vx)$:
\begin{align}
\tilde{p}(\vx) = \frac{1}{Z} p_g(\vx) \frac{D(\vx)}{1 - D(\vx)},
\quad \text{where} \quad
Z = \sum_{\vx} p_g(\vx) \frac{D(\vx)}{1 - D(\vx)}
\label{eq:target}
\end{align}
is the normalization constant that guarantees that $\tilde{p}(\vx)$ is a proper probability distribution.
This distribution was discovered independently from us by \citet{che2017maximum}.
Note that the optimum for the generator occurs at $D(\vx) = D^{\star}(\vx) = 1/2$, so that $Z = 1$ and \mbox{$\tilde{p}(\vx) = p_g(\vx) = p(\vx)$}.
$\tilde{p}(\vx)$ is a potentially biased estimator for the true density; however, {\em the bias only depends on the quality of $D(\vx)$}: the closer $D(\vx)$ is to $D^{\star}(\vx)$, the lower the bias. This is an important consideration because 
it is likely that training the discriminator (a standard probabilistic binary classifier)
is much easier than training the generator (whose objective function is a moving
target as the discriminator adapts to the generator and vice-versa).
The optimum for the generator exists 
when generated samples have equal probability of being classified as data or as samples.
This occurs at the decision boundary of the discriminator, so that we call this approach boundary-seeking generative adversarial networks (BGAN).


\subsection{BGAN with discrete variables}
For discrete variables, parameterizing the generating distribution directly reveals a method for training the generator without relying on back-propagation through the generator's 
discrete output.
As with previous work on evaluating GANs~\citep{wu2016quantitative}, we parameterize $p_g(\vx)$ as a the marginalization of a joint density, $p_g(\vx) = \sum_{\vz} g(\vx | \vz) p(\vz)$, rephrasing the generator function, $G(\vz)$, as a conditional distribution, $g(\vx | \vz)$.
To minimize the distance between $\tilde{p}(\vx)$ and $p_g(\vx)$, we can minimize the exclusive KL divergence, so that the gradients w.r.t. the generator parameters, $\PS$, are (the full derivation can be found in the Appendix):
\begin{align}
\nabla_{\PS} D_{KL}(\tilde{p}(\vx) || p_g(\vx)) = -\frac{1}{Z} \mathbb{E}\left[\sum_\vx g(\vx | \vz) \frac{D(\vx)}{1 - D(\vx)} \nabla_{\PS} \log p_g(\vx)\right]_{\vz \sim p(\vz)},
\label{eq:grad-KL}
\end{align}
where $\tilde{p}(\vx)$ is held fixed.
Note that, while $\tilde{p}(\vx)$ depends on the generator parameters, $\PS$, we interpret this distribution as the current estimate of the true density and hence a {\em target} for the generator.
The gradients would require some sort of Monte-Carlo (MC) estimator across the input and output noise, and will have high variance, notably due to the partition function, $Z$.
The intuition here is to note that, as the conditional density, $g(\vx | \vz)$, is unimodal, it can be used to define a low-variance estimator analogous to that of Equation~\ref{eq:target}.
Following this intuition and by inspection of Eq.~\ref{eq:grad-KL}, we propose updating the joint distribution, $p(\vx, \vz) = g(\vx | \vz) p(\vz)$, to fit the joint target distribution:
\begin{align}
\tilde{p}(\vx, \vz) = \tilde{p}(\vx | \vz) p(\vz) = \frac{1}{Z_{|\vz}} g(\vx | \vz) \frac{D(\vx)}{1 - D(\vx)} p(\vz),
\label{eq:target2}
\end{align}
where $\tilde{p}(\vx |\vz)$ is an estimate of the data in the neighborhood of the generated samples defined by $\vz$ and $Z_{|\vz} = \sum_{\vx} g(\vx | \vz) \frac{D(\vx)}{1 - D(\vx)}$ is the partition function that ensures $\tilde{p}(\vx |\vz)$ is a proper probability distribution.
Note we are no longer targeting marginal of Equation~\ref{eq:target}, as the marginal of the joint given by Equation~\ref{eq:target2} over $\vz$,
%\begin{align}
%\tilde{p}(\vx) = \sum_{\vz} \frac{1}{Z_{|\vz}} g(\vx | \vz) \frac{D(\vx)}{1 - D(\vx)} p(\vz),
%\end{align}
which is also normalized, has the same convergence behavior as Equation~\ref{eq:target}.
The gradients then can be estimated as the exclusive KL divergence of the joint distributions:
\begin{align}
&\nabla_{\PS} D_{KL}\left(\tilde{p}(\vx, \vz) || p_g(\vx, \vz)\right) 
\approx -\mathbb{E} \left[\tilde{p}(\vx | \vz) \nabla_{\PS} \log g(\vx | \vz) \right]_{\vz \sim p(\vz)} 
\nonumber \\
&= -\mathbb{E} \left[\frac{1}{Z_{|\vz}} \sum_\vx g(\vx | \vz) \frac{D(\vx)}{1 - D(\vx)} \nabla_{\PS} \log g(\vx | \vz) \right]_{\vz \sim p(\vz)}
\approx -\mathbb{E}\left[ \sum_m \tilde{w}^{(m)} \nabla_{\PS} \log g(\vx^{(m)} | \vz)\right]_{\vz \sim p(\vz)},
\nonumber \\
&\text{where} \quad
\tilde{w}^{(m)} = \frac{w^{(m)}}{\sum_{m'} w^{(m')}} 
\quad
\text{and} \quad
w^{(m)} = \frac{D(\vx^{(m)})}{1 - D(\vx^{(m)})} \nonumber
\end{align}
are the normalized and unnormalized importance weights respectively (note
how $Z_{|\vz}$ cancels in the normalization) and $\vx^{(m)}$
are samples from the generator for the given $\vz$.
The normalization can help generate a relative learning signal, which will help in the case that the samples are all fairly bad from the point of view of $D(\vx)$.
The benefit of using Equation~\ref{eq:target2} over Equation~\ref{eq:target} is clear: the normalized weights, $\tilde{w}^{(m)}$, when $\vx^{(m)}$ is sampled from a unimodal distribution, will be of low variance compared to those estimated by marginalizing over $\vz$.
The gradients can be computed over a mini-batch of samples from $p(\vz)$, % so that the updates to the generator parameters, $\PS$, become:
%\begin{align}
%\Delta \PS \propto \frac{1}{N} \sum_n \sum_m \tilde{w}^{(m)} \nabla_{\PS} \log g(\vx^{(m)} | \vz^{(n)}).
%\end{align}
%This 
which reveals a straightforward procedure for training discrete GANs with a parametric conditional generator without relying on back-propagation.
Compared to normal GANs, this method requires $M$ times more space, 
but computing the $M$ (instead of 1) scalar values of $D(\vx^{(m)})$ can be parallelized,
as samples from $g(\vx|\vz)$ can be drawn independently.

One could have derived a similar learning objective using the {\em inclusive} KL divergence (rather than the exclusive in our formulation), so that the gradients become (see Appendix for details):
\begin{align}
\nabla_{\PS} D_{KL}(p_g(\vx, \vz) || \tilde{p}(\vx, \vz)
\approx - \mathbb{E} \left[\frac{1}{M} \sum_m  (\log{w^{(m)}} - \log{Z_{|\vz}} - 1) \nabla \log{g(\vx^{(m)} | \vz)}\right]_{\vz \sim p(\vz)}.
\label{eq:bgan_reinforce}
\end{align}
This gradient strongly resembles REINFORCE, and in fact we can interpret $\log{Z_{|\vz}}$ as an input-dependent baseline (using a deep neural network trained with MSE to predict $\log{Z_{|\vz}}$, using $\vz$ as input), as in neural variational inference and learning~\citep[NVIL,][]{mnih2014neural}.

\subsection{Continuous variables}
While we originally set out to find a means for training discrete GANs, the insight above can also be used to provide a learning algorithm in the continuous case.
For continuous variables, we could introduce a parametric conditional distribution (such as Gaussian) as we did in the discrete case, defining importance weights, and computing a gradient directly at the generator output using the KL divergence.
However, this would abandon some of the strengths central to GANs relative to other generative models with continuous data~\citep{goodfellow2016nips}.

While a tractable form of $p_g(\vx)$ is unavailable to us, a simple analysis of Equation~\ref{eq:target} shows that the distance (by a number of metrics) between $\tilde{p}(\vx)$ and $p_g(\vx)$ is minimized when $\frac{1}{Z} \frac{D(\vx)}{1 - D(\vx)} = 1$.
This indicates an alternative learning objective to training GANs for a generator of continuous data.
There are a number of loss functions that would be suitable, and we 
have experimented with a simple squared-error loss:
\begin{align}
\label{eq:bs_gan_cont}
\mathcal{L}_g(\vx) = \left( \log D(\vx) - \log (1 - D(\vx)) - \log{Z} \right) ^ 2.
\end{align}
An alternative is to use the cross-entropy with a target of $1 / 2$~\citep[this has been done in the context of domain adaptations, e.g.,][]{tzeng2015simultaneous}, though the above loss works very well in practice. In the case of discriminators with a simple sigmoid output, $D(\vx) = \sigma(r(\vx))$, where $\sigma(r) = \frac{1}{1 + e^{-r}}$, the loss simplifies to:
\begin{align}
\mathcal{L}_g(\vx) = \left( r(\vx) - \log{Z} \right) ^ 2.
\label{eq:bs_gan_cont2}
\end{align}
As the ratio $\frac{D(\vx)}{1 - D(\vx)}$ converges to one, so does the partition function, which can be used to formulate an alternative objective to Equation~\ref{eq:bs_gan_cont2}, but without the $\log{Z}$ term.
Though this also works in practice, in our experiments using an estimate of $\log{Z}$ can improve stability and results.
When used, we estimate $\log{Z}$ using a moving average of Monte-Carlo estimates for each update, $t$:
\begin{align}
\log{\tilde{Z}}_{t+1} = \gamma \log{\tilde{Z}}_{t} + (1 - \gamma) \frac{1}{N}\log{\left(\sum_n \frac{D(\vx^{(n)}_t)}{1 - D(\vx^{(n)}_t)}\right)},
\label{eq:log_Z_est}
\end{align}
where $\gamma$ is hyper-parameter controlling the moving average, and $\vx^{(n)}_t$ are the batch of $N$ generated samples at the $t$-th step.

While the continuous-case loss has a very different form from that of the discrete cases, the optimum is the same and corresponds exactly with the minimum of the KL divergence.
The above loss function can be used as an alternative to the usual GAN generator objective for continuous data, and we demonstrate its effectiveness in our experiments below.

\section{Related Work}

\paragraph{GAN for discrete variables}

Our approach introduces new learning algorithms that allows for training generative adversarial networks (GANs) on discrete data. Discrete data with GANs is an active and unsolved area of research, particularly with language model data involving recurrent neural network (RNN) generators~\citep{yu2016seqgan, li2017adversarial}. Our importance sampling-based solution is analogous to reweighted wake-sleep~\citep[RWS,][]{bornschein2014reweighted}, a successful approach for training Helmholtz machines with discrete variables (but not in a GAN framework).

%While this discrete case of BGAN resembles REINFORCE, the motivations and approach are quite different, relying on normalized importance sampling rather than baselines to reduce variance.
Other REINFORCE-based methods have been proposed for language modeling~\citep{yu2016seqgan, che2017maximum}, but the solution we introduce is derived from the inclusive KL divergence following directly from the intuition of BGAN.
The REINFORCE estimator works, though less well compared to the importance sampling-based BGAN in our experiments of the following section.
This situation is analogous to performance of RWS vs NVIL in sigmoid belief networks (SBMs), where RWS outperforms the input-dependent baseline method.
Alternatively, advanced techniques~\citep{mnih2016variational} may improve the REINFORCE-based method, though we leave this for future work.
%From our framework, we were able to derive a new objective for training a generator in the case of continuous variables.

The Gumbel-Softmax technique is another approach to train Helmholz machines with discrete variables~\citep{jang2016categorical,maddison2016concrete}, which effectively allows for training them as variational autoencoders~\citep[VAEs,][]{kingma2013auto}.
However, this technique has only been shown to work in very limited settings, and has yet to been shown to work well with GANs with discrete data.
In our experiments, we find that this estimator does not help train GANs, and these results can be found in the Appendix.
More recent approaches, such as REBAR~\cite{tucker2017rebar}, may prove to be more successful, but this is left for future work.

\paragraph{Learning algorithms for GAN}

Since its introduction by \citet{goodfellow2014generative}, there have been a number of novel objectives for improving GAN training. Recently, \citet{arjovsky2017wasserstein}, proposed a modification to the learning procedure of GAN based on the earth-mover's distance. This method works very well for stabilizing GAN training, but necessarily limits the capacity of the discriminator to limit its Lipschitz constant. 
\citet{mao2016least} proposed a mean-squared error-based approach that improves mode coverage. 
Our work contributes to this pile of recent works, however, with a distinct goal of building a unified learning framework for both discrete and continuous variables.
In addition, LSGAN shares the same optimal discriminator as BGAN in the form of Equation~\ref{eq:opt_disc}.
This indicates an alternative learning objective for the generator in LSGANs, such that the target lies at the average of the real and fake targets of the discriminator (e.g., $1 / 2$ in a 0-1 coding).
Finally, traditional GANs rely on the generator pushing the generated samples in the direction of highest certainty as ``true" data, or towards infinity in the logit space.
This causes the well-known pathology of GANs sometimes pushing the samples beyond the true samples, thus causing learning to fail.
This pathology is explicitly absent from the GAN objective, which removes one possible source of instability (see Appendix for more details).

\paragraph{Implicit inference}
There is a strong connection to implicit variational inference with GANs~\citep{tran2017deep, huszar2017variational}, where the logit of a binary GAN ($r$ in Equation~\ref{eq:bs_gan_cont2}) can be used as a proxy for the log-ratio in the variational lower bound.
This indicates an alternative approach to encoder / decoder type GANs~\citep{dumoulin2016adversarially}, so that the objective is to ensure that the encoding and decoding distributions match.
This is left for future work.

\section{Discrete variables}

\subsection{Datasets and setup}
We first test our approach to training discrete generative adversarial networks (GANs) using two imaging benchmarks: the common discretized MNIST dataset~\citep{salakhutdinov2008quantitative} and a new quantized version of the CelebA dataset \citep[see ][for the original CelebA dataset]{liu2015deep}.
For CelebA quantization, we first downsampled the images from $64\times64$ to $32\times32$.
We then generated a $16$-color palette using Pillow, a fork of the Python Imaging Project (\href{https://python-pillow.org}{https://python-pillow.org}).
This palette was then used to quantize the RGB values of the CelebA samples to a one-hot representation of $16$ colors.
%The quantized CelebA dataset is available for download from \url{http://tdb.url}.
Next, we test BGAN with natural language generation with the 1-billion word dataset~\citep{chelba2013one} at the character-level, limiting the dataset to sentences of at least and truncating to $32$ characters.
As we found better results with the importance-weighted version of BGAN, we use this version for discrete data unless otherwise noted.
Our implementation for discrete BGAN as well as the continuous case in the next Section can be found at \href{https://github.com/rdevon/BGAN}{https://github.com/rdevon/BGAN}.

For imaging data, our models used deep convolutional GANs~\citep[DCGAN,][]{radford2015unsupervised}.
The generator is fed a vector of $64$ i.i.d. random variables drawn from a uniform distribution, $[0, 1]$.
%For MNIST, the generator was composed of dense layers with $1024$ and $128\times7\times7$ units followed by $2$ fractionally-strided convolutional layers with $64$ filters and $1$ filter, respectively.
%For quantized CelebA, the generator was composed of a dense layer with $512 \times 4 \times 4$ units followed by $3$ fractionally-strided convolutional layers with $256$, $128$, and $16$ filters respectively.
%The fractional stride was chosen to ensure that the filter size doubled at each layer, so that the output was $14$ channels of size $32\times32$.
%All layers other than the output layer of the generator were rectified linear units (ReLU), while all layers other than the output layer of the discriminator were leaky rectifiers.
The output nonlinearity was sigmoid for MNIST to model the Bernoulli centers $g(x_i | \vz)$ for each pixel, while the output was softmax for quantized CelebA.
%The discriminator used $3$ convolutional layers, the structure of which was the transpose of that in the generator, followed by a dense layer with a single-unit output and a sigmoid nonlinearity.
For character-level language generation, we follow recent work with advanced training of WGANs~\citep{gulrajani2017improved}, and use deep CNNs for both the generator and discriminator.
For the input-dependent baseline REINFORCE method in Equation~\ref{eq:bgan_reinforce}, we use a three layer fully-connected neural network with $256$ hidden units, leaky-ReLU units, and batch normalization~\citep[BN,][]{ioffe2015batch}.
We applied BN to the generator, as is consistent in the literature.
However, across our experiments with discrete datasets, we found that BN in the discriminator made for unstable learning and worse results.
We therefore omitted the BN from the discriminator.

The model was trained using the Adam method for stochastic optimization~\citep{kingma2014method} with exponential decay rates of $\beta_1 = 0.95$ and $\beta_2 = 0.5$.
MNIST was trained for $100$ epochs, while quantized CelebA was trained for $50$ epochs.
Unlike what is commonly done in the literature for training GANs~\citep{goodfellow2014generative, arjovsky2017wasserstein}, the discriminator was not updated multiple times for every generator update.
We trained the model with learning rates of \num{1e-4}, \num{1e-5}, and \num{1e-6} to test convergence properties.
The number of samples per generated examples, $M$, was 
set to $20$, which was used to generate the normalized weights and train the input-dependent baseline.

%\begin{figure}
%\begin{minipage}{0.49\textwidth}
%\centering
%\includegraphics[width=0.9\linewidth]{extra_figures/BS-GAN.png}
%\end{minipage}
%\hfill
%\begin{minipage}{0.49\textwidth}
%\caption{The evolution of the generator cost with varying learning rates when training a boundary-seeking GAN (BGAN). We observe a smooth convergence of the generator cost for all the learning rates, demonstrating the stability of learning with the proposed BGAN.}
%\label{fig:BGAN_cost}
%\end{minipage}
%\end{figure}

\begin{figure*}[bt]
\begin{minipage}{0.32\textwidth}
\includegraphics[width=0.99\columnwidth]{extra_figures/rand_gen_trunc}
\end{minipage}
\hfill
\begin{minipage}{0.66\textwidth}
\caption{
Left: Random samples from the generator trained as a boundary-seeking GAN (BGAN) with discrete MNIST data.
Shown are the Bernoulli centers of the generator conditional distribution.
Samples show high variability, with realistic generated hand-drawn digits.}
\label{fig:rand_gen}
\end{minipage}

\begin{minipage}{0.32\textwidth}
\includegraphics[width=0.99\columnwidth]{extra_figures/rand_gen_nvil}
\end{minipage}
\hfill
\begin{minipage}{0.66\textwidth}
\caption{
Left: Random samples from the generator trained using the REINFORCE-like algorithm with an input-dependent baseling.
Samples are of good quality, but the model is missing modes.}
\label{fig:rand_gen_nvil}
\end{minipage}

\begin{minipage}{0.32\textwidth}
\includegraphics[width=0.99\columnwidth]{extra_figures/celeba_gt}
\end{minipage}
\hfill
\begin{minipage}{0.32\textwidth}
\includegraphics[width=0.99\columnwidth]{extra_figures/celeba_gen}
\end{minipage}
\hfill
\begin{minipage}{0.32\textwidth}
\caption{
Left: Ground-truth 16-color (4-bit) quantized CelebA images downsampled to $32\times32$.
Right: Samples produced from the generator trained as a boundary-seeking GAN on the quantized CelebA for $50$ epochs.}
\label{fig:celeb_gt}
\end{minipage}
\vspace{-3mm}
\end{figure*}

\subsection{Generation result}

\begin{table}
\caption{Random samples drawn from a generator trained with the discrete BGAN objective.
The model is able to successfully learn many important character-level English language patterns.}
\small
\begin{tabular}{c | c | c}
And it 's miant a quert could he 
& He weirst placed produces hopesi 
& What 's word your changerg bette\\
" We pait of condels of money wi 
& Sance Jory Chorotic , Sen doesin
& In Lep Edger 's begins of a find", \\
Lankard Avaloma was Mr. Palin , 
& What was like one of the July 2 
& " I stroke like we all call on a\\
Thene says the sounded Sunday in 
& The BBC nothing overton and slea
& With there was a passes ipposing\\
About dose and warthestrinds fro 
& College is out in contesting rev
& And tear he jumped by even a roy
\end{tabular}
\label{tab:lm_res}
\end{table}


Our results showed that training the importance-weighted BGAN on discrete MNIST data is stable and produces realistic and highly variable generated handwritten digits.
Figure~\ref{fig:rand_gen} shows example digits from the model trained with a learning rate of \num{1e-4}, sampled randomly from the generator to demonstrate individual digit quality, respectively.
The model was  relatively robust against learning rate, showing stable learning for the generator across learning rates tested without any noticeable sensitivity to hyperparameters.
For quantized CelebA, the generator trained as a BGAN produced reasonably realistic images, which resemble the original dataset well.
In addition, the faces produced are highly variable, and show the type of diversity we expect from a good generator.

For language data, training with BGAN yielded stable, reliably good character-level generation (Table~\ref{tab:lm_res}), though generation is poor compared to recurrent neural network-based methods~\citep{sutskever2011generating,mikolov2012statistical}.
We are however not aware of any previous work in which a discrete GAN, without any continuous relaxation~\citep{gulrajani2017improved}, was successfully trained from scratch without any pretraining and without any auxiliary supervised loss to generate any sensible text . Despite the low quality of the text relative to the existing supervised recurrent language model, we find it a favourable evidence showing the stability and capability of the proposed boundary-seeking criterion for training discrete GAN.

\section{Continuous Variables}
\subsection{Datasets and setup}
We tested the boundary-seeking objective on street view house numbers~\citep[SVHN][]{netzer2011reading}, CIFAR-10~\citep{krizhevsky2009learning}, CelebA, and the Caltech-USCB birds dataset~\citep[][scaled to 64x64]{WelinderEtal2010}. 
As with our experiments with discrete data, we used the DCGAN architecture.
We used this same parameterization, except with one additional convolutional layer for both the generator and discriminator, as is consistent with the literature.
We also used $100$ i.i.d. random variables for the 64x64 image datasets and $64$ variables for the 32x32 image datasets, each drawn from a uniform distribution.
All models were trained with the same Adam hyper-parameters and a learning rate of \num{1e-3}.

For the 32x32 CIFAR-10 and SVHN datasets, BGAN setting $\log{Z} = 0$ was sufficient.
For the larger 64x64 images from CelebA and Caltech-UCSB birds, we found that estimating $\log{Z}$ from Equation~\ref{eq:log_Z_est} improved stability and achieved better samples.

Next, we used CelebA to compare between various GAN variants: normal GAN, least-squares GAN~\citep[LSGAN][]{mao2016least}, and Wasserstein GAN~\citep[WGAN][]{arjovsky2017wasserstein}.
In addition, we train on a boundary-seeking version of LSGANs, setting the generator target to $1 / 2$ with a 0-1 coding discriminator.
We tested on a wider range learning rates and selected those for which the models performed the best qualitatively.
Although \citet{arjovsky2017wasserstein} recommends otherwise, we trained all models, including WGAN,  with one discriminator update per generator update, as in the context of our initial experiments we found there to be no discernible difference.
All models were trained for a maximum of $30$ epochs, though models were stopped early in the case of collapse.

\subsection{Generation result}
\begin{figure*}[t]
\begin{minipage}{0.45\textwidth}
\centering
\includegraphics[width=0.99\columnwidth]{extra_figures/svhn_clip}

(a) SVHN
\end{minipage}
\hfill
\begin{minipage}{0.45\textwidth}
\centering
\includegraphics[width=0.99\columnwidth]{extra_figures/bgan_new_celeba_clip}

(b) CelebA (Continuous)
\end{minipage}

\begin{minipage}{0.45\textwidth}
\centering
\includegraphics[width=.99\linewidth]{extra_figures/bgan_old_cifar_clip}

(c) CIFAR-10
\end{minipage}
\hfill
\begin{minipage}{0.45\textwidth}
\centering
\includegraphics[width=.99\linewidth]{extra_figures/bgan_new_birds_clip}

(d) Caltech-UCSB Birds
\end{minipage}
\caption{
Samples generated from the continuous BGANs trained on various real-valued benchmark datasets; (a) SVHN (32x32), (b) CelebA (64x64 without quantization), (c) CIFAR-10 (32x32), and (d) rescaled Caltech-USBC Birds dataset (64x6x).
Both SVHN and CIFAR-10 were generated from BGAN trained setting $\log{Z} = 0$, while CelebA and Caltech-UCSB birds were generated using the moving average in Equation~\ref{eq:log_Z_est}.}
\label{fig:svhn}
\vspace{-.2cm}
\end{figure*}

\begin{figure*}[t]
\begin{minipage}{0.24\textwidth}
\centering
\includegraphics[width=0.99\columnwidth]{extra_figures/GAN_cifar}

(a) GAN
\end{minipage}
\hfill
\begin{minipage}{0.24\textwidth}
\centering
\includegraphics[width=0.99\columnwidth]{extra_figures/LSGAN_cifar}

(b) LSGAN
\end{minipage}
\begin{minipage}{0.24\textwidth}
\centering
\includegraphics[width=.99\linewidth]{extra_figures/WGAN_cifar}

(c) WGAN
\end{minipage}
\hfill
\begin{minipage}{0.24\textwidth}
\centering
\includegraphics[width=.99\linewidth]{extra_figures/BLSGAN_cifar}

(d) BGAN-LSGAN
\end{minipage}
\caption{
Examples generated from other GAN objectives with the same architecture; (a) GAN, (b) LSGAN, (c) WGAN, and (d) LSGAN trained with a boundary-seeking objective, setting the generator target to $1 / 2$.}
\label{fig:celeba_comp}
\vspace{-.5cm}
\end{figure*}
Example generations from BGAN are presented in Figure~\ref{fig:svhn} (Full sets can be found in the Appendix).
Of all of the models shown, only normal GANs collapsed within the number of epochs run, so we show the last best samples for normal GANs.
BGAN performs well generating images from the imaging datasets and performs comparably in image quality to that of LSGANs and WGANs (Figure~\ref{fig:celeba_comp}), but better than normal GANs within the constraints of our hyperparameter search and training procedures.
Surprisingly, LSGAN with the $1 / 2$ generator target performs very well, generating samples that are at least of equal quality to WGAN, BGAN, and LSGAN.

Though we found that BGAN with continuous data did not easily collapse, it did so more than WGAN, which seems relatively robust against collapse in our experiments (though WGAN can produce strong artifacts or be slow to train, depending on the hyper-parameters).
Particularly for larger datasets like CelebA, we found that BGAN performed better with higher learning rates (\num{1e-3}), higher ``leaking" on the discriminator rectifier nonlinearities, and at least $2$ discriminator updates per generator update during learning.
However, none of the models we tested were absolutely reliable in image quality across a wide range of learning rates, and we conclude that BGAN can produce high-dimensional images as well as WGAN and LSGAN.

\section{Conclusion}

Reinterpreting the generator objective to match the proposal target distribution reveals a novel learning algorithm for training a generative adversarial network~\citep[GANs, ][]{goodfellow2014generative}. 
%At the core of this algorithm is the boundary seeking objective for a generator that stabilizes learning greatly. 
This proposed approach of boundary-seeking provides us with a unified framework under which learning algorithms for both discrete and continuous variables are derived. Empirically, we showed the effectiveness of training a GAN with the proposed learning algorithm, to which we refer as a boundary-seeking GAN (BGAN), on both discrete and continuous variables. 
%In due course, we observed that the proposed approach is more stable compared to the recently proposed re-parametrization technique for discrete variables, called the Gumbel-Softmax technique. 

%The proposed learning algorithm can be extended for the case of continuous variables by incorporating the idea of importance sampling, which was a core recipe of the version of the proposed algorithm for discrete variables. This can be done by using the normalized weights as a multinomial distribution over a batch of generated samples. This amounts
%to filtering out poor samples generated from a generator according to a discriminator, and can be used both during training and inference. We leave this for the future.

\section*{Acknowledgements}

RDH thanks IVADO, MILA, UdeM, NIH grants R01EB006841 and P20GM103472, and NSF grant 1539067 for support.
APJ thanks UWaterloo, Waterloo AI lab and MILA for their support and Michael Noukhovitch, Pascal Poupart for constructive discussions.
KC thanks TenCent, eBay, Google (Reward Awards 2015, 2016), NVIDIA (NVAIL) and Facebook for their support.
YB thanks CIFAR, NSERC, IBM, Google, Facebook and Microsoft for their support.
Finally, we wish to thank the developers of Theano~\citep{team2016theano}, Lasagne~\url{http://lasagne.readthedocs.io}, and Fuel~\citep{van2015blocks} for their valuable code-base.

\bibliography{main}
\bibliographystyle{icml2017}

\section{Appendix}
\subsection{Derivation of importance-weighted discrete BGAN}
Here we derive the gradient of the exclusive KL divergence between the joint target distribution, $\tilde{p}(\vx, \vz)$, which is kept fixed w.r.t. the generative parameters, $\PS$, and the joint generator distribution, $p_g(\vx, \vz) = g(\vx | \vz) p(\vz)$.
\begin{align}
&\nabla_{\PS} D_{KL}(\tilde{p}(\vx) || p_g(\vx)) 
= \nabla_{\PS} \sum_{\vx} \tilde{p}(\vx) \log \frac{\tilde{p}(\vx)}{p_g(\vx)} 
\nonumber\\
&= -\sum_{\vx} \tilde{p}(\vx) \nabla_{\PS} \log p_g(\vx)
= -\sum_{\vx}  \frac{1}{Z} p_g(\vx) \frac{D(\vx)}{1 - D(\vx)} \nabla_{\PS} \log p_g(\vx)
\nonumber\\
&= -\sum_{\vx} \sum_{\vz}  \frac{1}{Z} g(\vx | \vz) p(\vz) \frac{D(\vx)}{1 - D(\vx)} \nabla_{\PS} \log p_g(\vx),
\end{align}

\subsection{REINFORCE-based algorithm}
Here we derive the REINFORCE-based algorithm from the inclusive KL divergence between the joint target distribution and the joint generation distribution.
\begin{align}
&\nabla_{\PS} D_{KL}(p_g(\vx, \vz) || \tilde{p}(\vx, \vz)) 
= \nabla_{\PS} \sum_{\vx, \vz} p(\vz) g(\vx | \vz) \log{\frac{p_g(\vx, \vz)}{\tilde{p}(\vx, \vz)}} \nonumber\\
&= \sum_{\vx, \vz} p(\vz) \left( \nabla_{\PS} g(\vx | \vz) \log{\frac{p_g(\vx, \vz)}{\tilde{p}(\vx, \vz)}} + g(\vx | \vz) \nabla_{\PS} \log{p_g(\vx, \vz)} \right) \nonumber\\
&= \sum_{\vx, \vz} p(\vz) \left( \nabla_{\PS} g(\vx | \vz) \left( \log {g(\vx | \vz')} - \log{\tilde{p}(\vx | \vz)}\right) + g(\vx | \vz) \nabla_{\PS} \log{p(\vz) g(\vx | \vz)} \right) \nonumber\\
&= \sum_{\vx, \vz} p(\vz) g(\vx | \vz) \left( \nabla_{\PS} \log{g(\vx | \vz)} \left( \log{g(\vx | \vz)} - \log{\tilde{p}(\vx)_{|\vz}}\right) + \nabla_{\PS} \log{ g(\vx | \vz)} \right) \nonumber\\
&= -\sum_{\vx, \vz} p(\vz) g(\vx | \vz) \nabla_{\PS} \log{ g(\vx | \vz)} \left(\log{\frac{D(\vx)}{1 - D(\vx)}} - \log{Z_{\vz|}} + 1 \right) \nonumber\\
&\approx - \mathbb{E}\left[\frac{1}{M} \sum_m \left(\log{w^{(m)}} - \log{Z_{|\vz}} - 1\right) \nabla \log{g(\vx^{(m)} | \vz^{(n)})}\right]_{\vz \sim p(\vz)}.
\end{align}

The log-partition function can be estimated a number of ways.
The simplest is to estimate using a logarithm of the Monte-Carlo estimate:
\begin{align}
\log{Z_{\vz|}} = \log \sum_{\vx} g(\vx | \vz) \frac{D(\vx)}{1 - D(\vx)}
\approx \log \sum_m \frac{D(\vx^{(m)})}{1 - D(\vx)^{(m)}} - \log M,
\end{align}
where $\vx^{(m)} \sim g(\vx | \vz)$ are $M$ samples drawn from the generator given $\vz$.
To reduce variance, we can use a moving average, as in Equation~\ref{eq:log_Z_est}.
However, these estimates do not capture the dependence of $\vz$.
As in \citet{mnih2014neural}, we can use an input-dependent baseline with parameters $\PP$, $C(\vz; \PP)$, using a neural network with input $\vz$.
This network is trained with the simple MSE loss function:
\begin{align}
\mathcal{L}_{\PP} = \mathbb{E}\left[C(\vz; \PP) - \log \sum_m \frac{D(\vx^{(m)})}{1 - D(\vx)^{(m)}} + \log M)^2\right]_{\vz \sim p(\vz)}.
\end{align}

\subsection{Visual comparison: GAN vs. BGAN}

In the case of continuous variables, we can qualitatively analyze the difference between the conventional GAN and the proposed boundary-seeking GAN. We consider an one-dimensional variable, and draw 20 samples as each of real and generated sets of samples from two Gaussian distributions. We consider two cases; (a) early stage of learning, and (b) late stage of learning. We use $-2$ and $2$ in the first case, and $-0.1$ and $0.1$ in the second case, for the centers of those two Gaussians. The variances were set to 0.3 for both distributions. Instead of learning, we simply set our discriminator $D(x)$ to
\begin{align*}
    D(x) = \frac{1}{1 + \exp(-c x)},
\end{align*}
where $c$ is set to $0.8$ and $10$ for the cases (a) and (b), respectively. These values were selected to illustrate sub-optimal discriminators, which is almost always true when training a GAN on a real data.

In Fig.~\ref{fig:example_1d}, we plot both real and generated samples on the x-axis ($y=0$). The solid red curve corresponds to the discriminator $D(x)$ above, and its log-gradient ($\partial \log D(x) / \partial x$) is drawn with a dashed red curve. It is clear that maximizing $\log D(x)$, as conventionally done with GAN, pushes the generator beyond the real samples (orange circles). On the other hand, the proposed criterion from Eq.~\eqref{eq:bs_gan_cont} has its minimum at the decision boundary of the discriminator. Minimizing this criterion has the effect of pushing the generated samples, or correspondingly the generator, toward the real samples, but never beyond the region occupied by them. The issue is much more apparent in Fig.~\ref{fig:example_1d}~(b), where the real and generated samples are extremely close to each other. The proposed BGAN encourages the generator to stay close to the center of real samples, while the conventional objective pushes the generated samples beyond the real samples. We conjecture that this property avoids a well-known failure mode of GANs and improves learning. 

\begin{figure*}[t]
    \centering
    \begin{minipage}{0.48\textwidth}
    \centering
        \includegraphics[width=.99\columnwidth]{./extra_figures/example_1d.pdf}
        
        (a)
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
    \centering
        \includegraphics[width=.99\columnwidth]{./extra_figures/example_1d_2.pdf}
        
        (b)
    \end{minipage}
    
    \caption{
    Qualitative comparison between the conventional GAN and the proposed BGAN in 1-D examples. The discriminator $D$ is fixed to a logistic regression classifier with a coefficient $0.8$. Note that the maximum of the generator objective from the conventional GAN is at the positive infinity (red curve), while the minimum from the proposed BGAN is at the decision boundary (blue curve). Unlike the proposed BGAN, the learning gradient of the conventional GAN (red dashed curve) pushes the generated samples beyond the real samples.
    }
    \label{fig:example_1d}
\vspace{-3mm}
\end{figure*}

\subsection{Training discrete GANs with Gumbel softmax}
\begin{table}[h]
\begin{center}
\caption{
Hyperparameters used in our sweep of discrete GANs trained using the Gumbel-Softmax and straight-through Gumbel-Softmax}
\begin{tabular}{ |c|c| } 
 \hline
 \textbf{Optimizer} & [Adam, RMSProp, SGD] \\ 
 \hline
 \textbf{Learning Rate} & [\num{1e-2}, \num{1e-3}, \num{1e-4}, \num{1e-5}]\\
 \hline
 \textbf{Anneal Rate ($r$)} & [$0.01$, $0.001$, $0.0001$]\\ 
 \hline
 \textbf{Anneal Interval ($N$)} & [$200$, $300$, $500$]\\ 
 \hline
\end{tabular}
\label{table:GUMB-GRID-SEARCH}
\end{center}

\vspace{-4mm}
\end{table}

The ''Gumbel technique``~\citep{gumbel1954statistical, jang2016categorical} has been shown to be successful in some tasks where back-propogation through discrete random variables is not possible.
Using the same model parameterization as our MNIST experiments above, we compare our approach to Gumbel-Softmax~\citep{maddison2016concrete, jang2016categorical} and straight-through Gumbel-Softmax~\citep{maddison2016concrete, jang2016categorical} on the binarized MNIST dataset. 
The temperature was annealed using the schedule $\tau = \max(0.5, \exp(-rt))$ as a function of the global training step $t$, where $\tau$ is updated every $N$ steps. The values for the grid search of $r$ and $N$ are listed in Table~\ref{table:GUMB-GRID-SEARCH}.

We observed that the generator cost of Gumbel-Softmax and straight-through Gumbel diverged across all hyperparameters (Figure~\ref{fig:GUMBEL}).
No set of hyperparameters produced a generator that was able to produce satisfactory representative samples of the original dataset.

\begin{figure}
\begin{minipage}{0.49\textwidth}
\centering
\includegraphics[width=0.9\linewidth,clip=True,trim=0 110 0 20]{extra_figures/gumbel_softmax.png}
(a) Gumbel-Softmax
\end{minipage}
\begin{minipage}{0.49\textwidth}
\centering
\includegraphics[width=0.9\linewidth,clip=True,trim=0 110 0 20]{extra_figures/gumbel_softmax_st.png}
(b) Gumbel-Softmax (Straight-Through)
\end{minipage}

\begin{minipage}{0.49\textwidth}
{
\centering
\scriptsize
\begin{tabular}{c l| c c c c}
& & {\color{blue} \bf b} & {\color{yellow}\bf y} & {\color{red}\bf r} & {\color{green}\bf g} \\
\hline\hline
\multirow{4}{*}{\rotatebox{90}{(a)}} &  & adam & rmsprop & adam & rmsprop \\
& lr & $10^{-4}$ & $10^{-4}$ & $10^{-5}$ & $10^{-5}$ \\
& $r$ & $10^{-3}$ & $10^{-4}$ & $10^{-3}$ & $10^{-3}$ \\
& $N$ & $500$ & $200$ & $200$ & $300$ \\
\hline
\multirow{4}{*}{\rotatebox{90}{(b)}}&  & adam & rmsprop & adam & rmsprop \\
& lr & $10^{-4}$ & $10^{-4}$ & $10^{-5}$ & $10^{-5}$ \\
& $r$ & $10^{-3}$ & $10^{-5}$ & $10^{-3}$ & $10^{-3}$ \\
& $N$ & $300$ & $500$ & $200$ & $300$ 
\end{tabular}
}
\end{minipage}
\begin{minipage}{0.49\textwidth}
\caption{
The diverging behaviour of the generator training when using Gumbel-Softmax technique, both (a) with and (b) without straight-through estimation. The curves are a running average and are sampled from our hyperparameter search:
}
\label{fig:GUMBEL}
\end{minipage}
\end{figure}

\subsection{Generation results}
Provided are the complete set of random samples, which were truncated in the main text.
\begin{figure*}[t]
\begin{minipage}{0.45\textwidth}
\centering
\includegraphics[width=0.99\columnwidth]{extra_figures/BGAN_old_svhn_full}

(a) SVHN
\end{minipage}
\hfill
\begin{minipage}{0.45\textwidth}
\centering
\includegraphics[width=0.99\columnwidth]{extra_figures/BGAN_cifar_full}

(b) CelebA (Continuous)
\end{minipage}

\begin{minipage}{0.45\textwidth}
\centering
\includegraphics[width=.99\linewidth]{extra_figures/BGAN_old_cifar_full}

(c) CIFAR-10
\end{minipage}
\hfill
\begin{minipage}{0.45\textwidth}
\centering
\includegraphics[width=.99\linewidth]{extra_figures/BGAN_new_birds_full}

(d) Caltech-UCSB Birds
\end{minipage}
\caption{
Samples generated from the continuous BGANs trained on various real-valued benchmark datasets; (a) SVHN (32x32), (b) CelebA (64x64 without quantization), (c) CIFAR-10 (32x32), and (d) rescaled Caltech-USBC Birds dataset (64x6x).}
\vspace{-.5cm}
\end{figure*}

\begin{figure*}[t]
\begin{minipage}{0.45\textwidth}
\centering
\includegraphics[width=0.99\columnwidth]{extra_figures/GAN_cifar_full}

(a) GAN
\end{minipage}
\hfill
\begin{minipage}{0.45\textwidth}
\centering
\includegraphics[width=0.99\columnwidth]{extra_figures/LSGAN_cifar_full}

(b) LSGAN
\end{minipage}

\begin{minipage}{0.45\textwidth}
\centering
\includegraphics[width=.99\linewidth]{extra_figures/WGAN_cifar_full}

(c) WGAN
\end{minipage}
\hfill
\begin{minipage}{0.45\textwidth}
\centering
\includegraphics[width=.99\linewidth]{extra_figures/BLSGAN_cifar_full}

(d) BGAN-LSGAN
\end{minipage}
\caption{
Examples generated from other GAN objectives with the same architecture; (a) GAN lr=\num{1e-4}, (b) LSGAN lr=\num{1e-3}, (c) WGAN lr=\num{1e-4}, and (d) LSGAN trained with a boundary-seeking objective, setting the generator target to $1 / 2$, lr=\num{1e-3}.}
\end{figure*}

\end{document}


We introduce a novel training principle for probabilistic models that is an alternative to maximum likelihood. The proposed Generative Stochastic Networks (GSN) framework is based on learning the transition operator of a Markov chain whose stationary distribution estimates the data distribution.  Because the transition distribution is a conditional distribution generally involving a small move, it has fewer dominant modes, being unimodal in the limit of small moves. Thus, it is easier to learn, more like learning to perform supervised function approximation, with gradients that can be obtained by back-propagation. The theorems provided here generalize recent work on the probabilistic interpretation of denoising auto-encoders and provide an interesting justification for dependency networks and generalized pseudolikelihood (along with defining an appropriate joint distribution and sampling mechanism, even when the conditionals are not consistent). We study how GSNs can be used with missing inputs and can be used to sample subsets of variables given the rest.  Successful experiments are conducted, validating these theoretical results, on two image datasets and with a particular architecture that mimics the Deep Boltzmann Machine Gibbs sampler but allows training to proceed with backprop, without the need for layerwise pretraining.



\icmlauthor{Yoshua Bengio$^*$}{find.us@on.the.web}
%\icmladdress{}
\icmlauthor{\'Eric Thibodeau-Laufer}{}
\icmlauthor{Guillaume Alain}{}
%\ifsavetrees
%\icmladdress{D\'epartement d'informatique et recherche op\'erationnelle, Universit\'e de Montr\'eal,$^*$also Canadian Institute for Advanced Research}
%\else
\icmladdress{D\'epartement d'informatique et recherche op\'erationnelle, Universit\'e de Montr\'eal,$^*$\& Canadian Inst. for Advanced Research}
%CIFAR Senior Fellow}
%\fi
\icmlauthor{Jason Yosinski}{}
\icmladdress{Department of Computer Science, Cornell University}



%\section{--- Outline ---}
%
%\begin{enumerate}
%\item We want to learn a model for $P(x)$
%  
%  \begin{enumerate}
%  \item of course, this is hard (and there have been many approaches)...
%   The main challenges when there are latent variables (which allow one to capture complex dependencies)
%   are (1) the computation of the normalization constant (partition function)
%   and its gradient (always an issue in undirected models) and/or (2) inference
%   (summing or maximizing over configurations of the latent variables, given 
%   an input observation), which is always an issue in directed models, but also
%   in undirected models where the latent variables are not conditionally independent.
%   Both sums (either over all variables or just over latent variables) are intractable 
%   and different families of approximation exist (MCMC, MAP, variational).
%  \item many powerful (parsimonious) models take a long time to train due to: 
%   (1) there could be a huge number of important modes (either
%  in the posterior over latent variables, for inference, or over all the variables,
%  for the gradient of the partition function)
%  (2) Exploration of these modes is hard, i.e., 
%  mixing between major modes (or equivalently, finding the most likely mode) can be exponentially difficult 
%  when the major modes are separated by vast zones of low density, which corresponds to the
%  commonly made manifold assumption, that data concentrate near different low-dimensional manifolds 
%  and that the manifolds of different classes are well separated (although they could be highly curved,
%  making the task of learning to classify difficult).
%  \end{enumerate}
%
%\item We propose a radically new approach
%  \begin{enumerate}
%  \item Define an ``incomplete, noisy representation'' function $f$, which depends
%   on an instance of the observed random variable $X$. Can be almost anything (weak conditions 
%   suffice to guarantee that the Markov chain associated with the learner converges) so
%   long as it stochastically destroys information about $X$.
%  \item Learn the (potentially) much easier $P(X|f(X))$
%    
%    \begin{enumerate}
%    \item train *this* function via gradient descent
%    \item this reduces to function approximation (for simple $P(X|f(X))$ models with one mode, right? yes)
%    \item we know a lot about function approx
%    \item bring in here the part about general priors / ANN being a good function approx model
%    \end{enumerate}
%
%  \end{enumerate}
%
%  \item Use the noising function and denoising function to define a transition operator, 
%  which defines a Markov Chain. Furthermore, the Markov Chain can be extended with
%  {\em stochastic hidden variables $H$}, by making1 $f$ depend on the previous $H$
%  and produce a next $H$, and we learn $P(X_t=x | f(X_{t-1}, H_{t-1}))$ while $H_t = g(X_{t-1},H_{t-1})$
%  and we initialize the chain with $X_0=X$.
%
%  \item If your $P(X|f)$ converges to true denoising dist, then under some
%    reasonable assumptions, we show that the stationary distribution of the chain, $\pi_n$
%    converges to the true data-generating distribution ${\calP}(X)$. 
%     (Of course, we expect that near $P(X|f)$ convergence would
%    correspond to near $\pi_n \rightarrow {\calP}(X)$)
%
%   \item By the way, this can also deal with missing inputs, 
%   sampling from the conditional, and structured output too! (pull in section on this)
%
%\end{enumerate}
%
%Proofs
%
%Demos
%
%Conclusion





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\svs{3}
\section{Introduction}
\svs{2}

\begin{figure}[htpb]
\vs{3}
\centering
\includegraphics[width=0.8\linewidth]{crop_px+dae2.pdf}
\includegraphics[width=0.8\linewidth]{crop_px+gsn2.pdf}
\vs{3}
\caption{
{\em Top:} A denoising auto-encoder defines an estimated Markov chain where the transition 
operator first samples a corrupted $\tilde{X}$ from ${\cal C}(\tilde{X}|X)$ and then samples a reconstruction from $P_\theta(X|\tilde{X})$, which is trained to estimate the ground truth 
${\calP}(X|\tilde{X})$. Note how for any given $\tilde{X}$,
${\calP}(X|\tilde{X})$ is a much simpler (roughly unimodal) distribution than the
ground truth ${\calP}(X)$ and its partition function is thus easier to approximate.
{\em Bottom:} More generally, a GSN allows the use of arbitrary 
latent variables $H$ in addition to $X$, with the Markov chain state 
(and mixing) involving both $X$ and $H$. Here $H$ is the angle about the origin.
The GSN inherits the benefit of a simpler conditional and adds 
latent variables, which allow more powerful deep representations in which mixing is easier~\citep{Bengio-et-al-ICML2013}.
}
\label{fig:data_px}
\vs{3}
\end{figure}


Research in deep learning~(see \citet{Bengio-2009-book}
and~\citet{Bengio-Courville-Vincent-TPAMI2013} for reviews) grew from
breakthroughs in unsupervised learning of representations, based
mostly on the Restricted Boltzmann Machine (RBM)~\citep{Hinton06},
auto-encoder
variants~\citep{Bengio-nips-2006-small,VincentPLarochelleH2008-small}, and
sparse coding variants~\citep{HonglakLee-2007,ranzato-07-small}.
However, the most impressive recent results have been obtained with purely
supervised learning techniques for deep networks, in particular for speech
recognition~{\citep{dahl2010phonerec-small,Deng-2010,Seide2011}
and object recognition~\citep{Krizhevsky-2012-small}.
The latest breakthrough in object
recognition~\citep{Krizhevsky-2012-small} was achieved with fairly deep
convolutional networks with a form of noise injection in the input and hidden
layers during training,
called dropout~\citep{Hinton-et-al-arxiv2012}.

\ifLongversion
In all of these cases, the availability of large quantities of labeled data was critical.

On the other hand, progress with deep unsupervised architectures has been
slower, with the established approaches with a probabilistic footing
being the Deep Belief Network (DBN)~\citep{Hinton06} and the Deep Boltzmann
Machine (DBM)~\citep{Salakhutdinov+Hinton-2009-small}. 
Although single-layer unsupervised learners are fairly well
developed and used to pre-train these deep models, 
jointly training all the layers with respect to a single
unsupervised criterion remains a challenge, with a few
techniques arising to reduce that difficulty~\citep{Montavon2012,Goodfellow-et-al-NIPS2013}.
In contrast to recent progress toward joint supervised training of models with many layers, joint 
unsupervised training of deep models remains a difficult task.
\fi

\ifLongversion
In particular, the normalization constant
involved in complex multimodal probabilistic models is often intractable
and this is dealt with using various approximations (discussed below)
whose limitations may be an important part of the difficulty for training
and using deep unsupervised, semi-supervised or structured output models.
\fi

Though the goal of training large unsupervised networks has turned out to
be more elusive than its supervised counterpart, the vastly larger
available volume of unlabeled data still beckons for efficient methods to
model it.  Recent progress in training supervised models raises the question:
can we take advantage of this progress to improve our ability to train
deep, generative, unsupervised, semi-supervised or structured output models?

This paper lays theoretical foundations
for a move in this direction through the following main contributions:

{\bf  1 -- Intuition: } In Section~\ref{sec:unsup_hard} we discuss what we view as
basic motivation for studying alternate ways of training unsupervised probabilistic models, i.e., avoiding
the intractable sums or maximization involved in many approaches.


{\bf   2 -- Training Framework: } We start Section~\ref{sec:gsn} by presenting our recent work on the generative view of denoising auto-encoders (Section~\ref{sec:DAE-model-prob-density}). We present the \emph{walkback} algorithm which addresses some of the training difficulties with denoising auto-encoders (Section~\ref{sec:walkback-description}). 

We then generalize those results by introducing
latent variables in the framework to define Generative Stochastic Networks
(GSNs) (Section~\ref{sec:from-DAE-to-GSN}). GSNs aim to estimate the data-generating
distribution indirectly, by parametrizing the transition operator of a
Markov chain rather than directly parametrizing a model $P(X)$ of the observed
random variable $X$.
\iffalse
We provide a statistically consistent way of
estimating the underlying data distribution
% (in fact benefiting from the rate of convergence of maximum likelihood) 
based on a denoising-like criterion where the noise can be injected not
just in the input but anywhere in the computational graph that 
predicts the distribution of the denoised input.
%Our approach combines two simple components to create a Markov Chain whose stationary distribution models the density of data.
%One component creates a noisy, incomplete representation of a data point in an arbitrary new space, and the other component reconstructs the original data from the noisy representation.
\fi
\iffalse
Briefly, we create a Markov chain whose transition operator consists of
alternately sampling from a noisy representation distribution
$P(h_{t+1}|X,h_t)$ and a reconstruction distribution
$P(X|h_{t+1})$, where $h_t$ are stochastic latent variables.
\fi
Most critically, {\em this framework transforms the unsupervised density estimation
problem into one which is more similar to supervised function approximation}.
This enables training by (possibly regularized) maximum likelihood
and gradient descent computed via simple back-propagation,
avoiding the need to compute intractable partition functions. Depending
on the model, this may allow us to draw from any number of recently demonstrated
supervised training tricks. 
\ifLongversion
For example, one could use a convolutional
architecture with max-pooling for parametric parsimony and computational
efficiency, % REMOVED Adagrad \cite{duchi2011adaptive} for quick gradient descent, and
or dropout ~\citep{Hinton-et-al-arxiv2012} to prevent co-adaptation of hidden
representations.
\fi

{\bf   3 -- General theory:}
Training the generative (decoding / denoising) component of a GSN 
$P(X|h)$ with noisy representation $h$ is often far easier than modeling $P(X)$ explicitly
(compare the blue and red distributions in \mbox{Figure~\ref{fig:data_px})}. We prove that if our estimated $P(X|h)$
is consistent (e.g. through maximum likelihood), then the
stationary distribution of the resulting Markov chain is a consistent estimator of the
data-generating density, ${\calP}(X)$ (Section~\ref{sec:DAE-model-prob-density} and
Appendix \ref{sec:local-consistency}).

{\bf   4 -- Consequences of theory: } We show that the model is general and extends to a wide range of architectures,
including sampling procedures whose computation can be unrolled
as a Markov Chain, i.e., architectures that add noise during intermediate
computation in order to produce random samples of a desired distribution (Theorem~\ref{thm:noisy-reconstruction}).
An exciting frontier in machine learning is the problem
of modeling so-called structured outputs, i.e., modeling
a conditional distribution where the output is high-dimensional
and has a complex multimodal joint distribution (given the input variable).
We show how GSNs can be used to support such structured output and missing values (Section~\ref{sec:missing_inputs}).

{\bf   5 -- Example application: } In Section~\ref{sec:gsn_experiment} we show an example application of the GSN theory to create a
deep GSN whose computational graph 
resembles the one followed by Gibbs sampling in deep Boltzmann machines (with continuous latent variables),
but that can be trained efficiently with back-propagated gradients
and without layerwise pretraining. 
Because the Markov Chain is defined over a state
$(X,h)$ that includes latent variables, we reap the dual advantage
of more powerful models for a given number of
parameters and better mixing in the chain as we add noise to variables
representing higher-level information, first suggested by the results
obtained by~\citet{Bengio-et-al-ICML2013}
and~\citet{Luo+al-AISTATS2013-small}. The experimental results show that
such a model with latent states indeed mixes better
than shallower models without them (Table~\ref{tab:LL}).


{\bf   6 -- Dependency networks: } Finally, an unexpected result falls out of the GSN theory:
it allows us to provide a novel justification for dependency networks~\citep{HeckermanD2000}
and for the first time define a proper joint distribution between all the visible
variables that is learned by such models (Section~\ref{sec:dependency-nets}).





%\editbox{maybe mention that this puts requirements on the user-supplied model for $P(X|h)$...}
% what requirements? the usual requirement in function approximation is that the model class is 'large enough', nothing new here I think.
% JBY: The requirement is that the user can sample from their provided $P(X|h)$ conditioned on some of the inputs, right?
%\vspace*{-.5ex}
%\end{itemize}

\svs{2}
\section{Summing over too many major modes}
\label{sec:unsup_hard}
\svs{2}

The approach presented in this paper is motivated by a difficulty often encountered with 
probabilistic models, especially those containing anonymous latent variables.
They are called anonymous
because no a priori semantics are assigned to them, like in Boltzmann machines,
and unlike in many knowledge-based graphical models. Whereas inference
over non-anonymous latent variables is required to make sense of the model,
anonymous variables are only a device to capture the structure of the distribution
and need not have a clear human-readable meaning.

However, graphical models with latent variables often require dealing
with either or both of the following fundamentally difficult problems in
the inner loop of training, or to actually use the model for making
decisions: inference (estimating the posterior distribution over latent
variables $h$ given inputs $x$) and sampling (from the joint model of $h$
and $x$). However, if the posterior $P(h|x)$ has a huge number of modes
that matter, then the approximations made may break down.

Many of the computations involved in graphical models (inference, sampling,
and learning) are made intractable and difficult to approximate because
of the large number of non-negligible modes in the modeled distribution
(either directly $P(x)$ or a joint distribution $P(x,h)$ involving latent variables $h$).
In all of these cases, what is intractable is the computation or approximation
of a sum (often weighted by probabilities), such as a marginalization or the estimation
of the gradient of the normalization constant. If only a few terms in this
sum dominate (corresponding to the dominant modes of the distribution), then
many good approximate methods can be found, such as Monte-Carlo Markov
chains (MCMC) methods.

Deep Boltzmann machines~\citep{Salakhutdinov+Hinton-2009-small} combine the difficulty of
inference (for the {\em positive phase} where one tries to push the
energies associated with the observed $x$ down) and also that of
sampling (for the {\em negative phase} where one tries to push up the
energies associated with $x$'s sampled from $P(x)$).  
Sampling for the
negative phase is usually done by MCMC, although some unsupervised learning
algorithms~\citep{CollobertR2008-small,Gutmann+Hyvarinen-2010-small,Bordes-et-al-LSML2013}
involve ``negative examples'' that are sampled through simpler procedures
(like perturbations of the observed input, in a spirit reminiscent of
the approach presented here). 
Unfortunately, using an MCMC method to sample from $P(x,h)$ in order to
estimate the gradient of the partition function may be seriously hurt by
the presence of a large number of important modes, as argued below.

To evade the problem of highly multimodal joint or posterior
distributions, the currently known
approaches to dealing with the above intractable sums make very strong explicit 
assumptions (in the parametrization) or
implicit assumptions (by the choice of approximation methods) on the form of the distribution of interest.
In particular, MCMC methods are more likely to produce a good estimator
if the number of non-negligible modes is small: otherwise the
chains would require at least as many MCMC steps as the number of such
important modes, times a factor that accounts for the mixing time
between modes. Mixing time itself can be  very problematic as a trained model
becomes sharper, as it approaches a data-generating distribution
that may have well-separated and sharp modes (i.e., manifolds)~\citep{Bengio-et-al-ICML2013}.

\iffalse
%To illustrate the inherent difficulty of sampling 
\vs{3}
\subsection{An illustration of the multimodal problem}
\vs{2}
%To illustrate the problem of multimodality...

%\edit{Maybe shorten the part from here...}

\ifLongversion
Imagine for example that $h$
represents many explanatory variables of a rich audio-visual ``scene'' with a
highly ambiguous raw input $x$, including the presence of several objects
with ambiguous attributes or categories, such that one cannot really
disambiguate one of the objects independently of the others (the so-called
``structured output'' scenario, but at the level of latent explanatory
variables). Clearly, a factorized or unimodal representation would be
inadequate (because these variables are not at all independent, given $x$)
while the number of modes could grow
exponentially with the number of ambiguous factors present in the
scene. 
\fi
For example, consider $x$ being the audio of speech pronounced
in a foreign language that you do not know well, so that you cannot
easily segment and parse each of the words. The number of
plausible interpretations (given your poor knowledge of that foreign
language) could be exponentially large (in the length of the utterance),
and the individual factors (words) would certainly not be conditionally
independent (actually having a very rich structure which corresponds to
a language model). Say there are $10^6$ possible segmentations
into around 10 word segments, each associated with 100 different
plausible candidates (out of a million, counting proper nouns), but,
due to the language model, only 1 out of $10^6$ of their combinations are
plausible (i.e., the posterior does not factorize or fit a tree structure).
So one really
has to consider $\frac{1}{10^6} \times 100^{10} \times 10^6 = 10^{20}$ {\em plausible configurations} 
of the latent variables (high-probability modes).
Making a decision $y$ based on $x$,
e.g., $P(y\mid x)=\sum_h P(y\mid h) P(h\mid x)$,
 involves summing over a huge
number of non-negligible terms of the posterior $P(h\mid x)$, which
we can consider as important modes.
One way or another, {\em summing explicitly over that many
modes seems implausible}, and assuming single mode (MAP) or a factorized
distribution (mean-field) could yield very poor results.

\ifLongversion
Under some assumptions on the underlying data-generating process,
it might well be possible to do inference that is exact or a
provably good approximation, and searching for graphical models
with these properties is an interesting avenue to deal with this problem.
Basically, these assumptions work because we assume a specific structure
in the form of the underlying distribution.
Also, if we are lucky, a few Monte-Carlo
samples from $P(h\mid x)$ might suffice to obtain an acceptable approximation
for our $y$, because somehow, as far as $y$ is concerned, many probable values of $h$
yield the same answer $y$ and a Monte-Carlo sample will well represent these
different ``types'' of values of $h$.
That is one form of regularity that could be exploited (if it exists) to
approximately solve that problem.
\else
In some problems
of interest, we may be lucky enough to discover an estimator whose implicit
assumptions still allow us
to do a good job of approximating such a sum.
\fi


But what if these assumptions are not
appropriate to solve challenging AI problems?
%Another, more general assumption (and thus one more likely to be appropriate for these problems) may suffice: that function approximation can generalize well despite the space of functions being combinatorially large.
%As is similar to what we usually do with machine learning: function approximation, i.e.,  although the space of functions is combinatorially large, we are able to generalize by postulating a rather large and flexible family of functions (such as a deep neural net).
\fi

We propose to make another assumption that might suffice to bypass
this multimodality problem: the
effectiveness of function approximation. 
As is typical in machine learning,
we postulate a rather large and flexible family of functions (such as deep
neural nets) and then use all manner of tricks to pick a member from that
combinatorially large family (i.e. to train the neural net) that both fits
observed data and generalizes to unseen data well.

In particular, the GSN approach presented in the next section relies on estimating the transition operator
of a Markov chain, e.g. $P(x_t | x_{t-1})$ or $P(x_t, h_t | x_{t-1}, h_{t-1})$.
Because each step of the Markov chain is generally local, these transition
distributions will often include only a very small number of important modes
(those in the neighborhood of the previous state). Hence the gradient of their
partition function will be easy to approximate. For example consider the
denoising transitions studied by~\citet{Bengio-et-al-NIPS2013} and illustrated
in Figure~\ref{fig:data_px},
where $\tilde{x}_{t-1}$ is a stochastically corrupted version of $x_{t-1}$
and we learn the denoising distribution $P(x | \tilde{x})$.
In the extreme case (studied empirically here) where $P(x | \tilde{x})$
is approximated by a unimodal distribution, the only form of training
that is required involves function approximation (predicting the clean $x$
from the corrupted $\tilde{x}$).

Although having the true $P(x | \tilde{x})$ turn out to be unimodal
makes it easier to find an appropriate family of models for it,
unimodality is by no means required by the GSN framework itself.  One
may construct a GSN using any multimodal model for output (e.g. mixture
of Gaussians, RBMs, NADE, etc.), provided that gradients for the parameters of
the model in question can be estimated (e.g. log-likelihood gradients).

% TODO: determine fate of this next paragraph
% JBY: When I read the arXiv version of the paper, it was during this paragraph that I understood why function approximation might be a good idea.

%Thus an interesting avenue is to assume that there exists a
%computationally tractable function that computes ${P(y\mid x)}$
%in spite of the apparent complexity of going through the intermediate
%steps involving $h$, and that we may learn ${P(y\mid x)}$ through $(x,y)$ examples.
%In fact, this is exactly what we do when we train a deep supervised neural
%net or any black-box supervised machine learning algorithm. 

%The question we want to address here is that of unsupervised learning: could
%we take advantage of this idea of bypassing explicit latent variables in the
%realm of unsupervised probabilistic models of the data?
The approach proposed here thus avoids the need for a poor approximation
of the gradient of the partition function in the inner loop of training,
but still has the potential of capturing very rich distributions by relying
mostly  on ``function approximation''.

Besides the approach discussed here, there may well be other
very different ways of evading this problem of 
intractable marginalization, including
approaches such as sum-product
networks~\citep{Poon+Domingos-2011b}, which are based on learning a probability function that
has a tractable form by construction and yet is from a flexible enough family
of distributions. Another interesting direction of investigation
that avoids the need for MCMC and intractable partition functions
is the variational auto-encoder~\citep{Kingma+Welling-ICLR2014,Gregor-et-al-ICML2014,Mnih+Gregor-ICML2014,Rezende-et-al-arxiv2014} and related directed models~\citep{Bornschein+Bengio-arxiv2014-small,Ozair+Bengio-arxiv2014},
which rely on learned approximate inference.

\svs{2}
\section{Generative Stochastic Networks}
\label{sec:gsn}
\svs{2}

In this section we work our way from denoising auto-encoders (DAE) to generative stochastic networks (GSN).
We illustrate the usefulness of denoising auto-encoders being applied iteratively as a way
to generate samples (and model a distribution). We introduce the {\em walkback} training algorithm
and show how it can facilitate the training.

We generalize the theory to GSNs, and provide a theorem that serves as a recipe 
as to how they can be trained. We also reference a classic result from matrix perturbation theory
to analyze the behavior of GSNs in terms of their stationary distribution.

We then study how GSNs may be used to fill missing values and theoretical
conditions for estimating associated conditional samples. 
Finally, we connect GSNs to dependency nets
and show how the GSN framework fixes one of the main problems with the theoretical
analysis of dependency nets and propose a particular way of sampling from them.

\svs{2}
\subsection{Denoising auto-encoders to model probability distributions}
\label{sec:DAE-model-prob-density}
\svs{2}

%JBY
Assume the problem we face is to construct a model for some unknown
data-generating distribution ${\calP}(X)$ given only examples of $X$ drawn
from that distribution. In many cases, the unknown distribution
${\calP}(X)$ is complicated, and modeling it directly can be difficult.
%For example, see the distribution pictured in Figure~\ref{fig:data_px}(a). -- YB: it is not really difficult to model.

%Though the unsupervised problem of modeling ${\calP}(X)$ is often difficult, 
A recently proposed approach using denoising auto-encoders (DAE) transforms the 
difficult task of modeling ${\calP}(X)$ into a supervised learning problem that may be much easier to solve.
The basic approach is as follows: given a clean example data point $X$ from ${\calP}(X)$,
we obtain a corrupted version $\tilde{X}$ by sampling from some corruption distribution ${\cal C}(\tilde{X}|X)$.
For example, we might take a clean image, $X$, and add random white noise to produce $\tilde{X}$.
We then use supervised learning methods to train a function to reconstruct, as accurately as possible,
any $X$ from the data set given only a noisy version $\tilde{X}$. As shown in Figure~\ref{fig:data_px},
the reconstruction distribution ${\calP}(X|\tilde{X})$ may often be much easier to learn than the data distribution ${\calP}(X)$,
{\em because ${\calP}(X|\tilde{X})$ tends to be dominated by a single or few major modes}
(such as the roughly Gaussian shaped density in the figure). What we call a major
mode is one that is surrounded by a substantial amount of probability mass. There
may be a large number of minor modes that can be safely ignored in the context
of approximating a distribution, but the major modes should not be missed.

But how does learning the reconstruction distribution help us solve our
original problem of modeling ${\calP}(X)$? The two problems are clearly
related, because if we knew everything about ${\calP}(X)$, then our
knowledge of the ${\cal C}(\tilde{X}|X)$ that we chose would allow us to
precisely specify the optimal reconstruction function via Bayes rule:
${\calP}(X|\tilde{X}) = \frac{1}{z} {\cal C}(\tilde{X}|X){\calP}(X)$,
where $z$ is a normalizing constant that does not depend on $X$. As one
might hope, the relation is also true in the opposite direction: once we
pick a method of adding noise, ${\cal C}(\tilde{X}|X)$, knowledge of the
corresponding reconstruction distribution ${\calP}(X|\tilde{X})$ is
sufficient to recover the density of the data ${\calP}(X)$.

In a recent paper, \citet{Alain+Bengio-ICLR2013} showed that denoising auto-encoders
with small Gaussian corruption and squared error loss estimated the
score (derivative of the log-density with respect to the input) of 
continuous observed random variables, thus implicitly estimating $P(X)$.
The following Proposition \ref{prop:markov-chain-basic-gsn}
generalizes this to arbitrary variables (discrete, continuous or both),
arbitrary corruption (not necessarily asymptotically small), and arbitrary
loss function (so long as they can be seen as a log-likelihood).

\begin{proposition}
\label{prop:markov-chain-basic-gsn}
Let $P(X)$ be the training distribution for which we only have empirical samples.
Let ${\cal C}(\tilde{X}|X)$ be the fixed corruption distribution and $P_\theta(X|\tilde{X})$
be the trained reconstruction distribution (assumed to have sufficient capacity).
We define a Markov chain that starts at some $X_0 \sim P(X)$ and then iteratively samples pairs of values
$(X_k, \tilde{X}_k)$ by alternatively sampling from 
${\cal C}(\tilde{X}_k|X_k)$ and from $P_\theta(X_{k+1}|\tilde{X}_k)$.

\begin{center}
\includegraphics[width=0.85\linewidth]{basic_long_markov_chain_plain_X_Xtilde_0_to_k.pdf}
\end{center}

Let $\pi$ be the stationary distribution of this Markov chain when we consider only the
sequence of values of $\{X_k\}_{k=0}^\infty$.

If we assume that this Markov chain is irreducible, that its stationary distribution exists,
and if we assume that $P_\theta(X|\tilde{X})$ is the distribution
that minimizes optimally the following expected loss
\begin{equation*}
\mathcal{L} = \int_{\tilde{X}} \int_X P(X)\mathcal{C}(\tilde{X}|X) \log P_\theta(X | \tilde{X}) dX d\tilde{X},
\end{equation*}
then we have that the stationary distribution $\pi$ is the same as the training distribution $P(X)$.
\end{proposition}
\begin{proof}
If we look at the density $P(\tilde{X})=\int P(X) \mathcal{C}(\tilde{X}|X) d\tilde{X}$ that
we get for $\tilde{X}$ by applying $\mathcal{C}(\tilde{X}|X)$ to the training data from $P(X)$,
we can rewrite the loss as a KL divergence
\begin{equation*}
\int_{\tilde{X}} \int_X P(X)\mathcal{C}(\tilde{X}|X) \log P_\theta(X | \tilde{X}) dX d\tilde{X} =
- \textrm{KL}\left( P(X)\mathcal{C}(\tilde{X}|X) \| P_\theta(X | \tilde{X}) P(\tilde{X}) \right) + \textrm{cst}
\end{equation*}
where the constant is independent of $P_\theta(X | \tilde{X})$.
This expression is maximized when
we have a $P_\theta(X | \tilde{X})$ that satisfies
\begin{equation}
\label{eqn:two-expressions-for-joint}
P(X)\mathcal{C}(\tilde{X}|X) = P_\theta(X | \tilde{X}) P(\tilde{X}).
\end{equation}
In that case, we have that
\begin{equation*}
P_{\theta^*}(X | \tilde{X}) = \frac{ P(X)\mathcal{C}(\tilde{X}|X) }{ P(\tilde{X} } = P(X | \tilde{X})
\end{equation*}
where $P(X | \tilde{X})$ represents the true conditional that we get through
the usual application of Bayes' rule.

Now, when we sample iteratively between $\mathcal{C}(\tilde{X}_k|X_k)$
and $P_{\theta^*}(X_{k+1} | \tilde{X}_k)$ to get the Markov chain illustrated above,
we are performing Gibbs sampling. We understand what Gibbs sampling does,
and here we are sampling using the two possible ways of expressing the joint from
equation (\ref{eqn:two-expressions-for-joint}). This means that the stationary
distribution $\pi$ of the Markov chain will have $P(X)$ as marginal density
when we look only at the $X_k$ component of the chain.

\end{proof}

Beyond proving that ${\calP}(X|\tilde{X})$ is sufficient to reconstruct the data density,
Proposition \ref{prop:markov-chain-basic-gsn} also demonstrates a method of sampling
from a learned, parametrized model of the density, $P_\theta(X)$, by running a Markov chain
that alternately adds noise using ${\cal C}(\tilde{X}|X)$ and denoises by sampling from
the learned $P_\theta(X|\tilde{X})$, which is trained to approximate the true ${\calP}(X|\tilde{X})$.
%The most important contribution of that paper was demonstrating that if a learned, parametrized reconstruction function $P_\theta(X|\tilde{X})$ converges to the true ${\calP}(X|\tilde{X})$, then under some relatively benign conditions the stationary distribution $\pi(X)$ of the resulting Markov chain will exist and will indeed converge to the data distribution ${\calP}(X)$.

Before moving on, we should pause to make an important point clear.
Alert readers may have noticed that ${\calP}(X|\tilde{X})$ and ${\calP}(X)$
can each be used to reconstruct the other given knowledge of ${\cal C}(\tilde{X}|X)$.
Further, if we assume that we have chosen a simple ${\cal C}(\tilde{X}|X)$
(say, a uniform Gaussian with a single width parameter),
then ${\calP}(X|\tilde{X})$ and ${\calP}(X)$ must both be of approximately the same complexity.
Put another way, we can never hope to combine a simple ${\cal C}(\tilde{X}|X)$ and
a simple ${\calP}(X|\tilde{X})$ to model a complex ${\calP}(X)$.
Nonetheless, it may still be the case that ${\calP}(X|\tilde{X})$ is easier to {\em model} than ${\calP}(X)$
due to reduced computational complexity in computing or approximating the
partition functions of the conditional distribution mapping corrupted input $\tilde{X}$ to 
the distribution of corresponding clean input $X$.
Indeed, because that conditional is going to be mostly assigning probability to $X$ locally around $\tilde{X}$,
${\calP}(X|\tilde{X})$ has only one or a few major modes, while ${\calP}(X)$ can have a very large number
of them.

So where did the complexity go? ${\calP}(X|\tilde{X})$ has fewer major modes than ${\calP}(X)$,
but {\em the location of these modes depends on the value of $\tilde{X}$}.
It is precisely this mapping from $\tilde{X} \rightarrow$ {\em mode location} that allows us
to trade a difficult density modeling problem for a supervised function approximation problem
that admits application of many of the usual supervised learning tricks.

%We pause here to make an important point clear: we do not claim that modeling ${\calP}(X|\tilde{X})$ is easier than modeling ${\calP}(X)$ because it is a simpler distribution. Indeed, both distributions will 
%the advantage is not due to an overall reduced complexity of the represented
%distribution
%(there should be about the same number of parameters, for example) but

\vspace{0.5cm}

In the Gaussian noise example, what happens is that the tails of the Gaussian
are exponentially damping all but the modes that are near $X$, thus preserving
the actual number of modes but considerably changing the number
of major modes. 
In the Appendix we also present one alternative line of reasoning based
on a corruption process $C(\tilde{X}|X)$ that has finite local support,
thus completely removing the modes that are not in the neighborhood of $X$.
We argue that even with such a corruption process,
the stationary distribution $\pi$ will match the original $P(X)$,
so long as one can still visit all the regions of interest through
a sequence of such local jumps.

Two potential issues with Proposition \ref{prop:markov-chain-basic-gsn}
are that 1) we are learning distribution $P_\theta(X | \tilde{X})$ based on
experimental samples so it is only asymptotically minimizing the desired loss,
and 2) we may not have enough capacity in our model to estimate $P_{\theta}(X | \tilde{X})$
perfectly.

The issue is that, when running a Markov chain for infinitely long using a
slightly imperfect $P_\theta(X | \tilde{X})$, these small differences may
affect the stationary distribution $\pi$ and compound over time. We are
not allowed to ``adjust'' the $P_\theta(X | \tilde{X})$ as the chain runs.

This is addressed by Theorem \ref{thm:schweitzer_inequality} cited in
the later Section \ref{sec:from-DAE-to-GSN}. That theorem gives us a result about continuity,
so that, for ``well-behaved'' cases, when $P_\theta(X | \tilde{X})$ is close
to $P(X | \tilde{X})$ we must have that the resulting stationary distribution
$\pi$ is close to the
original $P(X)$.

% One way to present this basic technique is to say that we are interested
% in minimizing the expected negative log-likelihood
% \begin{equation}
% \label{eqn:loss_basic_DAE_one_step}
% \mathcal{L} = \int_{\tilde{X}} \int_X P(X)\mathcal{C}(\tilde{X}|X) \log P_\theta(X | \tilde{X}) dX d\tilde{X}
% \end{equation}
% and that we are using an empirical estimate
% \begin{equation}
% \label{eqn:loss_basic_DAE_one_step_empirical}
% \mathcal{L} \simeq \frac{1}{N} \sum_{i=1}^N \log P_\theta(X^{(i)} | \tilde{X}^{(i)})
% \end{equation}
% with samples ${(X^{(i)},\tilde{X}^{(i)})}$ drawn from $P(X)\mathcal{C}(\tilde{X}|X)$.

% In the next section we present the walkback algorithm for which a very similar
% loss function is used. The main difference is that
% we will draw the empirical samples ${(X^{(i)},\tilde{X}^{(i)})}$
% from iteratively sampling from $\mathcal{C}(\tilde{X}|X)$ and 
% $\log P_\theta(X | \tilde{X})$ many times over. But first, we
% present the basic result that will later be used to justify the walkback algorithm :

\svs{2}
\subsection{Walkback algorithm for training denoising auto-encoders}
\label{sec:walkback-description}
\svs{2}

In this section we describe the walkback algorithm which is
very similar to the method from Proposition 1, but helps training
to converge faster. It differs in the
training samples that are used, and the fact that the solution
is obtained through an iterative process. The parameter update
changes the corruption function, which changes the $\tilde{X}$ in
the training samples, which influences the next parameter update, and so on.

Sampling in high-dimensional spaces (like in experiments
in Section \ref{sec:walkback_experiment})
using a simple local corruption process (such as Gaussian or
salt-and-pepper noise) suggests that
if the corruption is too local, the DAE's behavior far
from the training examples can create spurious modes in the regions
insufficiently visited during training. More training iterations or 
increasing the amount of corruption noise
helps to substantially alleviate that problem, but we discovered an even
bigger boost by {\em training
the Markov chain to walk back towards the training examples}
(see Figure \ref{fig:walkback_into_drain}).
\begin{SCfigure}
\centering
\includegraphics[width=0.49\textwidth]{second_attempt_with_walkback_path.pdf}
\caption{
Walkback samples get attracted by spurious modes and contribute to removing them.
Segment of data manifold in violet and example walkback path in red dotted line,
starting on the manifold and going towards a spurious attractor.
The vector field represents expected moves of the chain,
for a unimodal $P(X| \tilde{X})$, with arrows from $\tilde{X}$ to $X$.
The name {\bf walkback} is because this procedure forces the model
to learn to {\em walk back from the random walk it generates,
towards the $X$'s in the training set}.
}
\label{fig:walkback_into_drain}
\vspace{-0.5cm}
\end{SCfigure}
We exploit
knowledge of the currently learned model $P_\theta(X|\tilde{X})$ to define the
corruption, so as to pick values of $\tilde{X}$ that would be
obtained by following the generative chain: wherever
the model would go if we sampled using the generative Markov chain starting
at a training example $X$,
we consider to be a kind of ``negative example'' $\tilde{X}$
from which the
auto-encoder should move away (and towards $X$).
The spirit of this procedure is thus very similar
to the CD-$k$ (Contrastive Divergence with $k$ MCMC steps) procedure
proposed to train RBMs~\citep{Hinton99-small,Hinton06}.

\vspace{1cm}

We start by defining the modified corruption process $\mathcal{C}_k(\tilde{X}|X)$
that samples $k$ times alternating between $\mathcal{C}(\tilde{X}|X)$ and the
current $P_\theta(X | \tilde{X})$.

We can express this recursively if we let $\mathcal{C}_1(\tilde{X}|X)$ be
our original $\mathcal{C}(\tilde{X}|X)$, and then define
\begin{equation}
\mathcal{C}_{k+1}(\tilde{X}|X) = \int_{\tilde{X'}} \int_{X'} \mathcal{C}(\tilde{X} | X') P_\theta( X' | \tilde{X}') \mathcal{C}_k(\tilde{X}'|X) dX' d\tilde{X}'
\end{equation} 
Note that this corruption distribution $\mathcal{C}_k(\tilde{X}|X)$ now involves
the distribution $P_\theta(X|\tilde{X})$ that we are learning. 


With the help of the above definition of $\mathcal{C}_k(\tilde{X}|X)$, we define
the walkback corruption process $\mathcal{C}_{\textrm{\textsc{wb}}}(\tilde{X}|X)$.
To sample from $\mathcal{C}_{\textrm{\textsc{wb}}}$, we first draw a $k$
distributed according to some distribution, e.g., a geometric distribution 
with parameter $p=0.5$ and support on $k\in\{1,2,\ldots\}$),
and then we sample according to the corresponding $\mathcal{C}_k(\tilde{X}|X)$.
Other values than $p=0.5$ could be used, but we just want something convenient
for that hyperparameter.
Conceptually, the corruption process
$\mathcal{C}_{\textrm{\textsc{wb}}}$ means that, from a starting point $X$
we apply iteratively the original $\mathcal{C}$ and $P_\theta$, and then we flip
a coin to determine if we want to do it again. We re-apply until we lose the coin flip,
and then this gives us a final value for the sample $\tilde{X}$ based on $X$.

The walkback loss is given by
\begin{equation}
\label{eqn:loss_walkback_empirical}
\mathcal{L}_{\textrm{\textsc{wb}}} \simeq \frac{1}{N} \sum_{i=1}^N \log P_\theta(X^{(i)} | \tilde{X}^{(i)})
\end{equation}
for samples ${(X^{(i)}, k^{(i)}, \tilde{X}^{(i)})}$ drawn from $X \sim P(X)$, $k \sim \textrm{Geometric(0.5)}$ and
$\tilde{X} \sim \mathcal{C}_k(\tilde{X}|X)$.
Minimizing this loss is an iterative process because the samples used in the empirical expression
depend on the parameter $\theta$ to be learned. This iterated minimization is what we
call the \emph{walkback algorithm}. Samples are generated with the current parameter value $\theta_t$,
and then the parameters are modified to reduce the loss and yield $\theta_{t+1}$. We repeat until
the process stabilizes. In practical applications, we do not have infinite-capacity models
and we do not have a guarantee that the walkback algorithm should converge to some $\theta^*$.

\subsubsection{Reparametrization Trick}
\label{sec:reparametrization}

Note that we do not need to analytically marginalize over the latent variables involved:
we can back-propagate through the chain, considering it like a recurrent neural network
with noise (the corruption) injected in it. This is an instance of 
the so-called reparametrization trick, already proposed in~\citep{Bengio-arxiv2013,Kingma-arxiv2013,Kingma+Welling-ICLR2014}.
The idea is that we can consider sampling from a random variable conditionally on others (such as $\tilde{X}$
given $X$) as equivalent to applying a deterministic function taking as argument the conditioning variables as
well as some i.i.d. noise sources.
This view is particularly useful for the more general
GSNs introduced later, in which we typically choose the latent variables to be continuous, i.e.,
allowing to backprop through their sampling steps when exploiting the reparametrization trick.


\subsubsection{Equivalence of the Walkback Procedure}

%The name comes from the idea that the samples generated
%walk away from $X$ for a few steps, and then we ask $P_\theta$ to learn how to walk back to
%that $X$ from the training set (see Figure \ref{fig:walkback_into_drain}).
With the walkback algorithm, one can also decide to include or not in the loss function
all the intermediate reconstruction distributions through which the trajectories pass. That is,
starting from some $X_0$, we sample
\begin{eqnarray*}
X_0 \sim P(X)                                     & \tilde{X_0} \sim \mathcal{C}(\tilde{X_0} | X_0), \\
X_1 \sim P_\theta( X_1 | \tilde{X}_0)             & \tilde{X_1} \sim \mathcal{C}(\tilde{X_1} | X_1) \\
X_2 \sim P_\theta( X_2 | \tilde{X}_1)             & \tilde{X_2} \sim \mathcal{C}(\tilde{X_2} | X_2) \\
\vdots                                            & \vdots \\
X_{k-1} \sim P_\theta( X_{k-1} | \tilde{X}_{k-2}) & \tilde{X}_{k-1} \sim \mathcal{C}(\tilde{X}_{k-1} | X_{k-1})
\end{eqnarray*}
and we use all the pairs $(X, \tilde{X_k})$ as training data for the
walkback loss at equation (\ref{eqn:loss_walkback_empirical}).


\iffalse
More precisely, the modified corruption process $\mathcal{C}_{\textrm{\textsc{wb}}}$ we
propose is the following, based on the original corruption process $\cal
C$. We use it in a version of the training algorithm called {\bf walkback},
where we replace the corruption process ${\cal C}$ of
Algorithm~\ref{alg:DAE} by the walkback process ${\cal \tilde{C}}$ of
Algorithm~\ref{alg:walkback}. This also provides extra training examples (taking
advantage of the $\tilde{X}$ samples generated along the walk away from
$X$). 

\begin{algorithm}[ht]
\caption{{\sc The walkback algorithm} \sl is based on the walkback corruption
  process ${\cal \tilde{C}}(\tilde{X}|X)$, defined below in terms of a generic
  original corruption process ${\cal C}(\tilde{X}|X)$ and the current
  model's reconstruction conditional distribution $P(X|\tilde{X})$.  For
  each training example $X$, it provides a sequence of additional training examples
  $(X,\tilde{X}^*)$ for the DAE. It has a hyper-parameter
  that is a geometric distribution parameter $0<p<1$ controlling the
  length of these walks away from $X$, with
  $p=0.5$ by default. Training by Algorithm~\ref{alg:DAE} is the same, 
  but using all $\tilde{X}^*$ in the returned list $L$
  to form the pairs $(X,\tilde{X}^*)$ as
  training examples instead of just $(X,\tilde{X})$.  }
\begin{algorithmic} \label{alg:walkback}
\STATE $X^* \leftarrow X$, $L \leftarrow [\;]$
\STATE Sample $\tilde{X}^* \sim {\cal C}(\tilde{X}|X^*)$
\STATE Sample $u \sim {\rm Uniform}(0,1)$
\IF{$u > p$} 
  \STATE Append $\tilde{X}^*$ to $L$ and {\bf return} $L$
\ENDIF
\STATE If during training, append $\tilde{X}^*$ to $L$, so  $(X,\tilde{X}^*)$ will be an additional training example.
\STATE Sample $X^* \sim P(X | \tilde{X}^*)$
\STATE {\bf goto} 2.
\end{algorithmic}
\vs{1}
\end{algorithm}
\fi

The following proposition looks very similar to Proposition \ref{prop:markov-chain-basic-gsn},
but it uses the walkback corruption instead of the original corruption ${\cal C}(\tilde{X}|X)$.
It is also an iterated process through which the current value of the parameter $\theta_t$
sets the loss function that will be minimized by the updated $\theta_{t+1}$.

\begin{proposition}
\label{prop:walkback}
Let $P(X)$ be the training distribution for which we only have empirical samples.
Let $\pi(X)$ be the implicitly defined asymptotic distribution of the Markov chain alternating
sampling from $P_\theta(X|\tilde{X})$ and ${\cal C}(\tilde{X}|X)$, where ${\cal C}$
is the original local corruption process.

If we assume that $P_\theta(X|\tilde{X})$ has sufficient capacity and
that the walkback algorithm converges (in terms of being stable in the updates
to $P_\theta(X|\tilde{X})$), then $\pi(x)=P(X)$.

That is, the Markov chain defined by alternating $P_\theta(X|\tilde{X})$ and ${\cal C}(\tilde{X}|X)$
gives us samples that are drawn from the same distribution as the training data.
\vs{1}
\end{proposition}
\vs{3}
\begin{proof}

Consider that during training, we produce a sequence of estimators
$P_{\theta_t}(X|\tilde{X})$ where $P_{\theta_t}$ corresponds to the $t$-th training iteration
(modifying the parameters after each iteration). With the walkback
algorithm, $P_{\theta_{t-1}}$ is used to obtain the corrupted samples $\tilde{X}$
from which the next model $P_{\theta_{t-1}}$ is produced.

If training converges in terms of ${\theta_t} \rightarrow \theta^*$, it means that
we have found a value of $P_{\theta^*}(X|\tilde{X})$ such that
\begin{equation*}
\theta^* = \textrm{argmin}_{\theta} \frac{1}{N} \sum_{i=1}^N \log P_\theta(X^{(i)} | \tilde{X}^{(i)})
\end{equation*}
for samples ${(X^{(i)}, \tilde{X}^{(i)})}$ drawn from $X \sim P(X)$,
$\tilde{X} \sim \mathcal{C}_{\textrm{\textsc{wb}}}(\tilde{X}|X)$.

By Proposition \ref{prop:markov-chain-basic-gsn}, we know that, regardless of the the corruption
$\mathcal{C}_{\textrm{\textsc{any}}}(\tilde{X}|X)$ used,
when we have a $P_{\theta}(X|\tilde{X})$ that minimizes optimally the loss
\begin{equation*}
\int_{\tilde{X}} \int_X P(X)\mathcal{C}_{\textrm{\textsc{any}}}(\tilde{X}|X) \log P_\theta(X | \tilde{X}) dX d\tilde{X}
\end{equation*}
then we can recover $P(X)$ by alternating between
$\mathcal{C}_{\textrm{\textsc{any}}}(\tilde{X}|X)$ and $P_\theta(X | \tilde{X})$.

Therefore, once the model is trained with walkback, the stationary distribution $\pi$
of the Markov chain that it creates has the same distribution $P(X)$ as the training data.

% In particular, running the Markov chain with the walkback corruption is exactly the same
% as running the chain with the original corruption $\mathcal{C}(\tilde{X}|X)$. We just happen
% to generally walk more steps when doing walkback, but the distribution of the $X$ is the same.

Hence if we alternate between the original corruption $\mathcal{C}(\tilde{X}|X)$
and the walkback solution $P_{\theta^*}( X | \tilde{X} )$, then the stationary distribution
with respect to $X$ is also $P(X)$.
\vs{3}
\end{proof}

Note that this proposition applies regardless of the value of geometric distribution used
to determine how many steps of corruption will be used. It applies whether we keep
all the samples along to the way, or only the one at the last step. It applies regardless
of if we use a geometric distribution to determine which $\mathcal{C}_k$ to select, or any
other type of distribution.

A consequence is that {\em the walkback training
algorithm estimates the same distribution as the original denoising algorithm}, but
may do it more efficiently (as we observe in the experiments),
by exploring the space of corruptions in a way that spends more time
where it most helps the model to kill off spurious modes.

The Markov chain that we get with walkback should also generally mix faster, be less
susceptible to getting stuck in bad modes, but it will require a $P_{\theta^*}( X | \tilde{X} )$
with more capacity than originally. This is because $P_{\theta^*}( X | \tilde{X} )$
is now less local, covering the values of the initial $X$ that could have given rise to the $\tilde{X}$
resulting from several steps of the Markov chain.

\svs{2}
\subsection{Walkbacks with individual scaling factors to handle uncertainty} \label{sec:swb}
The use of the proposed walkback training procedure is effective in suppressing the spurious modes 
in the learned data distribution. Although the convergence is guaranteed asymptotically, in practice, 
given limited model capacity and training data, it has been observed that 
the more walkbacks in training, the more difficult it is to 
maximize $P_{\theta}( X | \tilde{X} )$. 
This is simply because more and more noise is added in this procedure, 
resulting in $\tilde{X}$ that 
is further away from $X$, therefore a potentially more complicated reconstruction 
distribution. 

In other words, $P_{\theta}( X | \tilde{X} )$ needs to have the capacity to 
model increasingly 
complex reconstruction distributions. 
As a result of training, a simple, or usually unimodal $P_{\theta}( X | \tilde{X} )$ 
is most likely to 
learn a distribution with a larger 
uncertainty than the one learned without walkbacks in order to distribute some 
probability mass to the 
more complicated and multimodal distributions implied by the walkback training procedure. 
One possible 
solution to this problem is to use a multimodal reconstruction distribution such as in 
\citet{Sherjil-et-al-arxiv2014}, \cite{Larochelle+Murray-2011-small}, 
or~\citet{Dinh-et-al-arxiv2014}. We propose here another solution, which can be combined
with the above, that consists in allowing a different level of entropy for
different steps of the walkback.

\subsubsection{Scaling trick in binary $X$}\label{sec:swb_binary}
In the case of binary $X$, the most common choice of the 
reconstruction distribution is the factorized Multinoulli distribution where 
$P_{\theta}( X | \tilde{X} )=\prod_{i=1}^d P_{\theta}(X^i | \tilde{X})$ 
and $d$ is the dimensionality of $X$.
Each factor $P_{\theta}(X^i | \tilde{X})$ is modeled by a Bernoulli distribution 
that has its parameter $p_i=\sigmoid(f_i(\tilde{X}))$ where $f_i(\cdot)$ is a general nonlinear 
transformation realized by a neural network. We propose to use a different scaling factor 
$\alpha_k$ for different walkback steps, resulting in a new parameterization 
$p_i^k=\sigmoid(\alpha_k f_i(\tilde{X}))$ for the k-th walkback step, with $\alpha_k>0$
being learned.
$\alpha_k$ effectively scales the pre-activation of the sigmoid function according to 
the uncertainty or entropy associated with different walkback steps. 
Naturally, later reconstructions in the walkback sequence are less accurate because
more noise has been injected. Hence, given the $k_i$-th and $k_j$-th walkback steps 
that satisfy  $k_i < k_j$, the learning will tend to
result in $\alpha_{k_i} > \alpha_{k_j}$ because larger $\alpha_k$ correspond
to less entropy.

\subsubsection{Scaling trick in real-valued $X$}\label{sec:swb_real}
In the case of real-valued $X$, the most common choice of $P_{\theta}( X | \tilde{X} )$ 
is the factorized Gaussian. In particular, each factor $P_{\theta}(X^i | \tilde{X})$ is 
modeled by a Normal distribution with its parameters $\mu_i$ and $\sigma_i$. Using the 
same idea of learning separate scaling factors, we can parametrize it as 
$P_{\theta}(X^i | \tilde{X}) = \mathcal{N}(\mu_i, \alpha_k \sigma_i^2)$ for the $k$-th 
walkback step. $\alpha_k$ is positive and also learned. However, Given the $k_i$-th and 
$k_j$-th walkback steps 
that satisfy  $k_i < k_j$, the learning will 
result $\alpha_{k_i} < \alpha_{k_j}$, since in this case, larger $\alpha_k$ 
indicates larger entropy.

\subsubsection{Sampling with the learned scaling factors}
After learning the scaling factors $\alpha_k$ for $k$ different walkback steps, the 
sampling is straightforward. One noticeable difference is that we have learned $k$ 
Markov transition operators. Although, asymptotically all $k$ Markov chains 
generate the same distribution of $X$, in practice, they result in different distributions 
because of the different $\alpha_k$ learned. In fact, using $\alpha_1$ results the 
samples that are sharper and more faithful to the data distribution. We verify the 
effect of learning the scaling factor further in the experimental section.
    
\subsection{Extending the denoising auto-encoder to more general GSNs}
\label{sec:from-DAE-to-GSN}
\svs{2}

%Consistent Estimation of the Underlying Data-Generating Distribution through Reconstruction from Arbitrary Noisy Information}

The denoising auto-encoder Markov chain is defined by 
$\tilde{X}_t \sim C(\tilde{X}|X_t)$ and 
$X_{t+1} \sim P_\theta(X | \tilde{X}_t)$, where $X_t$ alone can
serve as the state of the chain. The GSN framework generalizes the DAE in
two ways:
\begin{enumerate}
\item the ``corruption'' function is not fixed anymore but a parametrized function
that can be learned and corresponds to a ``hidden'' state 
(so we write the output of this function $H$ rather than $\tilde{X}$); and
\item that intermediate variable $H$ is now considered part of the state of the Markov chain,
i.e., its value of $H_t$ at step $t$ of the chain
depends not just on the previous visible $X_{t-1}$ but also 
on the previous state $H_{t-1}$.
\end{enumerate}
For this purpose, we define the
Markov chain associated with a GSN in terms of a visible $X_t$ and a 
latent variable $H_t$ as state
variables, of the form
%\vspace{-0.4em}
\begin{eqnarray*}
   H_{t+1} &\sim& P_{\theta_1}(H | H_t, X_t) \\
   X_{t+1} &\sim& P_{\theta_2}(X | H_{t+1}).
\end{eqnarray*}
\begin{center}
\includegraphics[width=0.7\textwidth]{H0_plus_first_step.pdf} 
\end{center}
This definition makes denoising auto-encoders a special case of GSNs.
Note that, given that the distribution of $H_{t+1}$ may depend on a previous value of $H_t$,
we find ourselves with an extra $H_0$ variable added at the beginning of the chain.
This $H_0$ complicates things when it comes to training, but when
we are in a sampling regime we can simply wait a sufficient
number of steps to burn in.

\subsubsection{Main result about GSNs}
%\label{sec:presenting-the-GSN-theorem}

The next theoretical results give conditions for making the stationary distributions
of the above Markov chain match a target data-generating distribution. It basically
says that, in order to estimate the data-generating distribution $P(X_0)$, it is
enough to achieve two conditions.

The first condition is similar to the one we
obtain when minimizing denoising reconstruction error, i.e., we must
make sure that the reconstruction distribution $P(X_1 | H_1)$ approaches
the conditional distribution $P(X_0 | H_1)$, i.e., the $X_0$'s that could
have given rise to $H_1$.

The second condition is novel and regards the
initial state $H_0$ of the chain, which influences $H_1$. It says that
$P(H_0|X_0)$ must match $P(H_1|X_0)$. One way to achieve that is to initialize
$H_0$ associated with a training example $X_0$ with the previous value of $H_1$
that was sampled when example $X_0$ was processed. In the graphical model
in the statement of Theorem \ref{thm:noisy-reconstruction},
note how the arc relating $X_0$ and $H_0$ goes in the $X_0 \rightarrow H_0$ direction,
which is different from the way we would sample from the GSN (graphical
model above), where we have $H_0 \rightarrow X_0$. Indeed, during training,
$X_0$ is given, forcing it to have the data-generating distribution.

Note that Theorem \ref{thm:noisy-reconstruction} is there to provide us
with a guarantee about what happens when those two conditions are satisfied.
It is not originally meant to describe a training method.

In section \ref{sec:how-to-train-theorem3} we explain how to these conditions
could be approximately achieved.
%We encourage the reader to
%consider skipping over to that section if the details of
%Theorem \ref{thm:noisy-reconstruction} are getting in the way of understanding
%the proposed training approach. 

\hspace{1em}

\pagebreak[2]

\begin{theorem}
\label{thm:noisy-reconstruction}
Let ${(H_t,X_t)}_{t=0}^\infty$ be the Markov chain defined by the following graphical model.
\begin{center}
\includegraphics[width=0.7\textwidth]{H0_reversed_plus_first_step.pdf} 
\end{center}
If we assume that the chain has a stationary distribution $\pi_{H,X}$, and that for every value of $(x,h)$ we have that
\begin{itemize}
\item all the $P(X_t = x | H_t=h) = g(x|h)$ share the same density for $t \geq 1$
\item all the $P(H_{t+1} = h | H_t=h', X_t=x) = f(h| h', x)$ shared the same density for $t \geq 0$
\item $P(H_0=h|X_0=x)=P(H_1=h|X_0=x)$
\item $P(X_1=x|H_1=h)=P(X_0=x|H_1=h)$
\end{itemize}
then for every value of $(x,h)$ we get that
\begin{itemize}
\item $P(X_0 = x | H_0=h) = g(x|h)$ holds, which is something that was assumed only for $t\geq 1$
\item $P(X_t = x, H_t = h) = P(X_0 = x, H_0 = h)$ for all $t\geq 0$
\item the stationary distribution $\pi_{H,X}$ has a marginal distribution $\pi_X$ such that $\pi\left(x\right) = P\left(X_0=x\right)$.
\end{itemize}
Those conclusions show that our Markov chain has the property that its samples in $X$ are drawn from the same distribution as $X_0$.
\end{theorem}

\begin{proof}
The proof hinges on a few manipulations done with the first variables to show
that $P(X_t = x | H_t=h) = g(x|h)$, which is assumed for $t \geq 1$, also holds for $t=0$.

For all $h$ we have that
\begin{eqnarray*}
P(H_0=h) & = & \int P(H_0=h|X_0=x)P(X_0=x) dx \\
         & = & \int P(H_1=h|X_0=x)P(X_0=x) dx \hspace{1em} \textrm{(by hypothesis)} \\
         & = & P(H_1=h).
\end{eqnarray*}

The equality in distribution between $(X_1,H_1)$ and $(X_0,H_0)$ is obtained with
\begin{eqnarray*}
P(X_1=x, H_1=h) & = & P(X_1=x | H_1=h) P(H_1=h) \\
                & = & P(X_0=x | H_1=h) P(H_1=h) \hspace{1em} \textrm{(by hypothesis)} \\
                & = & P(X_0=x, H_1=h) \\
                & = & P(H_1=h | X_0=x)P(X_0=x) \\
                & = & P(H_0=h | X_0=x)P(X_0=x) \hspace{1em} \textrm{(by hypothesis)} \\
                & = & P(X_0=x, H_0=h).
\end{eqnarray*}

Then we can use this to conclude that
\begin{eqnarray*}
         & P(X_0 = x, H_0 = h) = P(X_1 = x, H_1 = h) \\
\implies & P(X_0 = x |H_0 = h) = P(X_1 = x | H_1 = h) = g(x|h)
\end{eqnarray*}
so, despite the arrow in the graphical model being turned the other way, we have that
the density of $P(X_0 = x |H_0 = h)$ is the same as for all other $P(X_t = x |H_t = h)$ with $t \geq 1$.

Now, since the distribution of $H_1$ is the same as the distribution of $H_0$, and
the transition probability $P(H_1=h|H_0=h')$ is entirely defined by the $(f,g)$ densities
which are found at every step for all $t\geq 0$, then we know that $(X_2, H_2)$ will have the
same distribution as $(X_1, H_1)$. To make this point more explicitly,
\begin{eqnarray*}
 P(H_1 = h | H_0 = h') & = & \int P(H_1 = h | H_0 = h', X_0 = x)  P(X_0 = x | H_0 = h') dx \\
                       & = & \int f(h | h', x)  g(x| h') dx \\
                       & = & \int P(H_2 = h | H_1 = h', X_1 = x)  P(X_1 = x | H_1 = h') dx \\
                       & = & P(H_2 = h | H_1 = h')
\end{eqnarray*}
This also holds for $P(H_3|H_2)$ and for all subsequent $P(H_{t+1}|H_t)$.
This relies on the crucial step where we demonstrate that $P(X_0 = x | H_0 = h) = g(x|h)$.
Once this was shown, then we know that we are using the
same transitions expressed in terms of $(f,g)$ at every step.

Since the distribution of $H_0$ was shown above to be the same as the distribution of $H_1$,
this forms a recursive argument that shows that all the $H_t$ are equal in distribution to $H_0$.
Because $g(x|h)$ describes every $P(X_t = x | H_t = h)$, we have that
all the joints $(X_t, H_t)$ are equal in distribution to $(X_0, H_0)$.

This implies that the stationary distribution $\pi_{X,H}$ is the same as that of $(X_0, H_0)$.
Their marginals with respect to $X$ are thus the same.
\end{proof}

Intuitively, the proof of Theorem \ref{thm:noisy-reconstruction} achieves its objective by
forcing all the $(H_t, X_t)$ pairs to share the same joint distribution, thus making
the marginal over $X_t$ as $t \rightarrow \infty$ (i.e. the stationary distribution of the chain $\pi$) 
be the same as $P(X_0)$, i.e., the data distribution. On the other hand, because it is
a Markov chain, its stationary distribution does not depend on the initial conditions,
making the model generate from an estimator of $P(X_0)$ for any initial condition.

To apply Theorem \ref{thm:noisy-reconstruction} in a context
where we use experimental data to learn a model, we would like
to have certain guarantees concerning the robustness of
the stationary density $\pi_X$. When a model lacks capacity, or
when it has seen only a finite number of training examples,
that model can be viewed as a perturbed version of the exact
quantities found in the statement of
Theorem \ref{thm:noisy-reconstruction}.

\subsubsection{A note about consistency}
\label{sec:a-note-about-consistency-from-Schweitzer}

A good overview of results from perturbation theory
discussing stationary distributions in finite state Markov chains
can be found in \citep{Cho2000comparisonperturbation}.
We reference here only one of those results.

\begin{theorem}
\label{thm:schweitzer_inequality}
Adapted from \citep{Schweitzer1968perturbation}

Let $K$ be the transition matrix of a finite state, irreducible,
homogeneous Markov chain. Let $\pi$ be its stationary distribution
vector so that $K\pi=\pi$. Let $A=I-K$ and \mbox{$Z=\left(A+C\right)^{-1}$}
where $C$ is the square matrix whose columns all contain $\pi$.
Then, if $\tilde{K}$ is any transition matrix (that also satisfies
the irreducible and homogeneous conditions) with stationary distribution
$\tilde{\pi}$, we have that
\[
\left\Vert \pi-\tilde{\pi}\right\Vert _{1}\leq\left\Vert Z\right\Vert _{\infty}\left\Vert K-\tilde{K}\right\Vert _{\infty}.
\label{eqn:schweitzer_inequality}
\]

\end{theorem}

This theorem covers the case of discrete data by showing how the stationary
distribution is not disturbed by a great amount when the transition
probabilities that we learn are close to their correct values. We are
talking here about the transition between steps of the chain
$(X_0, H_0), (X_1, H_1), \ldots, (X_t, H_t)$, which are defined
in Theorem \ref{thm:noisy-reconstruction} through the $(f,g)$ densities.

\subsubsection{Training criterion for GSNs}
\label{sec:how-to-train-theorem3}

So far we avoided discussing the training criterion for a GSN.
Various alternatives exist, but this analysis is for future work.
Right now Theorem \ref{thm:noisy-reconstruction} suggests the following
rules :
\begin{itemize}
\item Define $g(x|h) = P(X_1=x|H_1=h)$, i.e., the {\em decoder}, to be the estimator for \mbox{$P(X_0=x|H_1=h)$},
e.g. by training an estimator of this conditional distribution from the samples $(X_0, H_1)$,
with reconstruction likelihood, $\log P(X_1 = x_0 | H_1)$, as this
would asymptotically achieve the condition  $P(X_0 | H_1) = P(X_1 | H_1)$. To see that this 
is true, consider the following.

We sample $X_0$ from $P(X_0)$ (the data-generating distribution) and $H_1$ from $P(H_1|H_0,X_0)$. 
Refer to one of the next bullet points for an explanation
about how to get values for $H_0$ to be used when sampling
from $P(H_1 | H_0, X_0)$ here.
This creates a joint distribution over $(X_0, H_1)$
that has $P(X_0 | H_1)$ as a derived conditional. 
Then we train the parameters of a model $P_\theta(X_1|H_1)$ to maximize the log-likelihood
\begin{align}
    & \mathbb{E}_{x_0\sim P(X_0), h_1\sim P(H_1|x_0)} [  \log P_\theta(X_1=x_0 | h_1) ]  \nonumber \\
   =& \int_{x_0, h_1} P(x_0,h_1) \log P_\theta(X_1=x_0 | H_1=h_1) dx_0 dh_1 \nonumber\\
   =& \int_{h_1} P(h_1) \int_{x_0} P(X_0=x_0 | H_1 = h_1) \log P_\theta(X_1=x_0 | h_1) dx_0 dh_1 \nonumber\\
   =& -\mathbb{E}_{H_1}[ \textrm{KL}(P(X_0|H_1) || P_\theta(X_1|H_1)) ] + {\rm const.}
\end{align}  
where the constant does not depend on $\theta$, and thus the log-likelihood is maximized when 
\[
  P_\theta(X_1=x|H_1=h) = P(X_0=x|H_1=h).
\]

\item Pick the transition distribution $f(h | h', x)$ to be useful, i.e., training it towards the
same objective, i.e., sampling an $h'$ that makes it easy to reconstruct $x$. One can think 
of $f(h | h', x)$ as the {\em encoder}, except that it has a state which depends on its previous
value in the chain.
\item To approach the condition $P(H_0 | X_0) = P(H_1 | X_0)$, one interesting possibility is the following.
For each $X_0$ in the training set, iteratively sample $H_1 | (H_0, X_0)$ and 
substitute the value of $H_1$ as the updated value of $H_0$. Repeat until you have achieved a kind of ``burn in''.
Note that, after the training is completed, when we use the chain for sampling, the samples that we get from 
its stationary distribution do not depend on $H_0$. Another option is to store the value of $H_1$ that was
sampled for the particular training example $x_0$, and re-use it as the initial $H_0$ the next time that $x_0$
is presented during training. These techniques of substituting $H_1$ into $H_0$ are only
required during training. In our experiments, we actually found that a fixed $H_0=0$
worked as well, so we have used this simpler approach in the reported experiments.
\item The rest of the chain for $t \geq 1$ is defined in terms of $(f,g)$.
\end{itemize}

\iffalse
As much as we would like to simply learn $g$ from pairs $(H_0, X_0)$,
the problem is that the training samples $X_0^{(i)}$ are descendants
of the corresponding values of $H_0^{(i)}$ in the original
graphical model that describes the GSN. Those $H_0^{(i)}$ are hidden quantities
in GSN and we have to find a way to deal with them. Setting them all to be
some default value would not work because the relationship between $H_0$ and
$X_0$ would not be the same as the relationship later between $H_t$ and $X_t$
in the chain.
\fi

\svs{2}
\subsection{Random variable as deterministic function of noise}
\label{sec:deterministic-function-of-noise}
\svs{2}

%\vspace{2.0cm}

There several equivalent ways of
expressing a GSN. One of the interesting
formulations is to use deterministic functions
of random variables to express the densities $(f,g)$
used in Theorem \ref{thm:noisy-reconstruction}.
With that approach, we define $H_{t+1}=\otherf_{\theta_1}(X_t,Z_t,H_t)$
for some independent noise source $Z_t$,
and we insist that $X_t$ cannot be recovered
exactly from $H_{t+1}$, to avoid a situation in which the Markov
chain would not be ergodic. The advantage of that formulation
is that one can directly back-propagate the reconstruction
log-likelihood $\log P(X_1=x_0 | H_1=f(X_0,Z_0,H_0))$ into all the parameters
of $f$ and $g$, using the reparametrization trick discussed above
in Section~\ref{sec:reparametrization}.
This method is described in \mbox{\citep{Williams-1992}}.


%For the rest of this paper, we will use such
%a deterministic function $\otherf$ instead of having
%$f$ refer to a probability density function. We apologize if it
%causes any confusion.

In the setting described at the beginning of Section \ref{sec:gsn},
the function playing the role of the ``encoder'' was fixed for the purpose
of the theorem, and we showed that learning only the ``decoder'' part (but a
sufficiently expressive one) sufficed. In this
setting we are learning both, which can cause certain
broken behavior.

One problem would be if the created Markov
chain failed to converge to a stationary distribution.
Another such problem could be that the function $\otherf(X_t,Z_t,H_t)$
learned would try to ignore the noise $Z_t$, or not make the best use
out of it. In that case, the reconstruction distribution would
simply converge to a Dirac at the input $X$.  This is
the analogue of the constraint on auto-encoders that is needed to prevent
them from learning the identity function. Here, we must design the family
from which $f$ and $g$ are learned 
such that when the noise $Z$ is injected, there are always
several possible values of $X$ that could have been the correct original
input.

Another extreme case to think about is when $\otherf(X,Z,H)$ is overwhelmed
by the noise and has lost all information about $X$. In that case the theorems
are still applicable while giving uninteresting results: the learner must
capture the full distribution of $X$ in $P_{\theta_2}(X|H)$ because
the latter is now equivalent to $P_{\theta_2}(X)$, since $\otherf(X,Z,H)$
no longer contains information about $X$. This illustrates
that when the noise is large, the reconstruction distribution (parametrized
by $\theta_2$) will need to have the expressive power to represent
multiple modes. Otherwise, the reconstruction will tend to capture 
an average output, which would visually look like a fuzzy combination
of actual modes. In the experiments performed here, we have only
considered unimodal reconstruction distributions (with factorized 
outputs), because we expect that even if ${\calP}(X|H)$
is not unimodal, it would be dominated by a single mode when the
noise level is small. However, 
future work should investigate multimodal alternatives.

A related element to keep in mind is that one should pick the
family of conditional distributions $P_{\theta_2}(X|H)$ so that
one can sample from them and one can easily train them when given
$(X,H)$ pairs, e.g., by maximum likelihood.



%- a better motivating story for the GSN training criterion in its own right
%- extending f to include a latent state variable, so as the make the Markov Chain have state (x,h) and not just x
%- showing an example of this that emulates the general structure of the DBM, and can be trained without layer wise pre training
%- showing that this training criterion can be used to train with missing inputs and handle structured output

%Also, regarding the motivations, I do believe that the story about many important modes is important.

%
%My intuition of the GSN vs DAE advantage:
%- it more naturally allows recurrent computation to compute high-level representations (shared by several consecutive x's in the chain)
%- it allows to mix at the higher levels of representation (this is something we observed empirically), making everything work better (less spurious modes because the space is better visited by the walkback algorithm, faster mixing between modes)









%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\section*{XXXXXXXXXXXXXXXXX}

%\section{--- OLD Introduction ---}

%Non-linearities different in
%shape from the traditional sigmoid and hyperbolic tangent were used in
%several of these new results with
%dropout~\citep{Krizhevsky-2012-small,Goodfellow+al-ICML2013-small,Zeiler+et+al-ICLR2013-small}.

%Since the amount of unlabeled data
%potentially available is very large, it would be very interesting to
%develop unsupervised learning algorithms for generative deep architectures
%that can take advantage of the progress in techniques for training deep
%supervised ones, i.e., based on back-propagated gradients.  The approach
%presented here is one step in this direction, allowing to train jointly
%train all the levels of representation of a deep unsupervised probabilistic
%model solely by back-propagating gradients.




%\section{--- OLD A Potential Problem with Anonymous Latent Variable Models ---}
%\label{sec:problem}



%\section{--- OLD Denoising Auto-Encoders as Generative Models ---}
%label{sec:dae}

%...moved...

%\section{--- OLD Reconstruction with Noise Injected in the Reconstruction Function: Consistent Estimation of the Underlying Data-Generating Distribution ---}

%...moved...



\svs{2}
\subsection{Handling missing inputs or structured output}
\label{sec:missing_inputs}
\svs{2}

In general, a simple way to deal with missing inputs is to clamp the
observed inputs and then run the Markov chain with the constraint
that the observed inputs are fixed and not resampled at each time
step, whereas the unobserved inputs are resampled each time,
\emph{conditioned on the clamped inputs}.

In the context of the GSN described in Section \ref{sec:from-DAE-to-GSN}
using the two distributions
\begin{eqnarray*}
H_{t+1} & \sim & P_{\theta_{1}}(H|H_{t},X_{t})\\
X_{t+1} & \sim & P_{\theta_{2}}(X|H_{t+1})
\end{eqnarray*}
we need to make some adjustments to $P_{\theta_{2}}(X|H_{t+1})$ to
be able to sample $X$ conditioned on some of its components being
clamped. We also focus on the case where there are no connections
between the $H_t \rightarrow H_{t+1}$. That is, we study the
more basic situation where we train an denoising auto-encoder
instead of a GSN that has connections between the hidden units.

Let $\mathcal{S}$ be a set of values that $X$ can take. For example, $\mathcal{S}$
can be a subset of the units of $X$ that are fixed to given values.
We can talk about clamping $X\in \mathcal{S}$, or just ``clamping $\mathcal{S}$''
when the meaning is clear. In order to sample from a distribution
with clamped $\mathcal{S}$, we need to be able to sample from
\begin{eqnarray*}
H_{t+1} & \sim & P_{\theta_{1}}(H|X_{t})\\
X_{t+1} & \sim & P_{\theta_{2}}(X|H_{t+1},X\in S).
\end{eqnarray*}
This notation might be strange at first, but it's as legitimate as
conditioning on $0<X$ when sampling from any general distribution.
It involves only a renormalization of the resulting distribution
$P_{\theta_{2}}(X|H_{t+1},X\in \mathcal{S})$.

In a general scenario with two conditional distributions
$(P_{\theta_{1}},P_{\theta_{2}})$
playing the roles of $f(x|h)$ and $g(h|x)$, i.e. the encoder and
decoder, we can make certain basic assumptions so that the asymptotic
distributions of $(X_{t},H_{t})$ and $(X_{t},H_{t+1})$ both exist.
There is no reason to think that those two distributions are the same,
and it is trivial to construct counter-examples where they differ
greatly.

However, when we train a DAE with infinite capacity,
Proposition \ref{prop:markov-chain-basic-gsn} shows
that the optimal solution leads to those two joints being the same.
That is, the two trained conditional distributions $f(h|x)$ and $g(x|h)$
are \emph{mutually compatible}. They form a single joint
distribution over $(X,H)$. We can sample from it by the usual Gibbs
sampling procedure. Moreover, the marginal distribution over $X$
that we obtain will match that of the training data. This is the motivation
for Proposition \ref{prop:markov-chain-basic-gsn}.

Knowing that Gibbs sampling produces the desired joint distribution
over $(X,H)$, we can now see how it would be possible to sample from
$(X,H)|(X\in \mathcal{S})$ if we are able to sample from $f(h|x)$ and $g(x|h,x\in \mathcal{S})$.
Note that it might be very hard to sample from $g(x|h,x\in \mathcal{S})$, depending
on the particular model used. We are not making any assumption on
the factorization of $g(x|h)$, much like we are not making any assumption
on the particular representation (or implementation) of $g(x|h)$.

In section \ref{sec:a-note-about-consistency-from-Schweitzer}
we address a valid concern about the possibility
that, in a practical setting, we might not train $g(x|h)$ to achieve
an exact match the density of $X|H$. That $g(x|h)$ may be very close
to the optimum, but it might not be able to achieve it due to its
finite capacity or its particular parametrization. What does that
imply about whether the asymptotic distribution of the Markov chain
obtained experimentally compared to the exact joint $(X,H)$ ?

We deal with this issue in the same way as we dealt with it
when it arose in the context of Theorem \ref{thm:noisy-reconstruction}.
The best that we can do
is to refer to Theorem \ref{thm:schweitzer_inequality} and rely
on an argument made in the context of discrete states that would
closely approximate our situation (which is in either discrete or
continuous space).

Our Markov chain is homogeneous because it does not change with time.
It can be made irreducible my imposing very light constraints on
$f(h|x)$ so that $f(h|x)>0$ for all $(x,h)$. This happens automatically
when we take $f(h|x)$ to be additive Gaussian noise (with fixed parameters)
and we train only $g(x|h)$. In that case, the optimum $g(x|h)$ will
assign non-zero probability weight on all the values of $x$.

We cannot guarantee that a non-optimal $g(x|h)$ will not be broken
in some way, but we can often get $g(x|h)$ to be non-zero by selecting
a parametrized model that cannot assign a probability of exactly zero to an $x$.
Finally, to use Theorem \ref{thm:schweitzer_inequality} we need to
have that the constant $\left\Vert Z\right\Vert _{\infty}$ from that
Theorem \ref{thm:schweitzer_inequality} to be non-zero.
This is a bit more complicated to enforce, but it is something
that we will get if the transition matrix stays away from the identity matrix.
That constant is zero when the chain is close to being degenerate.

Theorem \ref{thm:schweitzer_inequality} says that,
with those conditions verified, we have
that an arbitrarily good $g(x|h)$ will lead to an arbitrarily good
approximation of the exact joint $(X,H)$.

Now that we know that this approach is grounded in sound theory, it
is certainly reasonable to try it in experimental settings in which
we are not satisfying all the requirements, and see if the results
are useful or not. We would refer the reader to our experiment
shown in Figure \ref{fig:samples+inpainting_small}
where we clamp certain units and resample the rest.

To further understand the conditions for obtaining the appropriate
conditional distributions on some of the visible inputs when others
are clamped, we consider below sufficient and necessary conditions
for making the stationary distribution of the clamped chain
correspond to the normalized distribution (over the allowed values)
of the unclamped chain.

\begin{proposition}
\label{prop:clamping}

Let $f(h|x)$ and $g(x|h)$ be the encoder and decoder functions
such that they are mutually compatible (i.e. they represent a
single joint distribution for which we can sample using Gibbs sampling).
Let $\pi(X,H)$ denote that joint.

Note that this happens when we minimize
\[
\mathbb{E}_{X} \left[ \log \int g(x|h)f(h|x) dh \right]
\]
or when we minimize the walkback loss (see Proposition \ref{prop:walkback}).

Let $\mathcal{S} \subseteq \mathcal{X}$ be a set of values that $X$ can take
(e.g. some components of $X$ can be assigned certain fixed values),
and such that $\mathbb{P}(X\in\mathcal{S})>0$.
Let $\pi(x | x \in \mathcal{S})$ denote the conditional distribution of $\pi(X,H)$
on which we marginalize over $H$ and condition on $X \in \mathcal{S}$. That is,
\[
\pi(x | x \in \mathcal{S}) \propto \pi(x) \mathbb{I}(x \in \mathcal{S}) = 
\frac{\int_h \pi(x,h) \mathbb{I}(x \in \mathcal{S}) dh}
{\int_x \int_h \pi(x,h) \mathbb{I}(x \in \mathcal{S}) dh dx}.
\]

Let $g(x|h,x \in \mathcal{S})$ denote a restriction of the decoder function
that puts probability weight only on the values of $x\in\mathcal{S}$. That is,
\[
g(x|h,x \in \mathcal{S}) \propto g(x|h) \mathbb{I}(x \in \mathcal{S}).
\]

\textbf{If} we start from some $x_0 \in \mathcal{S}$ and we run a Markov chain
by alternating between $f(h|x)$ and $g(x|h,x \in\mathcal{S})$,
\textbf{then} the asymptotic distribution of that chain with respect to $X$
will be the same as $\pi(x | x \in \mathcal{S})$.

\end{proposition}

\svs{2}
\subsection{General conditions for clamping inputs}
\label{sec:missing_inputs_condition}
\svs{2}

In the previous section we gave a \emph{sufficient} condition for ``clamping $\mathcal{S}$''
to work in the context of a Markov chain based on an encoder
distribution with density $f(h|x)$ and a decoder distribution with density $g(x|h)$.

In this section, we will give a \emph{sufficient} and \emph{necessary} condition
on the sufficient and necessary conditions for handling
missing inputs by clamping observed inputs.
\begin{proposition}
\label{prop:clamp_sufficient}
Assume we have an \textit{ergodic} Markov chain with transition operators
having density $f(h | x)$ and $g(x | h)$.
Its unique stationary distribution is $\pi(x, h)$ over $\cal{X}\times\cal{H}$ which satisfies:
\begin{equation}
\nonumber \int_{\cal{X}\times\cal{H}}\pi(x, h)
				f(h' | x)g(x' | h') dxdh= \pi(x', h').
\end{equation}
Assume that we start from $(X_0, H_0) = (x_0, h_0)$ where $x_0 \in \cal{S}$,
$\cal{S} \subseteq \cal{X}$ ($\cal{S}$ can be considered as a constraint over $X$)
and we sample $(X_{t+1}, H_{t+1})$ by first sampling $H_{t+1}$ with 
encoder $f(H_{t+1}|X_{t})$ and then sampling $X_{t+1}$ with decoder $g(X_{t+1}|H_{t+1}, X_{t+1}\in \cal{S})$,
the new stationary distribution we reach is $\pi_{\cal{S}}(x, h)$.

Then a sufficient condition for
\[
\pi_{\cal{S}}(x) = \pi(x|x \in S)
\]
is for $\pi(x|x \in S)$ to satisfy
\begin{equation}
\label{th3_cond}
\int_{\cal{S}} \pi(x | x \in {\cal{S}})f(h'|x)dx = \pi(h'|x\in\cal{S})
\end{equation}

where $\pi(x | x \in {\cal{S}})$ and $\pi(h'|x\in\cal{S})$ are conditional distributions 
\begin{equation}
\nonumber \pi(x | x \in {\cal{S}}) =\frac{\pi(x)}{\int_{\cal{S}}\pi(x')dx'},\quad
\pi(h' | x \in {\cal{S}}) =\frac{\int_{\cal{S}}\pi(x, h')dx}{\int_{\cal{S}\times{\cal{H}}}\pi(x, h)dxdh}.
\end{equation}

\end{proposition}
\begin{proof}
Based on the assumption that the chain is ergodic, we have that
$\pi_{\cal{S}}(X, H)$ 
is the unique distribution satisfying
\begin{equation}
\label{th3}
\int_{\cal{S}\times\cal{H}}\pi_{\cal{S}}(x, h)f(h'|x) g(x'|h', x'\in{\cal{S}})dxdh = \pi_{\cal{S}}(x', h').
\end{equation}
Now let us check if $\pi(x, h|x \in \cal{S})$ satisfies the equation above.

The Markov chain described in the statement of the Theorem
is defined by looking at the slices $(X_t, H_t)$.
This means that, by construction, the conditional density $\pi(x|h)$
is just given by $g(x|h)$.

This relation still holds even if we put the $\cal{S}$ constraint on $x$ 
\begin{equation}
\nonumber g(x'|h', x'\in{\cal{S}}) = \pi(x'|h', x'\in\cal{S}).
\end{equation}
Now if we substitute $\pi_{\cal{S}}(x, h)$ by $\pi(x, h|x \in \cal{S})$ in Equation \ref{th3},
the left side of Equation \ref{th3} becomes 
\begin{eqnarray}
\nonumber 
&&\int_{\cal{S}\times {\cal{H}}} \pi(x, h|x \in {\cal{S}}) f(h'|x) \pi(x'|h', x'\in{\cal{S}})dxdh\\
\nonumber
&=&\pi(x'|h', x'\in{\cal{S}} )\int_{\cal{S}}(\int_{\cal{H}} \pi(x, h|x \in {\cal{S}})dh) f(h'|x)dx\\
\nonumber
&=&\pi(x'|h', x'\in{\cal{S}} )\int_{\cal{S}}\pi( x|x \in {\cal{S}}) f(h'|x)dx\\
\nonumber
&=&\pi(x'|h', x'\in{\cal{S}} )\pi(h'| x \in {\cal{S}}) \hspace{2em} \textrm{(using Equation \ref{th3_cond})}  \\
\nonumber
&=&\pi(x'|h', x'\in{\cal{S}} )\pi(h'| x' \in {\cal{S}})\\
\nonumber &=&\pi(x',h'| x'\in{\cal{S}} ).
\end{eqnarray}
This shows that $\pi(x, h|x\in{\cal{S}})$ satisfies Equation \ref{th3}.
Due to the ergodicity of the chain, the distribution $\pi_{\cal{S}}(x,h)$
that satisfies Equation \ref{th3} is unique,
so we have $\pi_{\cal{S}}(x,h) =  \pi(x, h|x\in{\cal{S}})$.
By marginalizing over $h$ we get
\begin{equation}
\nonumber \pi_{\cal{S}}(x) =  \pi(x|x\in{\cal{S}}).
\end{equation}
\end{proof}
Proposition \ref{prop:clamp_sufficient} gives a sufficient condition
for dealing missing inputs by clamping observed inputs.
Note that this condition is weaker than the \textit{mutually compatible} condition
discussed in Section \ref{sec:missing_inputs}.
Furthermore, under certain circumstances, this sufficient condition becomes necessary,
and we have the following proposition :
\begin{proposition}
\label{prop:clamp_necessary}
Assume that the Markov chain in Proposition \ref{prop:clamp_sufficient} has finite discrete state space for both $X$ and $H$.
The condition in Equation \ref{th3_cond} in Proposition \ref{prop:clamp_sufficient} becomes a necessary condition when
all discrete conditional distributions $g(x|h, x \in \cal{S})$ are linear independent.
\end{proposition}
\begin{proof}
We follow the same notions in Proposition \ref{prop:clamp_sufficient} and now we have $\pi_{\cal{S}}(x) = \pi(x|x \in S)$.
Because $\pi_{\cal{S}}(x)$ is the marginal of the stationary distribution
reached by alternatively sampling with encoder $f(H|X)$ and decoder $g(X|H, X \in {\cal{S}})$, 
we have that $\pi(x|x \in {\cal{S}})$ satisfies
\begin{equation}
\nonumber \int_{\cal{S}}\pi(x| x\in {\cal{S}})(\int_{\cal{H}}f(h'|x) \pi(x'|h', x'\in{\cal{S}})dh')dx = \pi(x'|x'\in \cal{S})
\end{equation}
which is a direct conclusion from Equation \ref{th3} when considering the fact that
$\pi_{\cal{S}}(x) = \pi(x|x \in S)$ and $g(x'|h', x'\in{\cal{S}}) = \pi(x'|h', x'\in{\cal{S}})$.
If we re-arrange the integral of equation above, we get:
\begin{equation}
\label{th3_c1_2}
\int_{\cal{H}} \pi(x'|h', x'\in{\cal{S}})(\int_{\cal{S}}\pi(x| x\in {\cal{S}})f(h'|x)dx)dh' = \pi(x'|x'\in \cal{S}).
\end{equation}
Note that $\int_{\cal{S}}\pi(x|x\in{\cal{S}})f(h'|x)dx$ is the same as the left side
of Equation \ref{th3_cond} in Proposition \ref{prop:clamp_sufficient} and it can be seen as some function  $F(h')$ satisfying
$\int_{\cal{H}}F(h')dh' = 1$. Because we have considered a GSN over a finite discrete state space
${\cal{X}} = \{x_1, \cdots, x_N\}$ and ${\cal{H}} = \{h_1, \cdots, h_M\}$,
the integral in Equation \ref{th3_c1_2} becomes the linear matrix equation
\begin{equation}
\nonumber \mathbf{G}\cdot\mathbf{F}=\mathbf{P}_x,
\end{equation}
where $\mathbf{G}(i,j) = g(x'_i|h'_j, x'\in{\cal{S}}) = \pi(x'_i|h'_j, x'\in{\cal{S}})$, $\mathbf{F}(i) = F(h'_i)$
and $\mathbf{P}_x(i) =\pi(x_i'|x'\in{\cal{S}}) $.
In other word, $\mathbf{F}$ is a solution of the linear matrix equation
\begin{equation}
\nonumber \mathbf{G}\cdot\mathbf{Z}=\mathbf{P}_x.
\end{equation}
From the definition of $\mathbf{G}$ and $\mathbf{P}_x$, it is obvious that 
$\mathbf{P}_h$ is also a solution of this linear matrix equation, if $\mathbf{P}_h(i) = \pi(h_i'|x'\in{\cal{S}}) $.
Because all discrete conditional distributions $g(x|h, x\in \cal{S})$ are linear independent,
which means that all the column vectors of $\mathbf{G}$ are linear independent, then this
linear matrix equation has no more than one solution. Since $\mathbf{P}_h$ is the solution,
we have $\mathbf{F} = \mathbf{P}_h$, equivalently in integral form
\begin{equation}
\nonumber F(h') = \int_{\cal{S}} \pi(x | x \in {\cal{S}})f(h'|x)dx = \pi(h'|x\in\cal{S})
\end{equation}
which is the condition Equation \ref{th3_cond} in Proposition \ref{prop:clamp_sufficient}.

\end{proof}
Proposition \ref{prop:clamp_necessary} says that at least in discrete finite state space,
if the $g(x|h, x\in \cal{S})$ satisfies some reasonable condition like linear independence, then
along with Proposition \ref{prop:clamp_sufficient}, the condition in Equation \ref{th3_cond} is 
the necessary and sufficient condition for handling missing inputs by clamping the observed
part for at least one subset $\cal{S}$. If we want this result to hold
for any subset $\cal{S}$, we have the following proposition:
\begin{proposition}
\label{prop:clamp_any}
If the condition in Equation \ref{th3_cond} in Proposition \ref{prop:clamp_sufficient} holds for any subset of $\cal{S}$ that $\cal{S}\subseteq \cal{X}$,
then we have 
\begin{equation}
\nonumber f(h'|x) = \pi(h'|x)
\end{equation}
In other words, $f(h|x)$ and $g(x|h)$ are two conditional distributions 
marginalized from a single joint distribution $\pi(x, h)$.
\end{proposition}

\begin{proof}
Because $\cal{S}$ can be any subset of $\cal{X}$, of course that $\cal{S}$
can be a set which only has one element $x_0$, i.e., ${\cal{S}} = \{x_0\}$.
Now the condition in Equation \ref{th3_cond} in Proposition \ref{prop:clamp_sufficient} becomes 
\begin{equation}
\nonumber 1 \cdot f(h'|x=x_0) = \pi(h'|x=x_0).
\end{equation}
Because $x_0$ can be an arbitrary element in $\cal{X}$, we have
\begin{equation}
\nonumber f(h'|x) = \pi(h'|x), \quad {\rm or} \quad f(h|x) = \pi(h|x). 
\end{equation}
Since from Proposition \ref{prop:clamp_sufficient} we already know
that $g(x|h)$ is $\pi(x|h)$, we have that
$f(h|x)$ and $g(x|h)$ are \textit{mutually compatible}, that is, 
they are two conditional distributions 
obtained by normalization from a single joint distribution $\pi(x, h)$.
\end{proof}
According to Proposition \ref{prop:clamp_any},
if condition in Equation \ref{th3_cond} holds for any subset $\cal{S}$,
then $f(h|x)$ and $g(x|h)$ must be \textit{mutually compatible} to the single joint
distribution $\pi(x,h)$. 

%\svs{2}
\subsection{Dependency Networks as GSNs}
\label{sec:dependency-nets}
\svs{2}
Dependency networks~ \mbox{\citep{HeckermanD2000}} are models in which one
estimates conditionals $P_i(x_i | x_{-i})$, where $x_{-i}$ denotes $x \setminus x_i$,
i.e., the set of variables other than the $i$-th one, $x_i$. Note that
each $P_i$ may be parametrized separately, thus not guaranteeing that
there exists a joint of which they are the conditionals. Instead of the
ordered pseudo-Gibbs sampler defined in~ \mbox{\citet{HeckermanD2000}}, which
resamples each variable $x_i$ in the order $x_1, x_2, \ldots$, we can view
dependency networks in the GSN framework by defining a proper Markov chain
in which at each step one randomly chooses which variable to resample. The
corruption process therefore just consists of $H=f(X,Z)=X_{-s}$ 
where $X_{-s}$ is the complement of $X_{s}$, with $s$
a randomly chosen subset of elements of $X$ (possibly constrained to be
of size 1).  Furthermore, we parametrize the reconstruction
distribution as $P_{\theta_2}(X=x|H) = \delta_{x_{-s}=X_{-s}}P_{\theta_2,s}(X_s=x_s | x_{-s})$ 
where the estimated conditionals
$P_{\theta_2,s}(X_s=x_s | x_{-s})$ are not constrained to be consistent
conditionals of some joint distribution over all of $X$.

\begin{proposition}
If the above GSN Markov chain has a stationary distribution, then
the dependency network defines a joint distribution (which is that
stationary distribution), which does not have to be known in closed
form. Furthermore, if the conditionals $P(X_s | X_{-s})$ are consistent estimators
of the ground truth conditionals, then that stationary distribution 
is a consistent estimator of the ground truth joint distribution.
\vs{1}
\end{proposition}
The proposition can be proven by immediate application of Proposition~\ref{prop:markov-chain-basic-gsn}
with the above particular GSN model definitions.

This joint stationary
distribution can exist even if the conditionals are not consistent.
To show that, assume that some choice of (possibly inconsistent)
conditionals gives rise to a stationary distribution $\pi$.
Now let us consider the set of all conditionals (not necessarily
consistent) that could have given rise to that $\pi$.
Clearly, the conditionals derived from $\pi$ by Bayes rule are part of that
set, but there are infinitely many others (a simple counting
argument shows that the fixed point equation of $\pi$ introduces
fewer constraints than the number of degrees of freedom that
define the conditionals). To better understand why the ordered
pseudo-Gibbs chain does not benefit from the same properties, we can consider an extended case
by adding an extra component of the state $X$, being
the index of the next variable to
resample. In that case, the Markov chain associated with the ordered pseudo-Gibbs procedure
would be periodic, thus violating the ergodicity assumption
of the theorem. However, by introducing randomness in the choice
of which variable(s) to resample next, we obtain aperiodicity
and ergodicity, yielding as stationary distribution
a mixture over all possible resampling orders. These results also
show in a novel way (see e.g. ~\citet{Hyvarinen-2006-small} for earlier results)
that training by pseudolikelihood or generalized pseudolikelihood
provides a consistent estimator of the associated joint, so long as the GSN
Markov chain defined above is ergodic. This result can
be applied to show that the multi-prediction deep Boltzmann machine (MP-DBM)
training procedure introduced by~\citet{Goodfellow-et-al-NIPS2013} also
corresponds to a GSN. This has been exploited in order to obtain
much better samples using the associated GSN Markov chain than
by sampling from the corresponding DBM~\citep{Goodfellow-et-al-NIPS2013}.
Another interesting conclusion that one can draw from that paper
and its GSN interpretation is that state-of-the-art classification
error can thereby be obtained: 0.91\% on MNIST without fine-tuning (best 
comparable previous DBM results was well above 1\%) and 10.6\% on permutation-invariant
NORB (best previous DBM results was 10.8\%). 


\svs{2}
\section{Experimental results}
\label{sec:experiment}
\svs{2}

The theoretical results on Generative Stochastic Networks (GSNs)
open for exploration a large class of possible
parametrizations and training procedures which share the property that they can capture the
underlying data distribution through the GSN Markov chain. What
parametrizations will work well?  Where and how should one inject noise to best balance fast mixing with making the implied conditional easy to model? We present
results of preliminary experiments with specific selections for each of these choices, but
the reader should keep in mind that the space of possibilities is vast.

We start in Section \ref{sec:walkback_experiment} with results involving
GSNs without latent variables (denoising auto-encoders in Section \ref{sec:DAE-model-prob-density} and
the walkback algorithm presented in Section \ref{sec:walkback-description}).
Then in Section \ref{sec:gsn_experiment} we proceed with experiments
related to GSNs with latent variables (model described in Section \ref{sec:from-DAE-to-GSN}). 
Section \ref{sec:swb_experiment} extends experiments of the walkback algorithm 
with the scaling factors discussed in 
Section \ref{sec:swb}. 
A Theano\footnote{http://deeplearning.net/software/theano/}
 \citep{bergstra+al:2010-scipy-short} 
implementation
is available\footnote{https://github.com/yaoli/GSN}, including the links of datasets.
\svs{2}
\subsection{Experimental results regarding walkback in DAEs}
\label{sec:walkback_experiment}
\svs{2}

We present here an experiment performed with a non-parametric estimator on two types of data
and an experiment done with a parametric neural network on the MNIST dataset.

{\bf Non-parametric case.}
The mathematical results presented here
apply to any denoising training criterion where the reconstruction
loss can be interpreted as a negative log-likelihood. This
remains true whether or not the denoising machine $P(X|\tilde{X})$
is parametrized as the composition of an encoder and decoder.
This is also true of the asymptotic estimation results
in~\citet{Alain+Bengio-ICLR2013}.
%
We experimentally validate the above theorems in a case where the asymptotic limit
(of enough data and enough capacity) can be reached, i.e., in a
low-dimensional non-parametric setting.
Fig.~\ref{fig:histogram} shows the distribution
recovered by the Markov chain for {\bf discrete data}
with only 10 different values.  The conditional
$P(X|\tilde{X})$ was estimated by multinomial models and maximum likelihood (counting) from 5000
training examples. 5000 samples were generated from the chain to
estimate the asymptotic distribution $\pi_n(X)$.  For 
{\bf continuous data}, Figure~\ref{fig:histogram} also
shows the result of 5000 generated samples and 500 original training examples
with $X \in \mathbb{R}^{10}$, with scatter plots of pairs of
dimensions. The estimator is also non-parametric (Parzen density estimator
of $P(X|\tilde{X})$). 
\begin{figure}[htb]
\centering
\vs{8}
%\hspace*{-2mm}
\includegraphics[width=0.40\textwidth]{toy_plot_for_paper_5k_examples.png}
\includegraphics[width=0.40\textwidth]{manifold_1.png}
\vs{2}
\includegraphics[width=0.40\textwidth]{manifold_3.png}
\includegraphics[width=0.40\textwidth]{manifold_5.png}
\vs{1}
\caption{\sl Top left: histogram of a data-generating distribution (true, blue),
the empirical distribution (red), and the estimated distribution using
a denoising maximum likelihood estimator. Other figures: pairs of variables
(out of 10) showing the training samples and the model-generated samples.}
\label{fig:histogram}
\vs{4}
\end{figure}

{\bf MNIST digits.}  We trained a DAE on the
binarized MNIST data (thresholding at 0.5). 
The 784-2000-784 auto-encoder is trained for 200 epochs with the 50000 training examples and salt-and-pepper noise
(probability 0.5 of corrupting each bit, setting it to 1 or 0 with
probability 0.5). It has 2000 tanh hidden units and is trained by minimizing cross-entropy loss,
i.e., maximum likelihood on a factorized Bernoulli reconstruction distribution.
With walkback training, a chain of 5
steps was used to generate 5 corrupted examples for each training
example. Figure~\ref{fig:MNIST} shows samples generated with and without walkback.
The quality of the samples was also estimated quantitatively by measuring
the log-likelihood of the test set under a non-parametric density
estimator $\hat{P}(x)={\rm mean}_{\tilde{X}} P(x|\tilde{X})$
constructed from 10,000 consecutively generated samples
($\tilde{X}$ from the Markov chain). The expected value of $\mathbb{E}[\hat{P}(x)]$
over the samples can be shown~\citep{Bengio+Yao-arxiv-2013} to be
a lower bound (i.e. conservative estimate) of the true (implicit) model density $P(x)$.
The test set log-likelihood bound
was not used to select among model architectures, but visual inspection of
samples generated did guide the preliminary search reported here.
Optimization hyper-parameters (learning rate, momentum, and
learning rate reduction schedule) were selected based on the 
training objective. We compare against a state-of-the-art RBM~\citep{NECO_cho_2013_enhanced}
with an AIS log-likelihood estimate of -64.1 (AIS estimates tend to be optimistic).
We also drew samples from the RBM and applied the same estimator (using the mean of the RBM's $P(x|h)$ with $h$
sampled from the Gibbs chain), and obtained a log-likelihood non-parametric bound of -233,
skipping 100 MCMC steps between samples (otherwise
numbers are very poor for the RBM, which mixes poorly).
The DAE log-likelihood bound
with and without walkback is respectively -116 and -142,
confirming visual inspection suggesting that
the walkback algorithm produces less spurious samples. However, the
RBM samples can be improved by a spatial blur. By tuning the amount of
blur (the spread of the Gaussian convolution), we obtained a bound of -112
for the RBM. Blurring did not help the auto-encoder. 

\begin{figure}[htb]
\centering
%\hspace*{-2mm}
\vs{2}
\hspace*{-1mm}\includegraphics[width=0.5\textwidth]{mnist_plain_8rows.png} \hspace*{-1mm} \includegraphics[width=0.5\textwidth]{mnist_walkback_8rows_better.png}
\vs{5}
\caption{\sl Successive samples generated by Markov chain associated with
the trained DAEs
according to the plain sampling scheme (left) and walkback sampling scheme (right).
There are less ``spurious'' samples with the walkback algorithm.}
\label{fig:MNIST}
\vs{2}
\end{figure}

\svs{2}
\subsection{Experimental results for GSNs with latent variables}
\label{sec:gsn_experiment}
\svs{2}

We propose here to explore families of
parametrizations which are similar to existing deep stochastic architectures
such as the Deep Boltzmann Machine (DBM)~\citep{Salakhutdinov+Hinton-2009-small}.
Basically, the
idea is to construct a computational graph that is similar to the
computational graph for Gibbs sampling or variational inference
in Deep Boltzmann Machines. However, we have
to diverge a bit from these architectures in order to accommodate the
desirable property that it will be possible to back-propagate
the gradient of reconstruction log-likelihood with respect to the
parameters $\theta_1$ and $\theta_2$. Since the gradient of a binary
stochastic unit is 0 almost everywhere, we have to consider related
alternatives. An interesting source of inspiration regarding this
question is a recent paper on estimating or propagating gradients
through stochastic neurons~\citep{Bengio-arxiv2013}.
Here we consider the following stochastic non-linearities:
$h_i=\eta_{\rm out}+\tanh(\eta_{\rm in} + a_i)$
where $a_i$ is the linear activation for unit $i$ (an affine transformation
applied to the input of the unit, coming from the layer below, the layer
above, or both) and $\eta_{\rm in}$ and $\eta_{\rm out}$ are 
zero-mean Gaussian noises.

To emulate a sampling procedure similar to Boltzmann machines 
in which the filled-in missing values can depend on the
representations at the top level, the computational graph allows
information to propagate both upwards (from input to higher levels)
and downwards, giving rise to the
computational graph structure illustrated in Figure~\ref{fig:comp-graph},
which is similar to that explored for {\em deterministic} recurrent
auto-encoders~\citep{SeungS1998,Behnke-2001,Savard-master-small}. Downward
weight matrices have been fixed to the transpose of corresponding
upward weight matrices.


\begin{figure*}[ht]
\vs{3}
\centering
%\hspace*{-2mm}
\begin{minipage}{0.35\textwidth}
\vspace*{-3mm}\hspace*{5mm}
\includegraphics[width=0.8\textwidth]{GSN.pdf} 
\end{minipage} % \hspace*{0.03\textwidth}%
\begin{minipage}{0.64\textwidth}
\includegraphics[width=0.99\textwidth]{comp_graph.pdf}
\end{minipage}
\vs{3}
\caption{{\em Left:} Generic GSN Markov chain with state variables
$X_t$ and $H_t$. {\em Right:} GSN Markov chain inspired by the
unfolded computational graph of the Deep Boltzmann
Machine Gibbs sampling process, but with backprop-able stochastic units
at each layer. The training example $X=x_0$ starts the chain. Either
odd or even layers are stochastically updated at each step. All $x_t$'s
are corrupted by salt-and-pepper noise before entering the graph
(lightning symbol). Each $x_t$ for $t>0$ is obtained by sampling
from the reconstruction distribution for that step, $P_{\theta_2}(X_t|H_t)$. The walkback
training objective is the sum over all steps of log-likelihoods of target
$X=x_0$ under the reconstruction distribution.
In the special case of a unimodal Gaussian reconstruction
distribution, maximizing the likelihood is equivalent to minimizing
reconstruction error; in general one trains to maximum
likelihood, not simply minimum reconstruction error.}
\label{fig:comp-graph}
\vs{3}
\end{figure*}

%[EDIT : I don't know what to do with the next three paragraphs. I'm not even sure where we're using walkback anymore, where we're using it. Now we obviously can't cite ~\citet{Bengio-et-al-NIPS2013} in here because it's included in this JMLR paper. I would just remove the paragraph, but I am really not sure about the whole message conveyed by all those experiments. Finally, I'm not sure if the reader will understand at all what we mean when we refer to Theorem \ref{thm:noisy-reconstruction} requiring something about $H_0$. We are kinda using this to confess that our experiments aren't really in line with the theory presented. I don't know if there is a better way to say this.]

With the {\em walkback} algorithm, a different reconstruction distribution
is obtained after each step of the short chain started at the training example $X$.
It means that the computational graph from $X$ to a reconstruction
probability at step $k$ actually involves generating intermediate samples as if we were
running the Markov chain starting at $X$. In the experiments, the
graph was unfolded so that $2 D$ sampled reconstructions would be produced,
where $D$ is the depth (number of hidden layers). The training loss is
the sum of the reconstruction negative log-likelihoods (of target $X$) 
over all $2 D$ reconstructions.

%\vs{1}
%\subsection{Experimental validation and results}
%\vs{1}

Experiments evaluating the ability of the GSN models to generate good samples
were performed on the MNIST dataset and the Toronto Face Database (TFD), following the setup 
in~\citet{Bengio-et-al-ICML2013}.

Theorem \ref{thm:noisy-reconstruction} requires $H_0$ to have the same distribution as $H_1$ (given $X_0$) during training,
and this may be achieved by initializing each training chain with
$H_0$ set to the previous value of $H_1$ when the same example $X_0$ was shown. However, 
it turned out that even with a dumb initialization of $H_0$, good results were obtained
in the experiments below. 

Networks with 2 and 3 hidden layers
were evaluated and compared to regular denoising auto-encoders. The latter has just 1
hidden layer and no state to state transition, i.e., the computational graph can be 
split into separate graphs
for each reconstruction step in the walkback algorithm. They all have $\tanh$
hidden units and pre- and post-activation Gaussian noise of standard
deviation 2, applied to all hidden layers except the first.
In addition, at each step in the
chain, the input (or the resampled $X_t$) is corrupted with salt-and-pepper
noise of 40\% (i.e., 40\% of the pixels are corrupted, and replaced with a
0 or a 1 with probability 0.5). Training is over 100 to 600 epochs at most, with
good results obtained after around 100 epochs, using stochastic gradient descent
(minibatch size of one example). Hidden layer sizes vary
between 1000 and 1500 depending on the experiments, and a learning rate of
0.25 and momentum of 0.5 were selected to approximately minimize the
reconstruction negative log-likelihood. The learning rate is reduced
multiplicatively by $0.99$ after each epoch.  Following~\citet{Breuleux+Bengio-2011},
the quality of the samples
was also estimated quantitatively by measuring the log-likelihood
of the test set under a Parzen density estimator constructed from
10,000 consecutively generated samples (using the real-valued mean-field reconstructions
as the training data for the Parzen density estimator). This can be
seen as a {\em lower bound on the true log-likelihood}, with the bound
converging to the true likelihood as we consider more samples and
appropriately set the smoothing parameter of the Parzen 
estimator.\footnote{However, in this paper, to be consistent with
the numbers given in \citet{Bengio-et-al-ICML2013} we used a Gaussian
Parzen density, which makes the numbers not comparable with the
AIS log-likelihood upper bounds for binarized images reported in other papers
for the same data.}

%The test set log-likelihood bound
%was not used to select among model architectures, but visual inspection of
%samples generated did guide the preliminary search reported here.
%Optimization hyper-parameters (learning rate, momentum, and
%learning rate reduction schedule) were selected based on the 
%training objective.

Results are summarized in Table~\ref{tab:LL}. As in Section~\ref{sec:walkback_experiment}, 
the test set Parzen log-likelihood bound
was not used to select among model architectures, but visual inspection of
generated samples guided this preliminary search.
Optimization hyper-parameters (learning rate, momentum, and
learning rate reduction schedule) were selected based on the 
reconstruction log-likelihood training objective. The Parzen log-likelihood bound obtained
with a two-layer model on MNIST is 214 ($\pm$ standard error of 1.1), while the log-likelihood
bound obtained by a single-layer model (regular denoising auto-encoder, DAE in
the table) is
substantially worse, at -152$\pm$2.2.

In comparison,~\citet{Bengio-et-al-ICML2013} report a log-likelihood bound of -244$\pm$54 for
RBMs and 138$\pm$2 for a 2-hidden layer DBN, using the same setup. We have
also evaluated a 3-hidden layer DBM~\citep{Salakhutdinov+Hinton-2009-small}, using
the weights provided by the author, and obtained a Parzen log-likelihood bound of 32$\pm$2.
See \url{http://www.utstat.toronto.edu/~rsalakhu/DBM.html} for details.


Interestingly, the GSN and the DBN-2 actually perform slightly better than
when using samples directly coming from the MNIST training set, perhaps because
the mean-field outputs we use are more ``prototypical'' samples.

Figure~\ref{fig:samples+inpainting} shows two runs of consecutive samples
from this trained model, illustrating that it mixes quite well (faster
than RBMs) and produces rather sharp digit images. The figure shows
that it can also stochastically complete missing values: the left half
of the image was initialized to random pixels and the right side was clamped
to an MNIST image. The Markov chain explores plausible variations of 
the completion according to the trained conditional distribution.

% A smaller set of experiments was also run on TFD, yielding a test set
% Parzen log-likelihood bound of 1890 $\pm 29$. The setup is exactly the same
% and was not tuned after the MNIST experiments. A DBN-2 yields a 
% Parzen log-likelihood bound of 1908 $\pm 66$, which is indistinguishable
% statistically, while an RBM yields 604 $\pm$ 15. A run of
% consecutive samples from the GSN-3 model are shown in Figure~\ref{fig:tfd-samples}.






% [EDIT : Need to set the counter here or not ?]
% \setcounter{figure}{5}     % start at Figure 6
\begin{figure}[htpb]
\vs{3}
\centering
%\hspace*{-2mm}
\includegraphics[width=0.5\linewidth]{more_samples_88_resampledx4.png} % \vspace*{-0.5mm} %

\vspace*{1mm}
\includegraphics[width=0.5\linewidth]{alot_of_samples_210_resampledx4.png}

\vspace*{1.5mm}
\includegraphics[width=0.5\linewidth]{inpainting1500-half_croppedhalf.png}
\vs{5}
\caption{Top: two runs of consecutive samples (one row after the other) generated
from 2-layer GSN model,
showing fast mixing between classes and nice sharp images. Note: only every fourth sample is shown.
Bottom: conditional Markov chain, with the right half of the image clamped to
one of the MNIST digit images and the left half successively resampled, illustrating
the power of the generative model to stochastically fill-in missing inputs.}
\label{fig:samples+inpainting_small}
\vs{5}
\end{figure}

\begin{figure*}[ht]

\centering
%\hspace*{-2mm}
\includegraphics[width=0.75\textwidth]{more_samples_88.png} % \vspace*{-0.5mm} %

\includegraphics[width=0.75\textwidth]{alot_of_samples_210.png}

\vspace*{1.5mm}
\includegraphics[width=.75\textwidth]{inpainting1500-half.png}
\caption{These are expanded plots of those in Figure~\ref{fig:samples+inpainting_small}.
{\em Top:} two runs of consecutive samples (one row after the other) generated
from a 2-layer GSN model,
showing that it mixes well between classes and produces nice sharp images. Figure~\ref{fig:samples+inpainting_small} contained only one in every four samples, whereas here every sample is shown.
{\em Bottom:} conditional Markov chain, with the right half of the image clamped to
one of the MNIST digit images and the left half successively resampled, illustrating
the power of the trained generative model to stochastically fill-in missing inputs.
Figure~\ref{fig:samples+inpainting_small} showed only 13 samples in each chain; here we show 26.}
\label{fig:samples+inpainting}
\end{figure*}


%%%%% \ifLong
\begin{figure*}[ht]
\centering
%\hspace*{-2mm}
\includegraphics[width=0.39\textwidth]{samples_epoch_10.png}  \vspace*{2mm} %
\includegraphics[width=0.39\textwidth]{samples_epoch_25.png}
\caption{Left: consecutive GSN samples obtained after 10 training epochs. Right: GSN
  samples obtained after 25 training epochs. This shows quick convergence to a model that samples well. The samples in
  Figure~\ref{fig:samples+inpainting} are obtained after 600 training
  epochs.}
\label{fig:early-samples}
\end{figure*}
%%%%% \fi



\begin{figure*}[ht]
\centering
%\hspace*{-2mm}
\includegraphics[width=1\textwidth]{tfd_samples__with_nearest.png}  \vspace*{2mm} %
\caption{Consecutive GSN samples from a model trained on the TFD dataset.  At the end of each row, we show the nearest example from the training set to the last sample on that row to illustrate that the distribution is not merely copying the training set.}
\label{fig:tfd-samples-no-sw}
\end{figure*}












%\section{--- OLD Experimental Validation of GSNs ---}

%\begin{figure}[!ht]

% \begin{figure}[htpb]
% \vs{3}
% \centering
% %\hspace*{-2mm}
% \includegraphics[width=0.5\linewidth]{tfd_samples_resampledx2__with_nearest.png}  \vspace*{2mm} %
% \vs{8}
% \caption{GSN samples from a model trained on the TFD dataset without learned scaling factors. Every second sample is shown; see supplemental material for every sample. At the end of each row, we show the nearest example from the training set to the last sample on that row, to illustrate that the distribution is not merely copying the training set.}
% \label{fig:tfd-samples_small}
% \vs{3}
% \end{figure}


\begin{table}[htpb]
\vs{3}
\caption{Test set log-likelihood lower bound (LL) obtained by a Parzen density estimator
constructed using 10,000 generated samples, for different generative models trained
on MNIST.
The LL is not directly comparable to AIS likelihood estimates
because we use a Gaussian mixture rather
than a Bernoulli mixture to compute the likelihood, but we can compare with
\citet{Rifai-icml2012,Bengio-et-al-ICML2013,Bengio-et-al-NIPS2013} (from which we took the last three columns).
A DBN-2 has 2 hidden layers, a CAE-1 has 1 hidden layer, and a CAE-2 has 2.
% and an DBM-3 has 3 hidden layers.
The DAE is basically a GSN-1, with no injection of noise inside the network.
\ifLongversion
The last column uses 10,000 MNIST training examples to train the Parzen density estimator.
\fi
}
\label{tab:LL}
%\vskip 0.15in
%\vs{3}
\begin{center}
\begin{small}
\begin{sc}
\ifLongversion
\begin{tabular}{lrrrrrr}
\else
\begin{tabular}{lrrrrr} % r}
\fi
\toprule
\ifLongversion
& GSN-2 & DAE & RBM & DBM-3 & DBN-2 & MNIST\\
Log-likelihood lower bound &  214 & -152 & -244 &  32 & 138 & 24\\
Standard error &  1.1 &  2.2 &   54 &  1.9&  2.0 & 1.6\\
\else
%& GSN-2 & DAE & RBM & DBM-3 & DBN-2 \\ % & MNIST\\
%\midrule
%LL &  214 & -152 & -244 &  32 & 138 \\ % & 24\\
%std.err. &  1.1 &  2.2 &   54 &  1.9&  2.0 \\ % & 1.6\\
& GSN-2 & DAE & DBN-2 & CAE-1 & CAE-2 \\ % & MNIST\\
\midrule
LL  &  214 & 144 & 138 & 68  & 121 \\ % & 24\\
std.err. &  1.1 & 1.6 &  2.0      & 2.9 & 1.6 \\ % & 1.6\\
\fi
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\svs{3}
\end{table}

\subsection{Experimental results for GSNs with the scaling factors for walkbacks}\label{sec:swb_experiment}
We present the experimental results regarding the discussion in Section \ref{sec:swb}. 
Experiments are done on both MNIST and TFD. For TFD, only the unsupervised part of the 
dataset is used, resulting 69,000 samples for train, 15,000 for validation, and 15,000 
for test. The training examples are normalized to have a mean 0 and a standard deviation 1. 

For MNIST the GSNs we used have 2 hidden layers with 1000 tanh units each. 
Salt-and-pepper noise is used to corrupt inputs. We have performed extensive hyperparameter 
search on both the input noise level between 0.3 and 0.7, and the hidden noise level 
between 0.5 and 2.0. The number of walkback steps is also randomly sampled between 2 and 6. 
All the experiments are done with learning the scaling factors, following the 
parameterization in Section \ref{sec:swb_binary}. Following previous experiments, the 
log-probability of the test set is estimated by the same Parzen density estimator on 
consecutive 10,000 samples generated from the trained model. The $\sigma$ parameter in the Parzen 
estimator is cross-validated on the validation set. The sampling is performed 
with $\alpha_1$, the learned scaling factor for the first walkback step. 
The best model achieves a log-likelihood LL=237.44 on 
MNIST test set, which can be compared with the best reported result LL=225 from 
\citet{goodfellow2014generative}.  

On TFD, we follow a similar procedure as in MNIST, but with larger model capacity 
(GSNs with 2000-2000 tanh units) and a 
wider hyperparameter range on the input noise level (between 0.1 and 0.7), 
the hidden noise level (between 0.5 and 5.0), and the number 
of walkback steps (between 2 and 6). For comparison, two types of models are trained, 
one with the scaling factor and one without. The evaluation metric is the same as the one 
used in MNIST experiments. We compute the Parzen density estimation on the first 10,000 
test set examples. The best model without learning the scaling factor results in $LL=1044$, and 
the best model with learning the scaling factor results in 1215 when the scaling factor 
from the first walkback step is used and 1189 when all the scaling factors are used together 
with their corresponding walkback steps. As two further comparisons, using 
the mean over training examples to train the Parzen density estimator results in
$LL=632$, and using the validation set examples to train the Parzen estimator obtains $LL=2029$ (this can be considered as 
an upper bound when the generated samples are almost perfect). 
Figure~\ref{fig:tfd-samples-yes-sw} shows the consecutive 
samples generated with the best model, compared with Figure~\ref{fig:tfd-samples-no-sw}
that is trained without the scaling factor. In addition, Figure~\ref{fig:swb_alpha} shows 
the learned scaling factor for both datasets that confirms the hypothesis on the 
effect of the scaling factors made in
Section~\ref{sec:swb}.
     
\begin{figure*}[ht]
\centering
%\hspace*{-2mm}
\includegraphics[width=0.6\textwidth]{plt_learned_scaling_factor.png}  \vspace*{2mm} %
\caption{Learned $\alpha_k$ values for each walkback step $k$.
Larger values of $\alpha_k$ correspond to \emph{greater} uncertainty for TFD (real-valued) and \emph{less} uncertainty for MNIST (binary), due to the differing methods of parameterization given in Section \ref{sec:swb_binary} and \ref{sec:swb_real}.
Thus, both learned factors reflect the fact that there is greater uncertainty after each consecutive walkback step.
}
\label{fig:swb_alpha}
\end{figure*}

\begin{figure*}[ht]
\centering
%\hspace*{-2mm}
\includegraphics[width=0.6\textwidth]{tfd_scaled_train_200samples.png}  \vspace*{2mm} %
\caption{Consecutive GSN samples from a model trained on the TFD dataset. The scaling 
factors are learned. The samples are generated by using the scaling factor from the 
first walkback step. Samples are sharper compared with Figure 
(\ref{fig:tfd-samples-no-sw}). This is also reflected by an improvement of 140 in 
Parzen-estimated log-likelihood.}
\label{fig:tfd-samples-yes-sw}
\end{figure*}

\section{Conclusion}
\svs{2}

We have introduced a new approach to training generative models, called
Generative Stochastic Networks (GSN), which includes generative denoising
auto-encoders as a special case (with no latent variable). It is an alternative to 
directly performing maximum likelihood on an explicit $P(X)$, with the objective of avoiding 
the intractable marginalizations and partition function that such direct likelihood methods often entail. The
training procedure is more similar to function approximation than to
unsupervised learning because the reconstruction distribution is simpler
than the data distribution, often unimodal (provably so in the limit of
very small noise).  This makes it possible to train unsupervised models that
capture the data-generating distribution simply using backprop and
gradient descent in a computational graph that includes noise injection.
The proposed theoretical results state that under mild conditions
(in particular that the noise injected in the networks prevents perfect reconstruction),
training a sufficient-capacity model to denoise and reconstruct
its observations (through a powerful family of reconstruction
distributions) suffices to capture the data-generating distribution through
a simple Markov chain. Another view is that we are training the
transition operator of a Markov chain whose stationary distribution
estimates the data distribution, which has the potential of corresponding
to an easier learning problem because the normalization constant for this
conditional distribution is generally dominated by fewer modes. These
theoretical results are extended to the case where the corruption is local
but still allows the chain to mix and to the case where some inputs are
missing or constrained (thus allowing to sample from a conditional
distribution on a subset of the observed variables or to learned structured
output models). The GSN framework is shown to lend to dependency networks a
valid estimator of the joint distribution of the observed variables even
when the learned conditionals are not consistent, also allowing to prove
in a new way the consistency of generalized pseudolikelihood training, associated with the
stationary distribution of a corresponding GSN (that randomly chooses a
subset of variables and then resamples it). Experiments have
been conducted to validate the theory, in the case where the GSN
architecture is a simple denoising auto-encoder and in the case
where the GSN emulates the Gibbs sampling process of a Deep Boltzmann
Machine. A quantitative evaluation of the samples
confirms that the training procedure works very well (in this case
allowing us to train a deep generative model without layerwise pretraining)
and can be used to perform conditional sampling of a subset of
variables given the rest. After early versions of this work
were published~\citep{Bengio+Laufer-arxiv-2013}, the GSN framework has been extended and
applied to classification problems in several different ways~\citep{Goodfellow-et-al-NIPS2013,Zhou+Troyanskaya-ICML2014,Zohrer+Pernkopf-NIPS2014-small}
yielding very interesting results. In addition to providing
a consistent generative interpretation to dependency networks, GSNs
have been used to provide one to Multi-Prediction Deep Boltzmann Machines~\citep{Goodfellow-et-al-NIPS2013}
and to provide a fast sampling algorithm for deep NADE~\citep{Yao-et-al-arXiv2014}.





\documentclass[english,twoside,11pt]{article}

\usepackage{jmlr2e}
\usepackage{amsmath}
\usepackage{url}
\usepackage{sidecap}
\usepackage{subfigure}

%\newif\ifsavetrees
%\savetreestrue
%%\savetreesfalse
%\ifsavetrees
%%\usepackage[]{savetrees} 
%\usepackage[normaltitle, normalsections, normalmargins, normalindent, normalbib]{savetrees} 
%\fi

% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure} 

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{caption}


% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2014} with
% \usepackage[nohyperref]{icml2014} above.
%\usepackage[draft]{hyperref}
\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\newcommand{\sigmoid}{\mbox{sigmoid}}

% JBY
% Custom packages / etc.
\usepackage[utf8]{inputenc}
\usepackage{booktabs}
%\usepackage{amsthm}
%\usepackage{amsmath}  % required if you use \overset

%\usepackage[tiny,compact]{titlesec}
%\titleformat{\section}{\small\bfseries}{\thesection}{2mm}{}
%\titleformat{\subsection}{\small\bfseries}{\thesection}{2mm}{}
%\titleformat{\subsubsection}{\small\bfseries}{\thesection}{2mm}{}

\usepackage{ownstyles}
%\newtheorem{proposition}{Proposition}
%\newtheorem{lemma}{Lemma}
%\newtheorem{corollary}{Corollary}
%\newtheorem{theorem}{Theorem}
%\newtheorem*{thmnonumber}{Theorem}

% Comment out those two indicators that were meant to
% squeeze as much content into the spacing for the ICML paper.
%\newcommand{\svs}[1]{\vspace*{-#1mm}}
%\newcommand{\vs}[1]{\vspace*{-#1mm}}
\newcommand{\svs}[1]{}
\newcommand{\vs}[1]{}


% We input some notation formatting to allow shared formatting between the main paper and the supplemental section
\input{notation}


% TODO : Get rid of those LongVersion things.
\newif\ifLongversion
%\Longversionfalse
\Longversiontrue

% added this because our paragraphs need more space to breathe,
% and because we are not limited by space in JMLR
\usepackage{parskip}
\setlength{\parindent}{0.4cm}

\usepackage{babel}

\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}

\begin{document}

\firstpageno{1}

\title{GSNs: Generative Stochastic Networks}

% TODO : Do we need to include $^*$\& Canadian Inst. for Advanced Research somewhere with Yoshua's name ?

\author{
\noindent  Guillaume Alain$^{*+}$, Yoshua Bengio$^{*+}$, Li Yao$^*$, 
 Jason Yosinski$^\dagger$, \'Eric Thibodeau-Laufer$^*$, Saizheng Zhang$^*$ and Pascal Vincent$^*$\\
\\
$^*$ Department of Computer Science and Operations Research\\
University of Montreal\\
Montreal, H3C 3J7, Quebec, Canada\\
\\
$^\dagger$ Department of Computer Science, Cornell University\\
}

\editor{  } % FIXTHIS }

\maketitle

\begin{abstract}
We introduce a novel training principle for generative probabilistic models that is an
alternative to maximum likelihood. The proposed Generative Stochastic
Networks (GSN) framework generalizes Denoising Auto-Encoders (DAE)
and is based on learning the transition operator of a
Markov chain whose stationary distribution estimates the data distribution.
The transition distribution is a conditional distribution that generally
involves a small move, so it has fewer dominant modes and is unimodal in the
limit of small moves. This simplifies the learning problem, making it less like
density estimation and more akin to supervised function approximation, with gradients that can be
obtained by backprop.
The theorems provided here provide a probabilistic interpretation for denoising autoencoders
and generalize them; seen in the context of this framework, auto-encoders that learn with injected noise
are a special case of GSNs and can be interpreted as generative models.
The theorems also provide an
interesting justification for dependency networks and generalized
pseudolikelihood and define an appropriate joint distribution and
sampling mechanism, even when the conditionals are not consistent. GSNs
can be used with missing inputs and can be used to sample subsets of
variables given the rest. Experiments validating these theoretical results
are conducted on both synthetic datasets and image datasets. The experiments employ a particular
architecture that mimics the Deep Boltzmann Machine Gibbs sampler but
that allows training to proceed with backprop through a recurrent neural
network with noise injected inside and without the need for layerwise
pretraining.
\end{abstract}

\blfootnote{$^+$The first two authors had an equally important contribution}

\input{body}

\subsubsection*{Acknowledgements}

The authors would like to acknowledge the stimulating discussions and
help from Vincent Dumoulin, Aaron Courville, Ian Goodfellow, and Hod Lipson,
as well as funding from NSERC, CIFAR (YB is a CIFAR Senior Fellow), NASA (JY is a Space Technology Research Fellow),
and the Canada Research Chairs and Compute Canada.

{\small
%\bibliography{strings,strings-shorter,ml,myrefs,aigaion-shorter}
\bibliography{strings,strings-shorter,ml,aigaion-shorter}
%\bibliographystyle{plain}
%\bibliographystyle{natbib}
}


\section{Appendix: Argument for consistency based on local noise}

\input{supplemental_body}

\end{document} 



% Guillaume added these two commands to change
% the notation of the paper in a way that would
% be easily changeable again if there is an argument
% about the desired notation.
\newcommand{\otherf}{\phi}
\newcommand{\calP}{P}


\newtheorem{innercustomthm}{Theorem}
\newenvironment{customthm}[1]
  {\renewcommand\theinnercustomthm{#1}\innercustomthm}
  {\endinnercustomthm}



\appendix

%\section{Argument for consistency based on local noise}
\label{sec:local-consistency}

This section presents one direction that we pursed initially
to demonstrate that we had certain consistency properties
in terms of recovering the correct stationary distribution
when using a finite training sample.
We discuss this issue when we cite Theorem \ref{thm:schweitzer_inequality}
from the literature in section \ref{sec:from-DAE-to-GSN} and
thought it would be a good idea to
include our previous approach in this Appendix.

\vspace{1em}

The main theorem in \citet{Bengio-et-al-NIPS2013} (stated in supplemental as Theorem S1) requires that the
Markov chain be ergodic. A set of conditions
guaranteeing ergodicity is given in the aforementioned paper, but
these conditions are restrictive in requiring that ${\cal C}(\tilde{X}|X)>0$ everywhere
that ${\calP}(X)>0$. The effect of these restrictions is that
$P_\theta(X|\tilde{X})$ must have the capacity to model every mode of
${\calP}(X)$, exactly the difficulty we were trying to avoid. We show
here how we may also achieve the
required ergodicity through other means, allowing us to choose a
${\cal C}(\tilde{X}|X)$ that only makes small jumps, which in turn
only requires $P_\theta(X|\tilde{X})$ to model a small part of the
space around each $\tilde{X}$.

%As in \citet{Bengio-et-al-NIPS2013},
%More formally, 
%Let ${\calP}(X)$ be the unknown data-generating distribution.
%Let ${\cal C}(\tilde{X}|X)$ be a corruption process that stochastically
%transforms an $X$ (such as sampled from ${\calP}$) 
%into a random variable $\tilde{X}$.
Let $P_{\theta_n}(X | \tilde{X})$
be a denoising auto-encoder that has been trained on $n$ training examples.
$P_{\theta_n}(X | \tilde{X})$
assigns a probability to $X$, given $\tilde{X}$,
when $\tilde{X} \sim {\cal C}(\tilde{X}|X)$.
%When $n$ training examples 
%are provided we obtain an estimator parametrized by $\theta_n$.
This estimator defines a Markov chain $T_n$ obtained by sampling
alternatively an $\tilde{X}$ from ${\cal C}(\tilde{X}|X)$ and
an $X$ from $P_\theta(X | \tilde{X})$. Let $\pi_n$ be the asymptotic
distribution of the chain defined by $T_n$, if it exists.
The following theorem is proven by~\citet{Bengio-et-al-NIPS2013}.
%\begin{theorem}
\begin{customthm}{S1}
\label{thm:consistency}
{\bf If}
$P_{\theta_n}(X | \tilde{X})$ is a consistent estimator of the true conditional
distribution ${\calP}(X | \tilde{X})$
{\bf and}
$T_n$ defines an 
ergodic Markov chain,
{\bf then}
as $n\rightarrow \infty$, the asymptotic distribution $\pi_n(X)$ of the generated
samples converges to the data-generating distribution ${\calP}(X)$. 
%\end{theorem}
\end{customthm}

In order for Theorem~\ref{thm:consistency} to apply, the chain must be ergodic. One set of conditions under which this occurs is given in the aforementioned paper. We slightly restate them here:

\begin{figure}[htpb]
\centering
\includegraphics[width=1\linewidth]{crop_modes_1d.pdf}
\caption{If ${\cal C}(\tilde{X}|X)$ is globally supported as required by Corollary~\ref{cor:ergonew} \citep{Bengio-et-al-NIPS2013}, then for $P_{\theta_n}(X|\tilde{X})$ to converge to ${\calP}(X|\tilde{X})$, it will eventually have to model all of the modes in ${\calP}(X)$, even though the modes are damped (see ``leaky modes'' on the left). However, if we guarantee ergodicity through other means, as in Corollary~\ref{cor:ergonew2}, we can choose a local ${\cal C}(\tilde{X}|X)$ and allow $P_{\theta_n}(X|\tilde{X})$ to model only the local structure of ${\calP}(X)$ (see right).}
\label{fig:modes_1d}
\end{figure}

\begin{corollary}
\label{cor:ergonew}
{\bf If} the support for both the data-generating distribution and
denoising model are contained in and non-zero in
a finite-volume region $V$ (i.e., $\forall \tilde{X}$, $\forall X\notin V,\; {\calP}(X)=0, P_\theta(X|\tilde{X})=0$ and $\forall \tilde{X}$, $\forall X\in V,\; {\calP}(X)>0, P_\theta(X|\tilde{X})>0,  {\cal C}(\tilde{X}|X)>0$)
{\bf and} these statements remain
true in the limit of $n\rightarrow \infty$, {\bf then}
the chain defined by $T_n$ will be ergodic.
\end{corollary}

If conditions in Corollary~\ref{cor:ergonew} apply, then the chain will be ergodic and Theorem~\ref{thm:consistency} will apply. However, these conditions are sufficient, not necessary, and in many cases they may be artificially restrictive. In particular, Corollary~\ref{cor:ergonew} 
defines a large region $V$ containing any possible $X$ allowed by the model and requires that we maintain the probability of jumping between any two points in a single move to be greater than 0.
While this generous condition helps us easily guarantee the ergodicity of the chain, it also has the unfortunate side effect of requiring that, in order for $P_{\theta_n}(X|\tilde{X})$ to converge to the conditional distribution ${\calP}(X|\tilde{X})$, it must have the capacity to model every mode of ${\calP}(X)$, exactly the difficulty we were trying to avoid. The left two plots in Figure~\ref{fig:modes_1d} show this difficulty: because ${\cal C}(\tilde{X}|X)>0$ everywhere in $V$, every mode of $P(X)$ will leak, perhaps attenuated, into $P(X|\tilde{X})$.

Fortunately, we may seek ergodicity through other means. The following
corollary allows 
us to choose a ${\cal C}(\tilde{X}|X)$ that only makes small jumps, which in turn only requires $P_\theta(X|\tilde{X})$ to model a small part of the space $V$ around each $\tilde{X}$.

Let $P_{\theta_n}(X | \tilde{X})$
be a denoising auto-encoder that has been trained on $n$ training examples
and ${\cal C}(\tilde{X}|X)$ be some corruption distribution.
$P_{\theta_n}(X | \tilde{X})$
assigns a probability to $X$, given $\tilde{X}$,
when $\tilde{X} \sim {\cal C}(\tilde{X}|X)$ and $X \sim {\cal P}(X)$.
Define a Markov chain $T_n$ by alternately sampling
an $\tilde{X}$ from ${\cal C}(\tilde{X}|X)$ and
an $X$ from $P_\theta(X | \tilde{X})$.

%\setcounter{corollary}{1}     % start at Corollary 2
\begin{corollary}
\label{cor:ergonew2}
{\bf If} the data-generating distribution is contained in and non-zero in
a finite-volume region $V$ (i.e., $\forall X\notin V,\; {\calP}(X)=0,$ and $\forall X\in V,\; {\calP}(X)>0$)
%
{\bf and} all pairs of points in $V$ can be connected by a finite-length path through $V$
%
{\bf and}
for some $\epsilon > 0$,
$\forall \tilde{X} \in V,\forall X\in V$ within $\epsilon$ of each other,
${\cal C}(\tilde{X}|X) > 0$ and $P_{\theta}(X|\tilde{X}) > 0$
%
{\bf and} these statements remain
true in the limit of $n\rightarrow \infty$,
%
{\bf then}
the chain defined by $T_n$ will be ergodic.
\end{corollary}

\begin{proof}
Consider any two points $X_a$ and $X_b$ in $V$.
By the assumptions of Corollary~\ref{cor:ergonew2}, there exists a finite length path between $X_a$ and $X_b$ through $V$. Pick one such finite length path $P$.
Chose a finite series of points $x = \{x_1, x_2, \ldots, x_k\}$ along $P$, with $x_1 = X_a$ and $x_k = X_b$ such that the distance between every pair of consecutive points $(x_i, x_{i+1})$ is less than $\epsilon$ as defined in Corollary~\ref{cor:ergonew2}.
%
Then the probability of sampling $\tilde{X} = x_{i+1}$ from ${\cal C}(\tilde{X}|x_i))$ will be positive, because ${\cal C}(\tilde{X}|X)) > 0$ for all $\tilde{X}$ within $\epsilon$ of $X$ by the assumptions of Corollary~\ref{cor:ergonew2}. Further, the probability of sampling $X = \tilde{X} = x_{i+1}$ from $P_{\theta}(X|\tilde{X})$ will be positive from the same assumption on $P$.
%
Thus the probability of jumping along the path from $x_i$ to $x_{i+1}$, $T_n(X_{t+1} = x_{i+1}|X_{t} = x_i)$, will be greater than zero for all jumps on the path.
Because there is a positive probability finite length path between all pairs of points in $V$, all states commute, and the chain is irreducible.
%
If we consider $X_a = X_b \in V$, by the same arguments $T_n(X_t = X_a|X_{t-1} = X_a) > 0$. Because there is a positive probability of remaining in the same state, the chain will be aperiodic.
%
Because the chain is irreducible and over a finite state space, it will be positive recurrent as well. Thus, the chain defined by $T_n$ is ergodic.
\end{proof}

Although this is a weaker condition that has the advantage of making
the denoising distribution even easier to model (probably having less
modes), we must be careful to choose the ball size $\epsilon$ large
enough to guarantee that one can jump often enough between the major modes of ${\calP}(X)$
when these are separated by zones of tiny probability. $\epsilon$ must be larger than half the 
largest distance one would have to travel across a desert of low probability
separating two nearby modes (which if not connected in this way would make
$V$ not anymore have a single connected component). Practically, there is
a trade-off between the difficulty of estimating ${\calP}(X|\tilde{X})$
and the ease of mixing between major modes separated by a very low density zone.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% ICML 2014 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use the following line _only_ if you're still using LaTeX 2.09.
%\documentstyle[icml2014,epsf,natbib]{article}
% If you rely on Latex2e packages, like most moden people use this:
\documentclass{article}

% use Times
\usepackage{times}
% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure} 

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2014} with
% \usepackage[nohyperref]{icml2014} above.
\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
%\usepackage{icml2014}
% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Proceedings of the...''
\usepackage[accepted]{icml2014}



% JBY
% Custom packages / etc.
\usepackage[utf8]{inputenc}
\usepackage{booktabs}
\usepackage{amsthm}
\usepackage{amsmath}  % required if you use \overset
\usepackage{xr} % to refer to external references from the main paper
\externaldocument{main}
\usepackage{ownstyles}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{theorem}{Theorem}
\newtheorem*{thmnonumber}{Theorem}

\newif\ifLongversion
\Longversionfalse

% We input some notation formatting to allow shared formatting between the main paper and the supplemental section
\input{notation}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Supplemental Material for: Deep Generative Stochastic Networks Trainable by Backprop}

\begin{document} 

\twocolumn[
\icmltitle{Supplemental Material for: Deep Generative Stochastic Networks Trainable by Backprop}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2014
% package.

% We input the author info so we can store the authors for the main and supplemental parts in the same location
\input{authors}

% You may provide any keywords that you 
% find helpful for describing your paper; these are used to populate 
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{deep learning, unsupervised learning, generative models, denoising autoencoder}

\vskip 0.3in
]



%\input{outline}

\input{supplemental_body}


%\bibliography{strings,strings-shorter,ml,aigaion-shorter,cultrefs}
%\bibliographystyle{icml2014}

{\small
\bibliography{strings,strings-shorter,ml,aigaion-shorter,cultrefs}
\bibliographystyle{icml2014}
}

\end{document} 



\documentclass{article} % For LaTeX2e
\usepackage{iclr2017_conference,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{enumitem}
\usepackage{defs}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{color}
\usepackage{multirow}
\usepackage{makecell}

\newcommand{\cmt}[1]{\textcolor{blue}{#1}}
\newcommand{\smla}[1]{{\scalebox{0.6}{{({$#1$})}}}}
\newcommand{\sml}[1]{{\scalebox{0.6}{{{$#1$}}}}}
\newcommand{\dotw}{{\dot{w}}}
\newcommand{\barw}{{\bar{w}}}
\newcommand{\dotx}{{\dot{x}}}
\newcommand{\eda}{{\end{align}}}


\title{Hierarchical Multiscale \\ Recurrent Neural Networks}


\author{Junyoung Chung, Sungjin Ahn \& Yoshua Bengio
\thanks{Yoshua Bengio is CIFAR Senior Fellow.} \\
D\'epartement d'informatique et de recherche op\'erationnelle\\
Universit\'e de Montr\'eal\\
\texttt{\{junyoung.chung,sungjin.ahn,yoshua.bengio\}@umontreal.ca} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
%\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version

\begin{document}


\maketitle

\begin{abstract}
Learning both hierarchical and temporal representation has been among the long-standing challenges of recurrent neural networks.
Multiscale recurrent neural networks have been considered as a promising approach to resolve this issue, 
yet there has been a lack of empirical evidence showing that this type of models can actually capture the 
temporal dependencies by discovering the latent hierarchical structure of the sequence. 
In this paper, we propose a novel multiscale approach, called the hierarchical multiscale recurrent neural network,
that can capture the latent hierarchical structure in the sequence by encoding the temporal dependencies 
with different timescales using a novel update mechanism. 
We show some evidence that the proposed model can discover underlying hierarchical structure in the sequences
without using explicit boundary information. 
We evaluate our proposed model on character-level language modelling and handwriting sequence generation.
\end{abstract}

\section{Introduction}
\label{sec:introduction}
One of the key principles of learning in deep neural networks as well as in the human brain 
is to obtain a hierarchical representation with increasing levels of abstraction~\citep{bengio2009learning,lecun2015deep,schmidhuber2015deep}. 
A stack of representation layers, learned from the data in a way to optimize the target task, 
make deep neural networks entertain advantages such as generalization to unseen examples~\citep{hoffman2013one},
sharing learned knowledge among multiple tasks,
and discovering disentangling factors of variation~\citep{kingma2013auto}. 
The remarkable recent successes of the deep convolutional neural networks are particularly based on this ability to 
learn hierarchical representation for spatial data~\citep{krizhevsky2012imagenet}. 
For modelling temporal data, the recent resurgence of recurrent neural networks (RNN) has led to 
remarkable advances~\citep{mikolov2010recurrent,graves2013generating,Cho-et-al-EMNLP2014,sutskever2014sequence,vinyals2015show}. 
However, unlike the spatial data, learning both hierarchical and temporal representation has been among the long-standing challenges of RNNs 
in spite of the fact that hierarchical multiscale structures naturally exist in many 
temporal data~\citep{schmidhuber1991neural,mozer1993induction,el1995hierarchical,lin1996learning,koutnik2014clockwork}. 

A promising approach to model such hierarchical and temporal representation is the multiscale RNNs~\citep{schmidhuber1992learning,el1995hierarchical,koutnik2014clockwork}.
Based on the observation that high-level abstraction changes slowly with temporal coherency 
while low-level abstraction has quickly changing features sensitive to the precise local timing~\citep{el1995hierarchical}, 
the multiscale RNNs group hidden units into multiple modules of different timescales. 
In addition to the fact that the architecture fits naturally to the latent hierarchical structures in many temporal data, 
the multiscale approach provides the following advantages that resolve some inherent problems of standard RNNs: 
(a) computational efficiency obtained by updating the high-level layers less frequently, 
(b) efficiently delivering long-term dependencies with fewer updates at the high-level layers, which mitigates the vanishing gradient problem, 
(c) flexible resource allocation (e.g., more hidden units to the higher layers that focus on modelling long-term dependencies and
less hidden units to the lower layers which are in charge of learning short-term dependencies).
In addition, the learned latent hierarchical structures can provide useful information to other downstream tasks such as module structures 
in computer program learning, sub-task structures in hierarchical reinforcement learning, and story segments in video understanding.

There have been various approaches to implementing the multiscale RNNs. The most popular approach is to set the timescales as hyperparameters~\citep{el1995hierarchical,koutnik2014clockwork,bahdanau2016end} 
instead of treating them as dynamic variables that can be learned from the data~\citep{schmidhuber1991neural,schmidhuber1992learning,chung2015gated,chung2016character}. 
However, considering the fact that non-stationarity is prevalent in temporal data, and that many entities of abstraction such as words and sentences are in variable length, 
we claim that it is important for an RNN to dynamically adapt its timescales to the particulars of the input entities of various length. 
While this is trivial if the hierarchical boundary structure is provided~\citep{sordoni2015hierarchical}, 
it has been a challenge for an RNN to discover the latent hierarchical structure in temporal data without explicit boundary information.  

In this paper, we propose a novel multiscale RNN model, which can learn the hierarchical multiscale structure from temporal data without explicit boundary information. 
This model, called a \textit{hierarchical multiscale recurrent neural network} (HM-RNN), 
does not assign fixed update rates, but adaptively determines proper update times corresponding to different abstraction levels of the layers. 
We find that this model tends to learn fine timescales for low-level layers and coarse timescales for high-level layers. 
To do this, we introduce a binary boundary detector at each layer. 
The boundary detector is turned on only at the time steps where a segment of the corresponding abstraction level is completely processed. 
Otherwise, i.e., during the within segment processing, it stays turned off. 
Using the hierarchical boundary states, we implement three operations, UPDATE, COPY and FLUSH, and choose one of them at each time step. 
The UPDATE operation is similar to the usual update rule of the long short-term memory (LSTM)~\citep{hochreiter1997long},
except that it is executed sparsely according to the detected boundaries. 
The COPY operation simply copies the cell and hidden states of the previous time step. 
Unlike the leaky integration of the LSTM or the Gated Recurrent Unit (GRU)~\citep{Cho-et-al-EMNLP2014}, 
the COPY operation retains the whole states without any loss of information. 
The FLUSH operation is executed when a boundary is detected, where it first ejects the summarized representation of the current segment to the upper layer
and then reinitializes the states to start processing the next segment. 
Learning to select a proper operation at each time step and to detect the boundaries, the HM-RNN discovers the latent hierarchical structure of the sequences. 
We find that the straight-through estimator~\citep{hinton2012coursera,bengio2013estimating,courbariaux2016binarized} is efficient 
for training this model containing discrete variables.

We evaluate our model on two tasks: character-level language modelling and handwriting sequence generation. 
For the character-level language modelling, the HM-RNN achieves the state-of-the-art results 
on the Text8 dataset, and comparable results to the state-of-the-art on the Penn Treebank and Hutter Prize Wikipedia datasets.
The HM-RNN also outperforms the standard RNN on the handwriting sequence generation using the IAM-OnDB dataset.
In addition, we demonstrate that the hierarchical structure found by 
the HM-RNN is indeed very similar to the intrinsic structure observed in the data. 
The contributions of this paper are:
\bitem
    \item We propose for the first time an RNN model that can learn a latent hierarchical structure of a sequence without using explicit boundary information.
    \item We show that it is beneficial to utilize the above structure through empirical evaluation. 
	\item We show that the straight-through estimator is an efficient way of training a model containing discrete variables. 
    \item We propose the {\it slope annealing} trick to improve the training procedure based on the straight-through estimator.
\eitem

\section{Related Work}
\label{sec:related_work}
Two notable early attempts inspiring our model are \citet{schmidhuber1992learning} and \citet{el1995hierarchical}. 
In these works, it is advocated to stack multiple layers of RNNs in a decreasing order of update frequency for computational and learning efficiency. 
In \citet{schmidhuber1992learning}, the author shows a model that can self-organize a hierarchical multiscale structure.
Particularly in \citet{el1995hierarchical}, the advantages of incorporating a priori knowledge,
``\textit{temporal dependencies are structured hierarchically}", into the RNN architecture is studied.
The authors propose an RNN architecture that updates each layer with a fixed but different rate, called a hierarchical RNN.

LSTMs~\citep{hochreiter1997long} employ the multiscale update concept, where
the hidden units have different forget and update rates and thus can operate with different timescales. 
However, unlike our model, these timescales are not organized hierarchically. 
Although the LSTM has a self-loop for the gradients that helps to capture the long-term dependencies by mitigating the vanishing gradient problem,
in practice, it is still limited to a few hundred time steps due to the leaky integration by which the contents 
to memorize for a long-term is gradually diluted at every time step. 
Also, the model remains computationally expensive because it has to perform the update at every time step for each unit. 
However, our model is less prone to these problems because it learns a hierarchical structure such that, by design, high-level layers learn to perform less frequent updates than low-level layers. 
We hypothesize that this property mitigates the vanishing gradient problem more efficiently while also being computationally more efficient. 

A more recent model, the clockwork RNN (CW-RNN)~\citep{koutnik2014clockwork} extends the hierarchical RNN~\citep{el1995hierarchical}
and the NARX RNN~\citep{lin1996learning}\footnote{The acronym NARX stands for Non-linear Auto-Regressive model with eXogenous inputs.}.
The CW-RNN tries to solve the issue of using soft timescales in the LSTM, by explicitly assigning hard timescales. 
In the CW-RNN, hidden units are partitioned into several modules, and different timescales are assigned to 
the modules such that a module $i$ updates its hidden units at every $2^{(i-1)}$-th time step. 
The CW-RNN is computationally more efficient than the standard RNN including the LSTM 
since hidden units are updated only at the assigned clock rates. 
However, finding proper timescales in the CW-RNN remains as a challenge
whereas our model \textit{learns} the intrinsic timescales from the data. 
In the biscale RNNs~\citep{chung2016character}, the authors proposed to model \textit{layer-wise} timescales
adaptively by having additional gating units, however this approach still relies on the \textit{soft} gating mechanism like LSTMs.

Other forms of Hierarchical RNN (HRNN) architectures have been proposed in the cases where the explicit hierarchical boundary structure is provided. 
In~\citet{ling2015character}, after obtaining the word boundary via tokenization, the HRNN architecture is used for neural machine translation by modelling the characters and words using the first and second RNN layers, respectively. 
A similar HRNN architecture is also adopted in~\citet{sordoni2015hierarchical} to model dialogue utterances. 
However, in many cases, hierarchical boundary information is not explicitly observed or expensive to obtain. 
Also, it is unclear how to deploy more layers than the number of boundary levels that is explicitly observed in the data.

While the above models focus on online prediction problems, 
where a prediction needs to be made by using only the past data, 
in some cases, predictions are made after observing the whole sequence. 
In this setting, the input sequence can be regarded as 1-D spatial data, 
convolutional neural networks with 1-D kernels are proposed in~\citet{kim2014convolutional} and \citet{kim2015character} 
for language modelling and sentence classification. 
Also, in~\citet{chan2016listen} and \citet{bahdanau2016end}, the authors proposed to obtain high-level representation of the sequences
of reduced length by repeatedly merging or pooling the lower-level representation of the sequences.

%Another group of models discover the intrinsic structure of the sequences with hierarchical architectures~\citep{fernandez2007sequence,kong2015segmental}.
%These models aim to solve segmentation problems by optimizing the relevant objective functions while our goal is to propose a general framework that discovers 
%the intrinsic hierarchical structure even when explicit label information of each segment is not given.

Hierarchical RNN architectures have also been used to discover the segmentation structure in sequences~\citep{fernandez2007sequence,kong2015segmental}.
It is however different to our model in the sense that they optimize the objective with explicit labels on the hierarchical segments while our model discovers the intrinsic structure only from the sequences without segment label information.

The COPY operation used in our model can be related to Zoneout~\citep{krueger2016zoneout} which is a recurrent generalization of stochastic depth~\citep{huang2016deep}.
In Zoneout, an identity transformation is randomly applied to each hidden unit at each time step according to a Bernoulli distribution.
This results in occasional copy operations of the previous hidden states. 
While the focus of Zoneout is to propose a regularization technique similar to dropout~\citep{srivastava2014dropout} 
(where the regularization strength is controlled by a hyperparameter), 
our model learns (a) to dynamically determine when to copy from the context inputs and (b) to discover the hierarchical multiscale structure and representation.
Although the main goal of our proposed model is not regularization, 
we found that our model also shows very good generalization performance.

\section{Hierarchical Multiscale Recurrent Neural Networks}
\label{sec:model}
\subsection{Motivation}
\label{sec:motivation}
\begin{figure}[t]
    %\vspace*{-1.0cm}
    \vspace*{-0.5cm}
	\begin{minipage}{1.\columnwidth}
		\begin{minipage}{0.48\columnwidth}
			\centering
             \includegraphics[width=1.\columnwidth]{motiv_1.pdf}
         \end{minipage}
         \hfill
         \begin{minipage}{0.48\columnwidth}
             \centering
             \includegraphics[width=1.\columnwidth]{motiv_2.pdf}
         \end{minipage}
     \end{minipage}
     \begin{minipage}{1.\columnwidth}
         \begin{minipage}{0.48\columnwidth}
             \centering
             (a)
         \end{minipage}
         \hfill
         \begin{minipage}{0.48\columnwidth}
             \centering
             (b)
         \end{minipage}
     \end{minipage}
     \caption{(a) The HRNN architecture, which requires the knowledge of the hierarchical boundaries.
              (b) The HM-RNN architecture that discovers the hierarchical multiscale structure in the data.}
     \label{fig:motivation} 
\end{figure}

To begin with, we provide an example of how a stacked RNN can model
temporal data in an ideal setting, 
i.e., when the hierarchy of segments is provided~\citep{sordoni2015hierarchical,ling2015character}. 
In Figure~\ref{fig:motivation}~(a), we depict a hierarchical RNN (HRNN) for language modelling with two layers:
the first layer receives characters as inputs and generates word-level
representations (C2W-RNN), and the second layer takes the word-level
representations as inputs and yields phrase-level representations
(W2P-RNN).

As shown, by means of the provided end-of-word labels, the C2W-RNN obtains
word-level representation after processing the last character of each word
and passes the word-level representation to the W2P-RNN. Then, the W2P-RNN
performs an update of the phrase-level representation. Note that the hidden
states of the W2P-RNN remains unchanged while all the characters of a word are
processed by the C2W-RNN. When the C2W-RNN starts to process the next word,
its hidden states are reinitialized using the latest hidden states of the W2P-RNN,
which contain summarized representation of all the words that have been
processed by that time step, in that phrase. 

From this simple example, we can see the
advantages of having a hierarchical multiscale structure: (1) as the W2P-RNN
is updated at a much slower update rate than the C2W-RNN, a considerable
amount of computation can be saved, (2) gradients are backpropagated through
a much smaller number of time steps, and (3) layer-wise capacity control
becomes possible (e.g., use a smaller number of hidden units in the first
layer which models short-term dependencies but whose updates are invoked much more often).

\textit{Can an RNN discover such hierarchical multiscale structure
        without explicit hierarchical boundary information?} 
Considering the fact that the boundary information is difficult to obtain
(for example, consider languages where words are not always cleanly separated
by spaces or punctuation symbols,
and imperfect rules are used to separately perform segmentation)
or usually not provided at all, this is a legitimate problem. It gets worse when
we consider higher-level concepts which we would like the RNN to discover
autonomously. In Section
\ref{sec:related_work}, we discussed the limitations of the existing RNN models
under this setting, which either have to update all units at every time step
or use fixed update frequencies~\citep{el1995hierarchical,koutnik2014clockwork}.
Unfortunately,  this kind of approach is not well suited to the case where different segments in the hierarchical decomposition
have different lengths: for example, different words have different lengths, so a fixed hierarchy
would not update its upper-level units in synchrony with the natural boundaries in the data.

\subsection{The Proposed Model}
\label{sec:proposed}
%\begin{figure}[t]
%    \vspace*{-0.3cm}
%	\begin{minipage}{1.\columnwidth}
%		\begin{minipage}{0.5\columnwidth}
%			\centering
%            \includegraphics[height=0.5\columnwidth]{one_step.pdf}
%        \end{minipage}
%        \hfill
%        \begin{minipage}{0.5\columnwidth}
%            \centering
%            \includegraphics[height=0.5\columnwidth]{output_module.pdf}
%        \end{minipage}
%    \end{minipage}
%    \caption{Left: The gating mechanism of the HM-RNN. Right: The output module when $L=3$.}
%    \label{fig:gating_mechanism_and_output_module}
%\end{figure}     

A key element of our model is the introduction of a parametrized boundary detector, 
which outputs a binary value, in each layer of a stacked RNN, and learns 
when a segment should end in such a way to optimize the overall target objective.
Whenever the boundary detector is turned on at a time step of layer $\ell$ (i.e., when the boundary state is $1$), 
the model considers this to be the end of a segment corresponding to the latent abstraction level of that layer (e.g., word or phrase)
and feeds the summarized representation of the detected segment into the upper layer ($\ell + 1$).
Using the boundary states, at each time step, each layer selects one of the following operations: UPDATE, COPY or FLUSH. 
The selection is determined by (1) the boundary state of the current time step in the layer below $z_t^{\ell-1}$ and 
(2) the boundary state of the previous time step in the same layer $z_{t-1}^\ell$. 

In the following, we describe an HM-RNN based on the LSTM update rule. 
We call this model a hierarchical multiscale LSTM (HM-LSTM). 
Consider an HM-LSTM model of $L$ layers ($\ell=1,\dots,L$) which, at each layer $\ell$, performs the following update at time step $t$: 
\bea
    \bh_t^\ell, \bc_t^\ell, z_t^\ell = f_\text{HM-LSTM}^\ell(\bc_{t-1}^\ell, \bh_{t-1}^{\ell}, \bh_t^{\ell-1}, \bh_{t-1}^{\ell+1}, z_{t-1}^\ell, z_t^{\ell-1}).
\eea
Here, $\bh$ and $\bc$ denote the hidden and cell states, respectively.
The function $f_\text{HM-LSTM}^\ell$ is implemented as follows. First, using the two boundary states $z_{t-1}^\ell$ and $z_t^{\ell-1}$, the cell state is updated by:
\bea
  \label{eq:update_rule}
  \bc^\ell_t \eqa 
  \begin{cases}
  \bff_t^\ell \odot \bc_{t-1}^\ell + \bi_t^\ell \odot \bg_t^\ell & \text{if } z^\ell_{t-1} = 0 \mbox{ and } z^{\ell-1}_{t} = 1 \text{ (UPDATE)}\\
  \bc^\ell_{t-1}  & \text{if } z^\ell_{t-1} = 0 \mbox{ and } z^{\ell-1}_{t} = 0 \text{ (COPY)}\\
  \bi_t^\ell \odot \bg_t^\ell & \text{if } z^\ell_{t-1} = 1 \text{ (FLUSH)},
  \end{cases}
\eea
and then the hidden state is obtained by:
\bea
\bh_t^\ell =
\begin{cases}
	\bh_{t-1}^\ell & \text{if COPY,} \\
	\bo_t^\ell \odot \mathtt{tanh}(\bc_{t}^\ell) & \text{otherwise.}
\end{cases}
\eea
Here, $(\bff, \bi, \bo)$ are forget, input, output gates, and $\bg$ is a cell proposal vector. 
Note that unlike the LSTM, it is not necessary to compute these gates and cell proposal values at every time step. 
For example, in the case of the COPY operation, we do not need to compute any of these values and thus can save computations. 

The COPY operation, which simply performs $(\bc_t^\ell, \bh_t^\ell) \law (\bc_{t-1}^\ell,\bh_{t-1}^\ell)$,
implements the observation that an upper layer should keep its state unchanged until it receives the summarized input from the lower layer. 
The UPDATE operation is performed to update the summary representation of the layer $\ell$ if the boundary $z^{\ell-1}_t$ is detected from the layer below but the boundary $z^\ell_{t-1}$ was not found at the previous time step. 
Hence, the UPDATE operation is executed sparsely unlike the standard RNNs where it is executed at every time step, making it computationally inefficient. 
If a boundary is detected, the FLUSH operation is executed. 
The FLUSH operation consists of two sub-operations: (a) EJECT to pass the current state to the upper layer and then 
(b) RESET to reinitialize the state before starting to read a new segment. 
This operation implicitly forces the upper layer to absorb the summary information of the lower layer segment, because otherwise it will be lost. 
Note that the FLUSH operation is a {\it hard} reset in the sense that it completely erases all the previous states of the same layer, 
%which is different from the {\it soft} reset or forget operation by leaky integration in the GRU or LSTM. 
which is different from the {\it soft} reset or {\it soft} forget operation in the GRU or LSTM. 

Whenever needed (depending on the chosen operation), the gate values ($\bff_t^\ell, \bi_t^\ell, \bo_t^\ell$), 
the cell proposal $\bg_t^\ell$, and the pre-activation of the boundary detector
$\tilde{z}_t^\ell$~\footnote{$\tilde{z}_t^\ell$ can also be implemented as a function of
$\bh_t^\ell$, e.g., $\tilde{z}_t^\ell=\mathtt{hard\;sigm}(U\bh_t^\ell)$.} are then obtained by:
\bea
  \begin{pmatrix}
  {\bff}^\ell_t\\ 
  {\bi}^\ell_t\\ 
  {\bo}^\ell_t\\
  {\bg}^\ell_t\\
  \tilde{z}^\ell_t
  \end{pmatrix} 
  \eqa
  \begin{pmatrix}
  \mathtt{sigm}\\ 
  \mathtt{sigm}\\ 
  \mathtt{sigm}\\
  \mathtt{tanh}\\ 
  \mathtt{hard\;sigm}
  \end{pmatrix} f_\mathtt{slice}\left(\bs^{\text{recurrent}(\ell)}_t + \bs^{\text{top-down}(\ell)}_t + \bs^{\text{bottom-up}(\ell)}_t + \bb^{(\ell)} \right),
\eea
where
\bea 
  \bs^{\text{recurrent}(\ell)}_t \eqa U_\ell^\ell\bh^\ell_{t-1},\\
  \label{eq:top_down}
  \bs^{\text{top-down}(\ell)}_t \eqa z^\ell_{t-1}U_{\ell+1}^{\ell}\bh^{\ell+1}_{t-1},\\ 
  \bs^{\text{bottom-up}(\ell)}_t \eqa z^{\ell-1}_t W_{\ell-1}^{\ell} \bh_t^{\ell-1}.
\eea
Here, we use $W_i^j \in \eR^{(4dim(\bh^\ell)+1) \times dim(\bh^{\ell-1})}, U_i^j\in\eR^{(4dim(\bh^\ell)+1) \times dim(\bh^\ell)}$
to denote state transition parameters from layer $i$ to layer $j$,
and $\bb \in \eR^{4dim(\bh^\ell)+1}$ is a bias term.
In the last layer $L$, the top-down connection is ignored, and we use $\bh_t^{0} = \bx_t$. 
Since the input should not be omitted, we set $z^0_t=1$ for all $t$. 
Also, we do not use the boundary detector for the last layer. 
The $\mathtt{hard\;sigm}$ is defined by {\small $\mathtt{hard\;sigm}(x) = \max\left(0,\min\left(1,\f{ax + 1}{2}\right)\right)$}
with $a$ being the slope variable. 

Unlike the standard LSTM, the HM-LSTM has a top-down connection from $(\ell+1)$ to $\ell$, which is allowed to be activated only if a boundary 
is detected at the previous time step of the layer $\ell$ (see Eq.~\ref{eq:top_down}). 
This makes the layer $\ell$ to be initialized with more long-term information after the boundary is detected and execute the FLUSH operation. 
In addition, the input from the lower layer ($\ell - 1$) becomes effective only when a boundary is 
detected at the current time step in the layer ($\ell - 1$) due to the binary gate $z_t^{\ell-1}$. 
Figure~\ref{fig:gating_mechanism_and_output_module} (left) shows the gating mechanism of the HM-LSTM at time step $t$.

Finally, the binary boundary state $z^\ell_t$ is obtained by:
\bea
  z^\ell_t = f_\mathtt{bound}(\tilde{z}^\ell_t).
\eea
For the binarization function $f_\mathtt{bound}: \eR\rightarrow \{0,1\}$, we can either use a deterministic step function: 
\bea
  \label{eq:step_func}
  z^\ell_t \eqa 
  \begin{cases}
  1 & \text{if } \tilde{z}^\ell_t > 0.5\\
  0 & \text{otherwise},
  \end{cases}
\eea
or sample from a Bernoulli distribution $z^\ell_t \sim\text{Bernoulli}(\tilde{z}^\ell_t)$.
Although this binary decision is a key to our model, it is usually difficult to use stochastic gradient descent to train 
such model with discrete decisions as it is not differentiable. 

\label{sec:proposed}
\begin{figure}[t]
    \vspace*{-0.3cm}
	\begin{minipage}{1.\columnwidth}
		\begin{minipage}{0.5\columnwidth}
			\centering
            \includegraphics[height=0.5\columnwidth]{one_step.pdf}
        \end{minipage}
        \hfill
        \begin{minipage}{0.5\columnwidth}
            \centering
            \includegraphics[height=0.5\columnwidth]{output_module.pdf}
        \end{minipage}
    \end{minipage}
    \caption{Left: The gating mechanism of the HM-RNN. Right: The output module when $L=3$.}
    \label{fig:gating_mechanism_and_output_module}
\end{figure}     


\subsection{Computing Gradient of Boundary Detector}
Training neural networks with discrete variables requires more efforts since the standard backpropagation 
is no longer applicable due to the non-differentiability. 
Among a few methods for training a neural network with discrete variables such as the REINFORCE~\citep{williams1992simple,mnih2014neural} 
and the straight-through  estimator~\citep{hinton2012coursera,bengio2013estimating}, we use the straight-through estimator to train our model. 
The straight-through estimator is a biased estimator because the non-differentiable function used in the forward pass 
(i.e., the step function in our case) is replaced by a differentiable function during the backward pass 
(i.e., the hard sigmoid function in our case). 
The straight-through estimator, however, is much simpler and often works more efficiently in practice 
than other unbiased but high-variance estimators such as the REINFORCE. 
The straight-through estimator has also been used in~\citet{courbariaux2016binarized} and \citet{vezhnevets2016strategic}. 

\textbf{The Slope Annealing Trick}. In our experiment, we use the slope annealing trick to reduce the bias of the straight-through estimator. 
The idea is to reduce the discrepancy between the two functions used during the forward pass and the backward pass. 
That is, by gradually increasing the slope $a$ of the hard sigmoid function, we make the hard sigmoid be close to the step function. 
Note that starting with a high slope value from the beginning can make the training difficult while it is 
more applicable later when the model parameters become more stable. 
In our experiments, starting from slope $a=1$, we slowly increase the slope until it reaches a threshold with an appropriate scheduling. 


\section{Experiments}
\label{sec:experiments}
We evaluate the proposed model on two tasks, character-level language modelling and handwriting sequence generation. 
Character-level language modelling is a representative example of discrete sequence modelling, 
where the discrete symbols form a distinct hierarchical multiscale structure. 
The performance on real-valued sequences is tested on the handwriting sequence generation in which a relatively
clear hierarchical multiscale structure exists compared to other data such as speech signals. 

\subsection{Character-Level Language Modelling}
A sequence modelling task aims at learning the probability distribution over sequences by minimizing the negative log-likelihood of the training sequences:
\bea
	\label{eq:sequence_task}
	\min_{\theta}-\f{1}{N}\sum^{N}_{n=1}\sum^{T^n}_{t=1}\log p\left(x^n_t\mid x^n_{<t};\theta\right),
\eea
where $\theta$ is the model parameter, $N$ is the number of training sequences, 
and $T^n$ is the length of the $n$-th sequence. A symbol at time $t$ of sequence $n$ is denoted by $x_t^n$,
and $x^n_{<t}$ denotes all previous symbols at time $t$.
We evaluate our model on three benchmark text corpora: (1) Penn Treebank, (2) Text8 and (3) Hutter Prize Wikipedia.
We use the bits-per-character (BPC), $\eE[-\log_2 p(x_{t+1} \mid x_{\leq t})]$, as the evaluation metric.
\begin{table*}[t]
    %\vspace*{-1.0cm}
    \vspace*{-0.5cm}
    \hspace*{-1.2cm}
    {\small
    \begin{minipage}{0.4\textwidth}
        \begin{tabular}{c c c }
            \Xhline{0.8pt}
            \multicolumn{3}{c}{\bf Penn Treebank}\\
            \hline
            \multicolumn{2}{c}{\bf Model} & {\bf BPC} \\
            \hline
            \multicolumn{2}{c}{Norm-stabilized RNN}~\citep{krueger2015regularizing} & 1.48 \\
            \multicolumn{2}{c}{CW-RNN}~\citep{koutnik2014clockwork}                 & 1.46 \\
            \multicolumn{2}{c}{HF-MRNN}~\citep{mikolov2012subword}                  & 1.41 \\
            \multicolumn{2}{c}{MI-RNN}~\citep{wu2016multiplicative}                 & 1.39 \\
            \multicolumn{2}{c}{ME $n$-gram}~\citep{mikolov2012subword}              & 1.37 \\
            \multicolumn{2}{c}{BatchNorm LSTM}~\citep{cooijmans2016recurrent}       & 1.32 \\
            \multicolumn{2}{c}{Zoneout RNN}~\citep{krueger2016zoneout}              & 1.27 \\
            \multicolumn{2}{c}{HyperNetworks}~\citep{ha2016hypernetworks}           & 1.27 \\
            \multicolumn{2}{c}{LayerNorm HyperNetworks}~\citep{ha2016hypernetworks} & {\bf 1.23} \\
            \hline
            \multicolumn{2}{c}{LayerNorm CW-RNN$^\dagger$}                          & 1.40 \\
            \multicolumn{2}{c}{LayerNorm LSTM$^\dagger$}                            & 1.29 \\
            LayerNorm HM-LSTM & Sampling                                            & 1.27 \\
            LayerNorm HM-LSTM & Soft$^*$                                            & 1.27 \\
            LayerNorm HM-LSTM & Step Fn.                                            & 1.25 \\
            LayerNorm HM-LSTM & Step Fn. \& Slope Annealing                         & 1.24 \\
            \Xhline{0.8pt}
        \end{tabular}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \begin{tabular}{c c }
            \Xhline{0.8pt}
            \multicolumn{2}{c}{\bf Hutter Prize Wikipedia}\\
            \hline
            {\bf Model} & {\bf BPC} \\
            \hline
            Stacked LSTM~\citep{graves2013generating}                    & 1.67 \\
            MRNN~\citep{sutskever2011generating}                         & 1.60 \\
            GF-LSTM~\citep{chung2015gated}                               & 1.58 \\
            Grid-LSTM~\citep{kalchbrenner2015grid}                       & 1.47 \\
            MI-LSTM~\citep{wu2016multiplicative}                         & 1.44 \\
            Recurrent Memory Array Structures~\citep{rocki2016recurrent} & 1.40 \\
            SF-LSTM~\citep{rocki2016surprisal}$^{\ddagger}$              & 1.37 \\
            HyperNetworks~\citep{ha2016hypernetworks}                    & 1.35 \\
            LayerNorm HyperNetworks~\citep{ha2016hypernetworks}          & 1.34 \\
            Recurrent Highway Networks~\citep{zilly2016recurrent}        & 1.32 \\
            \hline
            LayerNorm LSTM$^\dagger$                                     & 1.39 \\
            HM-LSTM                                                      & 1.34 \\
            LayerNorm HM-LSTM                                            & 1.32 \\
            \hline
            PAQ8hp12~\citep{mahoney2005adaptive}                         & 1.32 \\
            decomp8~\citep{mahoney2009large}                             & {\bf 1.28} \\
            \Xhline{0.8pt}
        \end{tabular}
    \end{minipage}
    }
    \caption{BPC on the Penn Treebank test set (left) and Hutter Prize Wikipedia test set (right).
             ($*$) This model is a variant of the HM-LSTM that does not discretize the boundary detector states.
             ($\dagger$) These models are implemented by the authors to evaluate 
             the performance using layer normalization~\citep{ba2016layer}
             with the additional output module. 
             ($\ddagger$) This method uses test error signals for predicting the next characters,
             which makes it not comparable to other methods that do not.
         }
    \label{tab:ptb_wiki_bpc}
\end{table*}

\paragraph{Model}
We use a model consisting of an input embedding layer, an RNN module and an output module.
The input embedding layer maps each input symbol into $128$-dimensional continuous vector without
using any non-linearity.
The RNN module is the HM-LSTM, described in Section~\ref{sec:model}, with three layers. 
The output module is a feedforward neural network with two layers, an output embedding layer
and a softmax layer. 
Figure~\ref{fig:gating_mechanism_and_output_module} (right) shows a diagram of the output module. 
At each time step, the output embedding layer receives the hidden states of the three RNN layers as input. 
In order to adaptively control the importance of each layer at each time step, 
we also introduce three scalar gating units $g_t^\ell \in \eR$ to each of the layer outputs:
\bea
	\label{eq:scaling_unit}
	g^\ell_t = \mathtt{sigm}(\bw^\ell[\bh^1_t;\cdots;\bh^L_t]),
\eea
where $\bw^\ell\in\mathbb{R}^{\sum^L_{\ell=1} dim(\bh^\ell)}$ is the weight parameter. 
The output embedding $\bh_t^\text{e}$ is computed by:
\bea
	\label{eq:output_module}
	\bh^{\text{e}}_t = \mathtt{ReLU}\left(\sum^{L}_{\ell=1}g^{\ell}_t W_{\ell}^{\text{e}}\bh^{\ell}_t\right),
\eea
where $L=3$ and $\mathtt{ReLU}(x)=\max(0, x)$~\citep{nair2010rectified}. 
Finally, the probability distribution for the next target character is computed by the softmax function,
{\small $\mathtt{softmax}(x_j)=\frac{e^{x_j}}{\sum^K_{k=1} e^{x_k}}$}, where each output class is a character.

\paragraph{Penn Treebank}
We process the Penn Treebank dataset~\citep{marcus1993building} by following the procedure introduced in~\citet{mikolov2012subword}. 
Each update is done by using a mini-batch of $64$ examples of length $100$ to prevent the memory overflow problem when 
unfolding the RNN in time for backpropagation. 
The last hidden state of a sequence is used to initialize the hidden state of the next sequence to approximate the full backpropagation. 
We train the model using Adam~\citep{kingma2014adam} with an initial learning rate of $0.002$.
We divide the learning rate by a factor of $50$ when the validation negative log-likelihood stopped decreasing.
The norm of the gradient is clipped with a threshold of $1$~\citep{mikolov2010recurrent,pascanu2012difficulty}.
We also apply layer normalization~\citep{ba2016layer} to our models.
For all of the character-level language modelling experiments, we apply the same procedure, 
but only change the number of hidden units, mini-batch size and the initial learning rate.

For the Penn Treebank dataset, we use $512$ units in each layer of the HM-LSTM and for the output embedding layer.
In Table~\ref{tab:ptb_wiki_bpc} (left), we compare the test BPCs of four variants of our model to other baseline models. 
Note that the HM-LSTM using the step function for the hard boundary decision outperforms the others using either {\it sampling}
or {\it soft} boundary decision (i.e., hard sigmoid). 
The test BPC is further improved with the slope annealing trick, which reduces the bias of the straight-through estimator. 
We increased the slope $a$ with the following schedule {\small $a=\min\left(5, 1+0.04\cdot N_{epoch}\right)$},
where $N_{epoch}$ is the maximum number of epochs. 
The HM-LSTM achieves test BPC score of $1.24$.
For the remaining tasks, we fixed the hard boundary decision using the step function 
without slope annealing due to the difficulty of finding a good annealing schedule on large-scale datasets.

\begin{table*}[t]
    %\vspace*{-1.0cm}
    \vspace*{-0.5cm}
	\centering
    {\small
    \begin{tabular}{c c }
        \Xhline{0.8pt}
        \multicolumn{2}{c}{\bf Text8}\\
        \hline
        {\bf Model} & {\bf BPC} \\
        \hline
        {\it td}-LSTM~\citep{zhang2016architectural}         & 1.63 \\
        HF-MRNN~\citep{mikolov2012subword}                   & 1.54 \\
        MI-RNN~\citep{wu2016multiplicative}                  & 1.52 \\
        Skipping-RNN~\citep{pachitariu2013regularization}    & 1.48 \\
        MI-LSTM~\citep{wu2016multiplicative}                 & 1.44 \\
        BatchNorm LSTM~\citep{cooijmans2016recurrent}        & 1.36 \\
        \hline
        HM-LSTM                                              & 1.32 \\
        LayerNorm HM-LSTM                             & {\bf 1.29} \\
        \Xhline{0.8pt}
    \end{tabular}
    }
   	\caption{BPC on the Text8 test set.}
    \label{tab:Text8_bpc}
\end{table*}

\paragraph{Text8}
The Text8 dataset~\citep{mahoney2009large} consists of $100$M characters extracted from the Wikipedia corpus. 
Text8 contains only alphabets and spaces, and thus we have total 27 symbols. 
In order to compare with other previous works, we follow the data splits used in~\citet{mikolov2012subword}.
We use $1024$ units for each HM-LSTM layer and $2048$ units for the output embedding layer.
The mini-batch size and the initial learning rate are set to $128$ and $0.001$, respectively. 
The results are shown in Table~\ref{tab:Text8_bpc}.
The HM-LSTM obtains the state-of-the-art test BPC $1.29$.

\paragraph{Hutter Prize Wikipedia} 
The Hutter Prize Wikipedia (\texttt{enwik8}) dataset~\citep{Hutter2012} contains $205$ symbols 
including XML markups and special characters.
We follow the data splits used in~\citet{graves2013generating}
where the first 90M characters are used to train the model, 
the next 5M characters for validation, and the remainders for the test set. 
We use the same model size, mini-batch size and the initial learning rate as in the Text8.
In Table~\ref{tab:ptb_wiki_bpc} (right), we show the HM-LSTM achieving the test BPC $1.32$, which is a tie
with the state-of-the-art result among the neural models.
Although the neural models, show remarkable performances, their compression performance is
still behind the best models such as PAQ8hp12~\citep{mahoney2005adaptive} and decomp8~\citep{mahoney2009large}.

\paragraph{Visualizing Learned Hierarchical Multiscale Structure}
\begin{figure}[t]
    \vspace*{-0.3cm}
    \hspace*{-1.5cm}
    \begin{minipage}{1.1\columnwidth}
    	\centering
     	\includegraphics[width=1.1\columnwidth]{wiki_boundary.pdf}
    \end{minipage}
    \caption{Hierarchical multiscale structure in the Wikipedia dataset captured by the boundary detectors of the HM-LSTM.}
     \label{fig:wiki_bound} 
\end{figure}
In Figure~\ref{fig:wiki_bound} and \ref{fig:ptb_hidden}, we visualize the boundaries detected by the boundary detectors of the HM-LSTM
while reading a character sequence of total length $270$ taken from the validation set of either the Penn Treebank or Hutter Prize Wikipedia dataset.
Due to the page width limit, the figure contains the sequence partitioned into three segments of length $90$. 
The white blocks indicate boundaries $z^{\ell}_t=1$ while the black blocks indicate the non-boundaries $z^{\ell}_t=0$.

Interestingly in both figures, we can observe that the boundary detector of the first layer, $z^1$, 
tends to be turned on when it sees a space or after it sees a space, which is a reasonable breakpoint to separate between words. 
This is somewhat surprising because the model self-organizes this structure without any explicit boundary information.
In Figure~\ref{fig:wiki_bound}, we observe that the $z^1$ tends to detect the boundaries of the words but also fires within the words,
where the $z^2$ tends to fire when it sees either an end of a word or $2,3$-grams.
In Figure~\ref{fig:ptb_hidden}, we also see flushing in the middle of a word, e.g., ``tele-FLUSH-phone''.
Note that ``tele'' is a prefix after which a various number of postfixes can follow. 
From these, it seems that the model uses to some extent the concept of \textit{surprise} to learn the boundary. 
Although interpretation of the second layer boundaries is not as apparent as the first layer boundaries, 
it seems to segment at reasonable semantic / syntactic boundaries, 
e.g.,  ``consumers may'' - ``want to move their telephones a'' - ``little closer to the tv set <unk>'', and so on.

Another remarkable point is the fact that we do not pose any constraint on the number of boundaries that the model can fire up. 
The model, however, learns that it is more beneficial to delay the information ejection to some extent. 
This is somewhat counterintuitive because it might look more beneficial to feed the fresh update to the upper layers at every time step without any delay. 
We conjecture the reason that the model works in this way is due to the FLUSH operation
that poses an implicit constraint on the frequency of boundary detection,
because it contains both a reward (feeding fresh information to upper layers) and a penalty (erasing accumulated information).
The model finds an optimal balance between the reward and the penalty.

To understand the update mechanism more intuitively, in Figure~\ref{fig:ptb_hidden},
we also depict the heatmap of the $\ell^2$-norm of the hidden states along with the states of the boundary detectors. 
As we expect, we can see that there is no change in the norm value within segments due to the COPY operation. 
Also, the color of $\|\bh^1\|$ changes quickly (at every time step) because there is no COPY operation in the first layer. 
The color of $\|\bh^2\|$ changes less frequently based on the states of $z^1_t$ and $z^2_{t-1}$. 
The color of $\|\bh^3\|$ changes even slowly, i.e., only when $z^2_t=1$.

\begin{figure}[t]
    %\vspace*{-1.0cm}
    \vspace*{-0.5cm}
	\hspace*{-2.5cm} 
	\begin{minipage}{1.2\columnwidth}
    	\centering
     	\includegraphics[width=1.1\columnwidth]{hidden_norm.pdf}
    \end{minipage}
    \caption{The $\ell^2$-norm of the hidden states shown together with the states of the boundary detectors of the HM-LSTM.}
    \label{fig:ptb_hidden} 

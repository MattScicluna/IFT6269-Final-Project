\end{figure}

A notable advantage of the proposed architecture is that the internal process of the RNN becomes more interpretable. 
For example, we can substitute the states of $z^1_t$ and $z^2_{t-1}$ into Eq.~\ref{eq:update_rule} 
and infer which operation among the UPDATE, COPY and FLUSH was applied to the second layer at time step $t$. 
We can also inspect the update frequencies of the layers simply by counting how many UPDATE and FLUSH operations were made in each layer. 
For example in Figure~\ref{fig:ptb_hidden}, we see that the first layer updates at every time step (which is $270$ UPDATE operations), 
the second layer updates $56$ times, and only $9$ updates has made in the third layer. 
Note that, by design, the first layer performs UPDATE operation at every time step and then the number of UPDATE operations decreases as the layer level increases. 
In this example, the total number of updates is $335$ for the HM-LSTM which is $60\%$ of reduction from the $810$ updates of the standard RNN architecture.

\subsection{Handwriting Sequence Generation}
We extend the evaluation of the HM-LSTM to a real-valued sequence modelling task using IAM-OnDB~\citep{liwicki2005iam} dataset.
The IAM-OnDB dataset consists of $12,179$ handwriting examples, each of which is a sequence of 
$(x,y)$ coordinate and a binary indicator $p$ for pen-tip location, giving us $(x_{1:T^n},y_{1:T^n}, p_{1:T^n})$, where $n$ is an index of a sequence.
At each time step, the model receives $(x_t,y_t,p_t)$, and the goal is to predict $(x_{t+1},y_{t+1},p_{t+1})$.
The pen-up ($p_t=1$) indicates an end of a stroke, and the pen-down ($p_t=0$)  indicates that a stroke is in progress. 
There is usually a large shift in the $(x,y)$ coordinate to start a new stroke after the pen-up happens.
We remove all sequences whose length is shorter than $300$. 
This leaves us $10,465$ sequences for training, $581$ for validation, $582$ for test. 
The average length of the sequences is $648$. 
We normalize the range of the $(x,y)$ coordinates separately with the mean and standard deviation obtained from the training set.
We use the mini-batch size of $32$, and the initial learning rate is set to $0.0003$.

We use the same model architecture as used in the character-level language model,
except that the output layer is modified to predict real-valued outputs. 
We use the mixture density network as the output layer following~\citet{graves2013generating},
and use $400$ units for each HM-LSTM layer and for the output embedding layer.
In Table~\ref{tab:iamondb_nll}, we compare the log-likelihood averaged over the test sequences of the IAM-OnDB dataset. 
We observe that the HM-LSTM outperforms the standard LSTM.
The slope annealing trick further improves the test log-likelihood of the HM-LSTM into $1167$ in our setting.
In this experiment, we increased the slope $a$ with the following schedule {\small $a=\min\left(3, 1+0.004\cdot N_{epoch}\right)$}.
In Figure~\ref{fig:iamondb_bound}, we let the HM-LSTM to read a randomly picked validation sequence 
and present the visualization of handwriting examples by segments based on either the states of $z^2$ or
the states of pen-tip location\footnote{\scriptsize The plot function could be found at \url{blog.otoro.net/2015/12/12/handwriting-generation-demo-in-tensorflow/}.}.

%\begin{table*}[t]
%    %\vspace*{-1.0cm}
%    \vspace*{-0.5cm}
%	\centering
%    {\small
%    \begin{tabular}{c | c | c | c}
%        \Xhline{0.8pt}
%        \multicolumn{4}{c}{\bf IAM-OnDB}\\
%        \hline
%        {\bf Model} & Standard LSTM & HM-LSTM & HM-LSTM \& Slope Annealing\\
%        \hline
%        {\bf Average Log-Likelihood} & 1081 & 1137 & {\bf 1167}\\
%        \Xhline{0.8pt}
%    \end{tabular}
%    }
%   	\caption{Average log-likelihood per sequence on the IAM-OnDB test set.}
%    \label{tab:iamondb_nll}
%\end{table*}
\begin{table*}[t]
    %\vspace*{-1.0cm}
    \vspace*{-0.5cm}
	\centering
    {\small
    %\begin{tabular}{c | c }
    \begin{tabular}{c c }
        \Xhline{0.8pt}
        \multicolumn{2}{c}{\bf IAM-OnDB}\\
        \hline
        {\bf Model} & {\bf Average Log-Likelihood} \\
        \hline
        Standard LSTM & 1081 \\
        %\hline
        HM-LSTM & 1137 \\
        %\hline
        HM-LSTM \& Slope Annealing & {\bf 1167} \\
        \Xhline{0.8pt}
    \end{tabular}
    }
   	\caption{Average log-likelihood per sequence on the IAM-OnDB test set.}
    \label{tab:iamondb_nll}
\end{table*}



\begin{figure}[t]
    \vspace*{-0.3cm}
	\begin{minipage}{1.\columnwidth}
    	\centering
     	\includegraphics[width=1.\columnwidth]{handwriting_boundary.pdf}
    \end{minipage}
    \caption{The visualization by segments based on either the given pen-tip location or states of the $z^2$.}
    \label{fig:iamondb_bound} 
\end{figure}

\section{Conclusion}
\label{sec:conclusion}
In this paper, we proposed the HM-RNN that can capture the latent hierarchical structure of the sequences.
We introduced three types of operations to the RNN, which are the COPY, UPDATE and FLUSH operations.
In order to implement these operations, we introduced a set of binary variables and a novel update rule that is dependent on the states of these
binary variables. 
Each binary variable is learned to find segments at its level, therefore, we call this binary variable, a boundary detector.
On the character-level language modelling,
the HM-LSTM achieved state-of-the-art result on the Text8 dataset and 
comparable results to the state-of-the-art results on the Penn Treebank and Hutter Prize Wikipedia datasets.
Also, the HM-LSTM outperformed the standard LSTM on the handwriting sequence generation.
Our results and analysis suggest that the proposed HM-RNN can discover the latent hierarchical structure of the sequences
and can learn efficient hierarchical multiscale representation that leads to better generalization performance.

\section*{Acknowledgments}
\label{sec:ack}
The authors would like to thank Alex Graves, Tom Schaul and Hado van Hasselt for their fruitful comments and discussion.
We acknowledge the support of the following agencies for research funding and
computing support: Ubisoft, Samsung, IBM, Facebook, Google, Microsoft, NSERC, Calcul Qu\'{e}bec, Compute Canada,
the Canada Research Chairs and CIFAR.
The authors thank the developers of Theano~\citep{team2016theano}.
JC would like to thank Arnaud Bergenon and Fr\'ed\'eric Bastien for their technical support.
JC would also like to thank Guillaume Alain, Kyle Kastner and David Ha for providing us useful pieces of code.

{\small
\bibliography{./junyoung_thesis}
\bibliographystyle{iclr2017_conference}
}
\end{document}



\documentclass{article} % For LaTeX2e
\usepackage{nips10submit_e,times}
\usepackage{graphicx} % more modern
\usepackage{subfigure} 
\usepackage{amsmath, amsthm}
\usepackage{amsfonts}
\usepackage{mlapa}
%\documentstyle[nips10submit_09,times,art10]{article} % For LaTeX 2.09


\title{Adaptive Parallel Tempering for Stochastic Maximum Likelihood Learning of RBMs}

\author{
Guillaume Desjardins, Aaron Courville and Yoshua Bengio\\
Dept. IRO\\
Universit\'e de Montr\'eal\\
Montr\'eal, QC  \\
\texttt{\{desjagui,courvila,bengioy\}@iro.umontreal.ca}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\nipsfinalcopy % Uncomment for camera-ready version

\newcommand{\E}{\mathbb{E}}
\newcommand{\Egy}{{\bf E}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\h}{\mathbf{h}}
\newcommand{\vis}{\mathbf{v}}
\newcommand{\nv}{\mathbf{v^{{\bf -}}}}
\newcommand{\nh}{\mathbf{h^{{\bf -}}}}
\newcommand{\s}{\mathbf{s}}
\renewcommand{\b}{\mathbf{b}}
\renewcommand{\c}{\mathbf{c}}
\newcommand{\W}{\mathbf{W}}
\newcommand{\T}{{\bf \mathcal T}}
\newcommand{\B}{{\bf \mathcal B}}


\begin{document}


\maketitle

\begin{abstract}
Restricted Boltzmann Machines (RBM) have attracted a lot of attention of late, as one the principle building blocks of deep networks. Training RBMs remains problematic however, because of the intractibility of their partition function. The maximum likelihood gradient requires a very robust sampler which can accurately sample from the model despite the loss of ergodicity often incurred during learning. While using Parallel Tempering in the negative phase of Stochastic Maximum Likelihood (SML-PT) helps address the issue, it imposes a trade-off between computational complexity and high ergodicity, and requires careful hand-tuning of the temperatures. In this paper, we show that this trade-off is unnecessary. The choice of optimal temperatures can be automated by minimizing average return time (a concept first proposed by \cite{Katzgraber06}) while chains can be spawned dynamically, as needed, thus minimizing the computational overhead. We show on a synthetic dataset, that this results in better likelihood scores.
\end{abstract}

\section{Introduction}

Restricted Boltzmann Machines (RBM)~\cite{Freund+Haussler-94,Welling05,Hinton06} have become a model of choice for learning
unsupervised features for use in deep feed-forward architectures \cite{Hinton06,Bengio-2009} as well
as for modeling complex, high-dimensional distributions \cite{Welling05,TaylorHintonICML2009,Larochelle+al-2010}. 
Their success can be explained in part through the bi-partite structure of their
graphical model. Units are grouped into a visible layer $\vis$ and a hidden layer
$\h$, prohibiting connections within the same layer. The use of latent
variables affords RBMs a rich modeling capacity, while the conditional
independence property yields a trivial inference procedure. 

RBMs are parametrized by an energy function $\Egy(\vis,\h)$ which is converted to
probability through the Boltzmann distribution, after marginalizing out the
hidden units. The probability of a given configuration $p(\vis)$ is thus given by
$p(\vis) = \frac{1}{Z} \sum_{\h} \exp(-\Egy(\vis,\h))$, where $Z$ is the partition function
defined as $Z=\sum_{\vis,\h} \exp(-\Egy(\vis,\h))$.

Despite their popularity, direct learning of these models through maximum
likelihood remains problematic. The maximum likelihood gradient
with respect to the parameters $\theta$ of the model is:
\begin{eqnarray}
\label{eq:maxll}
\frac{\partial \log p(\vis)}{\partial \theta} 
&=& -\sum_{\h} p(\h|\vis) \frac{\partial \Egy(\vis,\h)}{\partial \theta}
+ \sum_{\nv,\nh} p(\nv,\nh) \frac{\partial \Egy(\nv,\nh)}{\partial \theta}
\label{eq:pos-neg}
\end{eqnarray}

The first term is trivial to calculate and is referred to as the {\bf positive
phase}, as it raises the probability of training data. The second term or {\bf
negative phase} is intractable in most applications of interest, as it involves an
expectation over $p(\vis,\h)$. Many learning algorithms have been
proposed in the literature to address this issue:

\begin{itemize}
%===== CD =====%
\item Contrastive Divergence (CD)~\cite{Hinton99,Hinton2002} replaces the expectation with
a finite set of negative samples, which are obtained by running a short Markov
chain initialized at positive training examples. This yields a biased, but
low-variance gradient which has been shown to work well as a feature extractor
for deep networks such as the Deep Belief Network~\cite{Hinton06}.
%===== PCD =====%
\item Stochastic Maximum Likelihood (SML) or Persistent Contrastive Divergence (PCD)
\cite{Younes98onthe,Tieleman08} on the other hand, relies on a persistent Markov
chain to sample the negative particles. The chain is run for a small number of
steps between consecutive model updates, with the assumption that the Markov
chain will stay close to its equilibrium distribution as the parameters evolve.
Learning actually encourages this process, in what is called the
``fast-weight effect'' \cite{TielemanT2009}.
%====== RM / SM ======%
\item Ratio Matching and Score Matching~\cite{Hyvarinen-2005,Hyvarinen-2007} 
avoid the issue of the partition function
altogether by replacing maximum likelihood by another learning principle, based
on matching the change in likelihood to that implied by the empirical distribution.
\end{itemize}

\cite{Marlin10InductivePrinciples} recently compared these algorithms on a
variety of tasks and found SML to be the most attractive method when taking
computational complexity into account.
Unfortunately, these results fail to address the main shortcomings of SML.
First, it relies on Gibbs sampling to extract negative samples: a poor choice
when sampling from multi-modal distributions. Second, to guarantee convergence,
the learning rate must be annealed throughout learning in order to offset the
% TODO: define ergodicity
loss of ergodicity 
\footnote{We use the term ``ergodicity'' rather loosely, to reflect the amount
of time required for the states sampled by the Markov chain, to reflect the
true expectation we wish to measure.}
incurred by the Markov chain due to parameter updates
\cite{Younes98onthe,Desjardins+al-2010}.
Using tempering in the negative phase of
SML~\cite{Desjardins+al-2010,Cho10IJCNN,Salakhutdinov-2010,Salakhutdinov-ICML2010}
appears to address these issues to some extent. By performing a random
walk in the joint (configuration, temperature) space, negative particles can
escape regions of high probability and travel between disconnected modes. Also,
since high temperature chains are inherently more ergodic, the sampler as a
whole exhibits better mixing and results in better convergence properties than
traditional SML.

Tempering is still no panacea however. Great care must be taken to select the
set of temperatures $\T=\{T_1,...,T_M ; T_1 < T_i < T_M \ \forall i \in [1,M], M
\in \mathbb{N}\}$ over which to run the simulation.  Having too few or incorrectly
spaced chains can result in high rejection ratios (tempered transition), low
return rates (simulated tempering) or low swap rates between neighboring chains
(parallel tempering), which all undermine the usefulness of the method. In this
work, we show that the choice of $\T$ can be automated for parallel tempering,
both in terms of optimal temperature spacing, as well as the number of chains
to simulate. Our algorithm relies heavily on the work of \cite{Katzgraber06},
who were the first to show that optimal temperature spacing can be obtained
by minimizing the average {\bf return time} of particles under simulation. 
%This is defined as the number of steps required for a particle to perform a round-trip between $T_1$ and $T_M$.

The paper is organized as follows. We start with a brief review of SML, then
explore the details behind SML with Parallel Tempering (SML-PT) as described in
\cite{Desjardins+al-2010}. Following this, we show how the algorithm of
Katzgraber et al. can be adapted to the online gradient setting for use with
SML-PT and show how chains can be created dynamically, so as to maintain a
given level of ergodicity throughout training. We then proceed to show various
results on a complex synthetic dataset.

% Things to touch n
% * motivation: why is learning RBMs so important
% * cite inductive principles paper
% * cover previous work on tempering PCD
% * explain what is novel about this paper
% * give outline of paper

\section{SML with Optimized Parallel Tempering}

\subsection{Parallel Tempered SML (SML-PT)}

We start with a very brief review of SML, which will serve mostly to anchor our
notation. For details on the actual algorithm, we refer the interested reader to 
\cite{TielemanT2009,Marlin10InductivePrinciples}.
RBMs are parametrized by $\theta=\{\W, \b, \c\}$, 
where $\b_i$ is the $i$-th hidden bias,
$\c_j$ the $j$-th visible bias and
$W_{ij}$ is the weight connecting units $h_i$ to $v_j$. 
They belong to the family of log-linear models whose energy function is given
by $\Egy(\x) = -\sum_k \theta_k \phi_k(\x)$, where $\phi_k$ are 
functions associated with each parameter $\theta_k$. In the case of RBMs, $\x=(\vis,\h)$ and
$\phi(\vis,\h) = (\h\vis^T, \h, \vis)$. For this family of model, the gradient of
Equation~\ref{eq:maxll} simplifies to:
\begin{eqnarray}
\label{eq:maxll2}
\frac{\partial \log p(\vis)}{\partial \theta} 
&=& \E_{p(\h|\vis)}[\phi(\vis,\h)] - \E_{p(\vis,\h)}[\phi(\vis,\h)].
\end{eqnarray}

As was mentioned previously, SML approximates the gradient by drawing negative
phase samples (i.e. to estimate the second expectation) 
from a persistent Markov chain, which attempts to track changes
in the model. If we denote the state of this chain at timestep $t$ as
$\vis^{-}_t$ and the i-th training example as $\vis^{(i)}$, then the stochastic
gradient update follows $\phi(\vis^{(i)},\tilde{\h}) - \phi(\tilde{\vis}^{-}_{t+k},
\tilde{\h}^{-}_{t+k})$, where $\tilde{\h} = \E[\h|\vis=\vis^{(i)}]$, and
$\tilde{\vis}^{-}_{t+k}$ is obtained after $k$ steps of alternating Gibbs
starting from state $\vis^{-}_t$ and $\tilde{\h}^{-}_{t+k} =
\E[\h|\vis=\vis^{-}_{t+k}]$.

\smallskip
Training an RBM using SML-PT maintains the positive phase as is.  During the
negative phase however, we create and sample from an an extended set of $M$
persistent chains, $\{p_{\beta_i}(\vis,\h)| i \in [1,M], \beta_i \ge \beta_j \iff i < j\}$. 
Here each $p_{\beta_i}(\vis,\h) = \frac{\exp(-\beta_i \Egy(\x))}{Z(\beta_i)}$
represents a smoothed version of the distribution we wish to sample from, with
the inverse temperature $\beta_i = 1/T_i \in [0,1]$ controlling the degree of smoothing.
Distributions with small $\beta$ values are easier to sample from as they
exhibit greater ergodicity.

After performing $k$ Gibbs steps for each of the $M$ intermediate distributions,
cross-temperature state swaps are proposed between neighboring chains using a
Metropolis-Hastings-based swap acceptance criterion. If we denote by $\x_i$ the joint state (visible
and hidden) of the $i$-th chain, the swap acceptance ratio $r_i$ for swapping
chains ($i$,$i+1$) is given by:
\begin{align}
    r_i &= \max(1, \frac {p_{\beta_i}(\x_{i+1})p_{\beta_{i+1}}(\x_i)} 
                     {p_{\beta_i}(\x_i)p_{\beta_{i+1}}(\x_{i+1})})
\end{align}

Although one might reduce variance by using free-energies to compute swap
ratios, we prefer using energies as the above factorizes nicely into the
following expression:
\begin{align}
r_i = \exp((\beta_i - \beta_{i+1}) \cdot (\Egy(\x_i) - \Egy(\x_{i+1}))),
\end{align}

While many swapping schedules are possible, we use the Deterministic Even Odd
algorithm (DEO) \cite{Lingenheil200980}, described below.

\subsection{Return Time and Optimal Temperatures}
\label{sec:returntime}

Conventional wisdom for choosing the optimal set $\T$ has relied on the
``flat histogram'' method which selects the parameters $\beta_i$ such that
the pair-wise swap ratio $r_i$ is constant and independent of the index $i$.
Under certain conditions (such as when sampling from multi-variate Gaussian
distributions), this can lead to a geometric spacing of the temperature
parameters \cite{Neal94b}. \cite{Behrens2010} has recently shown that geometric
spacing is actually optimal for a wider family of distributions characterized by
$\E_{\beta}(\Egy(x)) = K_1/\beta + K_2$, where $\E_\beta$ denotes
the expectation over inverse temperature and $K_1,K_2$ are arbitrary
constants. 

%% TODO: TRIPLE-CHECK THIS
Since this is clearly not the case for RBMs, we turn to the work of
\cite{Katzgraber06} who propose a novel measure for optimizing $\T$.
Their algorithm directly maximizes the ergodicity of the sampler by minimizing
the time taken for a particle to perform a round-trip between $\beta_1$ and
$\beta_M$. This is defined as the average ``return time'' $\tau_{rt}$. The
benefit of their method is striking: temperatures automatically pool around
phase transitions, causing spikes in local exchange rates and maximizing the
``flow'' of particles in temperature space.

The algorithm works as follows. For $N_s$ sampling updates:
\begin{itemize}
\item assign a label to each particle: those swapped into $\beta_1$ are labeled
      as ``up'' particles. Similarly, any ``up'' particle swapped into $\beta_M$
      becomes a ``down'' particle.
\item after each swap proposal, update the histograms $n_{u}(i), n_{d}(i)$,
      counting the number of ``up'' and ``down'' particles for the Markov chain associated with $\beta_i$.
  \item define $f_{up}(i) = \frac {n_{u}(i)} {n_{u}(i) + n_{d}(i)}$, the fraction of
      ``up''-moving particles at $\beta_i$. By construction, notice that
      $f_{up}(\beta_1)=1$ and $f_{up}(\beta_M) = 0$. $f_{up}$ thus defines a
      probability distribution of ``up'' particles in the range $[\beta_1,\beta_M]$.
\item The new inverse temperature parameters $\beta'$ are chosen as the ordered
    set which assigns equal probability mass to each chain. This yields
    an $f_{up}$ curve which is linear in the chain index.
\end{itemize}

The above procedure is applied iteratively, each time increasing $N_s$ so as to
fine-tune the $\beta_i$'s. To monitor return time, we can simply maintain a
counter $\tau_i$ for each particle $\x_i$, which is (1) incremented at every
sampling iteration and (2) reset to 0 whenever $\x_i$ has label ``down'' and is
swapped into $\beta_1$. A lower-bound for return time is then given by 
${\hat{\tau}_{rt}} = \sum_{i=0}^M \tau_i$.

% if sufficient amount of time/space left, discuss the equivalence of flat
% histogram method and minizing return time in the case of distributions where,
% at a given energy level E, the configuration space can be sampled easily. We
% are unfortunately dealing with the "broken ergodicity". In this setting
% measuring average swap rates doesn't even make sense.
% \cite{Nadler07} 

\subsection{Optimizing $\T$ while Learning}

\subsubsection{Online Beta Adaptation}

While the above algorithm exhibits the right properties, it is not very well
suited to the context of learning. When training an RBM, the distribution we are
sampling from is continuously changing. As such, one would expect the optimal
set $\T$ to evolve over time. We also do not have the luxury
of performing $N_s$ sampling steps after each gradient update. 

Our solution is simple: the histograms $n_u$ and $n_d$ are updated
using an exponential moving average, whose time constant is in the order of the
return time $\hat{\tau}_{rt}$. Using $\hat{\tau}_{rt}$ as the time constant is crucial as
it allows us to maintain flow statistics at the proper timescale. If an ``up''
particle reaches the $i$-th chain, we update $n_u(i)$ as follows:
\begin{align}
    n_u^{t+1}(i) = n_u^{t}(i) (1-1/{\hat{\tau}_{rt}^t}) +  1/{\hat{\tau}_{rt}^t},
\end{align}
where $\hat{\tau}_{rt}^t$ is the estimated return time at time $t$.

Using the above, we can estimate the set
of optimal inverse temperatures $\beta_i'$. Beta values are updated by
performing a step in the direction of the optimal value: 
$\beta_i^{t+1} = \beta_i^t + \mu (\beta_i' - \beta_i^t)$, where $\mu$ is a
learning rate on $\beta$. The properties of \cite{Katzgraber06} naturally
enforce the ordering constraint on the $\beta_i$'s.

\subsubsection{Choosing $M$ and $\beta_M$}

Another important point is that \cite{Katzgraber06} optimizes the set
$\T$ while keeping the bounds $\beta_1$ and $\beta_M$ fixed. While 
$\beta_1=1$ is a natural choice, we expect the optimal $\beta_M$ to vary during
learning. For this reason, we err on the side of caution and use $\beta_M=0$,
relying on a {\bf chain spawning process} to maintain sufficiently high swap
rates between neighboring parallel chains. Spawning chains as required by the
sampler should therefore result in increased stability, as well as computational
savings.

\cite{Lingenheil200980} performed an interesting study where they compared the
round trip rate $1/\tau_{rt}$ to the average swap rate measured across all
chains. They found that the DEO algorithm, which alternates between proposing
swaps between chains $\{(i,i+1); \forall\, {\rm even}\, i\}$ followed by $\{(i,i+1);
\forall\, {\rm odd}\, i\}$), gave rise to a concave function with a broad maximum around
an average swap rate of $\bar{r} = \sum_i{r_i} \approx 0.4$

Our temperature adaptation therefore works in two phases:
\begin{enumerate}
    \item The algorithm of Katzgraber et. al is used to optimize 
        $\{\beta_i; 1 < i < M\}$, for a fixed M.
    \item Periodically, a chain is spawned whenever $\bar{r} < \bar{r}_{min}$, 
          a hyper-parameter of the algorithm.
\end{enumerate}

Empirically, we have observed increased stability when the index $j$ of the new
chain is selected such that $j=\mathrm{argmax}_i (|f_{up}(i) - f_{up}(i+1)|)$,
$i \in [1,M-1]$. To avoid a long burn-in period, we initialize the new chain
with the state of the $(j+1)$-th chain and choose its inverse temperature as
the mean $(\beta_j + \beta_{j+1})/2$. A small but fixed burn-in period allows
the system to adapt to the new configuration.


\section{Results and Discussion}

We evaluate our adaptive SML-PT algorithm (SML-APT) on a
complex, synthetic dataset. This dataset is heavily inspired from the one used
in \cite{Desjardins+al-2010} and was specifically crafted to push the limits
of the algorithm. 

It is an online dataset of $28$x$28$ binary images, where each example is
sampled from a mixture model with probability density function $f_{X}(x) =
\sum_{m=1}^{5} w_m f_{Y_m}(x)$. Our dataset thus consists of $5$ mixture
components whose weights $w_m$ are sampled uniformly in the unit interval and
normalized to one. Each mixture component $Y_m$ is itself a random $28$x$28$
binary image, whose pixels are independent random variables having a probability
$p_m$ of being flipped. 
%small $p_m$ is akin to the variance parameter of Gaussian distributions. 
From the point of view of a sampler performing a
random walk in image space, $p_m$ is inversely proportional to the difficulty
of finding the mode in question. The complexity of our synthetic dataset comes
from our particular choice of $w_m$ and $p_m$.
\footnote{$w=[0.3314, 0.2262, 0.0812, 0.0254, 0.3358]$ and $p=[0.0001, 0.0137, 0.0215, 0.0223, 0.0544]$}
Large $w_m$ and small $p_m$ lead
to modes which are difficult to sample and in which a Gibbs sampler would tend
to get trapped. Large $p_m$ values on the other hand will tend to intercept "down" moving particles and thus present a challenge for parallel tempering.

Figure~\ref{fig:fig11} compares the results of training a $10$ hidden unit RBM, using
standard SML, SML-PT with $\{10,20,50\}$ parallel chains and our new SML-APT
algorithm. We performed $10^5$ updates (followed by $2 \cdot 10^4$ steps of
sampling) with mini-batches of size 5 and tested learning rates in
$\{10^{-3},10^{-4}\}$, $\beta$ learning rates in $\{10^{-3},10^{-4},10^{-5}\}$. For each
algorithm, we show the results for the best performing hyper-parameters,
averaging over $5$ different runs. Results are plotted with respect to
computation time to show the relative computational cost of each algorithm.

% results take from:
% /data/lisa6/desjagui/smlpt/nips10_workshop/exactll/id9/subset
% plotresults -dir . -type hdf5 -xnode nll -x t -y nll -fname outputs.h5 -filter rbm.n_beta spawn_beta adapt_beta
 
\begin{figure}[ht]
    \centering
    \hspace*{-1.0cm}
    \subfigure[Log-likelihood]
    {
        \label{fig:fig11}
        \includegraphics[scale=0.4]{fig1_4.png}
    }
    \hspace*{-.2cm}
    \subfigure[Return Time]
    {
        \label{fig:rtime}
        \includegraphics[scale=0.38]{rtime2.png}
    }
    \hspace*{-.5cm}
    \caption[] {(a) Comparison of training likelihood as a function of time for
    standard SML, SML-PT with 10/20/50 chains and the proposed SML-APT
    (initialized with 10 chains). SML-APT adapts the temperature set $\T =
    \{T_1,...,T_M ; T_1 < T_i < T_M \}$ to minimize round
    trip time between chains $T_1$ and $T_M$, and modifies the number of chains
    $M$ to maintain a minimal average swap rate. The resulting sampler exhibits
    greater ergodicity and yields better likelihood scores than standard SML
    and SML-PT, without requiring a careful hand-tuning of $\T$.
    (b) Average return time of each algorithm. SML-APT successfully minimizes
    this metric resulting in a return time similar to SML-PT 10, while still
    outperforming SML-PT $50$ in terms of likelihood.
    Errors bars represent standard error on the mean.
    \label{fig:fig1}}
\end{figure}

As we can see, standard SML fails to learn anything meaningful: the Gibbs
sampler is unable to cope with the loss in ergodicity and the model diverges.
SML-PT on the other hand performs much better. Using more parallel chains in
SML-PT consistently yields a better likelihood score, as well as reduced
variance. This seems to confirm that using more parallel chains in SML-PT increases the
ergodicity of the sampler. Finally, SML-APT outperforms all other methods. As
we will see in Figure~\ref{fig:fup}, it does so using only $20$ parallel
chains. Unfortunately, the computational cost seems similar to 50 parallel
chains. We hope this can be reduced to the same cost as SML-PT with $20$ chains
in the near future.
Also interesting to note, while the variance of all methods increase with
training time, SML-APT seems immune to this issue.

We now compare the various metrics being optimized by our adaptive algorithm.
Figure~\ref{fig:rtime} shows the average return time for each of the
algorithms. We can see that SML-APT achieves a return time which is comparable
to SML-PT with 10 chains, while achieving a better likelihood score than SML-PT
50. 

We now select the best performing seeds for SML-PT with 50 chains and SML-APT,
and show in Figure \ref{fig:fup}, the resulting $f_{up}(i)$ curves obtained at
the end of training.

\begin{figure}[ht]
    \centering
    \hspace*{-0.7cm}
    \subfigure[SML-APT]
    {
        \label{fig:fup1}
        \includegraphics[scale=0.35]{fup_adapted.pdf}
    }
    \hspace*{-.5cm}
    \subfigure[SML-PT 50]
    {
        \label{fig:fup2}
        \includegraphics[scale=0.35]{fup_fixed.pdf}
    }
    \hspace*{-.5cm}
    \caption[] {
    Return time is minimized by tagging each particle with a label: ``up'' if
    the particle visited $T_1$ more recently than $T_M$ and ``down'' otherwise.
    Histograms $n_u(i)$ and $n_d(i)$ track the number of up/down particles at
    each temperature $T_i$. Temperatures are modified such that the ratio
    $f_{up}(i) = n_u(i) / (n_u(i) + n_d(i))$ is linear in the index $i$.
    (a) $f_{up}$ curve obtained with SML-APT, as a function of temperature
    index (blue) and inverse temperature (red). SML-APT achieves a linear
    $f_{up}$ in the temperature index $i$.
    (b) Typical $f_{up}$ curve obtained with SML-PT (here using 50
    chains). $f_{up}$ is not linear in the index $i$, which translates to
    larger return times as shown in Fig.~\ref{fig:rtime}.
    \label{fig:fup}}
\end{figure}

The blue curve plots $f_{up}$ as a function of beta index, while the red curves
plots $f_{up}$ as a function of $\beta$. We can see that SML-APT results in a
more or less linear curve for $f_{up}(i)$, which is not the case for SML-PT. In
Figure \ref{fig:swapstat1} we can see the effect on the pair-wise swap
statistics $r_i$. As reported in \cite{Katzgraber06}, optimizing $\T$
to maintain a linear $f_{up}$ leads to temperatures pooling around the
bottleneck. In comparison, SML-PT fails to capture this phenomenon regardless of whether it uses $20$ or $50$ parallel chains (figures~\ref{fig:swapstat2}-\ref{fig:swapstat3}).

\begin{figure}[ht]
    \centering
    \hspace*{-0.8cm}
    \subfigure[SML-APT]
    {
        \label{fig:swapstat1}
        \includegraphics[scale=0.25]{swapstat_adapted.pdf}
    }
    \hspace*{-0.8cm}
    \subfigure[SML-PT 20]
    {
        \label{fig:swapstat2}
        \includegraphics[scale=0.25]{swapstat_fixed_20.pdf}
    }
    \hspace*{-0.8cm}
    \subfigure[SML-PT 50]
    {
        \label{fig:swapstat3}
        \includegraphics[scale=0.25]{swapstat_fixed.pdf}
    }
    \hspace*{-.7cm}
    \caption[] {
    Pairwise swap statistics obtained after $10^5$ updates. Minimizing return
    time causes SML-APT to pool temperatures around bottlenecks, achieving
    large swap rates (0.9) around bottenecks with relatively few chains.
    SML-PT on the other hand results in a much flatter distribution, requiring
    around 50 chains to reach swap rates close to 0.8.
    \label{fig:swapstat}}
\end{figure}

Finally, Figure~\ref{fig:betas} shows the evolution of the inverse temperature
parameters throughout learning. We can see that the position of the bottleneck
in temperature space changes with learning. As such, a manual tuning of
temperatures would be hopeless in achieving optimal return times.

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.4]{betas.pdf}
    \caption{Graphical depiction of the set $\{\beta_i; i \in [1,M]\}$, of
    inverse temperature parameters used by SML-APT during learning.
    Temperatures pool around a bottleneck to minimize return time, while new
    chains are spawned to maintain a given average swap rate.
    Note that the last $20$k updates actually correspond to a pure
    sampling phase (i.e. a learning rate of 0).
    \label{fig:betas}}
\end{figure}


\section{Conclusion}

We have introduced a new adaptive training algorithm for RBMs, which we call
Stochastic Maximum Likelihood with Adaptive Parallel Tempering (SML-APT). It
leverages the benefits of PT in the negative phase of SML, but adapts and
spawns new temperatures so as to minimize return time. The resulting negative
phase sampler thus exhibits greater ergodicity. Using a synthetic dataset, we
have shown that this can directly translate to a better and more stable
likelihood score. In the process, SML-APT also greatly reduces the number of
hyper-parameters to tune: temperature set selection is not only automated, but optimal.
The end-user is left with very few dials: a standard learning rate on $\beta_i$ and a minimum average swap rate $\bar{r}_{min}$ below which to spawn.

Much work still remains. In terms of computational cost,
we would like a model trained with SML-APT and resulting in $M$ chains, to
always be upper-bounded by SML-PT initialized with $M$ chains. Obviously, the
above experiments should also be repeated with larger RBMs on natural datasets,
such as MNIST or Caltech
Silhouettes.\footnote{http://people.cs.ubc.ca/~bmarlin/data/index.shtml}.

\subsubsection*{Acknowledgments}

The authors acknowledge the support of the following agencies for research
funding and computing support: NSERC, Compute Canada and CIFAR. We would also
like to thank the developers of Theano
\footnote{http://deeplearning.net/software/theano/}, for developing such a
powerful tool for scientific computing, especially gradient-based learning.

%\subsubsection*{References}

\nocite{Nadler07}

\small{
\bibliography{strings,strings-shorter,ml,aigaion-shorter}
\bibliographystyle{mlapa}
}

\end{document}



\documentclass[12pt]{article}

\usepackage{amsmath}
\usepackage{times}
\usepackage{graphicx}
\usepackage{color}
\usepackage{multirow}
%
\usepackage[round]{natbib}
%
\usepackage{rotating}
\usepackage{bbm}
\usepackage{latexsym}
\usepackage{main}

\usepackage[hidelinks]{hyperref}
\usepackage{url}
\usepackage{times}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{multicol}
\usepackage{booktabs}
\usepackage{resizegather}
\usepackage{caption}

%\DeclareGraphicsExtensions{.eps,.png}

%%% margins 
\textheight 23.4cm
\textwidth 14.65cm
\oddsidemargin 0.375in
\evensidemargin 0.375in
\topmargin  -0.55in
%
\renewcommand{\baselinestretch}{1}
%
\interfootnotelinepenalty=10000
%

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}

%\renewcommand{\thesubsubsection}{\arabic{section}.\arabic{subsubsection}}
\newcommand{\myparagraph}[1]{\ \\{\em #1}.\ \ }
\newcommand{\citealtt}[1]{\citeauthor{#1},\citeyear{#1}}
\newcommand{\myycite}[1]{\citep{#1}}

% Different font in captions
\newcommand{\captionfonts}{\normalsize}

\makeatletter  
\long\def\@makecaption#1#2{%
  \vskip\abovecaptionskip
  \sbox\@tempboxa{{\captionfonts #1: #2}}%
  \ifdim \wd\@tempboxa >\hsize
    {\captionfonts #1: #2\par}
  \else
    \hbox to\hsize{\hfil\box\@tempboxa\hfil}%
  \fi
  \vskip\belowcaptionskip}
\makeatother   
%%%%%

\renewcommand{\thefootnote}{\normalsize \arabic{footnote}} 	

\begin{document}
\hspace{13.9cm}1

\ \vspace{20mm}\\

\noindent{\LARGE Dynamic Neural Turing Machine with\\ Continuous and Discrete Addressing Schemes}

\ \\
{\bf \large Caglar Gulcehre$^{\displaystyle 1}$, Sarath Chandar$^{\displaystyle 1}$, Kyunghyun Cho$^{\displaystyle 2}$, Yoshua Bengio$^{\displaystyle 1}$}\\
{$^{\displaystyle 1}$University of Montreal, {\small \tt name.lastname@umontreal.ca}}\\
{$^{\displaystyle 2}$New York University, {\small \tt name.lastname@nyu.edu}}\\
%

%\ \\[-2mm]
{\bf Keywords:} neural networks, memory, neural Turing machines, natural language processing

\thispagestyle{empty}
\markboth{}{NC instructions}
%
\ \vspace{-0mm}\\
%
%Abstract
\begin{center} {\bf Abstract} \end{center}
We extend neural Turing machine (NTM) model into a {\it
    dynamic neural Turing machine} (D-NTM) by introducing a trainable memory
    addressing scheme. This addressing scheme maintains for each memory cell two separate
    vectors, {\it content} and {\it address} vectors. This allows the D-NTM to
    learn a wide variety of location-based addressing strategies including both
    linear and nonlinear ones. We implement the D-NTM with both continuous,
    differentiable and discrete, non-differentiable read/write mechanisms. We investigate
    the mechanisms and effects of learning to read and  write into a memory through experiments 
    on Facebook bAbI tasks using both a {\bf feedforward} and {\bf GRU}-controller. The D-NTM
    is evaluated on a set of Facebook bAbI tasks and shown to outperform NTM and LSTM baselines.
    We have done extensive analysis of our model and different variations of NTM on bAbI task.
    We also provide further experimental results on sequential $p$MNIST, Stanford Natural 
    Language Inference, associative recall and copy tasks.
%%%%%%%%%%%

\section{Introduction}

Designing of general-purpose learning algorithms is one of the long-standing goals
of artificial intelligence. Despite the success of deep learning in this area
(see, e.g., \citep{Goodfellow-et-al-2016-Book}) there are still a set of complex tasks
that are not well addressed by conventional neural network based models. Those tasks often
require a neural network to be equipped with an explicit, external memory in
which a larger, potentially unbounded, set of facts need to be stored. They
include, but are not limited to, episodic question-answering~\citep{weston2014memory,hermann2015teaching,hill2015goldilocks}, compact algorithms~\citep{zaremba2015learning}, dialogue \citep{serban2016building,vinyals2015neural} and video caption generation~\citep{yao2015capgenvid}.

Recently two promising approaches that are based on neural networks for this type of tasks
have been proposed. Memory networks~\citep{weston2014memory} explicitly store all
the facts, or information, available for each episode in an external memory (as
continuous vectors) and use the attention-based mechanism to index them when
returning an output. On the other hand, neural Turing
machines (NTM,~\citep{graves2014neural}) read each fact in an episode and decides
whether to read, write the fact or do both to the external, differentiable memory.

A crucial difference between these two models is that the memory network does
not have a mechanism to modify the content of the external memory, while the NTM
does. In practice, this leads to easier learning in the memory network, which in
turn resulted in that it being used more in realistic tasks~\citep{bordes2015large,dodge2015}.
On the contrary, the NTM has mainly been tested on a series of small-scale,
carefully-crafted tasks such as copy and associative recall. However, NTM is more expressive, precisely 
because it can store and modify the internal state of the network as 
it processes an episode and we were able to use it without any modifications on the model 
for different tasks.


The original NTM supports two modes of addressing (which can be used
simultaneously.) They are content-based and location-based addressing. We notice
that the location-based strategy is based on linear addressing. The distance
between each pair of consecutive memory cells is fixed to a constant. We address
this limitation, in this paper, by introducing a learnable address vector for
each memory cell of the NTM with least recently used memory addressing mechanism, 
and we call this variant a {\it dynamic neural Turing machine} (D-NTM).

We evaluate the proposed D-NTM on the full set of Facebook bAbI
task~\citep{weston2014memory} using either {\bf continuous}, differentiable attention or
{\bf discrete}, non-differentiable attention~\citep{rlntm} as an addressing strategy. Our
experiments reveal that it is possible to use the discrete, non-differentiable
attention mechanism, and in fact, the D-NTM with the discrete attention and GRU controller
outperforms the one with the continuous attention. %After we published our paper on arXiv, a new extension of NTM called DNC \citep{graves2016hybrid} has also provided results on  bAbI task as well.
We also provide results on sequential $p$MNIST, Stanford Natural Language Inference (SNLI)
task and algorithmic tasks proposed by \citep{graves2014neural} in order to 
investigate the ability of our model when dealing with long-term dependencies.

We summarize our contributions in this paper as below,

\begin{itemize}
    \item We propose a variation of neural Turing machine called a dynamic
        neural Turing machine (D-NTM) which employs a learnable and location-based
        addressing.
    \item We demonstrate the application of neural Turing machines on more natural
        and less toyish tasks, episodic question-answering, natural language entailment, digit classification 
        from the pixes besides the toy tasks. We provide a detailed analysis of our model on the bAbI task.
    \item We propose to use the discrete attention mechanism and empirically show
        that, it can outperform the continuous attention based addressing for episodic QA task.
    \item We propose a curriculum strategy for our model with the feedforward controller
    and discrete attention that improves our results significantly.
\end{itemize}

In this paper, we avoid doing architecture engineering for each task we work on and focus on pure model's overall performance on each without task-specific modifications on the model. In that respect, we mainly compare our model against similar models such as NTM and LSTM  without task-specific modifications. This helps us to better understand the model's failures.

The remainder of this article is organized as follows. In Section 2, we describe the architecture of Dynamic Neural Turing Machine (D-NTM). In Section 3, we describe the proposed addressing mechanism for D-NTM. Section 4 explains the training procedure. In Section 5, we briefly  discuss some related models. In Section 6, we report results on episodic question answering task. In Section 7, 8, and 9 we discuss the results in sequential MNIST, SNLI, and algorithmic toy tasks respectively. Section 10 concludes the article. 

\section{Dynamic Neural Turing Machine}

The proposed dynamic neural Turing machine (D-NTM) extends the neural Turing
machine (NTM, \citep{graves2014neural}) which has a modular design. The D-NTM
consists of two main modules: a controller, and a memory. The controller, which is
often implemented as a recurrent neural network, issues a command to the memory
so as to read, write to and erase a subset of memory cells. %Although the memory was originally envisioned as an integrated module, it is not necessary, and the memory may be an external, black box~\citep{rlntm}.

\subsection{Memory}

D-NTM consists of an external memory $\mM_t$, where each memory cell $i$ in $\mM_t[i]$ is partitioned into two parts: a trainable address vector $\mA_t[i] \in \RR^{1 \times d_a}$ and a content vector $\mC_t[i] \in \RR^{1 \times d_c}$.
\[
    \mM_t[i] = \left[ \mA_t[i] ; \mC_t[i] \right].
\]
Memory $\mM_t$ consists of $N$ such memory cells and hence represented by a rectangular matrix $\mM_t \in \RR^{N \times (d_c + d_a)}$:
\[
    \mM_t = \left[ \mA_t ; \mC_t \right].
\]
The first part $\mA_t \in \RR^{N \times d_a}$ is a learnable address matrix, and the second $\mC_t \in \RR^{N \times d_c}$ a content matrix. The address part $\mA_t$ is considered a model parameter that is updated during
training. During inference, the address part is not overwritten by the
controller and remains constant. On the other hand, the content part $\mC_t$ is
both read and written by the controller both during training and inference. At
the beginning of each episode, the content part of the memory is refreshed to be an
all-zero matrix, $\mC_0= \mathbf{0}$. This introduction of the learnable address portion for each memory cell allows the model to learn sophisticated location-based addressing strategies. 

\subsection{Controller}

At each timestep $t$, the controller (1) receives an input value $\vx_t$, (2)
addresses and reads the memory and creates the content vector $\vr_t$, (3) erases/writes a
portion of the memory, (4) updates its own hidden state $\vh_t$, and (5) outputs
a value $\vy_t$ (if needed.) In this paper, we use both a gated recurrent unit (GRU,
\citep{cho2014learning}) and a feedforward-controller to implement the controller such that for a GRU controller
%\begin{align}
%    \vh^t &= \text{GRU}(\vx^t, \vh^{t-1}, \vm^t)~\text{or for a feedforward-controller}, \\
%    \vh^t &= \sigmoid(\vx^t, \vm^t).
%\end{align}
\begin{align}
    \vh_t &= \text{GRU}(\vx_t, \vh_{t-1}, \vr_t) 
\end{align}
and for a feedforward-controller
\begin{align}
    \vh_t &= \sigmoid(\vx_t, \vr_t).
\end{align}

\subsection{Model Operation}

At each timestep $t$, the controller receives an input value $\vx_t$. Then it generates the read weights $\vw^r_t \in \RR^{N \times 1}$. By using the read weights $\vw^r_t$, the content vector read from the memory $\vr_t \in \RR^{(d_a + d_c) \times 1}$ is computed as
\begin{align}
    \label{eq:read_with_attention}
    \vr_t =  (\mM_{t})^{\top} \vw^r_t,
\end{align}
The hidden state of the controller ($\vh_t$) is conditioned on the memory content vector $\vr_t$ and based on this current hidden state of the controller. The model predicts the output label $\vy_t$ for the input. 

The controller also updates the memory by erasing the old content and writing a new content into the memory. The controller computes three vectors: erase vector $\ve_t \in \RR^{d_c \times 1}$, write weights $\vw^w_t \in \RR^{N \times 1}$, and candidate memory content vector $\bar{\vc}_t \in \RR^{d_c \times 1}$. These vectors are used to modify the memory. Erase vector is computed by a simple MLP which is conditioned on the hidden state of the controller $\vh_t$. The candidate memory content vector $\bar{\vc}_t$ is computed based on the current hidden state of the controller $\vh_{t} \in \RR^{d_h \times 1}$ and the input of the controller which is scaled by a scalar gate $\alpha_t$. The $\alpha_t$ is a function of the hidden state and the input of the controller. 
\begin{align}
       \alpha_t &= f(\vh_t, \vx_t), \\
        \label{eqn:cand_memory_content}
       \bar{\vc}_t &= \text{ReLU}(\mW_m \vh_t + \alpha_t \mW_x \vx_t).
\end{align}
where $\mW_m$ and $\mW_x$ are trainable matrices and $\text{ReLU}$ is the rectified linear activation function \citep{nair2010rectified}. Given the erase, write and candidate memory content vectors ($\ve_t$, $w_t^w$, and $\bar{\vc}_t$ respectively), the memory matrix is updated by,
\begin{align}
\label{eq:write_with_attention}
\mC_t[j] = (1 - \ve_t w^w_t[j]) \odot \mC_{t-1}[j] + w^w_t[j] \bar{\vc}_{t}.
\end{align}
where the index $j$ in $\mC_t[j]$ denotes the $j$-th row of the content matrix $\mC_t$ of the memory matrix $\mM_t$.

\paragraph{No Operation (NOP)}

As found in \citep{joulin2015inferring}, an additional NOP operation can be useful
for the controller {\it not} to access the memory only once in a while. We model
this situation by designating one memory cell as a NOP cell to which the controller should
access when it does not need to read or write into the memory. Because reading from or writing
into this memory cell is completely ignored. 

We illustrate and elaborate more on the read and write operations of the D-NTM in Figure \ref{fig_main}.

\begin{figure}[htbp]
\centering
\includegraphics[scale=0.50]{NTM_QA_Overview_last}
\caption{A graphical illustration of the proposed dynamic neural Turing machine with the 
    recurrent-controller.  The controller receives the fact as a
    continuous vector encoded by a recurrent neural network, computes the
    read and write weights for addressing the memory. If the D-NTM automatically
    detects that a query has been received, it returns an answer
    and terminates.}
\label{fig_main}
\end{figure}

The computation of the read $\vw^r_t$ and write vector $\vw^w_t$ are the most crucial parts of the model since the controller decide where to read from and write into the memory by using those. We elaborate this in the next section. 

\section{Addressing Mechanism}

Each of the address vectors (both read and write) is computed in similar
ways. First, the controller computes a key vector:
\[
    \vk_t = \mW^\top_k \vh_t + \vb_k,
\]
Both for the read and the write operations, $\vk_t \in \RR^{(d_a + d_c) \times 1}$. $\mW_k \in \RR^{(d_a+d_c) \times N}$ and $\vb_k \in \RR^{(d_a + d_c) \times 1}$ are the learnable weight matrix and bias respectively of $\vk_t$.  Also, the sharpening factor $\beta_t \in \RR \ge 1$ is computed as follows:

\begin{align}
    \beta_t &= \text{softplus}(\vu_{\beta}^{\top} \vh^t + b_{\beta}) + 1.
\end{align}
where $\vu_{\beta}$ and $b_{\beta}$ are the parameters of the sharpening factor $\beta_t$ and softplus is defined as follows:
\begin{align}
\text{softplus}(x) &= \text{log}(\text{exp}(x) + 1)
\end{align}
Given the key $\vk_t$ and sharpening factor $\beta_t$, the logits for the address weights are then computed by,
\begin{align}
 z_t[i] & = \beta^t S\left(\vk_t, \mM_t[i]\right)
\end{align}
where the similarity function is basically the cosine distance where it is defined as $S\left(\vx, \vy\right) \in \RR$ and $1 \ge S\left(\vx, \vy\right) \ge -1$,
\[
    S\left(\vx, \vy\right) = \frac{\vx \cdot \vy}{||\vx||||\vy|| + \epsilon}.
\]
$\epsilon$ is a small positive value to avoid division by zero. We have used $\epsilon=1e-7$ in all our experiments. The address weight generation which we have described in this section is same with the content based addressing mechanism proposed in \citep{graves2014neural}. 


\subsection{Dynamic Least Recently Used Addressing}

We introduce a memory addressing operation that can learn to put more emphasis on the 
least recently used~(LRU)~memory locations. As observed in \citep{santoro2016one, dmnew},
we find it easier to learn the write operations with the use of LRU addressing.

To learn a LRU based addressing, first we compute the exponentially moving averages of the logits~($\vz_t$) as~$\vv_t$, where it can be computed as~$\vv_t~=~0.1 \vv_{t-1} + 0.9 \vz_{t}$. We rescale the accumulated $\vv_t$ with $\gamma_t$, such that the controller adjusts the influence of how much previously written memory locations should effect the attention weights of a particular time-step. Next, we subtract $\vv_t$ from $\vz_t$ in order to reduce the weights of previously read or written memory locations. $\gamma_t$ is a shallow MLP with a scalar output and it is conditioned on the hidden state of the controller. $\gamma_t$ is parametrized with the parameters $\vu_{\gamma}$ and $\vb_{\gamma}$,
\begin{align}
       \gamma_t &= \text{sigmoid}(\vu_{\gamma}^{\top} \vh_t + \vb_{\gamma}),\\
       \vw_t &= \text{softmax}(\vz_t - \gamma_t \vv_{t-1}).  \label{eq:softmax}
\end{align}

This addressing method increases the weights of the least recently used rows of the memory. The magnitude of the influence of the least-recently used memory locations is being learned and adjusted with $\gamma_t$. Our LRU addressing is dynamic due to the model's ability to switch between pure content-based addressing and LRU. During the training, we do not backpropagate through $\vv_t$. Due to the dynamic nature of this addressing mechanism, it can be used for both read and write operations. If needed, the model will automatically learn to disable LRU while reading from the memory.

The address vector defined in Equation ~\eqref{eq:softmax} is a continuous vector. This makes the addressing operation differentiable and we refer to such a D-NTM as continuous D-NTM.

\subsection{Discrete Addressing}
\label{sec:gen_disc_add_vecs}

By definition in Eq.~\eqref{eq:softmax}, every element in the address
vector $\vw_t$ is positive and sums up to one. In other words, we can treat this vector as
the probabilities of a categorical distribution $\mathcal{C}(\vw_t)$ with $\dim(\vw_t)$ choices:
\[
    p[j] = \vw_t[j],
\]
where $\vw_t[j]$ is the $j$-th element of $\vw_t$. We can readily sample from this
categorical distribution and form an one-hot vector $\tilde{\vw}_t$ such that
\[
    \tilde{\vw}_t[k] = I(k=j),
\]
where $j \sim \mathcal{C}(\vw)$, and $I$ is an indicator function. If we use $\tilde{\vw}_t$ instead of $\vw_t$, then we will read and write from only one memory cell at a time. This makes the addressing operation non-differentiable and we refer to such a D-NTM as discrete D-NTM. In discrete D-NTM we sample the one-hot vector during training. Once training is over, we switch to a deterministic strategy. We simply choose
an element of $\vw_t$ with the largest value to be the index of the target memory
cell, such that
\[
    \tilde{\vw}_t[k] = \mathbf{I}(k=\text{argmax}(\vw_t)).
\]


\subsection{Multi-step Addressing}

At each time-step, controller may require more than one-step for accessing to the memory. 
The original NTM addresses this by implementing multiple sets of read, erase and write heads.
In this paper, we explore an option of allowing each head to operate more than once at each
timestep, similar to the multi-hop mechanism from the end-to-end memory network~\citep{sukhbaatarend}. 


\section{Training D-NTM}
Once the proposed D-NTM is executed, it returns the output distribution $p(\vy^{(n)}|\vx_1^{(n)}, \ldots, \vx_T^{(n)};~\TT)$ for the $n^{th}$ example that is parameterized with $\TT$. We define our cost function as the negative log-likelihood:
\begin{align}
    \label{eq:cost}
C(\theta) = -\frac{1}{N} \sum_{n=1}^N \log p(\vy^{(n)}|\vx_1^{(n)}, \ldots, \vx_T^{(n)};~\TT),
\end{align}
where $\TT$ is a set of all the parameters of the model. 

Continuous D-NTM, just like the original NTM, is fully end-to-end differentiable and hence we can compute the gradient of this cost function by using backpropagation and learn the parameters of the model with a gradient-based optimization algorithm, such as stochastic gradient descent, to train it end-to-end. However, in discrete D-NTM, we use sampling-based strategy for all the heads during training. This clearly makes the use of backpropagation infeasible to compute the gradient, as the sampling procedure is not differentiable. 


\subsection{Training discrete D-NTM}

To train discrete D-NTM, we use
REINFORCE~\citep{williams92} together with the three variance reduction
techniques--global baseline, input-dependent baseline and variance
normalization-- suggested in \citep{mnih2014neural}. 

Let us define $R(\vx)=\log p (\vy|\vx_1, \ldots, \vx_T; \TT)$ as a reward. We first center and re-scale the reward by,
\[
\tilde{R}(\vx) = \frac{R(\vx) - b}{\sqrt{\sigma^2 + \epsilon}},
\]
where $b$ and $\sigma$ is running average and standard deviation of $R$. We can further center it for each input $\vx$ separately, i.e., 
\[
\bar{R}(\vx) = \tilde{R}(\vx) - b(\vx),
\]
where $b(\vx)$ is computed by a baseline network which takes as input $\vx$ and predicts its estimated reward. The baseline network is trained to minimize the Huber loss~\citep{huber1964} between the true reward $\tilde{R}(\vx)$ and the predicted reward $b(\vx)$. This is also called as input based baseline (IBB) which is introduced in \citep{mnih2014neural}.

We use the Huber loss to learn the baseline $b(\vx)$ which is defined by,
\begin{align*}
    H_{\delta}(z) = 
    \begin{cases}
 {z^2}                   & \text{for } |z| \le \delta, \\
 \delta (2|z| - \delta), & \text{otherwise,}
\end{cases}
\end{align*}
due to its robustness where $z$ would be $\bar{R}(\vx)$ in this case.
As a further measure to reduce the variance, we regularize
the negative entropy of all those category distributions to facilitate a
better exploration during training~\citep{xu2015show}.

Then, the cost function for each training example is approximated as in Equation \eqref{eqn:reinforced_cost}. In this equation, we write the terms related to compute the REINFORCE gradients that includes terms for the entropy regularization on the action space, the likelihood-ratio term to compute the REINFORCE gradients both for the read and the write heads.
%\begin{gather*}
\begin{align*}
    C^n(\theta) =& -\log p(\vy|\vx_{1:T},
    \tilde{\vw}^r_{1:J}, \tilde{\vw}^w_{1:J}) \\
    & - \sum_{j=1}^J \bar{R}(\vx^n) (\log p(\tilde{\vw}^r_j|\vx_{1:T}) + \log p(\tilde{\vw}^w_j|\vx_{1:T})  \\
    & - \lambda_H \sum_{j=1}^J (\mathcal{H}(\vw^r_j|\vx_{1:T}) + \mathcal{H}(\vw^w_j|\vx_{1:T})).\numberthis \label{eqn:reinforced_cost}
\end{align*}
%\end{gather}

where $J$ is the number of addressing steps, $\lambda_H$ is the entropy regularization coefficient, and $\mathcal{H}$ denotes the entropy.

\subsection{Curriculum Learning for the Discrete Attention}
\label{sec:curr_disc_att}
Training discrete attention with feedforward controller and REINFORCE is challenging. We propose to use a curriculum strategy for training with the discrete attention in order to tackle this problem. For each minibatch, the controller stochastically decides to choose either to use the discrete or continuous weights based on the random variable $\pi_n$ with probability $p_n$ where $n$ stands for the number of $k$ minibatch updates such that we only update $p_n$ every $k$ minibatch updates. $\pi_n$ is a Bernoulli random variable which is sampled with probability of $p_n$, $\pi_n \sim \text{Bernoulli}(p_n)$. The model will either use the discrete or the continuous-attention based on the $\pi_n$. We start the training procedure with $p_0=1$ and during the training $p_n$ is annealed to $0$ by setting $p_n=\frac{p_0}{\sqrt{1+n}}$.

We can rewrite the weights $\vw_t$ as in Equation \eqref{eqn:curr_comb}, where it is expressed as the combination of continuous attention weights $\bar{\vw}_t$ and discrete attention weights $\tilde{\vw}_t$ with $\pi_t$ being a binary variable that chooses to use one of them,
\begin{equation}
    \label{eqn:curr_comb}
    \vw_t = \pi_n \bar{\vw}_t + (1-\pi_n) \tilde{\vw}_t.    
\end{equation}


By using this curriculum learning strategy, at the beginning of the training, the model learns to use the memory mainly with the continuous attention. As we anneal the $p^t$, the model will rely more on the discrete attention.


\subsection{Regularizing D-NTM}
\label{sec:reg_dntm}

If the controller of D-NTM is a recurrent neural network, we find it to be
important to regularize the training of the D-NTM so as to avoid suboptimal
solutions in which the D-NTM ignores the memory and works as a simple recurrent
neural network. 

\paragraph{Read-Write Consistency Regularizer}


One such suboptimal solution we have observed in our preliminary experiments
with the proposed D-NTM is that the D-NTM uses the address part $\mA$ of the
memory matrix simply as an additional weight matrix, rather than as a means to
accessing the content part $\mC$. We found that this pathological case can be
effectively avoided by encouraging the read head to point to a memory cell which
has also been pointed by the write head. This can be implemented as the
following regularization term:
\begin{align}
    R_{\text{rw}}(\vw^r, \vw^w) = \lambda \sum_{t'=1}^{T}||1 -
    (\frac{1}{t'}\sum_{t=1}^{t'}\vw^w_t)^{\top}\vw^r_{t'} ||_2^2
\end{align}

In the equations above, $\vw^w_t$ is the write and $\vw^r_{t}$ is the read weights.

\paragraph{Next Input Prediction as Regularization}

Temporal structure is a strong signal that should be exploited by the controller based on a recurrent neural network. We exploit this structure by letting the controller {\it predict} the input in the future. We maximize the predictability of the next input by the controller during training. This is equivalent to minimizing the following regularizer:
\[
R_{\text{pred}}(\mW) = -\sum_{t=0}^T\log p(\vx_{t+1}|\vx_t, \vw^r_t, \vw^w_t, \ve_t, \mM_t; \TT)
\]
where $\vx_t$ is the current input and $\vx_{t+1}$ is the input at the next timestep. We find this regularizer to be effective in our preliminary experiments and use it for bAbI tasks.

\section{Related Work}

A recurrent neural network (RNN), which is used as a controller in the proposed
D-NTM, has an implicit memory in the form of recurring hidden states. Even with
this implicit memory, a vanilla RNN is however known to have difficulties in
storing information for long time-spans~\citep{bengio1994learning,hochreiter1991untersuchungen}. Long short-term memory
(LSTM, \citep{lstm1997}) and gated recurrent units (GRU, \citep{cho2014learning})
have been found to address this issue. However all these models based solely on
RNNs have been found to be limited when they are used to solve, e.g.,
algorithmic tasks and episodic question-answering. 

In addition to the finite random access memory of the neural Turing machine,
based on which the D-NTM is designed, other data structures have been proposed
as external memory for neural networks. In \citep{sun1997neural,grefenstette2015learning,joulin2015inferring}, a continuous,
differentiable stack was proposed. In \citep{zaremba2015learning,rlntm}, grid and
tape storage are used. These approaches differ from the NTM in that their
memory is unbounded and can grow indefinitely. On the other hand, they are
often not randomly accessible. \cite{zhang2015structured} proposed a variation of NTM
that has a structured memory and they have shown experiments on copy and associative
recall tasks with this model.

In parallel to our work \citep{yang2016lie} and \citep{graves2016hybrid} proposed
new memory access mechanisms to improve NTM type of models. \citep{graves2016hybrid}
reported superior results on a diverse set of algorithmic learning tasks.


Memory networks~\citep{weston2014memory} form another family of neural networks
with external memory. In this class of neural networks, information is stored
explicitly as it is (in the form of its continuous representation) in the
memory, without being erased or modified during an episode. Memory networks and
their variants have been applied to various tasks
successfully~\citep{sukhbaatarend,bordes2015large,dodge2015,dmn2, chandar2016hierarchical}. \cite{keyval} 
have also independently proposed the idea of having separate key and value vectors for memory networks. A similar addressing mechanism is also explored in \citep{reed2015neural} in the context of learning program traces.


Another related family of models is the attention-based neural networks. Neural
networks with continuous or discrete attention over an input have shown promising results
on a variety of challenging tasks, including machine
translation~\citep{bahdanau2014neural,luong2015effective}, speech
recognition~\citep{chorowski2015attention}, machine reading
comprehension~\citep{hermann2015teaching} and image caption
generation~\citep{xu2015show}.  

The latter two, the memory network and attention-based networks, are however
clearly distinguishable from the D-NTM by the fact that they do not modify the
content of the memory.  

%\section{Experiments}

%We provide experimental results to demonstrate the abilities of our model, first on Facebook bAbI task~\citep{weston2015towards}. We give detailed analysis and experimental results on this task. We also compare different variations of NTM on bAbI tasks. We have performed experiments on sequential permuted MNIST~\citep{le2015simple} and on toy tasks to compare other published models on these tasks with a recurrent controller. The details of our experiments are provided in the supplementary material.

\section{Experiments on Episodic Question-Answering}

In this section, we evaluate the proposed D-NTM on the synthetic episodic
question-answering task called Facebook bAbI~\citep{weston2015towards}. We use
the version of the dataset that contains 10k training examples per sub-task provided by
Facebook.\footnote{
    \url{https://research.facebook.com/researchers/1543934539189348}
} For each episode, the D-NTM reads a sequence of factual sentences followed by a
question, all of which are given as natural language sentences. The D-NTM is
expected to store and retrieve relevant information in the memory in order to
answer the question based on the presented facts. 

\subsection{Model and Training Details}

We use the same hyperparameters for all the tasks for a given model. We use a recurrent neural network with GRU units to encode a variable-length
fact into a fixed-size vector representation. This allows the D-NTM to exploit
the word ordering in each fact, unlike when facts are encoded as bag-of-words
vectors. We experiment with both a recurrent and feedforward neural network as the controller that generates the read and write weights. The controller has 180 units.
We train our feedforward controller using noisy-tanh activation function~\citep{gulcehre2016noisy} since we were experiencing training difficulties with $\text{sigmoid}$ and $\text{tanh}$ activation functions. We use both single-step and three-steps addressing with our GRU controller. The memory contains 120 memory cells. Each memory cell consists of a 16-dimensional address part and 28-dimensional content part.

We set aside a random $10\%$ of the training examples as a validation set for each
sub-task and use it for early-stopping and hyperparameter search. We train one
D-NTM for each sub-task, using Adam~\citep{adam} with its learning rate set to
$0.003$ and $0.007$ respectively for GRU and feedforward controller. The size of each 
minibatch is 160, and each minibatch is constructed uniform-randomly from the training set. 


\subsection{Goals}

The goal of this experiment is three-fold. First, we present for the first time the performance of a memory-based network that can {\it both} read and write dynamically on the Facebook bAbI tasks\footnote{Similar experiments were done in the recently published \citep{graves2016hybrid}, but D-NTM results for bAbI tasks were already available in arxiv by that time.}. We aim to understand whether a model that has to learn to write an incoming fact to the memory, rather than storing it as it is, is able to work well, and to do so, we compare both the original NTM and proposed D-NTM against an LSTM-RNN. 

Second, we investigate the effect of having to learn how to write. The fact that the NTM needs to learn to write likely has adverse effect on the overall performance, when compared to, for instance, end-to-end memory networks (MemN2N, \citep{sukhbaatarend}) and dynamic memory network (DMN+, \citep{dmn2}) both of which simply store the incoming facts as they are. We quantify this effect in this experiment. Lastly, we show the effect of the proposed learnable addressing scheme. 

We further explore the effect of using a feedforward controller instead of the GRU controller. In addition to the explicit memory, the GRU controller can use its own internal hidden state as the memory. On the other hand, the feedforward controller must solely rely on the explicit memory, as it is the only memory available.


\begin{table*}[htbp]
\vspace{-2mm}
  \centering
  \tiny 
\begin{tabular}{ | l || c | c | c || c |c |c|c|| c | c | c|c| }
\hline
& & & & 1-step & 1-step & 1-step & 1-step & 3-steps & 3-steps & 3-steps & 3-steps \\ 
& & & & LBA$^{\ast}$ & CBA & Soft & Discrete & LBA$^{\ast}$ & CBA & Soft & Discrete\\
Task & LSTM & MemN2N & DMN+ & NTM & NTM & D-NTM & D-NTM  & NTM & NTM & D-NTM & D-NTM \\ \hline

1   &   0.00    &   0.00    &   0.00    &   16.30   &   16.88   &   5.41    &   6.66    &   0.00    &   0.00    &   0.00    &   0.00\\   
2   &   81.90   &   0.30    &   0.30    &   57.08   &   55.70   &   58.54   &   56.04   &   61.67   &   59.38   &   46.66   &   62.29 \\ 
3   &   83.10   &   2.10    &   1.10    &   74.16   &   55.00   &   74.58   &   72.08   &   83.54   &   65.21   &   47.08   &   41.45  \\
4   &   0.20    &   0.00    &   0.00    &   0.00    &   0.00    &   0.00    &   0.00    &   0.00    &   0.00    &   0.00    &   0.00   \\
5   &   1.20    &   0.80    &   0.50    &   1.46    &   20.41   &   1.66    &   1.04    &   0.83    &   1.46    &   1.25    &   1.45   \\
6   &   51.80   &   0.10    &   0.00    &   23.33   &   21.04   &   40.20   &   44.79   &   48.13   &   54.80   &   20.62   &   11.04  \\
7   &   24.90   &   2.00    &   2.40    &   21.67   &   21.67   &   19.16   &   19.58   &   7.92    &   37.70   &   7.29    &   5.62   \\
8   &   34.10   &   0.90    &   0.00    &   25.76   &   21.05   &   12.58   &   18.46   &   25.38   &   8.82    &   11.02   &   0.74   \\
9   &   20.20   &   0.30    &   0.00    &   24.79   &   24.17   &   36.66   &   34.37   &   37.80   &   0.00    &   39.37   &   32.50  \\
10  &   30.10   &   0.00    &   0.00    &   41.46   &   33.13   &   52.29   &   50.83   &   56.25   &   23.75   &   20.00   &   20.83  \\
11  &   10.30   &   0.10    &   0.00    &   18.96   &   31.88   &   31.45   &   4.16    &   3.96    &   0.28    &   30.62   &   16.87  \\
12  &   23.40   &   0.00    &   0.00    &   25.83   &   30.00   &   7.70    &   6.66    &   28.75   &   23.75   &   5.41    &   4.58   \\
13  &   6.10    &   0.00    &   0.00    &   6.67    &   5.63    &   5.62    &   2.29    &   5.83    &   83.13   &   7.91    &   5.00   \\
14  &   81.00   &   0.10    &   0.20    &   58.54   &   59.17   &   60.00   &   63.75   &   61.88   &   57.71   &   58.12   &   60.20  \\
15  &   78.70   &   0.00    &   0.00    &   36.46   &   42.30   &   36.87   &   39.27   &   35.62   &   21.88   &   36.04   &   40.26  \\
16  &   51.90   &   51.80   &   45.30   &   71.15   &   71.15   &   49.16   &   51.35   &   46.15   &   50.00   &   46.04   &   45.41  \\
17  &   50.10   &   18.60   &   4.20    &   43.75   &   43.75   &   17.91   &   16.04   &   43.75   &   56.25   &   21.25   &   9.16   \\
18  &   6.80    &   5.30    &   2.10    &   3.96    &   47.50   &   3.95    &   3.54    &   47.50   &   47.50   &   6.87    &   1.66   \\
19  &   90.30   &   2.30    &   0.00    &   75.89   &   71.51   &   73.74   &   64.63   &   61.56   &   63.65   &   75.88   &   76.66  \\
20  &   2.10    &   0.00    &   0.00    &   1.25    &   0.00    &   2.70    &   3.12    &   0.40    &   0.00    &   3.33    &   0.00   \\\hline



Avg.Err. & 36.41 & 4.24 & \textbf{2.81} & 31.42 & 33.60 & 29.51 & \textbf{27.93} & 32.85 & 32.76 & 24.24 & \textbf{21.79} \\\hline
\end{tabular}
\caption{Test error rates (\%) on the 20 bAbI QA tasks for models using 10k training examples with the GRU and feedforward controller. FF stands for the experiments that are conducted with feedforward controller. Let us, note that LBA$^{\ast}$ refers to NTM that uses both LBA and CBA. In this table, we compare multi-step vs single-step addressing, original NTM with location based+content based addressing vs only content based addressing, and discrete vs continuous addressing on bAbI.}
\label{app:babi_10k_soft}
\end{table*}


\subsection{Results and Analysis}

In Table~\ref{app:babi_10k_soft}, we first observe that the NTMs are indeed capable of solving this type of episodic question-answering better than the vanilla LSTM-RNN. Although the availability of explicit memory in the NTM has already suggested this result, we note that this is the first time neural Turing machines have been used in this specific task.

All the variants of NTM with the GRU controller outperform the vanilla LSTM-RNN. However, not all of them perform equally well. First, it is clear that the proposed dynamic NTM (D-NTM) using the GRU controller outperforms the original NTM with the GRU controller (NTM, CBA only NTM vs. continuous D-NTM, Discrete D-NTM). As discussed earlier, the learnable addressing scheme of the D-NTM allows the controller to access the memory slots by location in a potentially nonlinear way. We expect it to help with tasks that have non-trivial access patterns, and as anticipated, we see a large gain with the D-NTM over the original NTM in the tasks of, for instance, 12 - Conjunction and 17 - Positional Reasoning. 

Among the recurrent variants of the proposed D-NTM, we notice significant improvements by using discrete addressing over using continuous addressing. We conjecture that this is due to certain types of tasks that require precise/sharp retrieval of a stored fact, in which case continuous addressing is in disadvantage over discrete addressing. This is evident from the observation that the D-NTM with discrete addressing significantly outperforms that with continuous addressing in the tasks of 8 - Lists/Sets and 11 - Basic Coreference. Furthermore, this is in line with an earlier observation in \citep{xu2015show}, where discrete addressing was found to generalize better in the task of image caption generation. 

In Table~\ref{app:babi_10k_soft2}, we also observe that the D-NTM with the feedforward controller and discrete attention performs worse than LSTM and D-NTM with continuous-attention. However, when the proposed curriculum strategy from Sec.~\ref{sec:gen_disc_add_vecs} is used, the average test error drops from 68.30 to 37.79.

We empirically found training of the feedforward controller more difficult than that of the recurrent controller. We train our feedforward controller based models four times longer (in terms of the number of updates) than the recurrent controller based ones in order to ensure that they are converged for most of the tasks. On the other hand, the models trained with the GRU controller overfit on bAbI tasks very quickly. For example, on tasks 3 and 16 the feedforward controller based model underfits~(i.e., high training loss) at the end of the training, whereas with the same number of units the model with the GRU controller can overfit on those tasks after 3,000 updates only.

We notice a significant performance gap, when our results are compared to the variants of the memory network~\citep{weston2014memory} (MemN2N and DMN+). We attribute this gap to the difficulty in learning to manipulate and store a complex input. 

\cite{graves2016hybrid} also has also reported results with differentiable neural computer (DNC) and NTM on bAbI dataset. However their experimental setup is different from the setup we use in this paper. This makes the comparisons between more difficult. The main differences broadly are, as the input representations to the controller, they used the embedding representation of each word whereas we have used the representation obtained with GRU for each fact. Secondly, they report only joint training results. However, we have only trained our models on the individual tasks separately. However, despite the differences in terms of architecture in DNC paper (see Table 1), the mean results of their NTM results is very close to ours 28.5\% with std of +/- 2.9 which we obtain 31.4\% error.

\begin{table*}[htbp]
\vspace{-2mm}
  \centering
  \tiny 
\begin{tabular}{ |c|c|c|c| }
\hline
& FF & FF & FF\\ 
& Soft & Discrete & Discrete$^*$\\
Task &  D-NTM & D-NTM & D-NTM\\ \hline
1   &   4.38    &   81.67   &   14.79\\
2   &   27.5    &   76.67   &   76.67\\
3   &   71.25   &   79.38   &   70.83\\
4   &   0.00    &   78.65   &   44.06\\
5   &   1.67    &   83.13   &   17.71\\
6   &   1.46    &   48.76   &   48.13\\
7   &   6.04    &   54.79   &   23.54\\
8   &   1.70    &   69.75   &   35.62\\
9   &   0.63    &   39.17   &   14.38\\
10  &   19.80   &   56.25   &   56.25\\
11  &   0.00    &   78.96   &   39.58\\
12  &   6.25    &   82.5    &   32.08\\
13  &   7.5     &   75.0    &   18.54\\
14  &   17.5    &   78.75   &   24.79\\
15  &   0.0     &   71.42   &   39.73\\
16  &   49.65   &   71.46   &   71.15\\
17  &   1.25    &   43.75   &   43.75\\
18  &   0.24    &   48.13   &   2.92\\
19  &   39.47   &   71.46   &   71.56\\
20  &   0.0     &   76.56   &   9.79\\\hline
Avg.Err. & \textbf{12.81} & 68.30 & 37.79\\\hline
\end{tabular}
\caption{Test error rates (\%) on the 20 bAbI QA tasks for models using 10k training examples with feedforward controller.}
\label{app:babi_10k_soft2}
\end{table*}


\subsection{Visualization of Discrete Attention}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.85\textwidth]{ntm_grucontroller_task20_ntm_ex5_read_write.png}
\caption{An example view of the discrete attention over the memory slots for both read (left)  and write heads(right). x-axis the denotes the memory locations that are being accessed and y-axis corresponds to the content in the particular memory location. In this figure, we visualize the discrete-attention model with 3 reading steps and on task 20. It is easy to see that the NTM with discrete-attention accesses to the relevant part of the memory. We only visualize the last-step of the three steps for writing. Because with discrete attention usually the model just reads the empty slots of the memory.}
\label{fig:ntm_hard_att}

\end{figure}

We visualize the attention of D-NTM with GRU controller with discrete attention in Figure \ref{fig:ntm_hard_att}. From this example, we can see that D-NTM has learned to find the correct supporting fact even without any supervision for the particular story in the visualization. 

\subsection{Learning Curves for the Recurrent Controller}
\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\linewidth]{learning_curves_hardvssoft.pdf}
\caption{A visualization for the learning curves of continuous and discrete D-NTM models trained on Task 1 using 3 steps. In most tasks, we observe that the discrete attention model with GRU controller does converge faster than the continuous-attention model.}
\label{fig:ntm_learn_curves}
\end{figure}

In Figure \ref{fig:ntm_learn_curves}, we compare the learning curves of the continuous and discrete attention D-NTM model with recurrent controller on Task 1. Surprisingly, the discrete attention D-NTM converges faster than the continuous-attention model. The main difficulty of learning continuous-attention is due to the fact that learning to write with continuous-attention can be challenging.


\subsection{Training with Continuous Attention and Testing with Discrete Attention}

In Table \ref{app:babi_10k_soft_ff_hard_test}, we provide results to investigate the effects of using discrete attention model at the test-time for a model trained with feedforward controller and continuous attention. Discrete$^{\ast}$ D-NTM model bootstraps the discrete attention with the continuous attention, using the curriculum method that we have introduced in Section \ref{sec:curr_disc_att}. Discrete$^{\dagger}$ D-NTM model is the continuous-attention model which uses discrete-attention at the test time. We observe that the Discrete$^{\dagger}$ D-NTM model which is trained with continuous-attention outperforms Discrete D-NTM model.

\begin{table*}[htbp]
\vspace{-2mm}
  \centering
  \footnotesize 
\begin{tabular}{ | l || c |c | c | c | c |}
\hline
&  continuous & Discrete & Discrete$^\ast$ & Discrete$^\dagger$  \\
Task & D-NTM & D-NTM  & D-NTM & D-NTM\\ \hline

1 &4.38 & 81.67 & 14.79 & 72.28\\
2 & 27.5 & 76.67 & 76.67 & 81.67 \\
3 & 71.25 & 79.38 & 70.83 & 78.95 \\
4 &0.00 & 78.65 & 44.06 & 79.69 \\
5 &1.67 & 83.13 & 17.71 & 68.54 \\
6 & 1.46 & 48.76 & 48.13 & 31.67 \\
7 & 6.04 & 54.79 & 23.54 & 49.17 \\
8 & 1.70 & 69.75 & 35.62 & 79.32 \\
9 & 0.63 & 39.17 & 14.38 & 37.71 \\
10 & 19.80 & 56.25 & 56.25 & 25.63 \\
11 & 0.00 & 78.96 & 39.58 & 82.08 \\
12 & 6.25 & 82.5 & 32.08 & 74.38 \\
13  & 7.5 & 75.0 & 18.54 & 47.08 \\
14 & 17.5 & 78.75 & 24.79 & 77.08 \\
15 & 0.0 & 71.42 & 39.73 & 73.96 \\
16 & 49.65 & 71.46 & 71.15 & 53.02 \\
17  & 1.25 & 43.75 & 43.75 & 30.42 \\
18 & 0.24 & 48.13 & 2.92 & 11.46\\
19 & 39.47 & 71.46 & 71.56 & 76.05\\
20 & 0.0 & 76.56 & 9.79 & 13.96 \\\hline
Avg & \textbf{12.81} & 68.30 & 37.79 & 57.21 \\\hline

\end{tabular}
\caption{Test error rates (\%) on the 20 bAbI QA tasks for models using 10k training examples with the feedforward controller. Discrete$^{\ast}$ D-NTM model bootstraps the discrete attention with the continuous attention, using the curriculum method that we have introduced in Section \ref{sec:gen_disc_add_vecs}. Discrete$^{\dagger}$ D-NTM model is the continuous-attention model which uses discrete-attention at the test time.}
\label{app:babi_10k_soft_ff_hard_test}
\end{table*}

\subsection{D-NTM with BoW Fact Representation}
In Table \ref{app:babi_10k_soft_bow}, we provide results for D-NTM using BoW with positional encoding~(PE) \cite{sukhbaatarend} as the representation of the input facts. The facts representations are provided as an input to the GRU controller. In agreement to our results with the 
GRU fact representation, with the BoW fact representation we observe improvements with multi-step of addressing over single-step and discrete addressing over continuous addressing.

\begin{table*}[htbp]
\vspace{-2mm}
  \centering
  \footnotesize 
\begin{tabular}{ | l || c |c | c | c | c |}
\hline
&  Soft & Discrete & Soft & Discrete  \\
Task & D-NTM(1-step) & D-NTM(1-step)  & D-NTM(3-steps) & D-NTM(3-steps)\\ \hline

1 & 0.00 & 0.00 & 0.00 & 0.00\\
2 & 61.04 & 59.37 & 56.87 & 55.62 \\
3 & 55.62 & 57.5 & 62.5 & 57.5 \\
4 & 27.29 & 24.89 & 26.45 & 27.08 \\
5 & 13.55 & 12.08 & 15.83 & 14.78 \\
6 & 13.54 & 14.37 & 21.87 & 13.33 \\
7 & 8.54 & 6.25 & 8.75 & 14.58 \\
8 & 1.69 & 1.36 & 3.01 & 3.02 \\
9 & 17.7 & 16.66 & 37.70 & 17.08 \\
10 & 26.04 & 27.08 & 26.87 & 23.95 \\
11 & 20.41 & 3.95 & 2.5 & 2.29 \\
12 & 0.41 & 0.83 & 0.20 & 4.16 \\
13  & 3.12 & 1.04 & 4.79 & 5.83 \\
14 & 62.08 & 58.33 & 61.25 & 60.62 \\
15 & 31.66 & 26.25 & 0.62 & 0.05 \\
16 & 54.47 & 48.54 & 48.95 & 48.95 \\
17  & 43.75 & 31.87 & 43.75 & 30.62 \\
18 & 33.75 & 39.37 & 36.66 & 36.04\\
19 & 64.63 & 69.21 & 67.23 & 65.46\\
20 & 1.25 & 0.00 & 1.45 & 0.00 \\\hline
Avg & 27.02 & 24.98 & 26.36 & {\bf 24.05} \\\hline

\end{tabular}
\caption{Test error rates (\%) on the 20 bAbI QA tasks for models using 10k training examples with the GRU controller and representations of facts are obtained with BoW using positional encoding.}
\label{app:babi_10k_soft_bow}
\end{table*}

\section{Experiments on Sequential $p$MNIST}


In sequential MNIST task, the pixels of the MNIST digits are provided to the model in scan line order, left to right and top to bottom \citep{le2015simple}. At the end of sequence of pixels, the model predicts the label of the digit in the sequence of pixels. We experiment D-NTM on the variation of sequential MNIST where the order of the pixels is randomly shuffled, we call this task as permuted MNIST ($p$MNIST). An important contribution of this task to our paper, in particular, is to measure the model's ability to perform well when dealing with long-term dependencies. We report our results in Table \ref{tbl:pmnist_dntm_comp}, we observe improvements over other models that we compare against. In Table \ref{tbl:pmnist_dntm_comp}, "discrete addressing with MAB" refers to D-NTM model using REINFORCE with baseline computed from moving averages of the reward. Discrete addressing with IB refers to D-NTM using REINFORCE with input-based baseline.

\begin{table}[htbp]
\centering
\footnotesize
\begin{tabular}{@{}ll@{}}
\toprule
                                                    & Test  Acc \\ \midrule
D-NTM discrete MAB     & 89.6   \\
D-NTM discrete  IB      &  92.3  \\ % TODO: (still running), update this result
Soft D-NTM                          & \textbf{93.4}           \\
NTM                                               & 90.9          \\\midrule
I-RNN~\citep{le2015simple}                                & 82.0           \\
Zoneout~\citep{krueger2016zoneout}                                             & 93.1           \\
LSTM~\citep{krueger2016zoneout}                                               & 89.8           \\
Unitary-RNN~\citep{arjovsky2015unitary}                  & 91.4           \\
Recurrent Dropout~\citep{krueger2016zoneout}                                   & 92.5           \\ 
Recurrent Batch Normalization~\citep{cooijmans2016recurrent}    & 95.6 \\ \bottomrule
\end{tabular}
\caption{ Sequential $p$MNIST.}
\label{tbl:pmnist_dntm_comp}

\end{table}


In Figure \ref{fig:dntm_reinforce_baselines_cmp}, we show the learning curves of input-based-baseline (ibb) and regular REINFORCE with moving averages baseline (mab) on the $p$MNIST task. We observe that input-based-baseline in general is much easier to optimize and converges faster as well. But it can quickly overfit to the task as well. Let us note that, recurrent batch normalization with LSTM \citep{cooijmans2016recurrent} with 95.6\% accuracy and it performs much better than other algorithms. However, it is possible to use recurrent batch normalization in our model and potentially improve our results on this task as well.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\linewidth]{train_valid_costs_cmp_hard.pdf}
\caption{We compare the learning curves of our D-NTM model using discrete attention on $p$MNIST task with input-based baseline and regular REINFORCE baseline. The x-axis is the loss and y-axis is the number of epochs.}
\label{fig:dntm_reinforce_baselines_cmp}
\end{figure}

In all our experiments on sequential MNIST task, we try to keep the capacity of our model to be close to our baselines. We use  100 GRU units in the controller and each content vector of size 8 and with address vectors of size 8. We use a learning rate of $1e-3$ and trained the model with Adam optimizer. We did not use the read and write consistency regularization in any of our models.

\section{Stanford Natural Language Inference~(SNLI) Task}

SNLI task \citep{bowman2015large} is designed to test the abilities of different machine learning algorithms for inferring the entailment between two different statements. Those two statements, can either entail, contradict or be neutral to each other. In this paper, we feed the premise followed by the end of premise~(EOP) token and the hypothesis in the same sequence as an input to the model. Similarly \cite{rocktaschel2015reasoning} have trained their model by providing the premise and the hypothesis in a similar way. This ensures that the performance of our model does not rely only on a particular preprocessing or architectural engineering. But rather we mainly rely on the model's ability to represent the sequence and the dependencies in the input sequence efficiently. The model proposed by \cite{rocktaschel2015reasoning}, applies attention over its previous hidden states over premise when it reads the hypothesis.

In Table \ref{tbl:snli_ntm_res}, we report results for different models with or without recurrent dropout \citep{semeniuta2016recurrent} and layer normalization \citep{ba2016layer}.

The number of input vocabulary we use in our paper is $41200$, we use GLOVE~\citep{pennington2014glove} embeddings to initialize the input embeddings. We use GRU-controller with 300 units and the size of the embeddings are also 300. We optimize our models with Adam. We have done a hyperparameter search to find the optimal learning rate via random search and sampling the learning rate from log-space between $1e-2$ and $1e-4$ for each model. We use layer-normalization in our controller \citep{ba2016layer}. 

We have observed significant improvements by using layer normalization and dropout on this task. Mainly because that the overfitting is a severe problem on SNLI. D-NTM achieves better performance compared to both LSTM and NTMs.

\begin{table}[htbp]
\centering
\footnotesize
\begin{tabular}{@{}ll@{}}
\toprule
                                                    & Test  Acc \\ \midrule
                                                                \hline
            Word by Word Attention\citep{rocktaschel2015reasoning} & \textbf{83.5} \\
            Word by Word Attention two-way\citep{rocktaschel2015reasoning} & 83.2 \\
            \hline
            LSTM + LayerNorm + Dropout & 81.7 \\
            NTM + LayerNorm + Dropout & 81.8 \\
            DNTM + LayerNorm + Dropout & \textbf{82.3} \\
            LSTM \citep{bowman2015large} & 77.6 \\
            D-NTM & 80.9 \\
            NTM & 80.2 \\ \bottomrule
\end{tabular}
\caption{ Stanford Natural Language Inference Task}
\label{tbl:snli_ntm_res}

\end{table}



\section{NTM Toy Tasks}

We explore the possibility of using D-NTM to solve algorithmic tasks such as copy and associative recall tasks. We train our model on the same lengths of sequences that is experimented in \citep{graves2014neural}. We report our results in Table \ref{tbl:ntm_toy_tasks}. We find out that D-NTM using continuous-attention can successfully learn the "Copy" and "Associative Recall" tasks.

In Table \ref{tbl:ntm_toy_tasks}, we train our model on sequences of the same length as the experiments in \citep{graves2014neural} and test the model on the sequences of the maximum length seen during the training. We consider a model to be successful on copy or associative recall if its validation cost (binary cross-entropy) is lower than $0.02$ over the sequences of maximum length seen during the training. We set the threshold to $0.02$ to determine whether a model is successful on a task. Because empirically we observe that the models have higher validation costs perform badly in terms of generalization over the longer sequences. "D-NTM discrete" model in this table is trained with REINFORCE using moving averages to estimate the baseline.

\begin{table}[ht!]

\centering
\footnotesize
\begin{tabular}{@{}lll@{}}
\toprule
           & Copy Tasks & Associative Recall \\ \midrule
Soft D-NTM & Success    & Success            \\
D-NTM discrete & Success    & Failure            \\
NTM        & Success     & Success            \\ \bottomrule
\end{tabular}
\caption{NTM Toy Tasks.}
\label{tbl:ntm_toy_tasks}
\end{table}

On both copy and associative recall tasks, we try to keep the capacity of our model to be close to our baselines. We use 100 GRU units in the controller and each content vector of has a size of 8 and using  address vector of size 8. We use a learning rate of $1e-3$ and trained the model with Adam optimizer. We did not use the read and write consistency regularization in any of our models. For the model with the discrete attention we use REINFORCE with baseline computed using moving averages.

\section{Conclusion and Future Work}

In this paper we extend neural Turing machines (NTM) by introducing a learnable addressing scheme which allows the NTM to be capable of performing highly nonlinear location-based addressing. This extension, to which we refer by dynamic NTM (D-NTM), is extensively tested with various configurations, including different addressing mechanisms (continuous vs. discrete) and different number of addressing steps, on the Facebook bAbI tasks. This is the first time an NTM-type model was tested on this task, and we observe that the NTM, especially the proposed D-NTM, performs better than vanilla LSTM-RNN. Furthermore, the experiments revealed that the discrete, discrete addressing works better than the continuous addressing with the GRU controller, and our analysis reveals that this is the case when the task requires precise retrieval of memory content. 

Our experiments show that the NTM-based models can be weaker than other variants of memory networks which do not learn but have an explicit mechanism of storing incoming facts as they are. We conjecture that this is due to the difficulty in learning how to write, manipulate and delete the content of memory. Despite this difficulty, we find the NTM-based approach, such as the proposed D-NTM, to be a better, future-proof approach, because it can scale to a much longer horizon (where it becomes impossible to explicitly store all the experiences.)

On $p$MNIST task, we show that our model can outperform other similar type of approaches proposed to deal with the long-term dependencies. On copy and associative recall tasks, we show that our model can solve the algorithmic problems that are proposed to solve with NTM type of models.

Finally we have shown some results on the SNLI task where our model performed better than NTM and the LSTM on this task. However our results do not involve any task specific modifications and the results can be improved further by structuring the architecture of our model according to the SNLI task.

The success of both the learnable address and the discrete addressing scheme suggests two future research directions. First, we should try both of these schemes in a wider array of memory-based models, as they are not specific to the neural Turing machines. Second, the proposed D-NTM needs to be evaluated on a diverse set of applications, such as text summarization~\citep{para}, visual question-answering~\citep{vqa} and machine translation, in order to make a more concrete conclusion.

\bibliographystyle{plainnat}
\bibliography{refs}

\end{document}


%%%%%%%%%%%%%%%%%%%%%%% file template.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is a general template file for the LaTeX package SVJour3
% for Springer journals.          Springer Heidelberg 2010/09/16
%
% Copy it to a new file with a new name and use it as the basis
% for your article. Delete % signs as needed.
%
% This template includes a few options for different layouts and
% content for various journals. Please consult a previous issue of
% your journal as needed.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% First comes an example EPS file -- just ignore it and
% proceed on the \documentclass line
% your LaTeX will extract the file if required
\begin{filecontents*}{example.eps}
%!PS-Adobe-3.0 EPSF-3.0
%%BoundingBox: 19 19 221 221
%%CreationDate: Mon Sep 29 1997
%%Creator: programmed by hand (JK)
%%EndComments
gsave
newpath
  20 20 moveto
  20 220 lineto
  220 220 lineto
  220 20 lineto
closepath
2 setlinewidth
gsave
  .4 setgray fill
grestore
stroke
grestore
\end{filecontents*}
%
\RequirePackage{fix-cm}
%
%\documentclass{svjour3}                     % onecolumn (standard format)
%\documentclass[smallcondensed]{svjour3}     % onecolumn (ditto)
\documentclass[smallextended]{svjour3}       % onecolumn (second format)
%\documentclass[twocolumn]{svjour3}          % twocolumn
%
\smartqed  % flush right qed marks, e.g. at end of proof
%
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{colortbl}
\usepackage{subfigure}
\usepackage{color}
%
% \usepackage{mathptmx}      % use Times fonts if available on your TeX system
%
% insert here the call for the packages your document requires
%\usepackage{latexsym}
% etc.
%
% please place your own definitions here and don't use \def but
% \newcommand{}{}
%
% Insert the name of "your journal" with
% \journalname{myjournal}
%
\begin{document}

\title{A Benchmark for Endoluminal Scene Segmentation of Colonoscopy Images
}
%\subtitle{Semantic Segmentation of Colonoscopy Images}
\titlerunning{EndoScene benchmark}        % Short form of title if too long for running head

\author{David V\'azquez$^{1,2}$ \and Jorge Bernal$^{1}$ \and F. Javier S\'anchez$^{1}$ \and Gloria Fern\'andez-Esparrach$^{4}$ \and Antonio M. L\'opez$^{1,2}$ \and Adriana Romero$^{2}$\and Michal Drozdzal$^{3, 5}$ \and Aaron Courville$^{2}$
}
\authorrunning{David V\'azquez et al.}

\institute{David V\'azquez \at \email{dvazquez@cvc.uab.es}\\
%            \and Jorge Bernal \at \email{jbernal@cvc.uab.es} \and
%            Adriana Romero \at \email{adriana.romero.soriano@umontreal.ca} \and
%            Michal Drozdzal \at \email{michal.drozdzal@gmail.com} \and
%            F. Javier S\'anchez \at \email{javier@cvc.uab.es} \and
%            Gloria Fern\'andez-Esparrach \at \email{mgfernan@clinic.ub.es} \and
%            Aaron Courville \at \email{aaron.courville@gmail.com}\\
$^{1}$ Computer Vision Center, Computer Science Department, Universitat Autonoma de Barcelona, Spain\\
$^{2}$ Montreal Institute for Learning Algorithms, Universit\'e de Montr\'eal, Canada\\
$^{3}$ \'{E}cole Polytechnique de Montr\'{e}al, Montr\'{e}al, Canada\\
$^{4}$ Endoscopy Unit, Gastroenterology Service, CIBERHED, IDIBAPS, Hospital Clinic, Universidad de Barcelona, Spain\\
$^{5}$ Imagia Inc., Montr\'{e}al, Canada\\
}

\date{Received: 07 Nov 2016 / Accepted: date}
% The correct dates will be entered by the editor


\maketitle

\begin{abstract}
Colorectal cancer (CRC) is the third cause of cancer death worldwide. Currently, the standard approach to reduce CRC-related mortality is to perform regular screening in search for polyps and colonoscopy is the screening tool of choice. The main limitations of this screening procedure are polyp miss-rate and inability to perform visual assessment of polyp malignancy. These drawbacks can be reduced by designing Decision Support Systems (DSS) aiming to help clinicians in the different stages of the procedure by providing endoluminal scene segmentation. Thus, in this paper, we introduce an extended benchmark of colonoscopy image, with the hope of establishing a \emph{new strong benchmark for colonoscopy image analysis research}. We provide new baselines on this dataset by training standard fully convolutional networks (FCN) for semantic segmentation  and \emph{significantly outperforming, without any further post-processing}, prior results in endoluminal scene segmentation. 
\keywords{Colonoscopy \and Polyp \and Semantic Segmentation \and Deep Learning}
\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction} 
\label{sec:intro} 
Colorectal cancer (CRC) is the third cause of cancer death worldwide \cite{cancerstats}. CRC arises from adenomatous polyps (adenomas) which are initially benign; however, with time some of them can become malignant. Currently, the standard approach to reduce CRC-related mortality is to perform regular screening in search for polyps and colonoscopy is the screening tool of choice. During the examination, clinicians visually inspect intestinal wall (see Figure \ref{fig:scene} for an example of intestinal scene) in search of polyps; once found they are resected for histological analysis.

The main limitations of colonoscopy are its associated polyp miss-rate (small/flat polyps or the ones hidden behind intestine folds can be missed \cite{leufkens2012factors}) and the fact that polyp's malignancy degree is only known after histological analysis. These drawbacks can be reduced by developing new colonoscopy modalities to improve visualization  (e.g. High Definition imaging, Narrow Band Imaging (NBI) \cite{machida2004narrow} and magnification endoscopes \cite{bruno2003magnification}) and/or by developing Decision Support Systems (DSS) aiming to help clinicians in the different stages of the procedure. Ideally, DSS should be able to detect, segment and assess the malignancy degree (e.g. by optical biopsy \cite{roy2011colonoscopic}) of polyps during colonoscopy procedure (Figure \ref{fig:pipeline} depicts steps of DSS in colonoscopy procedure). 

The development of DSS for colonoscopy has been an active research topic during the last decades. The majority of available works on optical colonoscopy are focused on polyp detection (e.g. see \cite{bernal2015wm,Silva2014,gross2009comparison,park2016colonoscopic,ribeiro2016colonic,tajbakhsh2015automated}) and only few works address the problems of endoluminal scene segmentation. 

Endoluminal scene segmentation is of crucial relevance for clinical applications \cite{bernal2014polyp,bernal2015wm,nunez2014impact,bernal2014discarding}. First, segmenting \emph{lumen} is relevant to help clinicians in navigation during the procedure. Second, segmenting \emph{polyps} is important to indicate the area covered by a potential lesion that should be carefully inspected by clinicians. Finally, \emph{specular highlight} have proven to be useful in reducing polyp detection false positive ratio in the context of hand-crafted methods \cite{bernal2015wm,bernal2013impact}. 

\begin{figure}[t!]
\centering
\subfigure[]{\includegraphics[width=0.55\textwidth]{figures/endoluminal_scene.png}\label{fig:scene}}\hfill
%\subfigure[]{\includegraphics[width=0.55\textwidth]{figures/pipeline.eps}\label{fig:pipeline}}\hfill
\subfigure[]{\includegraphics[width=0.75\textwidth]{figures/pipeline.png}\label{fig:pipeline}}\hfill
\caption{(a) Colonoscopy image and corresponding labeling: blue for lumen, red for background (mucosa wall), and green for polyp, (b) Proposed pipeline of a decision support system for colonoscopy.}
\label{fig:first_fig}
\end{figure}

In the recent years, convolutional neural networks (CNNs) have become a \emph{de facto} standard in computer vision, achieving state-of-the-art performance in tasks such as image classification, object detection and semantic segmentation; and making traditional methods based on hand-crafted features obsolete. Two major components in this groundbreaking progress were the availability of increased computational power (GPUs) and the introduction of large labeled datasets \cite{imagenet,mscoco}.

Despite the additional difficulty of having limited amounts of labeled data, CNNs have successfully been applied to a variety of medical imaging tasks, by resorting to aggressive data augmentation techniques \cite{ronneberger2015u,Drozdzal16}. More precisely, CNNs have excelled at semantic segmentation tasks in medical imaging, such as the EM ISBI 2012 dataset~\cite{EM_data}, BRATS~\cite{Brats} or MS lesions~\cite{MsLesions}, where the top entries are built on CNNs \cite{ronneberger2015u,0011QCH16,HavaeiDWBCBPJL15,Brosch,Drozdzal16}. Surprisingly, to the best of our knowledge, CNNs have not been applied to semantic segmentation of colonoscopy data. We associate this to the lack of large publicly available annotated databases, which is needed in order to train and validate such networks. 

In this paper, we aim to overcome this limitation and introduce an extended benchmark of colonoscopy images by combining the two largest datasets of colonoscopy images \cite{bernal2012towards,bernal2015wm} and incorporating additional annotations to segment lumen and specular highlights, with the hope of establishing a \emph{new strong benchmark for colonoscopy image analysis research}. We provide new baselines on this dataset by training standard fully convolutional networks (FCN) for semantic segmentation \cite{long2015fully}, and \emph{significantly outperforming, without any further post-processing}, prior results in endoluminal scene segmentation. 

\vspace{0.2cm}

Therefore, the contributions of this paper are two-fold:
\vspace{-.1cm}
\begin{enumerate}
\item Extended benchmark for colonoscopy image segmentation.
\item New state-of-the-art in colonoscopy image segmentation.
\end{enumerate}

The rest of the paper is organized as follows. In Section \ref{sec:bench}, we present the new extended benchmark, including the introduction of datasets as well as the performance metrics. After that, in Section \ref{sec:deep}, we introduce the FCN architecture used as baseline for the new endoluminal scene segmentation benchmark. Then, in Section \ref{sec:results}, we show qualitative and quantitative experimental results. Finally, Section \ref{sec:conclusions} concludes the paper. 

\section{Endoluminal scene segmentation benchmark}
\label{sec:bench} 
In this section, we describe the endoluminal scene segmentation benchmark, including evaluation metrics.

\subsection{Dataset}
\label{ssec:dataset}

Inspired by already published benchmarks for polyp detection, proposed within a challenge held in conjunction with MICCAI 2015\footnote{http://endovis.grand-challenge.org}, we introduce a benchmark for endoluminal scene object segmentation. 

We combine \textbf{CVC-ColonDB} and \textbf{CVC-ClinicDB} into a new dataset (\textbf{EndoScene}) composed of 912 images obtained from 44 video sequences acquired from 36 patients. 
\begin{itemize}
\item \textbf{CVC-ColonDB} contains 300 images with associated polyp and background (here, mucosa and lumen) segmentation masks obtained from 13 polyp video sequences acquired from 13 patients.
\item \textbf{CVC-ClinicDB} contains 612 images with associated polyp and background (here, mucosa and lumen) segmentation masks obtained from 31 polyp video sequences acquired from 23 patients. 
\end{itemize}

We extend the old annotations to account for lumen, specular highlights as well as a void class for black borders present in each frame. In the new annotations, background only contains mucosa (intestinal wall). Please refer to Table \ref{tab:databases} for datasets details and to Figure \ref{fig:databases} for a dataset sample. 

We split the resulting dataset into three sets: training, validation and test containing 60\%, 20\% and 20\% images respectively. We impose the constraint that one patient can not be in different sets. As a result, the final training set contains 20 patients and 547 frames, the validation set contains 8 patients and 183 frames; and test set contains 8 patients and 182 frames. The dataset is available online\footnote{http://adas.cvc.uab.es/endoscene}.

\begin{table}[t!]
\begin{center}
\caption{Summary of prior database content. All frames show at least one polyp.}
\begin{scriptsize}
\begin{tabular}{|c||c|c|c|c|c|}
  \hline
  \textbf{Database} & \textbf{\# patients} & \textbf{\# seq.} & \textbf{\# frames} & \textbf{Resolution} & \textbf{Annotations}\\
  \hline  \hline
  \begin{tabular}{@{}c@{}}CVC- \\ ColonDB\end{tabular} & 13 & 13 & 300 & 500x574 &  \begin{tabular}{@{}c@{}}polyp \\ background\end{tabular} \\\hline
  \begin{tabular}{@{}c@{}}CVC- \\ ClinicDB\end{tabular} & 23 & 31 & 612 & 384x288 & \begin{tabular}{@{}c@{}}polyp \\ background\end{tabular} \\
  \hline
   \hline
  EndoScene & 36 & 44 & 912 & 500x574 \& 384x288 & \begin{tabular}{@{}c@{}}polyp \\ lumen \\ background \\specularity \\ border (void)\end{tabular} \\
  \hline
\end{tabular}
\end{scriptsize}
\label{tab:databases}
\end{center}
\end{table}


\begin{figure}[t!]
\centering
\subfigure[]{\includegraphics[width=0.24\textwidth]{figures/orig291.eps}}
\subfigure[]{\includegraphics[width=0.24\textwidth]{figures/polyp291.eps}}
\subfigure[]{\includegraphics[width=0.24\textwidth]{figures/spec291.eps}}
\subfigure[]{\includegraphics[width=0.24\textwidth]{figures/lumen291.eps}}
\caption{Example of a colonoscopy image and its corresponding ground truth: (a) Original image, (b) Polyp mask, (c) Specular highlights mask and (d) lumen mask.}
\label{fig:databases}
\end{figure}

\subsection{Metrics}

We use Intersection over Union (IoU), also known as \emph{Jaccard index}, and per pixel accuracy as segmentation metrics. These metrics are commonly used in medical image segmentation tasks \cite{cha2016urinary,prastawa2004brain}. 

We compute the mean of per-class IoU. Each per-class IoU is computed over a validation/test set according to following formula:
\begin{equation}
IoU(\mathrm{PR}(class), \mathrm{GT}(class)) = \frac{|\mathrm{PR}(class) \bigcap \mathrm{GT}(class)|}{|\mathrm{PR}(class) \bigcup \mathrm{GT}(class)|},
\end{equation}
where $\mathrm{PR}$ represents the binary mask produced by the segmentation method, $\mathrm{GT}$ represents the ground truth mask, $\bigcap$ represents set intersection and $\bigcup$ represents set union. 

We compute the mean global accuracy for each set as follows:
\begin{equation}
Acc(\mathrm{PR}, \mathrm{GT}) = \frac{\# \mathrm{TP}}{\# \mathrm{pixels}},
\end{equation}
where $\mathrm{TP}$ represents the number of true positives. 

Notably, this new benchmark might as well be used for the relevant task of polyp localization. In that case, we follow Pascal VOC challenge metrics \cite{Everingham10} and determine that a polyp is localized if it has a high overlap degree with its associated ground truth, namely:
\begin{equation}
IoU(\mathrm{PR}(polyp), \mathrm{GT}(polyp)) > 0.5,
\end{equation}
where the metric is computed for each polyp independently, and averaged per set to give a final score. 

\section{Baseline} 
\label{sec:deep}

%% CNNs
CNN are a standard architecture used for tasks, where a single prediction per input is expected (e.g. image classification). Such architectures capture hierarchical representations of the input data by stacking blocks of convolutional, non-linearity and pooling layers on top of each other. Convolutional layers extract local features. Non-linearity layers allow deep networks to learn non-linear mappings of the input data. Pooling layers reduce the spatial resolution of the representation maps by aggregating local statistics.

%% FCNs
FCNs \cite{long2015fully,ronneberger2015u} were introduced in the computer vision and medical imaging communities in the context of semantic segmentation. FCNs naturally extend CNNs to tackle per pixel prediction problems, by adding upsampling layers to recover the spatial resolution of the input at the output layer. As a consequence, FCNs can process images of arbitrary size. In order to compensate for the resolution loss induced by pooling layers, FCNs introduce skip connections between their downsampling and upsampling paths. Skip connections help the upsampling path recover fine-grained information from the downsampling layers. 

%% Training details
We implemented FCN8 architecture from \cite{long2015fully}\footnote{https://github.com/jbernoz/deeppolyp}. and trained the network by means of stochastic gradient descent with rmsprop adaptive learning rate \cite{rmsprop}. The validation split is used to early-stop the training, we monitor mean IoU for validation set and use patience of 50. We used a mini-batch size of 10 images. The input image is normalized in the range 0-1. We randomly crop the training images to $224\times224$ pixels. As regularization, we use dropout\cite{Srivastava2014} of 0.5, as mentioned in the paper \cite{long2015fully}. We do not use any weight decay.

As described in Section \ref{ssec:dataset}, colonoscopy images have a black border that we consider as a void class. Void classes do not influence the computation of the loss nor the metrics of any set, since the pixels marked as void class are ignored. As the number of pixels per class is unbalanced, in some experiments, we apply the median frequency balancing of \cite{EigenF14}.

% data augmentation
During training, we experiment with data augmentation techniques such as random cropping, rotations, zooming, sharing and elastic transformations.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experimental results}
\label{sec:results}

In this section, we report semantic segmentation and polyp localization results on the new benchmark. 

\subsection{Endoluminal Scene Semantic Segmentation}

In this section, we first analyze the influence of different data augmentation techniques. Second, we evaluate the effect of having different number of endoluminal classes on polyp segmentation results. Finally, we compare our results with previously published methods.

\subsubsection{Influence of data augmentation}

Table \ref{tab:results_data_aug_valid} presents an analysis on the influence of different data augmentation techniques and their impact on the validation performance. We evaluate random zoom from 0.9 to 1.1, rotations from 0 to 180 degrees, shearing from 0 to 0.4 and warping with $\sigma$ ranging from 0 to 10. Finally, we evaluate the combination of all the data augmentation techniques. 

As shown in the table, polyps significantly benefit from all data augmentation methods, in particular, from warping. Note that warping applies small elastic deformation locally, accounting for many realistic variations in the polyp shape. Rotation and zoom also have a strong positive impact on the polyp segmentation performance. It goes without saying that such transformations are the least aggressive ones, since they do not alter the polyp appearance. Shearing is most likely the most aggressive transformation, since it changes the polyp appearance and might, in some cases, result in unrealistic deformations.

While for lumen it is difficult to draw any strong conclusions, it looks like zooming and warping slightly deteriorate the performance, whereas shearing and rotation slightly improve it. As for specularity highlights, all the data augmentation techniques that we tested significantly boost the segmentation results. Finally, background (mucosa) shows only slight improvement when incorporating data augmentations. This is not surprising, given its predominance throughout the data it could be even considered background.

Overall, combining all the discussed data augmentation techniques, leads to better results in terms of mean IoU and mean global accuracy. More precisely, we increase the mean IoU by $4.51\%$ and the global mean accuracy by $1.52\%$


%% data augmentation validation only
\begin{table}
\begin{center}
\begin{tabular}{|c|c|c|c|c||c|c|}
\hline
\begin{tabular}{@{}c@{}}\textbf{Data} \\ \textbf{Augmentation}\end{tabular}
& \begin{tabular}{@{}c@{}}\textbf{IoU} \\ \textbf{background}\end{tabular}
& \begin{tabular}{@{}c@{}}\textbf{IoU} \\ \textbf{polyp}\end{tabular}
& \begin{tabular}{@{}c@{}}\textbf{IoU} \\ \textbf{lumen}\end{tabular}
& \begin{tabular}{@{}c@{}}\textbf{IoU} \\ \textbf{spec.}\end{tabular}
& \begin{tabular}{@{}c@{}}\textbf{IoU} \\ \textbf{mean}\end{tabular}
& \begin{tabular}{@{}c@{}}\textbf{Acc} \\ \textbf{mean}\end{tabular}\\
\hline
\hline
None & 88.93 & 44.45 & 54.02 & 25.54 & 57.88 & 92.48 \\ 
% None & 80.39 (88.93) & 43.41 (44.45) & 36.85 (54.02) & 19.87 (25.54) & 50.46 (57.88) & 87.40 (92.48) \\ 
\hline
Zoom   & 89.89 & 52.73 & 51.15 & 37.10 & 57.72 & 90.72  \\
% Zoom   & 89.95 (89.89) & 40.64 (52.73) & 37.82 (51.15) & 33.23 (37.10) & 50.41 (57.72) & 90.40 (90.72)  \\
\hline
Warp   & 90.00 & 54.00 & 49.69 & \textbf{37.27} & 58.97 & 90.93  \\
% Warp   & 90.0 (90.00) & 49.6 (54.00) & 33.6 (49.69) & 34.7 (37.27) & 52.02 (58.97) & 90.58 (90.93)  \\
\hline
%Shear   & 87.8 () & 42.6 () & 45.4 () & 35.8 () & 52.95 () & 88.79 ()  \\
Shear   & 89.60 & 46.61 & 54.27 & 36.86 & 56.83 & 90.49  \\
%Shear   & 89.73 (89.60) & 46.36 (46.61) & 36.83 (54.27) & 33.89 (36.86) & 51.70 (56.83) & 90.28 (90.49)  \\
\hline
Rotation   & 90.52 & 52.83 & \textbf{56.39} & 35.81 & 58.89 & 91.38  \\
% Rotation   & 92.3 (90.52) & 59.8 (52.83) & 42.1 (56.39) & 27.0 (35.81) & 55.33 (58.89) & 92.78 (91.38)  \\
\hline
Combination & \textbf{92.62} & \textbf{54.82} & 55.08 & 35.75 & \textbf{59.57} & \textbf{93.02} \\
% Combination &  &  & &  &  62.39 & 94.00  \\
% Combination   & 88.81 () & 51.60 () & 41.21 () & 38.87 () & 55.13 (62.39) & 89.69 (94.00)  \\
\hline
\end{tabular}
\end{center}
\caption{FCN8 endoluminal scene semantic segmentation results for different data augmentation techniques. The results are reported on validation set.}
\label{tab:results_data_aug_valid}
\end{table}

\subsubsection{Influence of number of classes}

Table \ref{tab:nb_classes_valid} presents endoluminal scene semantic segmentation results for different number of classes. As shown in the table, using more underrepresented classes such as lumen or specular highlights makes the optimization problem more difficult. As expected and contrary to hand-crafted segmentation methods, when considering polyp segmentation, deep learning based approaches do not suffer from specular highlights, showing the robustness of the learnt features towards saturation zones in colonoscopy images.

Best results for polyp segmentation are obtained in the 2 classes scenario (polyp vs background). However, segmenting lumen is a relevant clinical problem as mentioned in Section \ref{sec:intro}. Results achieved in the 3 classes scenario are very encouraging, with a IoU higher than $50\%$ for both polyp and lumen classes.

%% Number of classes: validation only
\begin{table}
\begin{center}
% \begin{tabular}{|l|p{1.1cm}|p{1.1cm}|p{1.1cm}|p{1.1cm}||p{1.1cm}|p{1.1cm}|}
\begin{tabular}{|c|c|c|c|c||c|c|}
\hline
\begin{tabular}{@{}c@{}}\textbf{\#} \\ \textbf{classes}\end{tabular}
& \begin{tabular}{@{}c@{}}\textbf{IoU} \\ \textbf{background}\end{tabular}
& \begin{tabular}{@{}c@{}}\textbf{IoU} \\ \textbf{polyp}\end{tabular}
& \begin{tabular}{@{}c@{}}\textbf{IoU} \\ \textbf{lumen}\end{tabular}
& \begin{tabular}{@{}c@{}}\textbf{IoU} \\ \textbf{spec.}\end{tabular}
& \begin{tabular}{@{}c@{}}\textbf{IoU} \\ \textbf{mean}\end{tabular}
& \begin{tabular}{@{}c@{}}\textbf{Acc} \\ \textbf{mean}\end{tabular}\\
\hline \hline
% \multicolumn{7}{|l|}{Without class balancing} \\ \hline 
4& 92.07 & 39.37 & \textbf{59.55} & \textbf{40.52} &  57.88 & 92.48\\ 
\hline
% 3 & 89.41 & 24.74 & 53.91 &  --   &  56.02 & 90.05\\ 
3 & 92.19 & 50.70 & 56.48 &  --  & 66.46 & 92.82\\ 
\hline
2 & \textbf{96.63} & \textbf{56.07} &   --   &  --  &  \textbf{76.35} & \textbf{96.77}\\
\hline %\hline
%\multicolumn{7}{|l|}{With class balancing} \\ \hline
%4 & 88.93 & 44.45 & 54.02 & 25.54 & 53.24 & {\color{red}{92.48}} \\ 
%\hline
%3 & {\color{red}{92.19}} & {\color{red}{50.70}} & {\color{red}{56.48}} &  --  & %{\color{red}{66.46}} & {\color{red}{92.82}}\\ 
%\hline
%2 & 95.44 & 44.37 &   --  &  --  & 69.91 & 89.55\\ 
%\hline
\end{tabular}
\end{center}
\caption{FCN8 endoluminal scene semantic segmentation results for different number of classes. The results are reported on validation set. In all cases, we selected the model that provided best validation results (with or without class balancing).}
\label{tab:nb_classes_valid}
\end{table}

\subsubsection{Comparison to state-of-the-art}
%% final: test + sota
Finally, we evaluate the FCN model on the test set. We compare our results to the combination of previously published hand-crafted methods: 1) \cite{bernal2014polyp} an energy-map based method for polyp segmentation, 2) \cite{bernal2014discarding} a watershed-based method for lumen segmentation  and  3) \cite{bernal2013impact} for specular highlights segmentation.  

\begin{table}
\setlength\tabcolsep{4.5pt} % default value: 6pt
\begin{center}
\begin{tabular}{|c|c|c|c|c|c||c|c|}
\hline
& \begin{tabular}{@{}c@{}}\textbf{Data} \\ \textbf{Augmentation}\end{tabular}& 
\begin{tabular}{@{}c@{}}\textbf{IoU} \\ \textbf{background}\end{tabular}
& \begin{tabular}{@{}c@{}}\textbf{IoU} \\ \textbf{polyp}\end{tabular}
& \begin{tabular}{@{}c@{}}\textbf{IoU} \\ \textbf{lumen}\end{tabular}
& \begin{tabular}{@{}c@{}}\textbf{IoU} \\ \textbf{spec.}\end{tabular}
& \begin{tabular}{@{}c@{}}\textbf{IoU} \\ \textbf{mean}\end{tabular}
& \begin{tabular}{@{}c@{}}\textbf{Acc} \\ \textbf{mean}\end{tabular}\\
\hline \hline
\multicolumn{8}{|l|}{\emph{FCN8 performance}} \\ \hline
4 classes & None & 86.36 & 38.51 & \textbf{43.97} & 32.98 &  50.46 &  87.40\\ 
\hline
% 4 & Yes & 80.39 & 43.41 & 36.85 & 19.87 & 50.46 & 87.40 \\ 
% \hline
% 3 & No & 79.46 & 18.58 & 34.81 &  --    &  44.28 &  80.83\\ 
% \hline
3 classes & None & 84.66 & 47.55 & 36.93 &  --   & 56.38 & 86.08 \\ 
\hline
2 classes & None & \textbf{94.62} & 50.85 &   --   &  --   &  \textbf{72.74} & \textbf{94.91}\\ 
\hline
4 classes & Combination   & 88.81 & \textbf{51.60} & 41.21 & 38.87 & 55.13 & 89.69  \\
%\hline
%3 classes & Combination   & 80.67 & 24.47 & 35.02 & -- & 46.72 & 82.04 \\
% 2 & Yes & 92.17 & 32.58 &   --  &  --   & 45.13 & 82.19 \\ 
% \hline
\hline \hline
\multicolumn{8}{|l|}{\emph{State-of-the-art methods}} \\ \hline
\cite{bernal2014polyp,bernal2014discarding,bernal2013impact}& - &73.93 & 22.13 & 23.82 & \textbf{44.86} & 41.19& 75.58  \\ 
\hline
\end{tabular}
\end{center} 
\caption{Results on the tests set: FCN8 with respect to previously published methods.}
\label{tab:results_segmentation}
\setlength\tabcolsep{6pt} % default value: 6pt
\end{table}

The segmentation results on the test set are reported in Table \ref{tab:results_segmentation} and show a clear improvement of FCN8 over previously published methods. The following improvements can be observed when comparing previously published methods to 4-class FCN8 model trained with data augmentation: 15\% in IoU for background (mucosa), 29\% in IoU for polyps, 18\% in IoU for lumen, 14\% in mean IoU and 14\% in mean accuracy. FCN8 is still outperformed by traditional methods when it comes to specular highlights class. However, it is important to note that specular highlights class is used by hand-crafted methods to reduce false positive ratio of polyp detection and from our analysis it looks like FCN model is able to segment well polyps even when ignoring this class. For example, the best mean IoU of 72.74 \% and mean accuracy of 94.91\% are obtained by 2-class model without additional data augmentation.

Figure \ref{fig:predictions} shows qualitative results of 4-class FCN8 model trained with data augmentation. From left to right, each row shows a colonoscopy frame, followed by the corresponding ground truth annotation and FCN8 prediction. Rows 1 to 4 show correct segmentation masks, with very clean polyp segmentation. Rows 5 and 6 show failure modes of the model, where polyps have been missed or under-segmented. In row 5, the small polyp is missed by our segmentation methond while, in row 6, the polyp is under-segmented. All cases exhibit decent lumen segmentation and good background (mucosa) segmentation. 

\begin{figure}[t!]
\centering
\subfigure[]{\includegraphics[width=0.7\textwidth]{figures/Polyp1.png}}
\subfigure[]{\includegraphics[width=0.7\textwidth]{figures/Polyp2.png}}
\subfigure[]{\includegraphics[width=0.7\textwidth]{figures/Polyp3.png}}
\subfigure[]{\includegraphics[width=0.7\textwidth]{figures/Polyp4.png}}
\subfigure[]{\includegraphics[width=0.7\textwidth]{figures/Polyp5.png}}
\subfigure[]{\includegraphics[width=0.7\textwidth]{figures/Polyp6.png}}
\caption{Examples of predictions for 4-class FCN8 model. Each sub-figure represents a single frame, a ground truth annotation and a prediction image. We use the following color coding in the annotations: red for background (mucosa), blue for lumen, yellow for polyp and green specularity. (a, b, c, d) show correct polyp segmentation, whereas (e, d) show incorrect polyp segmentation.}
\label{fig:predictions}
\end{figure}

\subsection{Polyp localization}

\begin{figure}[t!]
\centering
\includegraphics[width=\textwidth]{figures/locratev2.eps}
\caption{Localization rate of polyps as a function of IoU. The x-axis represents the degree of overlap between ground truth and model prediction. The y-axis represents the percentage of correctly localized polyps. Different color plots represent different models: FCN8 with 4 classes, FCN8 with 3 classes, FCN8 with 2 classes and previously published method \cite{bernal2014polyp} (refereed to as state-of-the-art in the plot)}
\label{fig:local}
\end{figure}

Endoluminal scene segmentation can be seen as a proxy to proper polyp detection in colonoscopy video. In order to understand how well suited FCN is to localize polyps, we perform a last experiment. In this experiment, we compute the polyp localization rate as a function of IoU between the model prediction and the ground truth. We can compute this IoU per frame, since our dataset contains a maximum of one polyp per image. This analysis describes the ability of a given method to cope with polyp appearance variability and stability on polyp localization. 

The localization results are presented in Figure \ref{fig:local} and show a significant improvement when comparing FCN8 variants to the previously published method \cite{bernal2014polyp}. For example, when considering a correct polyp localization to have at least 50\% IoU, we observe an increase of 40\% in polyp localization rate. As a general trend, we observe that architectures trained using fewer number of classes achieve a higher IoU, though the polyp localization difference starts to be more visible when really high overlapping degrees are imposed. Finally, as one would expect, we observe that the architectures that show better results in polyp segmentation are the ones that show better results in polyp localization.


\section{Conclusions} 
\label{sec:conclusions}

In this paper, we have introduced an extended benchmark for endoluminal scene semantic segmentation. The benchmark includes extended annotations of polyps, background (mucosa), lumen and specular highlights. The dataset provides the standard training, validation and test splits for machine learning practitioners and will be publicly available upon paper acceptance. Moreover, standard metrics for the comparison haven been defined; with the hope to speed-up the research in the endoluminal scene segmentation area.

Together with the dataset, we provided new baselines based on fully convolutional networks, which outperformed by a large margin previously published results, without any further post-processing. We extended the proposed pipeline and used it as proxy to perform polyp detection. Due to the lack of non-polyp frames in the dataset, we reformulated the task as polyp localization. Once again, we highlighted the superiority of deep learning based models over traditional hand-crafted approaches. As expected and contrary to hand-crafted segmentation methods, when considering polyp segmentation, deep learning based approaches do not suffer from specular highlights, showing the robustness of the learnt features towards saturation zones in colonoscopy images. Moreover, given that FCN not only excels in terms of performance but also allows for nearly real-time processing, it has a great potential to be included in future DSS for colonoscopy.

Knowing the potential of deep learning techniques, efforts in the medical imaging community should be devoted to gather larger labeled datasets as well as designing deep learning architectures that would be better suited to deal with colonoscopy data. This paper pretends to make a first step towards novel and more accurate DSS by making all code and data publicly available, paving the road for more researcher to contribute to the endoluminal scene segmentation domain.


\begin{acknowledgements}
The authors would like to thank the developers of Theano \cite{Theano-2016short} and Keras \cite{chollet2015keras}. We acknowledge the support of the following agencies for research funding and computing support: Imagia Inc., Spanish government through funded project AC/DC TRA2014-57088-C2-1-R and iVENDIS (DPI2015-65286-R), SGR projects 2014-SGR-1506, 2014-SGR-1470 and 2014-SGR-135and TECNIOspring-FP7-ACCIÓ grant, by the FSEED and NVIDIA Corporation for the generous support in the form of different GPU hardware units.

\end{acknowledgements}

% BibTeX users please use one of
%\bibliographystyle{spbasic}      % basic style, author-year citations
\bibliographystyle{spmpsci}      % mathematics and physical sciences
%\bibliographystyle{spphys}       % APS-like style for physics
\bibliography{biblio}   % name your BibTeX data base

\end{document}


\documentclass{article} % For LaTeX2e

% From ICML example template
\usepackage{times}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{natbib}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% ICML
\usepackage[accepted]{icml2017} 

% Other
\usepackage{url}
\usepackage{amsmath}
\usepackage{placeins}
\usepackage{mathrsfs}
\usepackage[utf8]{inputenc}
\usepackage{amsfonts}
\usepackage{todonotes} 
\usepackage{bm}
\usepackage{upgreek}
\usepackage{mathtools,xparse}

\icmltitlerunning{On orthogonality and learning RNNs with long term dependencies}

\begin{document}

\twocolumn[
\icmltitle{On orthogonality and learning recurrent networks with long term dependencies}

\begin{icmlauthorlist}
\icmlauthor{Eugene Vorontsov}{poly,mila}
\icmlauthor{Chiheb Trabelsi}{poly,mila}
\icmlauthor{Samuel Kadoury}{poly,chum}
\icmlauthor{Chris Pal}{poly,mila}
\end{icmlauthorlist}

\icmlaffiliation{poly}{École Polytechnique de Montréal, Montréal, Canada}
\icmlaffiliation{mila}{Montreal Institute for Learning Algorithms, Montréal, Canada}
\icmlaffiliation{chum}{CHUM Research Center, Montréal, Canada}

\icmlcorrespondingauthor{Eugene Vorontsov}{eugene.vorontsov@gmail.com}
\vskip 0.3in
]

\printAffiliationsAndNotice{}

\begin{abstract}
It is well known that it is challenging to train deep neural networks and recurrent neural networks for tasks that exhibit long term dependencies. The vanishing or exploding gradient problem is a well known issue associated with these challenges. One approach to addressing vanishing and exploding gradients is to use either soft or hard constraints on weight matrices so as to encourage or enforce orthogonality. Orthogonal matrices preserve gradient norm during backpropagation and may therefore be a desirable property. This paper explores issues with optimization convergence, speed and gradient stability when encouraging or enforcing orthogonality. To perform this analysis, we propose a weight matrix factorization and parameterization strategy through which we can bound matrix norms and therein control the degree of expansivity induced during backpropagation. We find that hard constraints on orthogonality can negatively affect the speed of convergence and model performance.
\end{abstract}


\section{Introduction}

The depth of deep neural networks confers representational power, but also makes model optimization more challenging. Training deep networks with gradient descent based methods is known to be difficult as a consequence of the vanishing and exploding gradient problem ~\citep{hochreiter1997long}. Typically, exploding gradients are avoided by clipping large gradients ~\citep{pascanu2013difficulty} or introducing an $L_2$ or $L_1$ weight norm penalty. The latter has the effect of bounding the spectral radius of the linear transformations, thus limiting the maximal gain across the transformation. ~\citet{krueger2015regularizing} attempt to stabilize the norm of propagating signals directly by penalizing differences in successive norm pairs in the forward pass and ~\citet{pascanu2013difficulty} propose to penalize successive gradient norm pairs in the backward pass. These regularizers affect the network parameterization with respect to the data instead of penalizing weights directly.

Both expansivity and contractivity of linear transformations can also be limited by more tightly bounding their spectra. By limiting the transformations to be orthogonal, their singular spectra are limited to unitary gain causing the transformations to be norm-preserving. ~\citet{le2015simple} and ~\citet{henaff2016orthogonal} have respectively shown that identity initialization and orthogonal initialization can be beneficial. ~\citet{arjovsky2015unitary} have gone beyond initialization, building unitary recurrent neural network (RNN) models with transformations that are unitary by construction which they achieved by composing multiple basic unitary transformations. The resulting transformations, for some n-dimensional input, cover only some subset of possible $\mathit{n \times n}$ unitary matrices but appear to perform well on simple tasks and have the benefit of having low complexity in memory and computation. Similarly, \citep{jing2016tunable} introduce an efficient algorithm to cover a large subset.

The entire set of possible unitary or orthogonal parameterizations forms the Stiefel manifold. At a much higher computational cost, gradient descent optimization directly along this manifold can be done via geodesic steps \citep{nishimori2005note,tagare2011notes}. Recent work \citep{wisdom2016fullcap} has proposed the optimization of unitary matrices along the Stiefel manifold using geodesic gradient descent. To produce a full-capacity parameterization for unitary matrices they use some insights from \citet{tagare2011notes}, combining the use of canonical inner products and Cayley transformations. Their experimental work indicates that full capacity unitary RNN models can solve the copy memory problem whereas both LSTM networks and restricted capacity unitary RNN models having similar complexity appear unable to solve the task for a longer sequence length (${\mathit{T }}$ = 2000). \citet{hyland2017learning} and \citet{mhammedi2016efficient} introduced more computationally efficient full capacity parameterizations. \citet{harandi2016generalized} also find that the use of fully connected ``Stiefel layers'' improves the performance of some convolutional neural networks.

We seek to gain a new perspective on this line of research by exploring the optimization of real valued matrices within a configurable margin about the Stiefel manifold. We suspect that a strong constraint of orthogonality limits the model's representational power, hindering its performance, and may make optimization more difficult. We explore this hypothesis empirically by employing a factorization technique that allows us to limit the degree of deviation from the Stiefel manifold\footnote{Source code for the model and experiments located at \url{https://github.com/veugene/spectre_release}}. While we use geodesic gradient descent, we simultaneously update the singular spectra of our matrices along Euclidean steps, allowing optimization to step away from the manifold while still curving about it.

\subsection{Vanishing and Exploding Gradients}

The issue of vanishing and exploding gradients as it pertains to the parameterization of neural networks can be illuminated by looking at the gradient back-propagation chain through a network.

A neural network with ${\mathit{N}}$ hidden layers has pre-activations
\begin{equation}
 \mathbf{a}_i(\mathbf{h}_{i-1}) = \mathbf{W}_i \; \mathbf{h}_{i-1} + \; \mathbf{b}_i, \ \ i \in \{2, \cdots, N-1\}
\end{equation}
For notational convenience, we combine parameters ${\mathbf{W}_i}$ and ${\mathbf{b}_i}$ to form an affine matrix $\boldsymbol{\uptheta}$. We can see that for some loss function ${\mathit{L}}$ at layer ${\mathit{n}}$, the derivative with respect to parameters ${\boldsymbol{\uptheta}_{i}}$ is:

\begin{equation}\label{eq:dL_dtheta}
\frac{\partial L}{\partial \boldsymbol{\uptheta}_{i}} =
\frac{\partial \mathbf{a}_{n+1}}{\partial \boldsymbol{\uptheta} _{i}}
\frac{\partial L}{\partial \mathbf{a}_{n+1}}
\end{equation}

The partial derivatives for the pre-activations can be decomposed as follows:
\begin{equation}\label{eq:chain_rule}
\begin{aligned} 
\frac{\partial \mathbf{a}_{i+1}}{\partial \boldsymbol{\uptheta}_i}
&=
\frac{\partial \mathbf{a}_i}{\partial \boldsymbol{\uptheta}_i}
\frac{\partial \mathbf{h}_i}{\partial \mathbf{a}_i}
\frac{\partial \mathbf{a}_{i+1}}{\partial \mathbf{h}_i}
\\&=
\frac{\partial \mathbf{a}_i}{\partial \boldsymbol{\uptheta}_i} \; \mathbf{D}_i \mathbf{W}_{i+1}
\ \rightarrow \
\frac{\partial \mathbf{a}_{i+1}}{\partial \mathbf{a}_i} = \mathbf{D}_i \mathbf{W}_{i+1},
\end{aligned}
\end{equation}
where ${\mathbf{D_i}}$ is the Jacobian corresponding to the activation function, containing partial derivatives of the hidden units at layer ${\mathit{i \;+}}$ 1 with respect to the pre-activation inputs. Typically, ${\mathbf{D}}$ is diagonal. Following the above, the gradient in equation \ref{eq:dL_dtheta} can be fully decomposed into a recursive chain of matrix products:
\begin{equation}
\frac{\partial L}{\partial \boldsymbol{\uptheta}_{i}} =
\frac{\partial \mathbf{a}_i}{\partial \boldsymbol{\uptheta}_i}
\prod_{j=i}^{n} (\mathbf{D}_j \mathbf{W}_{j+1})
\frac{\partial L}{\partial \mathbf{a}_{n+1}}
\end{equation}

In \citet{pascanu2013difficulty}, it is shown that the 2-norm of ${\mathit{\dfrac{\partial \mathbf{a}_{i+1}}{\partial \mathbf{a}_i}}}$ is bounded by the product of the norms of the non-linearity's Jacobian and transition matrix at time ${\mathit{t\;}}$ (layer ${\mathit{i}}$), as follows:
\begin{equation}\label{eq:bounded_norm}
\begin{aligned}
\begin{gathered}
\left| \left | \frac{\partial \mathbf{a}_{t+1}}{\partial \mathbf{a}_{t}} \right| \right| \leq || \mathbf{D}_t || \; || \mathbf{W}_t || \leq \lambda_{\mathbf{D}_t} \; \lambda_{\mathbf{W}_t} = \eta_t , \\  \ \lambda_{\mathbf{D}_t} , \lambda_{\mathbf{W}_t} \in \mathbb{R}.
\end{gathered}
\end{aligned}
\end{equation}
where $\lambda_{\mathbf{D}_t}$ and $\lambda_{\mathbf{W}_t}$ are the largest singular values of the non-linearity's Jacobian ${\mathit{\mathbf{D}_t}}$ and the transition matrix ${\mathit{\mathbf{W}_t}}$. In RNNs, ${\mathit{\mathbf{W}_t}}$ is shared across time and can be simply denoted as ${\mathit{\mathbf{W}}}$.

Equation \ref{eq:bounded_norm} shows that the gradient can grow or shrink at each layer depending on the gain of each layer's linear transformation ${\mathbf{W}}$ and the gain of the Jacobian ${\mathbf{D}}$. The gain caused by each layer is magnified across all time steps or layers. It is easy to have extreme amplification in a recurrent neural network where ${\mathbf{W}}$ is shared across time steps and a non-unitary gain in ${\mathbf{W}}$ is amplified exponentially. The phenomena of extreme growth or contraction of the gradient across time steps or layers are known as the exploding and the vanishing gradient problems, respectively. It is sufficient for RNNs to have $\eta_t$ $\leq$ 1 at each time $t$ to enable the possibility of vanishing gradients, typically for some large number of time steps ${T}$. The rate at which a gradient (or forward signal) vanishes depends on both the parameterization of the model and on the input data. The parameterization may be conditioned by placing appropriate constraints on ${\mathbf{W}}$. It is worth keeping in mind that the Jacobian ${\mathbf{D}}$ is typically contractive, thus tending to be norm-reducing) and is also data-dependent, whereas ${\mathbf{W}}$ can vary from being contractive to norm-preserving, to expansive and applies the same gain on the forward signal as on the back-propagated gradient signal.

\section{Our Approach}
\label{sec:method}

%\subsection{Encouraging orthogonality vs. bounding expansivity and contractivity}

Vanishing and exploding gradients can be controlled to a large extent by controlling the maximum and minimum \emph{gain} of ${\mathit{\mathbf{W}}}$.
% Define gain here
The maximum gain of a matrix $\mathbf{W}$ is given by the spectral norm which is given by
\begin{equation}
||\mathbf{W}||_2= \max\Bigg[ \frac{||\mathbf{W}\mathbf{x}||}{||\mathbf{x}||} \Bigg].
\end{equation}

By keeping our weight matrix $\mathbf{W}$ close to orthogonal, one can ensure that it is close to a norm-preserving transformation (where the spectral norm is equal to one, but the minimum gain is also one). One way to achieve this is via a simple soft constraint or regularization term of the form:
\begin{equation}\label{eq:soft_orthogonality}
\lambda \sum_i ||\mathbf{W}_i^T \mathbf{W}_i - \mathbf{I}||^2.
\end{equation}
However, it is possible to formulate a more direct parameterization or factorization for $\bf{W}$ which permits \emph{hard bounds} on the amount of expansion and contraction induced by $\bf{W}$.   %transformation's singular values which determine the gain that it applies on an input along its basis vectors. 
This can be achieved by simply parameterizing ${\mathbf{W}}$ according to its singular value decomposition, which consists of the composition of orthogonal basis matrices ${\mathbf{U}}$ and ${\mathbf{V}}$ with a diagonal spectral matrix ${\mathbf{S}}$ containing the singular values which are real and positive by definition. We have
\begin{equation}
\mathbf{W}=\mathbf{U}\mathbf{S}\mathbf{V}^T.
\label{eq:factorization}
\end{equation}
Since the spectral norm or maximum gain of a matrix is equal to its largest singular value, this decomposition allows us to control the maximum gain or expansivity of the weight matrix by controlling the magnitude of the largest singular value. Similarly, the minimum gain or contractivity of a matrix can be obtained from the minimum singular value.
%as we have:
%\begin{equation}
%||\mathbf{W}||_2^2= \textrm{trace}(\mathbf{W}^T\mathbf{W}) = \sum_{i=1}^{min(m,n)} s_i^2, 
%\end{equation}
% where $\mathit{W^*}$ denotes the conjugate transpose of $\mathit{W}$, and $\mathit{s_i}$ are the singular values of W that constitute the diagonal of ${\mathit{S}}$. 
%when $\mathit{W}$ is complex one can use $\mathit{W^*}$, the conjugate transpose in place of ${W^T}$. 
%where ${\mathit{m}}$ and ${\mathit{n}}$ are respectively the rows of the squared matrices ${\mathbf{U}}$ and ${\mathbf{V}}$ and constitute also the dimension of $\mathbf{W}$.
%When $\mathbf{W}$ is real, 

We can keep the bases $\mathbf{U}$ and $\mathbf{V}$ orthogonal via \emph{geodesic gradient descent} along the set of weights that satisfy $\mathbf{U}^T\mathbf{U}=\mathbf{I}$ and $\mathbf{V}^T\mathbf{V}=\mathbf{I}$ respectively.  The submanifolds that satisfy these constraints are called Stiefel manifolds. We discuss how this is achieved in more detail below, then discuss our construction for bounding the singular values.

%We then bound the deviation of the ${\mathit{i^{th}}}$ singular value ${\mathit{s_i}}$ from unity by parameterizing each singular value with a sigmoid. The singular values are thus allowed to fall within some margin ${\mathit{m}}$ around one, i.e.
%\begin{equation}
%s_i = m(\sigma(s_{i,param}) - 0.5) + 1
%\end{equation}

%\subsection{Hard orthogonality constraints}

During optimization, in order to maintain the orthogonality of an orthogonally-initialized matrix $\bf{M}$, i.e. where $\bf{M}= \bf{U}$, $\bf{M}= \bf{V}$ or $\bf{M}= \bf{W}$ if so desired, we employ a Cayley transformation of the update step onto the Stiefel manifold of (semi-)orthogonal matrices, as in \citet{nishimori2005note} and \citet{tagare2011notes}. Given an orthogonally-initialized parameter matrix $\bf{M}$ and its Jacobian, $\bf{G}$ with respect to the objective function, an update is performed as follows:
\begin{equation}\label{eq:geoSGD}
\begin{aligned}
&\mathbf{A} = \mathbf{G}\mathbf{M}^T - \mathbf{M}\mathbf{G}^T\\
&\mathbf{M}_{new} = (\mathbf{I}+\frac{\eta}{2} \mathbf{A})^{-1}(\mathbf{I}-\frac{\eta}{2} \mathbf{A}) \mathbf{M},
\end{aligned}
\end{equation}
where $\bf{A}$ is a skew-symmetric matrix (that depends on the Jacobian and on the parameter matrix) which is mapped to an orthogonal matrix via a Cayley transform and ${\mathit{\eta}}$ is the learning rate.

While the update rule in (\ref{eq:geoSGD}) allows us to maintain an orthogonal hidden to hidden transition matrix $\bf{W}$ if desired, we are interested in exploring the effect of stepping away from the Stiefel manifold. As such, we parameterize the transition matrix $\bf{W}$ in factorized form, as a singular value decomposition with orthogonal bases $\bf{U}$ and $\bf{V}$ updated by geodesic gradient descent using the Cayley transform approach above.
%
%\begin{equation}\label{eq:factorization}
%\mathbf{W} = \mathbf{U}\mathbf{S}\mathbf{V}^T
%\end{equation}
%

If $\bf{W}$ is an orthogonal matrix, the singular values in the diagonal matrix ${\mathbf{S}}$ are all equal to one. However, in our formulation we allow these singular values to deviate from one and employ a sigmoidal parameterization to apply a hard constraint on the maximum and minimum amount of deviation. Specifically, we define a margin ${\mathit{m}}$ around 1 within which the singular values must lie. This is achieved with the parameterization
%
\begin{equation}
\begin{gathered}
s_i = 2m(\sigma(p_i) - 0.5) + 1, \hspace{.2in} 
~~s_i \in \{\textrm{diag}(\mathbf{S})\} , \;
m \in \mathbb[0, \; 1].
\label{eq:margin}
\end{gathered}
\end{equation}
%
The singular values are thus restricted to the range ${\mathit{[\textrm{1}-m, \; \textrm{1}+m]}}$ and the underlying parameters ${\mathit{p_i}}$ are updated freely via stochastic gradient descent. 
% Note that the parameterization of the singular values with a sigmoid is contractive toward unitary values.
Note that this parameterization strategy also has implications on the step sizes that gradient descent based optimization will take when updating the singular values -- they tend to be smaller compared to models with no margin constraining their values. Specifically, a singular value's progression toward a margin is slowed the closer it is to the margin. The sigmoidal parameterization can also impart another effect on the step size along the spectrum which needs to be accounted for. Considering \ref{eq:margin}, the gradient backpropagation of some loss ${\mathit{L}}$ toward parameters ${\mathit{p_i}}$ is found as
%
\begin{equation}
\begin{aligned}
\frac{dL}{dp_i} = \frac{ds_i}{dp_i} \frac{dL}{ds_i} = 2m \frac{d\sigma(p_i)}{dp_i} \frac{dL}{ds_i}.
\end{aligned}
\label{eq:backprop}
\end{equation}
%
From (\ref{eq:backprop}), it can be seen that the magnitude of the update step for ${\mathit{p_i}}$ is scaled by the margin hyperparameter ${\mathit{m}}$. This means for example that for margins less than one, the effective learning rate for the spectrum is reduced in proportion to the margin. Consequently, we adjust the learning rate along the spectrum to be independent of the margin by renormalizing it by ${\mathit{2m}}$. 

This margin formulation both guarantees singular values lie within a well defined range and slows deviation from orthogonality. Alternatively, one could enforce the orthogonality of $\bf{U}$ and $\bf{V}$ and impose a regularization term corresponding to a mean one Gaussian prior on these singular values. This encourages the weight matrix $\bf{W}$ to be norm preserving with a controllable strength equivalent to the variance of the Gaussian.  We also explore this approach further below.

\section{Experiments}

In this section, we explore hard and soft orthogonality constraints on factorized weight matrices for recurrent neural network hidden to hidden transitions. With hard orthogonality constraints on $\bf{U}$ and $\bf{V}$, we investigate the effect of widening the spectral margin or bounds on convergence and performance. Loosening these bounds allows increasingly larger margins within which the transition matrix $\bf{W}$ can deviate from orthogonality. We confirm that orthogonal initialization is useful as noted in \citet{henaff2016orthogonal}, and we show that although strict orthogonality guarantees stable gradient norm, loosening orthogonality constraints can increase the rate of gradient descent convergence. We begin our analyses on tasks that are designed to stress memory: a sequence copying task and a basic addition task \citep{hochreiter1997long}. We then move on to tasks on real data that require models to capture long-range dependencies: digit classification based on sequential and permuted MNIST vectors \citep{le2015simple,lecun1998gradient}. Finally, we look at a basic language modeling task using the Penn Treebank dataset \citep{marcus1993building}.

The copy and adding tasks, introduced by \citet{hochreiter1997long}, are synthetic benchmarks with pathologically hard long distance dependencies that require long-term memory in models. The copy task consists of an input sequence that must be remembered by the network, followed by a series of blank inputs terminated by a delimiter that denotes the point at which the network must begin to output a copy of the initial sequence. We use an input sequence of ${\mathit{T}+20}$ elements that begins with a sub-sequence of 10 elements to copy, each containing a symbol ${\mathit{a_i \in \{a_1, ..., a_p\}}}$ out of ${\mathit{p=8}}$ possible symbols. This sub-sequence is followed by ${\mathit{T}-1}$ elements of the blank category ${\mathit{a_0}}$ which is terminated at step ${\mathit{T}}$ by a delimiter symbol ${\mathit{a_{p+1}}}$ and 10 more elements of the blank category. The network must learn to remember the initial 10 element sequence for ${\mathit{T}}$ time steps and output it after receiving the delimiter symbol.

The goal of the adding task is to add two numbers together after a long delay. Each number is randomly picked at a unique position in a sequence of length ${\mathit{T}}$. The sequence is composed of ${\mathit{T}}$ values sampled from a uniform distribution in the range ${[0,1)}$, with each value paired with an indicator value that identifies the value as one of the two numbers to remember (marked 1) or as a value to ignore (marked 0). The two numbers are positioned randomly in the sequence, the first in the range ${[0, \frac{T}{2}-1]}$ and the second in the range ${[\frac{T}{2}, T-1]}$, where 0 marks the first element. The network must learn to identify and remember the two numbers and output their sum.

In the sequential MNIST task from \citet{le2015simple}, MNIST digits are flattened into vectors that can be traversed sequentially by a recurrent neural network. The goal is to classify the digit based on the sequential input of pixels. The simple variant of this task is with a simple flattening of the image matrices; the harder variant of this task includes a random permutation of the pixels in the input vector that is determined once for an experiment. The latter formulation introduces longer distance dependencies between pixels that must be interpreted by the classification model.

The English Penn Treebank (PTB) dataset from \citet{marcus1993building} is an annotated corpus of English sentences, commonly used for benchmarking language models. We employ a sequential character prediction task: given a sentence, a recurrent neural network must predict the next character at each step, from left to right. We use input sequences of variable length, with each sequence containing one sentence. We model 49 characters including lowercase letters (all strings are in lowercase), numbers, common punctuation, and an unknown character placeholder. We use two subsets of the data in our experiments: in the first, we first use 23\% of the data with strings with up to 75 characters and in the second we include over 99\% of the dataset, picking strings with up to 300 characters.


\subsection{Loosening Hard Orthogonality Constraints}

In this section, we experimentally explore the effect of loosening hard orthogonality constraints through loosening the spectral margin defined above for the hidden to hidden transition matrix.

In all experiments, we employed RMSprop \citep{Tieleman2012} when not using geodesic gradient descent. We used minibatches of size 50 and for generated data (the copy and adding tasks), we assumed an epoch length of 100 minibatches. We cautiously introduced gradient clipping at magnitude 100 (unless stated otherwise) in all of our RNN experiments although it may not be required and we consistently applied a small weight decay of 0.0001. Unless otherwise specified, we trained all simple recurrent neural networks with the hidden to hidden matrix factorization as in (\ref{eq:factorization}) using geodesic gradient descent on the bases (learning rate $10^{-6}$) and RMSprop on the other parameters (learning rate 0.0001), using a tanh transition nonlinearity, and clipping gradients of 100 magnitude. The neural network code was built on the Theano framework \citep{2016arXiv160502688short}. When parameterizing a matrix in factorized form, we apply the weight decay on the composite matrix rather than on the factors in order to be consistent across experiments. For MNIST and PTB, hyperparameter selection and early stopping were performed targeting the best validation set accuracy, with results reported on the test set.

\subsubsection{Convergence on Synthetic Memory Tasks}

For different sequence lengths ${\mathit{T}}$ of the copy and adding tasks, we trained a factorized RNN with 128 hidden units and various spectral margins ${\mathit{m}}$. For the copy task, we used Elman networks without a transition non-linearity as in \citet{henaff2016orthogonal}. We also investigated the use of nonlinearities, as discussed below.

\begin{figure*}[htb!]
\centering
\subfigure{
\includegraphics[width=0.204\textwidth]{figures/copy_200.pdf}
}
\subfigure{
\includegraphics[width=0.204\textwidth]{figures/copy_500.pdf}
}
\subfigure{
\includegraphics[width=0.204\textwidth]{figures/copy_1000.pdf}
}
\subfigure{
\includegraphics[width=0.287\textwidth]{figures/copy_10k.pdf}
}
\caption{
Accuracy curves on the copy task for different sequence lengths given various spectral margins. Convergence speed increases with margin size; however, large margin sizes are ineffective at longer sequence lengths (T=10000, right).
}
\label{fig:copy_acc}
\end{figure*}

As shown in Figure \ref{fig:copy_acc} we see an increase in the rate of convergence as we increase the spectral margin. This observation generally holds across the tested sequence lengths (${\mathit{T}=200}$, ${\mathit{T}=500}$, ${\mathit{T}=1000}$, ${\mathit{T}=10000}$); however, large spectral margins hinder convergence on extremely long sequence lengths. At sequence length ${\mathit{T}=10000}$, parameterizations with spectral margins larger than 0.001 converge slower than when using a margin of 0.001. In addition, the experiment without a margin failed to converge on the longest sequence length. This follows the expected pattern where stepping away from the Stiefel manifold may help with gradient descent optimization but loosening orthogonality constraints can reduce the stability of signal propagation through the network.

\begin{figure}[htb!]
\centering
\subfigure{
\includegraphics[width=0.35\textwidth]{figures/adding.pdf}
}
\caption{Mean squared error (MSE) curves on the adding task for different spectral margins ${\mathit{m}}$. A trivial solution of always outputting the same number has an expected baseline MSE of 0.167.}
\label{fig:adding}
\end{figure}

For the adding task, we trained a factorized RNN on ${\mathit{T}=1000}$ length sequences, using a ReLU activation function on the hidden to hidden transition matrix. The mean squared error (MSE) is shown for different spectral margins in Figure \ref{fig:adding}. Testing spectral margins ${\mathit{m}=0}$, ${\mathit{m}=1}$, ${\mathit{m}=10}$, ${\mathit{m}=100}$, and no margin, we find that the models with the purely orthogonal (${\mathit{m}=0}$) and the unconstrained (no margin) transition matrices failed to begin converging beyond baseline MSE within 2000 epochs.

We found that nonlinearities such as a rectified linear unit (ReLU) \citep{nair2010rectified} or hyperbolic tangent (tanh) made the copy task far more difficult to solve. Using tanh, a short sequence length (${\mathit{T}=100}$) copy task required both a soft constraint that encourages orthogonality and thousands of epochs for training. It is worth noting that in the unitary evolution recurrent neural network of \citet{arjovsky2015unitary}, the non-linearity (referred to as the "modReLU") is actually initialized as an identity operation that is free to deviate from identity during training. Furthermore, \citet{henaff2016orthogonal} derive a solution mechanism for the copy task that drops the non-linearity from an RNN. To explore this further, we experimented with a parametric leaky ReLU activation function (PReLU) which introduces a trainable slope ${\mathit{\alpha}}$ for negative valued inputs ${\mathit{x}}$, producing ${\mathit{f}(\mathit{x})=max(\mathit{x}, 0) + \mathit{\alpha} min(\mathit{x}, 0)}$ \citep{he2015delving}. Setting the slope ${\mathit{\alpha}}$ to one would make the PReLU equivalent to an identity function. We experimented with clamping ${\mathit{\alpha}}$ to 0.5, 0.7 or 1 in a factorized RNN with a spectral margin of 0.3 and found that only the model with ${\mathit{\alpha}=1}$ solved the ${\mathit{T}=1000}$ length copy task. We also experimented with a trainable slope ${\mathit{\alpha}}$, initialized to 0.7 and found that it converges to 0.96, further suggesting the optimal solution for the copy task is without a transition nonlinearity. Since the copy task is purely a memory task, one may imagine that a transition nonlinearity such as a tanh or ReLU may be detrimental to the task as it can lose information. Thus, we also tried a recent activation function that preserves information, called an orthogonal permutation linear unit (OPLU) \citep{chernodub2016norm}. The OPLU preserves norm, making a fully norm-preserving RNN possible. Interestingly, this activation function allowed us to recover identical results on the copy task to those without a nonlinearity for different spectral margins.

\subsubsection{Performance on Real Data}
\label{sec:real_data}

\begin{table*}
\centering
\setlength\tabcolsep{4pt}
\begin{tabular}{ cccccccc }
  & & MNIST & pMNIST & \multicolumn{2}{c}{PTBc-75} & \multicolumn{2}{c}{PTBc-300} \\
  margin & initialization & accuracy & accuracy & bpc & accuracy & bpc & accuracy \\
  \hline
  0   & orthogonal & 77.18 & 83.56 & 2.16 & 55.31 & 2.20 & 54.88 \\
  0.001 & orthogonal & 79.26 & 84.59 & - & - & - & - \\
  0.01 & orthogonal & 85.47 & 89.63 & 2.16 & 55.33 & 2.20 & 54.83 \\
  0.1 & orthogonal & 94.10 & 91.44 & 2.12 & 55.37 & 2.24 & 54.10 \\
  1 & orthogonal & 93.84 & 90.83 & 2.06 & 57.07 & 2.36 & 51.12 \\
  100 & orthogonal & - & - & 2.04 & 57.51 & 2.36 & 51.20 \\
  none& orthogonal & 93.24 & 90.51 & 2.06 & 57.38 & 2.34 & 51.30 \\
  none& Glorot normal & 66.71 & 79.33 & 2.08 & 57.37 & 2.34 & 51.04 \\
  none& identity & 53.53 & 42.72 & 2.25 & 53.83 & 2.68 & 45.35 \\
  \hline
  \multicolumn{2}{c}{LSTM} & 97.30 & 92.62 & 1.92 & 60.84 & 1.64 & 65.53 \\
\end{tabular}
\caption{Performance on MNIST and PTB for different spectral margins and initializations. Evaluated on classification of sequential MNIST (MNIST) and permuted sequential MNIST (pMNIST); character prediction on PTB sentences of up to 75 characters (PTBc-75) and up to 300 characters (PTBc-300).}
\label{tab:results_table}
\end{table*}

Having confirmed that an orthogonality constraint can negatively impact convergence rate, we seek to investigate the effect on model performance for tasks on real data. In Table \ref{tab:results_table}, we show the results of experiments on ordered and permuted sequential MNIST classification tasks and on the PTB character prediction task.

\begin{figure}[htb!]
\centering
\subfigure{
%0.416
\includegraphics[width=0.1885\textwidth]{figures/MNIST.pdf}
}
\subfigure{
%0.482
\includegraphics[width=0.2652\textwidth]{figures/pMNIST.pdf}
}
\caption{
Loss curves for different factorized RNN parameterizations on the sequential MNIST task (left) and the permuted sequential MNIST task (right). The spectral margin is denoted by m; models with no margin have singular values that are directly optimized with no constraints; Glorot refers to a factorized RNN with no margin that is initialized with Glorot normal initialization. Identity refers to the same, with identity initialization.
}
\label{fig:mnist}
\end{figure}

For the sequential MNIST experiments, loss curves  are shown in Figure \ref{fig:mnist} and reveal an increased convergence rate for larger spectral margins. We trained the factorized RNN models with 128 hidden units for 120 epochs. We also trained an LSTM with 128 hidden units (tanh activation) on both tasks for 150 epochs, configured with peephole connections, orthogonally initialized (and forget gate bias initialized to one), and trained with RMSprop (learning rate 0.0001, clipping gradients of magnitude 1).

For PTB character prediction, we evaluate results in terms of bits per character (bpc) and prediction accuracy. Prediction results are shown in \ref{tab:results_table} both for a subset of short sequences (up to 75 characters; 23\% of data) and for a subset of long sequences (up to 300 characters; 99\% of data). We trained factorized RNN models with 512 hidden units for 200 epochs with geodesic gradient descent on the bases (learning rate $10^{-6}$) and RMSprop on the other parameters (learning rate 0.001), using a tanh transition nonlinearity, and clipping gradients of 30 magnitude. As a rough point of reference, we also trained an LSTM with 512 hidden units for each of the data subsets (configured as for MNIST). On sequences up to 75 characters, LSTM performance was limited by early stopping of training due to overfitting.

\begin{figure*}[htb!]
\centering
\subfigure{
\includegraphics[width=0.3\textwidth]{figures/spectrum_pMNIST_hsb_0_001.pdf}
}
\subfigure{
\includegraphics[width=0.3\textwidth]{figures/spectrum_pMNIST_hsb_0_01.pdf}
}
\subfigure{
\includegraphics[width=0.3\textwidth]{figures/spectrum_pMNIST_hsb_0_1.pdf}
}
\subfigure{
\includegraphics[width=0.3\textwidth]{figures/spectrum_pMNIST_hsb_1.pdf}
}
\subfigure{
\includegraphics[width=0.3\textwidth]{figures/spectrum_pMNIST_hsb_none.pdf}
}
\subfigure{
\includegraphics[width=0.3\textwidth]{figures/spectrum_pMNIST_glorot.pdf}
}
\caption{
Singular value evolution on the permuted sequential MNIST task for factorized RNNs with different spectral margin sizes (m). The singular value distributions are summarized with the mean (green line, center) and standard deviation (green shading about mean), minimum (red, bottom) and maximum (blue, top) values. All models are initialized with orthogonal hidden to hidden transition matrices except for the model that yielded the plot on the bottom right, where Glorot normal initialization is used.
}
\label{fig:MNIST_spectra}
\end{figure*}

Interestingly, for both the ordered and permuted sequential MNIST tasks, models with a non-zero margin significantly outperform those that are constrained to have purely orthogonal transition matrices (margin of zero). The best results on both the ordered and sequential MNIST tasks were yielded by models with a spectral margin of 0.1, at 94.10\% accuracy and 91.44\% accuracy, respectively. An LSTM outperformed the RNNs in both tasks; nevertheless, RNNs with hidden to hidden transitions initialized as orthogonal matrices performed admirably without a memory component and without all of the additional parameters associated with gates. Indeed, orthogonally initialized RNNs performed almost on par with the LSTM in the permuted sequential MNIST task which presents longer distance dependencies than the ordered task. Although the optimal margin appears to be 0.1, RNNs with large margins perform almost identically to an RNN without a margin, as long as the transition matrix is initialized as orthogonal. On these tasks, orthogonal initialization appears to significantly outperform Glorot normal initialization \citep{glorot2010understanding} or initializing the matrix as identity. It is interesting to note that for the MNIST tasks, orthogonal initialization appears useful while orthogonality constraints appear mainly detrimental. This suggests that while orthogonality helps early training by stabilizing gradient flow across many time steps, orthogonality constraints may need to be loosened on some tasks so as not to over-constrain the model's representational ability.

Curiously, larger margins and even models without sigmoidal constraints on the spectrum (no margin) performed well as long as they were initialized to be orthogonal, suggesting that evolution away from orthogonality is not a serious problem on MNIST. It is not surprising that orthogonality is useful for the MNIST tasks since they depend on long distance signal propagation with a single output at the end of the input sequence. On the other hand, character prediction with PTB produces an output at every time step. Constraining deviation from orthogonality proved detrimental for short sentences and beneficial when long sentences were included. Furthermore, Glorot normal initialization did not perform worse than orthogonal initialization for PTB. Since an output is generated for every character in a sentence, short distance signal propagation is possible. Thus it is possible that the RNN is first learning very local dependencies between neighbouring characters and that given enough context, constraining deviation from orthogonality can help force the network to learn longer distance dependencies.

\subsubsection{Spectral and Gradient Evolution}

It is interesting to note that even long sequence lengths (T=1000) in the copy task can be solved efficiently with rather large margins on the spectrum. In Figure \ref{fig:gradients} we look at the gradient propagation of the loss from the last time step in the network with respect to the hidden activations. We can see that for a purely orthogonal parameterization of the transition matrix (when the margin is zero), the gradient norm is preserved across time steps, as expected. We further observe that with increasing margin size, the number of update steps over which this norm preservation survives decreases, though surprisingly not as quickly as expected.

\begin{figure}[htb!]
\centering
\includegraphics[width=0.5\textwidth]{figures/grad_copy_200.png}
\caption{The norm of the gradient of the loss from the last time step with respect to the hidden units at a given time step for a length 220 RNN over 1000 update iterations for different margins. Iterations are along the abscissa and time steps are denoted along the ordinate. The first column margins are: 0, 0.001, 0.01. The second column margins are: 0.1, 1, no margin. Gradient norms are normalized across the time dimension.}
\label{fig:gradients}
\end{figure}

Although the deviation of singular values from one should be slowed by the sigmoidal parameterizations, even parameterizations without a sigmoid (no margin) can be effectively trained for all but the longest sequence lengths. This suggests that the spectrum is not deviating far from orthogonality and that inputs to the hidden to hidden transitions are mostly not aligned along the dimensions of greatest expansion or contraction. We evaluated the spread of the spectrum in all of our experiments and found that indeed, singular values tend to stay well within their prescribed bounds and only reach the margin when using a very large learning rate that does not permit convergence. Furthermore, when transition matrices are initialized as orthogonal, singular values remain near one throughout training even without a sigmoidal margin for tasks that require long term memory (copy, adding, sequential MNIST). On the other hand, singular value distributions tend to drift away from one for PTB character prediction which may help explain why enforcing an orthogonality constraint can be helpful for this task, when modeling long sequences. Interestingly, singular values spread out less for longer sequence lengths (nevertheless, the T=10000 copy task could not be solved with no sigmoid on the spectrum).

\subsection{Exploring Soft Orthogonality Constraints}

\begin{figure*}[tb!]
\centering
\subfigure{
\includegraphics[width=0.192\textwidth]{figures/Worth_copy_200.pdf}
}
\subfigure{
\includegraphics[width=0.257\textwidth]{figures/Worth_factorized_copy_500.pdf}
}
\subfigure{
\includegraphics[width=0.189\textwidth]
{figures/sorth_factorized_copy200_hsb_none.pdf}
}
\subfigure{
\includegraphics[width=0.262\textwidth]
{figures/sorth_factorized_copy200_hsb_1.pdf}
}
\caption{Accuracy curves on the copy task for different strengths of soft orthogonality constraints. All sequence lengths are $\mathit{T}=200$, except in (B) which is run on $\mathit{T}=500$. A soft orthogonality constraint is applied to the transition matrix $\mathbf{W}$ of a regular RNN in (A) and that of a factorized RNN in (B). A mean one Gaussian prior is applied to the singular values of a factorized RNN in (C) and (D); the spectrum in (D) has a sigmoidal parameterization with a large margin of 1. Loosening orthogonality speeds convergence.}
\label{fig:soft_copy}
\end{figure*}

We visualize the spread of singular values for different model parameterizations on the permuted sequential MNIST task in Figure \ref{fig:MNIST_spectra}. Curiously, we find that the distribution of singular values tends to shift upward to a mean of approximately 1.05 on both the ordered and permuted sequential MNIST tasks. We note that in those experiments, a tanh transition nonlinearity was used which is contractive in both the forward signal pass and the gradient backward pass. An upward shift in the distribution of singular values of the transition matrix would help compensate for that contraction. Indeed, \citet{saxe2013exact} describe this as a possibly good regime for learning in deep neural networks. That the model appears to evolve toward this regime suggests that deviating from it may incur a cost. This is interesting because the cost function cannot take into account numerical issues such as vanishing or exploding gradients (or forward signals); we do not know what could make this deviation costly.

Unlike orthgonally initialized models, the RNN on the bottom right of Figure \ref{fig:MNIST_spectra} with Glorot normal initialized transition matrices begins and ends with a wide singular spectrum. While there is no clear positive shift in the distribution of singular values, the mean value appears to very gradually increase for both the ordered and permuted sequential MNIST tasks. If the model is to be expected to positively shift singular values to compensate for the contractivity of the tanh nonlinearity, it is not doing so well for the Glorot-initialized case; however, this may be due to the inefficiency of training as a result of vanishing gradients, given that initialization.

That the transition matrix may be compensating for the contraction of the tanh is supported by further experiments: applying a 1.05 pre-activation gain appears to allow a model with a margin of 0 to nearly match the top performance reached on both of the MNIST tasks. Furthermore, when using the OPLU norm-preserving activation function \citep{chernodub2016norm}, we found that orthogonally initialized models performed equally well with all margins, achieving over 90\% accuracy on the permuted sequential MNIST task.

It is reasonable to assume that the sequential MNIST task benefits from a lack of a transition nonlinearity because it is a pure memory task. However, using no transition nonlinearity results in reduced accuracy. Furthermore, while a nonlinear transition may lose information, thus countering the task of memory preservation, it is critical on more advanced tasks like PTB character prediction. Indeed, tasks that require more than just memory preservation in the hidden state can benefit from forgetting. \citet{jing2017gated} demonstrated that adding a forgetting mechanism to RNNs constrained to have an orthogonal transition matrix improved performance on such tasks. Some forgetting could be achieved by allowing deviation from orthogonality.

Having established that it may indeed be useful to step away from orthogonality, here we explore two forms of soft constraints (rather than hard bounds as above) on hidden to hidden transition matrix orthogonality. The first is a simple penalty that directly encourages a transition matrix $\bf{W}$ to be orthogonal, of the form
%\begin{equation}
$\lambda||\mathbf{W}^T \mathbf{W} - \mathbf{I}||_2^2$.
%\end{equation}
This is similar to the orthogonality penalty introduced by \citet{henaff2016orthogonal}. In subfigures (A) and (B) of Figure \ref{fig:soft_copy}, we explore the effect of weakening this form of regularization. We trained both a regular non-factorized RNN on the $\mathit{T}=200$ copy task (A) and a factorized RNN with orthogonal bases on the $\mathit{T}=500$ copy task (B). For the regular RNN, we had to reduce the learning rate to $10^{-5}$. Here again we see that weakening the strength of the orthogonality-encouraging penalty can increase convergence speed.

The second approach we explore replaces the sigmoidal margin parameterization with a mean one Gaussian prior on the singular values. In subfigures (C) and (D) of Figure \ref{fig:soft_copy}, we visualize the accuracy on the length 200 copy task, using geoSGD (learning rate $10^{-6})$ to keep $\mathbf{U}$ and $\mathbf{V}$ orthogonal and different strengths $\gamma$ of a Gaussian prior with mean one on the singular values $s_i$: $\gamma \sum_i ||s_i-1||^2$. We trained these experiments with regular SGD on the spectrum and other non-orthogonal parameter matrices, using a $10^{-5}$ learning rate. We see that strong priors lead to slow convergence. Loosening the strength of the prior makes the optimization more efficient. Furthermore, we compare a direct parameterization of the spectrum (no sigmoid) in (C) with a sigmoidal parameterization, using a large margin of 1 in (D). Without the sigmoidal parameterization, optimization quickly becomes unstable; on the other hand, the optimization also becomes unstable if the prior is removed completely in the sigmoidal formulation (margin 1). These results further motivate the idea that parameterizations that deviate from orthogonality may perform better than purely orthogonal ones, as long as they are sufficiently constrained to avoid instability during training.

\section{Conclusions}
We have explored a number of methods for controlling the expansivity of gradients during backpropagation based learning in RNNs through manipulating orthogonality constraints and regularization on weight matrices. Our experiments indicate that while orthogonal initialization may be beneficial, maintaining hard constraints on orthogonality can be detrimental. Indeed, moving away from hard constraints on matrix orthogonality can help improve optimization convergence rate and model performance. However, we also observe with synthetic tasks that relaxing regularization which encourages the spectral norms of weight matrices to be close to one too much, or allowing bounds on the spectral norms of weight matrices to be too wide, can reverse these gains and may lead to unstable optimization.

\subsubsection*{Acknowledgments}
We thank the Natural Sciences and Engineeering Research Council (NSERC) of Canada and Samsung for supporting this research.


\bibliography{icml2017}
\bibliographystyle{icml2017}

\end{document}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% ICML 2017 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use the following line _only_ if you're still using LaTeX 2.09.
%\documentstyle[icml2017,epsf,natbib]{article}
% If you rely on Latex2e packages, like most moden people use this:
\documentclass[11pt,letterpaper]{article}
\usepackage{emnlp2017}
\usepackage{times}
\usepackage{latexsym}

% Uncomment this line for the final submission:
\emnlpfinalcopy

%  Enter the EMNLP Paper ID here:
%\def\emnlppaperid{***}
\def\emnlppaperid{1462}
% 1462

\newcommand\BibTeX{B{\sc ib}\TeX}

\usepackage{floatrow} % Alex: has to go here in order to compile...

% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
%\usepackage{subfigure} 

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2017} with
% \usepackage[nohyperref]{icml2017} above.
\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
%\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage{caption}
\usepackage{subcaption}
%\usepackage[dvips]{graphicx}
%\usepackage[T1]{fontenc}
%\usepackage[utf8]{inputenc}
%\usepackage{authblk}
\usepackage{booktabs}
%\PassOptionsToPackage{hyphens}{url}
\usepackage{multirow}
%\usepackage{afterpage}
\usepackage{amsmath}
\usepackage{amsfonts}
%\usepackage{adjustbox}
%\usepackage{titlesec}
%\usepackage{url}
\usepackage{anyfontsize}
\usepackage{stfloats}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''

% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Proceedings of the...''
%\usepackage[accepted]{icml2017}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
%\icmltitlerunning{Variational Encoder-Decoders with Piecewise Latent Variables}
%\icmltitlerunning{Variational Autoencoders with Piecewise Constant Latent Variables}
%\icmltitlerunning{Piecewise Latent Variables for Natural Language Processing}

\begin{document} 

\title{Piecewise Latent Variables for Neural Variational Text Processing}

\author{Iulian V. Serban$^1$\thanks{ \hspace{0.1cm} The first two authors contributed equally.} \and Alexander G. Ororbia II$^2$\footnotemark[1] \and Joelle Pineau$^3$ \and Aaron Courville$^1$ \\ $^1$ Department of Computer Science and Operations Research, Universite de Montreal \\ $^2$College of Information Sciences \& Technology, Penn State University \\ $^3$School of Computer Science, McGill University \\ \texttt{iulian\hspace{0.1cm}[DOT]\hspace{0.1cm}vlad\hspace{0.1cm}[DOT]\hspace{0.1cm}serban\hspace{0.1cm}[AT]\hspace{0.1cm}umontreal\hspace{0.1cm}[DOT]\hspace{0.1cm}ca} \\ \texttt{ago109\hspace{0.1cm}[AT]\hspace{0.1cm}psu\hspace{0.1cm}[DOT]\hspace{0.1cm}edu} \\ \texttt{jpineau\hspace{0.1cm}[AT]\hspace{0.1cm}cs\hspace{0.1cm}[DOT]\hspace{0.1cm}mcgill\hspace{0.1cm}[DOT]\hspace{0.1cm}ca} \\ \texttt{aaron\hspace{0.1cm}[DOT]\hspace{0.1cm}courville\hspace{0.1cm}[AT]\hspace{0.1cm}umontreal\hspace{0.1cm}[DOT]\hspace{0.1cm}ca}}

\setlength\titlebox{2.50in}

%AO: I updated the title to reflect one reviewer's post-rebuttal comment
%    I also chose this title since I think it keeps things somewhat more general (since we train various encoder-decoder models) and we do start from general neural variational inference and work our way towards showing how the piecewise variables fit in and how the models change as a result :)
\maketitle


%\twocolumn[
%\icmltitle{Variational Encoder-Decoders with Piecewise Latent Variables}
%\icmltitle{Variational Autoencoders with Piecewise Latent Variables}
%\icmltitle{Variational Autoencoders with Piecewise Latent Variables for Text Processing}
%\icmltitle{Piecewise Latent Variables for Natural Language Processing}
%\title{Piecewise Latent Variables for Natural Language Processing}



% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2017
% package.

% list of affiliations. the first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% you can specify symbols, otherwise they are numbered in order
% ideally, you should not use this facility. affiliations will be numbered
% in order of appearance and this is the preferred way.

%\icmlsetsymbol{equal}{*}

%\begin{icmlauthorlist}
%\icmlauthor{Iulian V. Serban}{equal,mont}
%\icmlauthor{Alexander G. Ororbia II}{equal,psu}
%\icmlauthor{Joelle Pineau}{mcgill}
%\icmlauthor{Aaron Courville}{mont}
%\end{icmlauthorlist}

%\icmlaffiliation{mont}{Department of Computer Science and Operations Research, Universite de Montreal, Canada}
%\icmlaffiliation{psu}{College of Information Sciences and Technology, Penn State University, USA}
%\icmlaffiliation{mcgill}{School of Computer Science, McGill University, Canada}

%\icmlcorrespondingauthor{To be added}{blank}

%\icmlcorrespondingauthor{Iulian V. Serban}{iulian.vlad.serban@umontreal.ca}
%\icmlcorrespondingauthor{Alexander G. Ororbia II}{ago109@psu.edu}

% You may provide any keywords that you 
% find helpful for describing your paper; these are used to populate 
% the "keywords" metadata in the PDF but will not be shown in the document
%\icmlkeywords{variational encoder-decoder, machine learning, piecewise latent variables}

%\vskip 0.3in
%]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.
%\footnotetext{hi}

\begin{abstract}
Advances in neural variational inference have facilitated the learning of powerful directed graphical models with continuous latent variables, such as variational autoencoders.
%The hope is that such models will learn to represent latent factors in a wide range of real-world data.
The hope is that such models will learn to represent rich, multi-modal latent factors in real-world data, such as natural language text.
%However, latent factors in real-world data --- such as natural language text -- often posses latent factors, which follow highly non-linear, multi-modal distributions.
%, such as natural language text. %, audio, and images.
%However, latent factors in real-world data are often highly complex; for example, topics in newswire text and responses in conversational dialogue often posses latent factors, which follow non-linear (non-smooth), multi-modal distributions. % with many statistical outliers.
However, current models often assume simplistic priors on the latent variables --- such as the uni-modal Gaussian distribution --- which are incapable of representing complex latent factors efficiently. To overcome this restriction, we propose the simple, but highly flexible, piecewise constant distribution.
This distribution has the capacity to represent an exponential number of modes of a latent target distribution, while remaining mathematically tractable. Our results demonstrate that incorporating this new latent distribution into different models yields substantial improvements in natural language processing tasks such as document modeling and natural language generation for dialogue.
\end{abstract}


% OLD ABSTRACT
% \begin{abstract}
% Recent advances in neural variational inference have facilitated training of powerful directed graphical models with continuous latent variables, such as variational autoencoders.
% However, these models usually assume simple priors --- such as the uni-modal Gaussian distribution --- yet many real-world data distributions are highly complex and multi-modal.
% Examples of complex, multi-modal distributions range from topics in newswire text to conversational dialogue responses.
% When such latent variable models are applied to these domains, the restrictive priors make it difficult to capture non-linear, multi-modal probability manifolds over latent concepts.
% To overcome this critical restriction, we propose the simple, but highly flexible, piecewise constant distribution.
% The piecewise constant distribution has the capacity to represent an exponential number of modes of a latent target distribution, and --- as we will show --- it is mathematically tractable within the variational autoencoder framework.
% %In comparison to alternative approaches, such as stacked multivariate Gaussian distributions or auto-regressive approximate posteriors, our latent distribution does not require additional approximations or multiple sampling steps.
% We show that incorporating this new latent variable parametrization into neural models yields substantial performance improvements in natural language processing tasks such as document modeling and dialogue modeling.
% \end{abstract} 


\section{Introduction}
\label{intro}
The development of the variational autoencoder framework \citep{kingma2013auto,rezende2014stochastic} has paved the way for learning large-scale, directed latent variable models. This has led to significant progress in a diverse set of machine learning applications, ranging from computer vision \citep{gregor2015draw,larsen2015autoencoding} to natural language processing tasks \citep{mnih2014neural,miao2015neural,bowman2015generating,serban2016hierarchical}.
It is hoped that this framework will enable the learning of generative processes of real-world data --- including text, audio and images --- by disentangling and representing the underlying latent factors in the data. However, latent factors in real-world data are often highly complex. For example, topics in newswire text and responses in conversational dialogue often posses latent factors that follow non-linear (non-smooth), multi-modal distributions (i.e.\@ distributions with multiple local maxima).

Nevertheless, the majority of current models assume a simple prior in the form of a multivariate Gaussian distribution in order to maintain mathematical and computational tractability.
This is often a highly restrictive and unrealistic assumption to impose on the structure of the latent variables.
First, it imposes a strong uni-modal structure on the latent variable space; latent variable samples from the generating model (prior distribution) all cluster around a single mean.
Second, it forces the latent variables to follow a perfectly symmetric distribution with constant kurtosis; this makes it difficult to represent asymmetric or rarely occurring factors.
%Second, it encourages local smoothness on the latent variables, where the similarity between two latent variable samples decreases exponentially as their distance increase.
%Furthermore, variational autoencoding models naturally incorporate a Bayesian modeling perspective,
%by enabling the integration of problem-dependent knowledge within the prior on the generating distribution.
Such constraints on the latent variables increase pressure on the down-stream generative model, which in turn is forced to carefully partition the probability mass for each latent factor throughout its intermediate layers.
For complex, multi-modal distributions --- such as the distribution over topics in a text corpus, or natural language responses in a dialogue system --- the uni-modal Gaussian prior inhibits the model's ability to extract and represent important latent structure in the data.
In order to learn more expressive latent variable models, we therefore need more flexible, yet tractable, priors.

In this paper, we introduce a simple, flexible prior distribution based on the piecewise constant distribution.
We derive an analytical, tractable form that is applicable to the variational autoencoder framework and propose a differentiable parametrization for it.
We then evaluate the effectiveness of the distribution when utilized both as a prior and as approximate posterior across variational architectures in two natural language processing tasks: document modeling and natural language generation for dialogue.
We show that the piecewise constant distribution is able to capture elements of a target distribution that cannot be captured by simpler priors --- such as the uni-modal Gaussian.
We demonstrate state-of-the-art results on three document modeling tasks, and show improvements on a dialogue natural language generation.
Finally, we illustrate qualitatively how the piecewise constant distribution represents multi-modal latent structure in the data.


% OLD INTRODUCTION

% \section{Introduction}
% \label{intro}
% The development of the variational autoencoding framework \citep{kingma2013auto,rezende2014stochastic} has lead to a tremendous amount of progress in learning large-scale, directed latent variable models.
% Progress has been made in a diverse set of machine learning applications ranging from computer vision \citep{gregor2015draw,larsen2015autoencoding} to natural language processing tasks \citep{mnih2014neural,miao2015neural,bowman2015generating,serban2016hierarchical}.
% Furthermore, variational autoencoding models naturally incorporate a Bayesian modeling perspective,
% by enabling the integration of problem-dependent knowledge within the prior on the generating distribution.
% %(e.g.\@ strong or weak prior assumptions on the generating distribution).

% %In principle, this is a powerful and computationally efficient framework.
% However, the majority of models proposed assume a simple prior in the form of a multivariate Gaussian distribution in order to maintain mathematical and computational tractability.
% Although this assumption on the prior has lead to favorable results on several tasks, % \citep{miao2015neural},
% it is clearly a restrictive and often unrealistic assumption to impose on the structure of the latent variables.
% First, it imposes a strong uni-modal structure on the latent variable space;
% latent samples from the generating model (prior distribution) all cluster around a single mean.
% Second, it encourages local smoothness on the latent variables, where the similarity between two latent variable samples decreases exponentially as their distance increase.
% These restrictions increase the pressure on the down-stream generative model --- also called the decoder --- which is forced to non-linearly carve out separate probability mass for each latent concepts.
% %This pressure inevitably causes the model to underfit its representation of complex, multi-modal latent concepts.
% Thus, for complex, multi-modal distributions --- such as the distribution over topics in a text corpus, or natural language responses in a dialogue system --- the uni-modal Gaussian prior inhibits the model's ability to extract and represent important latent structure in the data.
% In order to learn more expressive latent variable models, %--- in particular, models with complex, multi-modal latent variable structures for natural language processing applications ---
% we therefore need a more powerful and more flexible and tractable prior.%, which has the ability to model multiple modes of a target distribution.
% %all the useful information and structure it can from a given sample.
% %, which clearly are multi-modal and complex in nature, such a uni-modal prior inhibits the model's ability to extract all the useful information and structure it can from a given sample.
% %we argue that for distributions such as that of topics over a text corpus, which clearly are multi-modal and complex in nature, such a uni-modal prior inhibits the model's ability to extract all the useful information and structure it can from a given sample.
% %This yields favorable results in practice --- and, more importantly, allows for a simple analytical derivation of the KL-divergence to further reduce variance of the gradient estimator --- as evidenced by the success reported in various applications \citep{miao2015neural}, we argue that for distributions such as that of topics over a text corpus, which clearly are multi-modal and complex in nature, such a uni-modal prior inhibits the model's ability to extract all the useful information and structure it can from a given sample.
% %To learn better, even more expressive models, especially those of discrete distributions such as text, we seek a suitable, flexible prior than can be automatically adapted to model multiple modes of a target distribution.

% In this paper, we introduce a tractable, efficient, flexible prior distribution based on the piecewise constant distribution. % that is suitable for distributions such as those found in natural language text. 
% We evaluate the effectiveness of the distribution when utilized both as a prior and approximate posterior across variational architectures in two representative tasks: document modeling and dialogue modeling.
% We find that the piecewise constant distribution prior is able to capture elements of a target distribution that simpler priors --- such as the uni-modal Gaussian --- cannot model, which allows the models to extract richer structure from data.
% In particular, we achieve state-of-the-art results on three document modeling tasks.
% Furthermore, we demonstrate substantial improvements on the dialogue modeling task.

\section{Related Work}
\label{related_work}
The idea of using an artificial neural network to approximate an inference model dates back to the early work of Hinton and colleagues \citep{hinton_mdl_1994,hinton1995wake,dayan1996varieties}.
%However, initial work lacked low-bias, low-variance estimators for the parameter updates. 
Researchers later proposed Markov chain Monte Carlo methods (MCMC) \citep{neal1992connectionist}, which do not scale well and mix slowly, as well as variational approaches which require a tractable, factored distribution to approximate the true posterior distribution \citep{jordan1999introduction}.
Others have since proposed using feed-forward inference models to initialize the mean-field inference algorithm for training Boltzmann architectures \citep{salakhutdinov2010efficient,ororbia2015online}.
%However, these approaches are limited by the mean-field inference's inability to model structured posteriors.
Recently, the variational autoencoder framework (VAE) was proposed by \citet{kingma2013auto} and \citet{rezende2014stochastic}, closely related to the method proposed by \citet{mnih2014neural}.
This framework allows the joint training of an inference network and a directed generative model, maximizing a variational lower-bound on the data log-likelihood and facilitating exact sampling of the variational posterior. 
Our work extends this framework. %in the next section.

With respect to document modeling, neural architectures have been shown to outperform well-established topic models such as Latent Dirichlet Allocation (LDA) \citep{hofmann1999probabilistic,blei2003latent}.
%For example, it has been demonstrated that models based on the Boltzmann machine, which learn semantic binary vectors (binary latent variables), perform very well\citep{hofmann1999probabilistic}.
Researchers have successfully proposed several models involving discrete latent variables \citep{salakhutdinov2009semantic,hinton_softmax_2009,srivastava2013modeling,larochelle_neural_2012,uria2014deep,lauly2016document,bornschein2014reweighted,mnih2014neural}.
%Work involving discrete latent variables include the constrained Poisson model \citep{salakhutdinov2009semantic}, the Replicated Softmax model \citep{hinton_softmax_2009} and the Over-Replicated Softmax model \citep{srivastava2013modeling}, as well as similar, auto-regressive neural architectures and deep directed graphical models \citep{larochelle_neural_2012,uria2014deep,lauly2016document,bornschein2014reweighted}.
%In particular, \citet{mnih2014neural} showed that using NVIL yields excellent generative models of documents.
The success of such discrete latent variable models --- which are able to partition probability mass into separate regions --- serves as one of our main motivations for investigating models with more flexible continuous latent variables for document modeling.
% , multi-modal, continuous latent variables for document modeling.
More recently, \citet{miao2015neural} proposed to use continuous latent variables for document modeling. %achieving state-of-the-art results.
%This model will be described later.

Researchers have also investigated latent variable models for dialogue modeling and dialogue natural language generation \citep{bangalore2008learning, crook2009unsupervised,zhai2014discovering}.
The success of discrete latent variable models in this task also motivates our investigation of more flexible continuous latent variables.
Closely related to our proposed approach is the Variational Hierarchical Recurrent Encoder-Decoder (\emph{VHRED}, described below) \citep{serban2016hierarchical}, a neural architecture with latent multivariate Gaussian variables.
In parallel with our work, \citet{zhao2017learning} has also proposed a latent variable model for dialogue modeling with the specific goal of generating diverse natural language responses.

Researchers have explored more flexible distributions for the latent variables in VAEs, such as autoregressive distributions, hierarchical probabilistic models and approximations based on MCMC sampling \citep{rezende2014stochastic,rezende2015variational,kingma2016improving,ranganath2016hierarchical,maaloe2016auxiliary,salimans2015markov,burda2015importance,chen2016variational,ruiz2016generalized}.
These are all complimentary to our approach; it is possible to combine them with the piecewise constant latent variables.
In parallel to our work, multiple research groups have also proposed VAEs with discrete latent variables \citep{maddison2016concrete,jang2016categorical,rolfe2016discrete,johnson2016composing}.
This is a promising line of research, however these approaches often require approximations which may be inaccurate when applied to larger scale tasks, such as document modeling or natural language generation.
Finally, discrete latent variables may be inappropriate for certain natural language processing tasks.

% % TODO: Julian: Alex, are you sure about including the reference suh2016gaussian? 
% % Alex: Nope, got rid of it upon further inspection - \citep{suh2016gaussian}
% Researchers have explored alternative distributions for the latent variables in VAEs, including autoregressive and discrete distributions.
% \citet{rezende2015variational} propose an approach called normalizing flows which computes a more complex, potentially multi-modal distribution, by projecting standard Gaussian variables through a sequence of non-linear transformations.
% This approach is similar to the inverse auto-regressive flow proposed by \citet{kingma2016improving}.
% Unfortunately, both normalizing flows and auto-regressive flow are only applicable to the approximate posterior distribution; typically these approaches require fixing the prior distribution to a uni-modal multivariate Gaussian.
% Furthermore, to the best of our knowledge, neither of these approaches have been tested in the context of large scale text processing tasks, such as the tasks we consider in this work.
% A complementary approach is to combine variational inference with MCMC sampling \citep{salimans2015markov,burda2015importance}, however this is computationally expensive and therefore difficult to scale up to many real-world tasks.
% %Enriching the latent variable distributions has also been investigated by \citep{maaloe2016auxiliary}.
% In parallel with our work, a VAE with autoregressive prior and posterior was proposed by \citet{chen2016variational}.
% Such an autoregressive prior is analogous to having a deep directed latent variable model \citep{rezende2014stochastic}.
% This approaches are promising for learning rich, flexible latent distributions, however, they are likely to suffer from two major drawbacks.
% First, the variance of the gradient estimator grows with the number of autoregressive steps or layers. %This makes it difficult to learn latent representations for rare concepts and for multi-modal representations.
% Second, it is not clear how many modes such models can represent or how their inductive biases will affect their performance on tasks containing multi-modal latent structure.
% The piecewise constant latent variables we propose in this work do not suffer from these drawbacks.
% The piecewise constant variables incur low variance in the gradient estimator, and can, in principle, represent a number of modes exponential in the number of latent variables.
% %However, due to the sequential sampling required by the autoregressive model, the parameter gradients will incur higher variance which could make it difficult to learn latent distributions for real-world natural language processing tasks.
% Also in parallel with our work, multiple research groups have proposed VAEs with discrete latent variables, which are learnt using numerical approximations \citep{maddison2016concrete,jang2016categorical,rolfe2016discrete}.
% This is a highly promising line of research, however it is not clear how accurate the approximations will be when applied to larger scale tasks, such as document modeling or dialogue modeling.
% Second, models with discrete latent variables may not generalize as well as models with continuous, distributed latent variables for certain types of natural language processing tasks.

%\subsection{Approaches for Learning Multi-modal Latent Variables}

%\textbf{Mixture of Gaussians} Perhaps the most direct and naive approach to learning multi-modal latent variables is to parametrize the latent variable prior and approximate posterior distributions as a mixture of Gaussians.
%However, the KL divergence between two mixtures of Gaussian distributions cannot be computed in closed form \citep{durrieu2012lower}.
%To train such a model, one would have to either resort to MCMC sampling, which may slow down and hurt the training process due to the high variance it incurs, or resort to approximations of the KL divergence, which may also hurt the training process.\footnote{Our lab has previously investigated incorporating mixture of Gaussian models into the autoencoder framework, but without any success. This work has not been published.}

%\textbf{Deep Directed Models}
%An alternative to a mixture of Gaussians parametrization is to construct a deep directed graphical model composed of multiple layers of uni-modal latent variables (e.g.\@ multivariate Gaussians) \citep{rezende2014stochastic}.
%Such models have the potential to capture highly complex, multi-modal latent variable representations through the marginal distribution of the top-layer latent variables.
%However, this approaches has two major drawbacks.
%First, the variance of the gradient estimator grows with the number of layers. %This makes it difficult to learn highly multi-modal latent representations. Second, it is not clear how many modes such models can represent or how their inductive biases will affect their performance on tasks containing multi-modal latent structure.
%The piecewise constant latent variables we propose do not suffer from either of these two drawbacks; the piecewise constant variables incur low variance in the gradient estimator, and can, in principle, represent a number of modes exponential in the number of latent variables.

%\textbf{Discrete Latent Variables}
%A third approach for learning multi-modal latent representations is to instead use discrete latent variables as discussed above.
%For example, the learning procedure proposed by \citep{mnih2014neural} for discrete latent variables can easily be combined with the variational autoencoder framework to learn models with both discrete and continuous latent variables.
%However, the major drawback of discrete latent variables is the high variance in the gradient estimator.
%Without further approximations, it might be difficult to scale up models with discrete latent variables for real-world tasks.

%what tasks their inductive biases %will be useful for.
% in comparison to the prior distribution we propose, 

\section{Neural Variational Models}
We start by introducing the neural variational learning framework.
%Then, we present the proposed piecewise constant distribution, which aims to be a tractable, highly flexible distribution for the model prior and approximate posterior.
We focus on modeling discrete output variables (e.g.\@ words) in the context of natural language processing applications.
However, the framework can easily be adapted to handle continuous output variables.%, such as images.

\subsection{Neural Variational Learning}
Let $w_1, \dots, w_N$ be a sequence of $N$ tokens (words) conditioned on a continuous latent variable $z$.
Further, let $c$ be an additional observed variable which conditions both $z$ and $w_1, \dots, w_N$.
Then, the distribution over words is:
{\fontsize{10}{12} % Alex: I used fontsize re-scaling, not sure if there's a better way :-(
\begin{align}
P_{\theta}(w_1, \dots, w_N | c) = \int \prod_{n=1}^N P_{\theta}(w_n | w_{<n}, z, c) P_{\theta}(z | c) dz, \nonumber
\end{align}
}%
\noindent
where $\theta$ are the model parameters.
The model first generates the higher-level, continuous latent variable $z$ conditioned on $c$.
Given $z$ and $c$, it then generates the word sequence $w_1, \dots, w_N$.
For unsupervised modeling of documents, the $c$ is excluded and the words are assumed to be independent of each other, when conditioned on $z$:
{\fontsize{10}{12}
\begin{align}
P_{\theta}(w_1, \dots, w_N) = \int \prod_{n=1}^N P_{\theta}(w_n | z) P_{\theta}(z) dz. \nonumber
\end{align}
}
Model parameters can be learned using the variational lower-bound \citep{kingma2013auto}:
{\fontsize{10}{12}
\begin{align}
\label{eq:variatonal_lower_bound}
\log P_{\theta} & (w_1, \dots, w_N | c) \nonumber \\
 \geq & \quad \text{E}_{z \sim Q_{\psi}(z | w_1, \dots, w_N, c) }[\log P_{\theta}(w_n | w_{<n}, z, c)] \nonumber \\
     & - \text{KL} \left [ Q_{\psi}(z | w_1, \dots, w_N, c) || P_{\theta}(z | c) \right ], 
\end{align}
}
%\begin{align}
%\log P_{\theta}(w_1, \dots, w_N, z)  \geq \text{E}_{z \sim Q_{\psi}(z | w_1, \dots, w_N) }[\log P_{\theta}(w_n | w_{<n}, z)] - \text{KL} \left [ Q_{\psi}(z | w_1, \dots, w_N) || P_{\theta}(z) \right ], \label{eq:variatonal_lower_bound}
%\end{align}
\noindent 
where we note that $Q_{\psi}(z | w_1, \dots, w_N, c)$ is the approximation to the intractable, true posterior $P_{\theta}(z | w_1, \dots, w_N, c)$.
$Q$ is called the \textit{encoder}, or sometimes the \textit{recognition model} or \textit{inference model}, and it is parametrized by $\psi$.
The distribution $P_{\theta}(z | c)$ is the prior model for $z$, where the only available information is $c$.
The VAE framework further employs the re-parametrization trick, which allows one to move the derivative of the lower-bound inside the expectation.
To accomplish this, $z$ is parametrized as a transformation of a fixed, parameter-free random distribution $z = f_\theta(\epsilon)$,
%\begin{align}
%z = f_\theta(\epsilon),
%\end{align}
where $\epsilon$ is drawn from a random distribution. %(e.g.\@ a standard Gaussian distribution with zero mean and unit standard deviation, or a uniform distribution in the interval $[0, 1]$).
Here, $f$ is a transformation of $\epsilon$, parametrized by $\theta$, such that $f_\theta(\epsilon) \sim P_{\theta}(z | c)$.
For example, $\epsilon$ might be drawn from a standard Gaussian distribution and $f$ might be defined as $f_\theta(\epsilon) = \mu + \sigma \epsilon$, where $\mu$ and $\sigma$ are in the parameter set $\theta$.
In this case, $z$ is able to represent any Gaussian with mean $\mu$ and variance $\sigma^2$.

Model parameters are learned by maximizing the variational lower-bound in eq.\@ \eqref{eq:variatonal_lower_bound} using gradient descent, where the expectation is computed using samples from the approximate posterior. %See also \citet {kingma2013auto}.

The majority of work on VAEs propose to parametrize $z$ as multivariate Gaussian distribtions.
However, this unrealistic assumption may critically hurt the expressiveness of the latent variable model.
See Appendix \ref{appendix_0} for a detailed discussion. This motivates the proposed piecewise constant latent variable distribution.

% Aaron: \paragraph{Gaussian priors are sometimes inappropriate.} Here being concrete is important. You do so with your Ubuntu example, but you need to be more explicit about how a Gaussian would get it wrong. The issue you raise is an important one: that in a dialogue, conditioning on context could easily result in a complicated prior as there may be genuine ambiguities that are not easily resolvable with a unimodal variable. 
%\subsubsection{Gaussian priors are sometimes inappropriate}

\subsection{Piecewise Constant Distribution}
\label{piecewise_prior}
We propose to learn latent variables by parametrizing $z$ using a piecewise constant probability density function (PDF).
This should allow $z$ to represent complex aspects of the data distribution in latent variable space, such as non-smooth regions of probability mass and multiple modes.
%In particular, it should enable $z$ to represent islands of probability mass relevant both for representing diverse topics in news articles and for representing ambiguity and uncertainty in natural language conversations.
%From a manifold learning perspective, this extension translates into expanding the set of manifolds representable by the model parameters to include more non-linear manifolds -- in particular, manifolds where there exists separate clusters of probability mass.

Let $n \in \mathbb{N}$ be the number of piecewise constant components. We assume $z$ is drawn from PDF:
{\fontsize{9.75}{11.7}
\begin{align}
P(z) = \dfrac{1}{K} \sum_{i=1}^n 1_{ \left (\dfrac{i-1}{n} \leq z \leq \dfrac{i}{n} \right )} a_i,\label{piecewise_pdf}
\end{align}
}
where $1_{(x)}$ is the indicator function, which is one when $x$ is true and otherwise zero.
The distribution parameters are $a_i > 0$, for $i=1,\dots,n$. The normalization constant is:
{\fontsize{9.75}{11.7}
\begin{align}
K & = \sum_{i=1}^n K_i, \ \text{where} \ K_0=0, K_i = \dfrac{a_i}{n}, \ \text{for} \ i=1,\dots,n. \nonumber
\end{align}
}
It is straightforward to show that a piecewise constant distribution with more than $n>2$ pieces is capable of representing a bi-modal distribution.
When $n>2$, a vector $z$ of piecewise constant variables can represent a probability density with $2^{|z|}$ modes.
Figure \ref{fig:joint_prob} illustrates how these variables help model complex, multi-modal distributions. %work with Gaussian latent variables in order to 

In order to compute the variational bound, we need to draw samples from the piecewise constant distribution using its inverse cumulative distribution  function (CDF).
Further, we need to compute the KL divergence between the prior and posterior.
The inverse CDF and KL divergence quantities are both derived in Appendix \ref{appendix_a}.
During training we must compute derivatives of the variational bound in eq.\@ \eqref{eq:variatonal_lower_bound}.
These expressions involve derivatives of indicator functions, which have derivatives zero everywhere except for the changing points where the derivative is undefined.
However, the probability of sampling the value exactly at its changing point is effectively zero. Thus, we fix these derivatives to zero. Similar approximations are used in training networks with rectified linear units. %\footnote{We thank Christian A.\@ Naesseth for pointing this out.}

%In order to learn models with piecewise constant latent variables, we need to draw samples from the distribution. This can be done through the inverse cumulative distribution function.

% the 

% using the inverse cumulative distribution:
% {\fontsize{9.75}{11.7}
% \begin{align}
% \phi^{-1}(\epsilon) &= \sum_{i=1}^n 1_{ \left ( \dfrac{1}{K} \sum_{j=0}^{i-1} K_j \leq \epsilon \leq \dfrac{1}{K} \sum_{j=0}^{i} K_j  \right )} \nonumber \\ & \quad \quad \qquad \cdot \left ( \dfrac{i-1}{n} + \dfrac{K}{a_i} \left ( \epsilon - \dfrac{1}{K} \sum_{j=0}^{i-1} K_j \right ) \right ) \nonumber \\
% z &= \phi^{-1}(\epsilon), \quad \text{where} \ \epsilon \sim \text{Uniform}(0, 1) \nonumber %\label{piece_inv_cdf}
% \end{align}
% }
% Furthermore, we need to compute the KL divergence between the prior and posterior.
% Since the cumulative distribution can be decomposed as a sum of integrals, the KL divergence can be computed in closed form:
% %{\fontsize{9.75}{11.7}
% %\begin{align}
% %& \text{KL} \left [ Q_{\psi}(z | w_1, \dots, w_N) || P_{\theta}(z) \right ] \nonumber \\ & = \dfrac{1}{n} \dfrac{1}{K^{\text{post}}} \sum_{i=1}^n a_i^{\text{post}} \left ( \log (a_i^{\text{post}}) - \log(a_i^{\text{prior}}) \right )  \nonumber \\
% % & \quad + \log(K^{\text{prior}}) - \log(K^{\text{post}}),
% %\end{align}
% %}
% {\fontsize{9.75}{11.7}
% \begin{align}
% & \text{KL} \left [ Q_{\psi}(z | w_1, \dots, w_N) || P_{\theta}(z) \right ] \nonumber \\ & = \dfrac{1}{n} \dfrac{1}{K^{\text{post}}} \sum_{i=1}^n a_i^{\text{post}} \log \left ( \dfrac{a_i^{\text{post}}}{a_i^{\text{prior}}} \right ) + \log \left (\dfrac{K^{\text{prior}}}{K^{\text{post}}} \right ), \nonumber
% \end{align}
% }
% where we use the \textit{prior} superscript to denote prior parameters and the \textit{post} superscript to denote approximate posterior parameters. See Appendix \ref{appendix_a} for detailed derivations.





% AO: do we need to mention theoretically how many modes our prior could capture --> 2^n where n is the number of pieces? (since this was mentioned in the abstract)

% Will need to re-design this diagram to make a tiny bit cleaner
%\begin{figure*}[!t]
%\centering
%\includegraphics[scale=0.25]{Images/PDF2DIllustration.png}
%\caption{The horizontal axis corresponds to $z_1$, which is a univariate Gaussian variable. The vertical axis corresponds to $z_2$, which is a piecewise constant variable. The PDF for each variable is shown along each axis, and their joint distribution is illustrated in grey color.}
% * <aaron.courville@gmail.com> 2016-11-10T18:33:52.821Z:
%
% I change the caption from x_1, x_2 to z_1, z_2 to match the axis labels in the fig.
%
% ^ <aaron.courville@gmail.com> 2016-11-16T16:17:05.244Z.
%\label{fig:joint_prob}
%\end{figure*}


\begin{figure}[!t]
\centering
\includegraphics[scale=0.35]{Images/PDF2DIllustration_v2.png}
\caption{Joint density plot of a pair of Gaussian and piecewise constant variables. The horizontal axis corresponds to $z_1$, which is a univariate Gaussian variable. The vertical axis corresponds to $z_2$, which is a piecewise constant variable.} %The density for each variable is shown along each axis, and their joint distribution is illustrated in grey color.}
% * <aaron.courville@gmail.com> 2016-11-10T18:33:52.821Z:
%
% I change the caption from x_1, x_2 to z_1, z_2 to match the axis labels in the fig.
%
% ^ <aaron.courville@gmail.com> 2016-11-16T16:17:05.244Z.
\label{fig:joint_prob}
\end{figure}

\section{Latent Variable Parametrizations}
\label{latent_params}
%The parametrization of the latent variables is crucial for learning.
In this section, we develop the parametrization of both the Gaussian variable and our proposed piecewise constant latent variable. %, which is critical for efficient learning.

Let $x$ be the current output sequence, which the model must generate (e.g.\@ $w_1, \dots, w_N$).
Let $c$ be the observed conditioning information.
%for the prior.
If the task contains additional conditioning information this will be embedded by $c$. For example, for dialogue natural language generation $c$ represents an embedding of the dialogue history, while for document modeling $c=\emptyset$.
% If no conditioning information is available, then  
%In document modeling there is no conditioning information available to the prior, so $c=\emptyset$.
%In dialogue modeling $c$ is the vector representation of the dialogue context, namely all previous utterances until the current time step.

\subsection{Gaussian Parametrization}
\label{gaussian_param}

Let $\mu^{\text{prior}}$ and $\sigma^{2,\text{prior}}$ be the prior mean and variance, and let $\mu^{\text{post}}$ and $\sigma^{2,\text{post}}$ be the approximate posterior mean and variance.
For Gaussian latent variables, the prior distribution mean and variances are encoded using linear transformations of a hidden state.
In particular, the prior distribution covariance is encoded as a diagonal covariance matrix using a softplus function:
% TODO: Consider dropping equation numbers here! This will save space, and help reduce the cognitive load on reviewers..
{\fontsize{9.75}{11.7}
\begin{align}
\mu^{\text{prior}} &= H_{\mu}^{\text{prior}} \text{Enc}(c) + b_{\mu}^{\text{prior}}, \nonumber \\ % \label{eq:mu_prior} \\
\sigma^{2,\text{prior}} &= \text{diag}(\log(1 + \exp ( H_{\sigma}^{\text{prior}} \text{Enc}(c) + b_{\sigma}^{\text{prior}}))), \nonumber % \label{eq:sigma_prior}
\end{align}
}%
where $\text{Enc}(c)$ is an embedding of the conditioning information $c$ (e.g.\@ for dialogue natural language generation this might, for example, be produced by an LSTM encoder applied to the dialogue history), which is shared across all latent variable dimensions. 
The matrices $H_{\mu}^{\text{prior}}, H_{\sigma}^{\text{prior}}$ and vectors $b_{\mu}^{\text{prior}}, b_{\sigma}^{\text{prior}}$ are learnable parameters.
For the posterior distribution, previous work has shown it is better to parametrize the posterior distribution as a linear interpolation of the prior distribution mean and variance and a new estimate of the mean and variance based on the observation $x$ \citep{fraccaro2016sequential}.
The interpolation is controlled by a gating mechanism, allowing the model to turn on/off latent dimensions:
{\fontsize{9.75}{11.7}
\begin{align}
\mu^{\text{post}} = & (1 - \alpha_{\mu}) \mu^{\text{prior}} + \alpha_{\mu} \left ( H_{\mu}^{\text{post}} \text{Enc}(c, x) + b_{\mu}^{\text{post}} \right ), \nonumber \\ %\label{eq:mu_post}\\
\sigma^{2,\text{post}} = & (1 - \alpha_{\sigma}) \sigma^{2,\text{prior}} \nonumber \\ & + \alpha_{\sigma} \text{diag}(\log(1 + \exp ( H_{\sigma}^{\text{post}} \text{Enc}(c, x) + b_{\sigma}^{\text{post}}))), \nonumber % \label{eq:sigma_post}
\end{align}
}%
where $\text{Enc}(c, x)$ is an embedding of both $c$ and $x$.
The matrices $H_{\mu}^{\text{post}}, H_{\sigma}^{\text{post}}$ and the vectors $b_{\mu}^{\text{post}}, b_{\sigma}^{\text{post}}, \alpha_{\mu}, \alpha_{\sigma}$ are parameters to be learned.
The interpolation mechanism is controlled by $\alpha_{\mu}$ and $\alpha_{\sigma}$, which are initialized to zero (i.e.\@ initialized such that the posterior is equal to the prior). %However, we found that the simpler was often better and thus do not report these results using more advanced mechanisms.}
%Optionally, $\alpha_{\mu}$ and $\alpha_{\sigma}$ can be defined as a linear function of $\text{Enc}(c, x)$. This should be even better

\subsection{Piecewise Constant Parametrization}
\label{piecewise_param}

We parametrize the piecewise prior parameters using an exponential function applied to a linear transformation of the conditioning information:
\begin{align}
a_i^{\text{prior}} = \exp ( H_{a,i}^{\text{prior}} \text{Enc}(c) + b_{a,i}^{\text{prior}}), \quad i=1,\dots,n, \nonumber% \label{eq:a_i_prior}
\end{align}
%\begin{align}
%a_i^{\text{prior}} = \log(1 + \exp ( H_{a,i}^{\text{prior}} \text{Enc}(c) + b_{a,i}^{\text{prior}})), \quad i=1,\dots,n, \label{eq:a_i_prior}
%\end{align}
where matrix $H_{a}^{\text{prior}}$ and vector $b_{a}^{\text{prior}}$ are learnable.
As before, we define the posterior parameters as a function of both $c$ and $x$:
{\fontsize{9.75}{11.7}
\begin{align}
a_i^{\text{post}} = & \exp ( H_{a,i}^{\text{post}} \text{Enc}(c,x) + b_{a,i}^{\text{post}}), \quad i=1,\dots,n, \nonumber% \label{eq:a_i_posterior}
\end{align}
}
where $H_{a}^{\text{post}}$ and $b_{a}^{\text{post}}$ are parameters.
%However, we found that this interpolation hurt performance and therefore fixed $\alpha_{a} = \mathbf{1}$.

%We may also constrain the piecewise constant posterior parameters to be an interpolation between the prior parameters and a new estimated parameter:
%\begin{align}
%a_i^{\text{post}} = & (1 - \alpha_{a,i}) a_i^{\text{prior}} \nonumber \\ & + \alpha_{a,i} \exp ( H_{a,i}^{\text{post}} \text{Enc}(c,x) + b_{a,i}^{\text{post}}), \nonumber \\ & \quad i=1,\dots,n, \nonumber% \label{eq:a_i_posterior}
%\end{align}
%where $H_{a}^{\text{post}}, b_{a}^{\text{post}}, \alpha_{a}$ are the parameters.
%However, we found that this interpolation hurt performance and therefore fixed $\alpha_{a} = \mathbf{1}$.
%with an initialization of $\alpha_{a} = \mathbf{0}$.

%To take advantage of the representational powers of each prior, we may concatenate the Gaussian and piecewise constant variables into a single vector of latent variables.

%of each priors, the Gaussian and piecewise constant variables may be combined, as was suggested in Section \ref{piecewise_prior}. In this work, we primarily experimented with their concatenation to create a hybrid model.

\section{Variational Text Modeling}
We now introduce two classes of VAEs. The models are extended by incorporating the Gaussian and piecewise latent variable parametrizations.%, and are used for our experiments on document modeling and the dialogue modeling.

%\subsection{Neural Variational Document Model (NVDM)}
\subsection{Document Model}
\label{nvdm}
The neural variational document model  (\emph{NVDM}) model has previously been proposed for document modeling \citep{mnih2014neural,miao2015neural}, where the latent variables are Gaussian.
Since the original \emph{NVDM} uses Gaussian latent variables, we will refer to it as \emph{G-NVDM}.
We propose two novel models building on \emph{G-NVDM}.
The first model we propose uses piecewise constant latent variables instead of Gaussian latent variables.
We refer to this model as \emph{P-NVDM}.
The second model we propose uses a combination of Gaussian and piecewise constant latent variables.
The models sample the Gaussian and piecewise constant latent variables independently and then concatenates them together into one vector.
We refer to this model as \emph{H-NVDM}.
% Since this is a hybrid model, 

Let $V$ be the vocabulary of document words.
Let $W$ represent a document matrix, where row $w_i$ is the 1-of-$|V|$ binary encoding of the $i$'th word in the document.
Each model has an encoder component $Enc(W)$,
which compresses a document vector into a continuous distributed representation upon which the approximate posterior is built.
For document modeling, word order information is not taken into account and no additional conditioning information is available.
Therefore, each model uses a bag-of-words encoder, defined as a multi-layer perceptron (MLP) $Enc(c = \emptyset, x) = Enc(x)$.
Based on preliminary experiments, we choose the encoder to be a two-layered MLP with parametrized rectified linear activation functions (we omit these parameters for simplicity). % defined by parameters  $\{E^0,b^0,E^1,b^1\}$.
For the approximate posterior, each model has the parameter matrix $W_{a}^{\text{post}}$ and vector $b_{a}^{\text{post}}$ for the piecewise latent variables, and the parameter matrices $W_{\mu}^{\text{post}}, W_{\sigma}^{\text{post}}$ and vectors $b_{\mu}^{\text{post}}, b_{\sigma}^{\text{post}}$ for the Gaussian means and variances.
For the prior, each model has parameter vector $b_{a}^{\text{prior}}$ for the piecewise latent variables, and vectors $b_{\mu}^{\text{prior}}, b_{\sigma}^{\text{prior}}$ for the Gaussian means and variances.
%and approximate posterior distributions, each model requires learning the parameters, , b_{a}^{\text{post}}\}$ for the piecewise variables, and learning the parameters  for the Gaussian variables.
% Julian: I changed this, because only the posterior depends on $x$ for the NVDM model.
%$Enc(x)$ is trained to compress a document vector into a continuous distributed representation upon which the prior and posterior models are built.
%The NVDM parametrization simplifies the prior and posterior distribution models described for both the Gaussian and piecewise constant latent variables, requiring that only the bias parameters, $b_{a}^{\text{prior}}, b_{a}^{\text{post}}$ for the piecewise and  $b_{\mu}^{\text{post}}, b_{\sigma}^{\text{post}}, b_{\mu}^{\text{prior}}, b_{\sigma}^{\text{prior}}$ for the Gaussian, are learned.
We initialize the bias parameters to zero in order to start with centered Gaussian and piecewise constant priors.
The encoder will adapt these priors as learning progresses, using the gating mechanism to turn on/off latent dimensions.

Let $z$ be the vector of latent variables sampled according to the approximate posterior distribution.
Given $z$, the decoder $Dec(w, z)$ outputs a distribution over words in the document:
%\begin{align*}
%Dec(w, z) = P_{\theta}(w|z) = \frac{\exp{( -w^{\text{T}} R z + b_w)}}{\sum_{w'} \exp{(-w^{\text{T}} R z + b_{w'})}},
%\end{align*}
\begin{align*}
Dec(w, z) = \frac{\exp{( -w^{\text{T}} R z + b_w)}}{\sum_{w'} \exp{(-w^{\text{T}} R z + b_{w'})}},
\end{align*}
where $R$ is a parameter matrix and $b$ is a parameter vector corresponding to the bias for each word to be learned.
This output probability distribution is combined with the KL divergences to compute the lower-bound in eq.\@ \eqref{eq:variatonal_lower_bound}.
See Appendix \ref{appendix_c}. % for further details.

Our baseline model \emph{G-NVDM} is an improvement over the original \emph{NVDM} proposed by \citet{mnih2014neural} and \citet{miao2015neural}.
We learn the prior mean and variance, while these were fixed to a standard Gaussian in previous work. This increases the flexibility of the model and makes optimization easier.
In addition, we use a gating mechanism for the approximate posterior of the Gaussian variables.
This gating mechanism allows the model to turn off latent variable (i.e.\@ fix the approximate posterior to equal the prior for specific latent variables) when computing the final posterior parameters.
Furthermore, \citet{miao2015neural} alternated between optimizing the approximate posterior parameters and the generative model parameters, while we optimize all parameters simultaneously.
%learn to interpolate between the generated prior and posterior models to calculate a new posterior.
% Julian: I cut a lot of this text, since we're discussing it above anyway. Alex, you can add back stuff you think is important in the appendix...
%is defined by parameters $\{R,c\}$. For example, in the case of the hybrid VAE we use eq.\@ \eqref{eq:mu_prior}--\eqref{eq:a_i_posterior} to generate the distribution parameters.
%In this case, to draw a sample from the Gaussian prior, we draw a standard Gaussian variable and then multiply it by the standard deviation and add the mean of the Gaussian prior.
%To draw a sample from the piecewise prior, we use eq.\@ \eqref{piece_inv_cdf}.
%As such, the complete architecture is:
%\begin{align*}
%\pi(W) &= f^0(E^0 W + b^0), \\
%Enc(W) &= f^1(E^1 \pi(W) + b^1), \\
%z_{Gaussian} &= \mu^{\text{post}} + \sqrt{ \sigma^{2,\text{post}} } \otimes \epsilon_0, \\
%z_{Piecewise} &= \phi^{-1,post}(\epsilon_1),\\
%z &= \langle z_{Gaussian}, z_{Piecewise} \rangle,\\
%Dec(w, z) &= g(-w^{\text{T}} R z),
%\end{align*}
%Dec(w, z) &= g((E^0 w)^{\text{T}} R z),
%where $\otimes$ is the Hadamard product, $\langle \circ, \circ \rangle$ is an operator that combines the Gaussian and the Piecewise variables and $Dec(w, z)$ is the decoder model.~\footnote{Operations include vector concatenation, summation, or averaging.}
%As a result of using the re-parametrization trick and choice of prior, we calculate the latent variable $z$ through the two samples, $\epsilon_0$ and $\epsilon_1$.
%$f(\circ)$ is a non-linear activation function.
%We choose it to be the softsign function, or $f(v) = v / (1 + |v|)$.
%The decoder model $Dec(z)$ outputs a probability distribution over words conditioned on $z$. 
%In this case, we define $g(\circ)$ as the softmax function (omitting the bias term $c$ for clarity) computed as:
%\begin{align*}
%Dec(w, z) = P_{\theta}(w|z) = \frac{\exp{( -w^{\text{T}} R z )}}{\sum_{w'} \exp{(-w^{\text{T}} R z)}}, \label{sample_y_given_h1_h2}
%\end{align*}
%Dec(w, z) = P_{\theta}(w|z) = \frac{\exp{( -(E^0 w)^{\text{T}} R z )}}{\sum_{w'} \exp{(-(E^0 w')^{\text{T}} R z)}}, \label{sample_y_given_h1_h2}
% Julian: I left out the bias term to avoid cluttering the equation.
% Alex: sounds good. I added a negative sign in my decoder during my debugging to more closely match Maio et al paper since I ended up using that: smx(-R h + b)
%The decoder's output is used to calculate the first term in the variational lower-bound: $\log P_{\theta}(W|z)$.
%The prior and posterior distributions are used to compute the KL term in the variational lower-bound.
%The lower-bound defined becomes:
%\begin{align*}
%\mathcal{L} = & \text{E}_{Q_{\psi}(z | W) } \Bigg[ \sum^{N}_{i=1} \log P_{\theta}(w_i | z) \Bigg] \nonumber \\ & - \text{KL} \left [ Q_{\psi}(z | W) || P_{\theta}(z) \right ],
%\end{align*}
%where the KL term is the sum of the Gaussian and piecewise KL-divergence measures:
%\begin{align*}
% \text{KL} & \left [ Q(z | W) || P(z) \right ] \\ = & \text{KL}_{Gaussian} \left [ Q(z | W) || P(z) \right ] \\ & + \text{KL}_{Piecewise} \left [ Q(z | W) || P(z) \right ].
%\end{align*}
%The KL-terms may be interpreted as regularizers of the parameter updates for the encoder model \citep{kingma2013auto}.
%These terms encourage the posterior distributions to be similar to their corresponding prior distributions, by limiting the amount of information the encoder model transmits regarding the output.
%For example, it encourages the uni-modal Gaussian posterior to move its mean close to the mean of the Gaussian prior, which makes it difficult for the Gaussian posterior to represent different modes conditioned on the observation.
%Similarly, this encourages the piecewise constant posterior to be similar to the piecewise constant prior.
%However, since the piecewise constant posterior is multi-modal, it may be able to shift some of its probability mass towards the prior distribution while keeping other probability mass on one or several modes dependent upon the output observation (e.g.\@ if the prior distribution is a uniform distribution and the true posterior concentrates all its probability mass in several small regions, then the approximate posterior could interpolate between the prior and the true posterior).

%\subsection{Variational Hierarchical Recurrent Encoder-Decoder (VHRED)}
\subsection{Dialogue Model}
\label{vhred}

The variational hierarchical recurrent encoder-decoder (\emph{VHRED}) model has previously been proposed for dialogue modeling and natural language generation~\citep{serban2016hierarchical,DBLP:conf/aaai/SerbanSBCP16}. % is an extension of the \emph{HRED} model (hierarchical recurrent encoder-decoder model) 
The model decomposes dialogues using a two-level hierarchy: sequences of utterances (e.g.\@ sentences), and sub-sequences of tokens (e.g.\@ words).
Let $\mathbf{w}_n$ be the $n$'th utterance in a dialogue with $N$ utterances.
Let $w_{n, m}$ be the $m$'th word in the $n$'th utterance from vocabulary $V$ given as a 1-of-$|V|$ binary encoding. Let $M_n$ be the number of words in the $n$'th utterance.
For each utterance $n=1,\dots,N$, the model generates a latent variable $z_n$.
Conditioned on this latent variable, the model then generates the next utterance:
%\begin{align}
%& P_\theta(\mathbf{w}_1, \ldots, \mathbf{w}_N, z_n) \nonumber \\ &= \prod_{n = 1}^N P_\theta(\mathbf{w}_n | \mathbf{w}_{< n}, z_n) P_{\theta}(z_n | \mathbf{w}_{< n}) \nonumber \\
%& = \prod_{n = 1}^N \prod_{m = 1}^{M_n} P_\theta(w_{n, m} | w_{n, < m}, \mathbf{w}_{< n}, z_n) P_{\theta}(z_n | \mathbf{w}_{< n}),
%\end{align}
\begin{align}
P_\theta(\mathbf{w}_1, z_1, \ldots, \mathbf{w}_N, z_N) = \prod_{n = 1}^N P_{\theta}(z_n | \mathbf{w}_{< n}) & \nonumber \\ % \prod_{n = 1}^N P_\theta(\mathbf{w}_n | \mathbf{w}_{< n}, z_n) P_{\theta}(z_n | \mathbf{w}_{< n}) \nonumber \\
 \times \prod_{m = 1}^{M_n} P_\theta(w_{n, m} | w_{n, < m}, \mathbf{w}_{< n}, z_n), & \nonumber
\end{align}
where $\theta$ are the model parameters.
\emph{VHRED} consists of three RNN modules: an \textit{encoder} RNN, a \textit{context} RNN and a \textit{decoder} RNN.
The \textit{encoder} RNN computes an embedding for each utterance.
This embedding is fed into the \textit{context} RNN, which computes a hidden state summarizing the dialogue context before utterance $n$: $h^{\text{con}}_{n-1}$.
This state represents the additional conditioning information, which is used to compute the prior distribution over $z_n$:
\begin{align}
P_{\theta}(z_n \mid 
\mathbf{w}_{<n}) = f^{\text{prior}}_\theta (z_n ; h_{n-1}^{con}), \nonumber
\end{align}
where $f^{\text{prior}}$ is a PDF parametrized by both $\theta$ and $h_{n-1}^{\text{con}}$.
A sample is drawn from this distribution: $z_n \sim P_\theta (z_n | \mathbf{w}_{<n})$.
This sample is given as input to the \textit{decoder} RNN, which then computes the output probabilities of the words in the next utterance.
The model is trained by maximizing the variational lower-bound, which factorizes into independent terms for each sub-sequence (utterance):
\begin{align}
\log P_{\theta} & (\mathbf{w}_1, \dots, \mathbf{w}_N) \nonumber \\ \geq  \sum_{n=1}^N & - \text{KL} \left [ Q_{\psi}(z_n \mid \mathbf{w}_1, \dots, \mathbf{w}_n) || P_{\theta}(z_n \mid \mathbf{w}_{<n} ) \right ] \nonumber \\
& + \mathbb{E}_{Q_{\psi}(z_n \mid \mathbf{w}_1, \dots, \mathbf{w}_n)} \left [ \log P_{\theta}(\mathbf{w}_n \mid z_n, \mathbf{w}_{< n}) \right ], \nonumber % \label{VHRED:lower_bound}
\end{align}
where distribution $Q_{\psi}$ is the approximate posterior distribution with parameters $\psi$, computed similarly as the prior distribution but further conditioned on the \textit{encoder} RNN hidden state of the next utterance.
% Aaron: We need more detail following equation 10 for the hybrid model. Make the factoring into the piecewise and Gaussian terms explicit.

%\begin{align}
%Q_{\psi}(z_n \mid \mathbf{w}_{\leq n}) = f^{\text{post}}_\psi (h_{n-1}^{con}, h_{n, M_n}^{enc}),
%\end{align}
%where $f^{\text{post}}$ is a PDF.

The original \emph{VHRED} model \citep{serban2016hierarchical} used Gaussian latent variables.
We refer to this model as \emph{G-VHRED}.
The first model we propose uses piecewise constant latent variables instead of Gaussian latent variables.
We refer to this model as \emph{P-VHRED}.
The second model we propose takes advantage of the representation power of both Gaussian and piecewise constant latent variables.
This model samples both a Gaussian latent variable $z_n^{\text{gaussian}}$ and a piecewise latent variable $z_n^{\text{piecewise}}$ independently conditioned on the \textit{context} RNN hidden state:
\begin{align}
P_{\theta}(z_n^{\text{gaussian}} \mid 
\mathbf{w}_{<n}) &= f^{\text{prior, gaussian}}_\theta (z_n^{\text{gaussian}} ; h_{n-1}^{con}), \nonumber \\
P_{\theta}(z_n^{\text{piecewise}} \mid 
\mathbf{w}_{<n}) &= f^{\text{prior, piecewise}}_\theta (z_n^{\text{piecewise}} ; h_{n-1}^{con}), \nonumber
\end{align}
where $f^{\text{prior, gaussian}}$ and $f^{\text{prior, piecewise}}$ are PDFs parametrized by independent subsets of parameters $\theta$.
We refer to this model as \emph{H-VHRED}.



%, parametrized by parameters $\psi$, which aims to approximate the intractable true posterior distribution:
%\begin{align}
% & Q_{\psi}(\mathbf{z}_n \mid \mathbf{w}_1, \dots, \mathbf{w}_N) \nonumber \\ 
% & =  \mathcal{N}(\boldsymbol{\mu}_{\text{posterior}}(\mathbf{w}_1, \dots, \mathbf{w}_n), \Sigma_{\text{posterior}}(\mathbf{w}_1, \dots, \mathbf{w}_n)) \nonumber \\
% & \approx P_{\psi}(\mathbf{z}_n \mid %\mathbf{w}_1, \dots, \mathbf{w}_N),
%\end{align}


%NOTE: Iulian, this might be where you want to give a reasonably quick sketch of the VHRED using our notation/definitions from the latent variable parametrization sub-sections...

% AO: These extension were cut out, as they proved not useful in our experiments
%
%\subsection{Extensions}
%The training signal for the piecewise constant posterior may have high variance.
%Consider the gradient of the reconstruction cost w.r.t. the latent variable $z_i$:
%\begin{align}
%\nabla_{z_i} \text{E}_{z \sim Q(z | w_1, \dots, w_N) }[\log P_{\theta}(w_n | w_{<n}, z)].
%\end{align}
%If the reconstruction cost is low (e.g.\@ $z_i$ explains the data well), then the probability mass of sampling $z_i$ will be increased by increasing the $a$ parameter corresponding to the piece of $z_i$ and decreasing all the other $a$ parameters through the normalization constant.
%Vice versa, if the reconstruction cost is high, the corresponding $a$ parameter will be decreased and all other $a$ parameters will be increased.
%This part of the update has a flavor similar to the REINFORCE learning algorithm, where probability mass of good $z_i$ examples are increased and all other probability mass is decreased.\footnote{Note that this analysis only applies to the latent variable parameters through the reconstruction cost. The closed-form KL divergence cost provides a very strong signal for matching the prior and posterior.}
%When $n$ is large, it may take a very long time to move all the piecewise constant into right position.

%To overcome this problem, following a discussion with Aaron Courville, I propose to use parameter sharing for the $a$ parameters.
%Let $\hat{a}$ be the parameters of either the prior or posterior given in equations \eqref{eq:a_i_posterior} or \eqref{eq:a_i_prior}.
%Now, redefine the $a$ parameters to be a Gaussian weighed combination of the $\hat{a}$ parameters:
%\begin{align}
%a_i = \sum_{j=1}^n \dfrac{e^{-\beta (i-j)^2}}{\sum_{l} e^{-\beta (i-l)^2}} \hat{a}_j,
%\end{align}
%where $\beta > 0$ is a smoothness parameter determining how much distant $\hat{a}$ parameters affect $a$.

%An alternative approach to improving the learning is by adjusting the gradient signal. If 
%\begin{align}
%\nabla_{z_i} \text{E}_{z \sim Q(z | w_1, \dots, w_N) }[\log P_{\theta}(w_n | w_{<n}, z)] > 0,
%\end{align}
%then we can apply the gradient update of $a$ corresponding to $z_i$, to all small $a$ which have pieces small than $z_i$. Vice versa, if the gradient is negative we can apply the current gradient update to all $a$ parameters, which are greater than the current $a$ corresponding to the $z_i$ piece.
%However, the downside of this approach is that it may hurt the learning of multi-modal distributions.

%Small note: Try using a spline as another possible alternative to piece-wise constant (i.e., this would be piece-wise polynomial), perhaps a quadratic spline (2nd order).  Need to derive a closed-form solution (AO: I still have derive this, might be a good follow-up for us down the road, perhaps a fuller journal article, in the framework I coded up for the piecewise-constant VAE I build for this paper)

\section{Experiments}
\label{experiments}
We evaluate the proposed models on two types of natural language processing tasks: document modeling and dialogue natural language generation.
%In order to evaluate the  the ability of our piecewise latent variables to capture complex aspects of data distributions, we conduct experiments with both the NVDM and VHRED models.
All models are trained with back-propagation using the variational lower-bound on the log-likelihood or the exact log-likelihood. We use the first-order gradient descent optimizer Adam \citep{kingma2014adampublished} with gradient clipping \citep{pascanu2012difficulty}\footnote{Code and scripts are available at \url{https://github.com/ago109/piecewise-nvdm-emnlp-2017} and \url{https://github.com/julianser/hred-latent-piecewise}.} % , where hyper-parameter choices varied depending on the task.
% Alex: now that we both used Adam, I've moved it back here and updated each sub-section accordingly =]
%The specifics of the design of the encoder and decoder differed between the two tasks, as described in Sections \ref{nvdm} and \ref{vhred}. 

\subsection{Document Modeling}
\label{doc_model}
% Furthermore, \textit{G-NVDM} outperform the previous state-of-the-art on 20-NG.
\begin{table}[t]
  \caption{Test perplexities on three document modeling tasks: 20-NewGroup (20-NG), Reuters corpus (RCV1) and CADE12 (CADE). Perplexities were calculated using 10 samples to estimate the variational lower-bound. The \textit{H-NVDM} models perform best across all three datasets.} \label{tabel:document_results}
  %\small
  \centering
    \begin{tabular}{lccccccccccccc}
    %\toprule
    \textbf{Model} & \textbf{20-NG} & \textbf{RCV1} & \textbf{CADE} \\
    \midrule
    \textit{LDA} & $1058$ & $--$ & $--$ \\
%    \textit{RSM} & $953$ & $--$ & $--$ \\
    \textit{docNADE} & $896$ & $--$ & $--$ \\
%    \textit{SBN} & $909$ & $--$ & $--$ \\
%    \textit{fDARN} & $917$ & $--$ & $--$ \\
    \textit{NVDM} & $836$ & $--$ & $--$ \\ \midrule
    \textit{G-NVDM} & $651$ & $905$ & $339$ \\
    \textit{H-NVDM-3} & $607$ & $865$ & $\mathbf{258}$ \\
    \textit{H-NVDM-5} & $\mathbf{566}$ & $\mathbf{833}$ & $294$ \\ \bottomrule
    \end{tabular}
\end{table}

\textbf{Tasks} We use three different datasets for document modeling experiments.
First, we use the 20 News-Groups (20-NG) dataset \citep{hinton_softmax_2009}.
Second, we use the Reuters corpus (RCV1-V2), using a version that contained a selected 5,000 term vocabulary. %\footnote{Scripts and code will be made available upon publication.}%\footnote{We will make the code and scripts used to create the final document input vectors and vocabulary files publicly available upon publication.}
As in previous work \citep{hinton_softmax_2009,larochelle_neural_2012}, we transform the original word frequencies using the equation $\log(1 + \text{TF})$, where TF is the original word frequency.
Third, to test our document models on text from a non-English language, we use the Brazilian Portuguese CADE12 dataset \citep{2007:phd-Ana-Cardoso-Cachopo}.
%We pre-processed the dataset by stemming words and removing stop words. We further filtered terms that occurred less than 130 times to obtain a vocabulary of 3,736 terms (over 26,991 training and 13,486 test documents).
For all datasets, we track the validation bound on a subset of 100 vectors randomly drawn from each training corpus.




\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.4}do
\caption{Word query similarity test on 20 News-Groups: for the query `space'', we retrieve the top $10$ nearest words in word embedding space based on Euclidean distance. \textit{H-NVDM-5} associates multiple meanings to the query, while \textit{G-NVDM} only associates the most frequent meaning.} %more general/abstract terms to the query, which may or may not always be what is desired.}
\label{word_query}
\centering
\begin{tabular}{lllll}
%\hline\hline
\multicolumn{1}{l}{\begin{tabular}[x]{@{}c@{}}\textbf{G-NVDM}\\\end{tabular}}&\multicolumn{1}{c}{\begin{tabular}[x]{@{}c@{}}\textbf{H-NVDM-3}\\\end{tabular}}&\multicolumn{1}{c}{\begin{tabular}[x]{@{}c@{}}\textbf{H-NVDM-5}\\\end{tabular}}\tabularnewline
\hline
environment & project & science \tabularnewline
project & gov & built \tabularnewline
flight & major & high \tabularnewline
lab & based & technology \tabularnewline
mission & earth & world \tabularnewline
launch & include & form \tabularnewline
field & science & scale \tabularnewline
working & nasa & sun \tabularnewline
build & systems & special \tabularnewline
gov & technical & area \tabularnewline
\hline
\end{tabular}
\end{table}

%In addition, we make use of the NIPS XXX collection, which has also been pre-processed and publicly available.\footnote{http://www.cs.nyu.edu/~roweis/data.html}. The data-set contains XXXX documents with a vocabulary of XXXX...

\textbf{Training} All models were trained using mini-batches with 100 examples each. % over 150 training set epochs.
A learning rate of $0.002$ was used.
Model selection and early stopping were conducted using the validation lower-bound, estimated using five stochastic samples per validation example.
Inference networks used 100 units in each hidden layer for 20-NG and CADE, and 100 for RCV1.
We experimented with both $50$ and $100$ latent random variables for each class of models, and found that $50$ latent variables performed best on the validation set.
For \emph{H-NVDM} we vary the number of components used in the PDF, investigating the effect that 3 and 5 pieces had on the final quality of the model.
The number of hidden units was chosen via preliminary experimentation with smaller models.
On 20-NG, we use the same set-up as \citep{hinton_softmax_2009} and therefore report the perplexities of a topic model (\emph{LDA}, \citep{hinton_softmax_2009}), 
%the Replicated Softmax (\emph{RSM}, \citep{hinton_softmax_2009}),
the document neural auto-regressive estimator (\emph{docNADE}, \citep{larochelle_neural_2012}),
%a sigmoid belief network (\emph{SBN}, \citep{mnih2014neural}), 
%a deep auto-regressive neural network (\emph{fDARN}, \citep{mnih2014neural}),
and a neural variational document model with a fixed standard Gaussian prior (\emph{NVDM}, lowest reported perplexity, \citep{miao2015neural}).

\textbf{Results} In Table \ref{tabel:document_results}, we report the test document perplexity: $\exp(-\frac{1}{D}\sum_{n}\frac{1}{L_n} \log P_{\theta}(x_n)$. 
We use the variational lower-bound as an approximation based on 10 samples, as was done in \citep{mnih2014neural}. 
First, we note that the best baseline model (i.e.\@ the \emph{NVDM}) is more competitive when both the prior and posterior models are learnt together (i.e.\@ the \emph{G-NVDM}), as opposed to the fixed prior of \citep{miao2015neural}.
Next, we observe that integrating our proposed piecewise variables yields even better results in our document modeling experiments, substantially improving over the baselines. More importantly, in the 20-NG and Reuters datasets, increasing the number of pieces from 3 to 5 further reduces perplexity. 
Thus, we have achieved a new state-of-the-art perplexity on 20 News-Groups task and --- to the best of our knowledge -- better perplexities on the CADE12 and RCV1 tasks compared to using a state-of-the-art model like the \emph{G-NVDM}.
%Furthermore, we found that using iterative inference yielded yet a further boost in performance, estimating a tighter bound (see Appendix).
We also evaluated the converged models using an non-parametric inference procedure, where a separate approximate posterior is learned for each test example in order to tighten the variational lower-bound.
\textit{H-NVDM} also performed best in this evaluation across all three datasets, which confirms that the performance improvement is due to the piecewise components.
See appendix for details.
%The second column \emph{SGD-Inf}, refers to the model's test-perplexity when the lower-bound is tightened using iterative inference to search for the optimal latent variable per document.
%See appendix for further details on the iterative inference procedure.

%, however, this form of inference is expensive and requires additional meta-parameters (e.g.\@ a step-size and an early-stopping criterion). %We remark a simpler, and more accurate, approach to inference would be to use importance sampling. 
%In some cases, 3 pieces appear to be sufficient, although this might simply indicate the need for yet further optimization of the piecewise variables, as hinted by the Reuters models (where iterative inference actually helped the 5-piece model achieve lowest perplexity). <-- since I redid the experiment, I was able to optimize the 5-piece model enough, it ended up performing the best in the end

\begin{figure}[!t]
\centering
\includegraphics[scale=0.20]{Images/TSNEVisualization.png}
\caption{Latent variable approximate posterior means t-SNE visualization on 20-NG for \textit{G-NVDM} and \textit{H-NVDM-5}. Colors correspond to the topic labels assigned to each document.}
\label{fig:tsne}
\end{figure}

In Table \ref{word_query}, we examine the top ten highest ranked words given the query term ``space'', using the decoder parameter matrix. %(since the decoder is directly affected by the latent variables in our document models).
The piecewise variables appear to have a significant effect on what is uncovered by the model.%, as each model returns different results for the query word.
In the case of ``space'', the hybrid with 5 pieces seems to value two senses of the word--one related to ``outer space'' (e.g., ``sun'', ``world'', etc.) and another related to the dimensions of depth, height, and width within which things may exist and move (e.g., ``area'', ``form'', ``scale'', etc.).
On the other hand, \textit{G-NVDM} appears to only capture the ``outer space'' sense of the word.
More examples are in the appendix.

Finally, we visualized the means of the approximate posterior latent variables on 20-NG through a t-SNE projection. As shown in Figure \ref{fig:tsne}, both \textit{G-NVDM} and \textit{H-NVDM-5} learn representations which disentangle the topic clusters on 20-NG. However, \textit{G-NVDM} appears to have more dispersed clusters and more outliers (i.e.\@ data points in the periphery) compared to \textit{H-NVDM-5}.
Although it is difficult to draw conclusions based on these plots, these findings could potentially be explained by the Gaussian latent variables fitting the latent factors poorly.



%In our current examples, it appears that the \emph{H-NVDM} with 5 pieces returns more general words. For example, in the case of ``government'', the baseline seems to value the plural form of the word (which is largely based on morphology) while the hybrid model actually pulls out meaningful terms such as ``federal'', ``policy'', and ``administration''. 

%We also qualitatively evaluate the quality of the latent document representations learned by the various document models through a t-SNE visualization \citep{maaten2008visualizing}, specifically employing the Barnes-Hutt approximation for scalability. As observed in Figure XXXX, it appears that...

\subsection{Dialogue Modeling}
\label{dialog_model}

% \begin{table}[t]
%   \caption{Ubuntu evaluation using precision (P), recall (R) and F1 metrics w.r.t.\@ activities and entities. \textit{G-VHRED}, \textit{P-VHRED} and \textit{H-VHRED} all outperform the baseline \textit{HRED}. \textit{G-VHRED} performs best w.r.t.\@ activities and \textit{H-VHRED} performs best w.r.t.\@ entities.} \label{tabel:ubuntu_results}
%   \small
%   \centering
%     \begin{tabular}{lccccccccccccc}
%     %\toprule
%      & \multicolumn{3}{c}{\textbf{Activity}} & \multicolumn{3}{c}{\textbf{Entity}} \\ \midrule
%     \textbf{Model} & \textbf{P\@} & \textbf{R\@} & \textbf{F1} & \textbf{P\@} & \textbf{R\@} & \textbf{F1} \\
%     \midrule
%     \textit{HRED} & $6.55$ & $4.49$ & $4.77$ & $3.09$ & $2.3$ & $2.43$ \\
%     \textit{G-VHRED} & $10.89$ & $\mathbf{10.11}$ & $\mathbf{9.24}$ & $3.11$ & $2.44$ & $2.49$ \\
%     \textit{P-VHRED} & $6.79$ & $4.67$ & $5$ & $3.08$ & $2.4$ & $2.49$ \\   
%     \textit{H-VHRED} & $\mathbf{10.92}$ & $8.31$ & $8.41$ & $\mathbf{4.81}$	& $\mathbf{3.51}$ & $\mathbf{3.72}$ \\ \bottomrule
%     \end{tabular}
% \end{table}

\begin{table}[t]
  \caption{Ubuntu evaluation using F1 metrics w.r.t.\@ activities and entities. \textit{G-VHRED}, \textit{P-VHRED} and \textit{H-VHRED} all outperform the baseline \textit{HRED}. \textit{G-VHRED} performs best w.r.t.\@ activities and \textit{H-VHRED} performs best w.r.t.\@ entities.} \label{tabel:ubuntu_results}
  %\small
  \centering
    \begin{tabular}{lccccccccccccc}
    %\toprule
    \textbf{Model} & \textbf{Activity} & \textbf{Entity} \\
    \midrule
    \textit{HRED} & $4.77$ & $2.43$ \\
    \textit{G-VHRED} & $\mathbf{9.24}$ & $2.49$ \\
    \textit{P-VHRED} & $5$ & $2.49$ \\   
    \textit{H-VHRED} & $8.41$ & $\mathbf{3.72}$ \\ \bottomrule
    \end{tabular}
\end{table}

\textbf{Task} We evaluate \emph{VHRED} on a natural language generation task, where the goal is to generate responses in a dialogue.
This is a difficult problem, which has been extensively studied in the recent literature \citep{ritter2011data,lowe2015ubuntu,sordoni2015aneural,li2015diversity,DBLP:conf/aaai/SerbanSBCP16,serban2016generative}.
Dialogue response generation has recently gained a significant amount of attention from industry, with high-profile projects such as Google SmartReply \citep{kannan2016smart} and Microsoft Xiaoice \citep{markoff2015forsymp}.
Even more recently, Amazon has announced the Alexa Prize Challenge for the research community with the goal of developing a natural and engaging chatbot system \citep{farber2015amazon}.

We evaluate on the technical support response generation task for the Ubuntu operating system.
We use the well-known Ubuntu Dialogue Corpus \citep{lowe2015ubuntu,lowe2017ubuntu}, which consists of about 1/2 million natural language dialogues extracted from the \#Ubuntu Internet Relayed Chat (IRC) channel.
The technical problems discussed span a wide range of software-related and hardware-related issues.
Given a dialogue history --- such as a conversation between a user and a technical support assistant --- the model must generate the next appropriate response in the dialogue.
For example, when it is the turn of the technical support assistant, the model must generate an appropriate response helping the user resolve their problem.

We evaluate the models using the activity- and entity-based metrics designed specifically for the Ubuntu domain \citep{serban2017multires}.
These metrics compare the \textit{activities} and \textit{entities} in the model generated responses with those of the reference responses; activities are verbs referring to high-level actions (e.g.\@ \textit{download}, \textit{install}, \textit{unzip}) and entities are nouns referring to technical objects (e.g.\@ \textit{Firefox}, \textit{GNOME}).
The more activities and entities a model response overlaps with the reference response (e.g.\@ expert response) the more likely the response will lead to a solution.%~\citep{serban2017multires}.

\textbf{Training} The models were trained to maximize the log-likelihood of training examples using a learning rate of $0.0002$ and mini-batches of size $80$.
We use a variant of truncated back-propagation.
%and apply gradient clipping \citep{pascanu2012difficulty}.
We terminate the training procedure for each model using early stopping, estimated using one stochastic sample per validation example.
We evaluate the models by generating dialogue responses: conditioned on a dialogue context, we fix the model latent variables to their median values and then generate the response using a beam search with size 5.
We select model hyper-parameters based on the validation set using the F1 activity metric, as described earlier.
%, of the generated responses on the validation set.

It is often difficult to train generative models for language with stochastic latent variables \citep{bowman2015generating,serban2016hierarchical}.
For the latent variable models, we therefore experiment with reweighing the KL divergence terms in the variational lower-bound with values $0.25$, $0.50$, $0.75$ and $1.0$.
In addition to this, we linearly increase the KL divergence weights starting from zero to their final value over the first $75000$ training batches.
Finally, we weaken the \textit{decoder} RNN by randomly replacing words inputted to the decoder RNN with the unknown token with $25\%$ probability.
These steps are important for effectively training the models, and the latter two have been used in previous work by \citet{bowman2015generating} and \citet{serban2016hierarchical}.
%At test time, we use beam search with $5$ beams for outputting responses with the RNN decoders.
%At the beginning of each beam search, samples of the latent variables are generated and conditioned on throughout the beam search. We fix the word embedding dimensionality to $400$.

{\bf HRED (Baseline): }
We compare to the \emph{HRED} model \citep{DBLP:conf/aaai/SerbanSBCP16}: a sequence-to-sequence model, shown to outperform other established models on this task, such as the LSTM RNN language model \citep{serban2017multires}.
The \emph{HRED} model's \textit{encoder} RNN uses a bidirectional GRU RNN encoder, where the forward and backward RNNs each have $1000$ hidden units.
The context RNN is a GRU encoder with $1000$ hidden units, and the decoder RNN is an LSTM decoder with $2000$ hidden units.\footnote{Since training lasted between 1-3 weeks for each model, we had to fix the number of hidden units during preliminary experiments on the training and validation datasets.}
The encoder and context RNNs both use layer normalization \citep{ba2016layer}.\footnote{We did not apply layer normalization to the decoder RNN, because several of our colleagues have found that this may hurt the performance of generative language models.}
We also experiment with an additional rectified linear layer applied on the inputs to the decoder RNN.
As with other hyper-parameters, we choose whether to include this additional layer based on the validation set performance.
\emph{HRED}, as well as all other models, use a word embedding dimensionality of size $400$.


{\bf G-HRED: }
We compare  to \emph{G-VHRED}, which is \emph{VHRED} with Gaussian latent variables \citep{serban2016hierarchical}.
\emph{G-VHRED} uses the same hyper-parameters for the encoder, context and decoder RNNs as the HRED model. 
The model has $100$ Gaussian latent variables per utterance.

{\bf P-HRED: }
The first model we propose is \emph{P-VHRED}, which is \emph{VHRED} model with piecewise constant latent variables.
We use $n=3$ number of pieces for each latent variable.
\emph{P-VHRED} also uses the same hyper parameters for the encoder, context and decoder RNNs as the \emph{HRED} model. 
Similar to \emph{G-VHRED}, \emph{P-VHRED} has $100$ piecewise constant latent variables per utterance.

{\bf H-HRED: }
The second model we propose is \emph{H-VHRED}, which has $100$ piecewise constant (with $n=3$ pieces per variable) and $100$ Gaussian latent variables per utterance.
\emph{H-VHRED} also uses the same hyper-parameters for the encoder, context and decoder RNNs as \emph{HRED}. 


{\bf Results: }
The results are given in Table \ref{tabel:ubuntu_results}.
All latent variable models outperform \emph{HRED} w.r.t.\@ both activities and entities.
This strongly suggests that the high-level concepts represented by the latent variables help generate meaningful, goal-directed responses.
Furthermore, each type of latent variable appears to help with a different aspects of the generation task.
\emph{G-VHRED} performs best w.r.t.\@ activities (e.g.\@ \textit{download}, \textit{install} and so on), which occur frequently in the dataset.
This suggests that the Gaussian latent variables learn useful latent representations for frequent actions.
On the other hand, \emph{H-VHRED} performs best w.r.t.\@ entities (e.g.\@ \textit{Firefox}, \textit{GNOME}), which are often much rarer and mutually exclusive in the dataset.
This suggests that the combination of Gaussian and piecewise latent variables help learn useful representations for entities, which could not be learned by Gaussian latent variables alone.
%This suggests that the Gaussian latent variables learn useful latent representations for frequent actions.
%On the other hand, \emph{H-VHRED} performs best w.r.t.\@ entities, which are rarer than the activities and often mutually exclusive.
%This suggests a combination of Gaussian latent variables and piecewise constant latent variables are necessary to represent the large and diverse set of objects in the Ubuntu domain.
We further conducted a qualitative analysis of the model responses, which supports these conclusions. See Appendix \ref{appendix_f}.\footnote{Results on a Twitter dataset are given in the appendix.}


%We also compare to the HRED model \citep{DBLP:conf/aaai/SerbanSBCP16}.
%The HRED model \textit{encoder} RNN has a bidirectional GRU RNN encoder, where the forward and backward RNNs each have $1000$ hidden units.
%The encoder and context RNNs use layer normalization \citep{ba2016layer}.
%We experiment with $500$ and $1000$ hidden units for the context RNN and with $1000$ and $2000$ hidden units for the LSTM RNN decoder.
%Based on the validation log-likelihood,
%we choose $1000$ hidden units for both the context RNN and decoder RNN.



\section{Conclusions}
\label{conc}
In this paper, we have sought to learn rich and flexible multi-modal representations of latent variables for complex natural language processing tasks.
We have proposed the piecewise constant distribution for the variational autoencoder framework.
We have derived closed-form expressions for the necessary quantities required for in the autoencoder framework,
and proposed an efficient, differentiable implementation of it.
%We have asserted the theoretical power and flexibility of the proposed piecewise constant distribution, and derived closed-form expressions for the necessary quantities required for in the autoencoder framework.
%developed the piecewise constant prior, which can be efficiently and flexibly adjusted to capture distributions with complex probability manifolds spanning many modes, such as those over topics.
We have incorporated the proposed piecewise constant distribution into two model classes --- \emph{NVDM} and \emph{VHRED} --- and evaluated the proposed models on document modeling and dialogue modeling tasks.
We have achieved state-of-the-art results on three document modeling tasks, and have demonstrated substantial improvements on a dialogue modeling task.
Overall, the results highlight the benefits of incorporating the flexible, multi-modal piecewise constant distribution into variational autoencoders.
%have shown the effectiveness of our framework in building models capable of learning richer structure from data.
%In particular, we have demonstrated new state-of-the-art results on several document modeling tasks.
Future work should explore other natural language processing tasks, where the data is likely to arise from complex, multi-modal latent factors.
%Two potential avenues such as online debates \citep{rosenthal2015couldn},
%as well as tasks where additional information is available, such as in semi-supervised document categorization \citep{ororbia2015learning}.
%Furthermore, the piecewise variables proposed in this work could prove useful in uncovering interesting and novel information in lesser-explored corpora.

\section*{Acknowledgments}
The authors acknowledge NSERC,
Canada  Research  Chairs, CIFAR, IBM Research, Nuance Foundation and Microsoft Maluuba for funding.
Alexander G. Ororbia II was funded by a NACME-Sloan scholarship.
The authors thank Hugo Larochelle for sharing the NewsGroup 20 dataset.
The authors thank Laurent Charlin, Sungjin Ahn, and Ryan Lowe for constructive feedback.
This  research  was enabled  in  part  by  support  provided  by  Calcul Qubec (\url{www.calculquebec.ca}) and Compute Canada (\url{www.computecanada.ca}).
\newpage
% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
%\nocitep{langley00}

%\bibliography{ref}
%\bibliographystyle{icml2017}

\bibliography{ref}
\bibliographystyle{emnlp_natbib}


\newpage
\appendix

%\newpage
%\ 
\newpage

\section{Appendix: Inappropriate Gaussian Priors} \label{appendix_0}
The majority of work on VAEs propose to parametrize $z$ --- both the prior and approximate posterior (encoder) --- as a multivariate Gaussian variable.
However, the multivariate Gaussian is a uni-modal distribution and can therefore only represent one mode in latent space.
Furthermore, the multivariate Gaussian is perfectly symmetric with a constant kurtosis. % Furthermore, the multivariate Gaussian is perfectly symmetric and has a fixed kurtosis. % 
% Aaron: This just isn't true for high dim. Gaussians. In fact, as the dim of z increases the probability mass close to the mode (let's say within 1 std-dev.) goes to zero (see e.g. http://math.stackexchange.com/questions/143377/3-sigma-rule-for-multivariate-normal-distribution).
These properties are problematic if the latent variables we aim to represent are inherently multi-modal, or if the latent variables follow complex, non-linear probability manifolds (e.g.\@ asymmetric distributions or heavy-tailed distributions).
For example. the frequency of topics in news articles could be represented by a continuous probability distribution, where each topic has its own island of probability mass; \textit{sports} and \textit{politics} topics might each be clustered on their own separate island of probability mass with zero or little mass in between them.
Due to its uni-modal nature, the Gaussian distribution can never represent such probability distributions.
As another example, ambiguity and uncertainty in natural language conversations could similarly be represented by islands of probability mass; given the question \textit{How do I install Ubuntu on my laptop?}, a model might assign positive probability mass to specific, unambiguous entities like \textit{Ubuntu 4.10} and to well-defined procedures like \textit{installation using a DVD}. % or \textit{installation using a USB key}.
In particular, certain entities like \textit{Ubuntu 4.10} are now outdated --- these entities occur rarely in practice and should be considered rare events.
When modeling such complex, multi-modal latent distributions, the mapping from multivariate Gaussian latent variables to outputs --- i.e.\@ the conditional distribution $P_{\theta}(w_n | z)$ --- has to be highly non-linear in order to compensate for the simplistic Gaussian distribution and capture the natural latent factors in an intermediate layer of the model.
However, it is difficult to learn such non-linear mappings 
%with existing stochastic optimization methods.
%such as mini-batch stochastic gradient descent and its variants.
%Learning is particularly difficult 
when using the variational bound in eq.\@ \eqref{eq:variatonal_lower_bound}, as it incurs additional variance from sampling the latent variable $z$.
Consequently, such models are likely to converge on solutions that do not capture salient aspects of the latent variables, which in turn leads to a poor fit of the output distribution.
%Consequently, such a model is very likely to converge on a solution which does not model multi-modality which then leads to a poor approximation of the output distribution.
% FOOTNOTE
%\footnote{Some work has in fact used a mixture of Gaussians as posterior approximation, but due to tractability, the number of mixture components (modes) is typically very small.}



\section{Appendix: Piecewise Constant Variable Derivations}
\label{appendix_a}

To train the model using the re-parametrization trick, we need to generate $z = f(\epsilon)$ where $\epsilon \sim \text{Uniform}(0, 1)$. To do so, we employ inverse transform sampling \citep{devroye1986sample},
%\url{https://en.wikipedia.org/wiki/Inverse_transform_sampling}
which requires finding the inverse of the cumulative distribution function (CDF).
We derive the CDF of eq.\@ \eqref{piecewise_pdf}:
\begin{align}
\phi(z) = \dfrac{1}{K} \sum_{i=1}^n & 1_{ \left (\dfrac{i}{n} \leq z \right )} K_i + 1_{ \left (\dfrac{i-1}{n} \leq z \leq \dfrac{i}{n} \right )} \nonumber \\ & * \left (z - \dfrac{i-1}{n} \right ) a_i. 
\end{align}
Next, we derive its inverse:
\begin{align}
\phi^{-1}(\epsilon) = \sum_{i=1}^n & 1_{ \left ( \dfrac{1}{K} \sum_{j=0}^{i-1} K_j \leq \epsilon \leq \dfrac{1}{K} \sum_{j=0}^{i} K_j  \right )} \nonumber \\ & * \left ( \dfrac{i-1}{n} + \dfrac{K}{a_i} \left ( \epsilon - \dfrac{1}{K} \sum_{j=0}^{i-1} K_j \right ) \right ) %\label{piece_inv_cdf}
\end{align}
Armed with the inverse CDF, we can now draw a sample $z$:
\begin{align}
z = \phi^{-1}(\epsilon), \quad \text{where} \ \epsilon \sim \text{Uniform}(0, 1).
\end{align}
In addition to sampling, we need to compute the Kullback-Leibler (KL) divergence between the prior and approximate posterior distributions of the piecewise constant variables.
We assume both the prior and the posterior are piecewise constant distributions.
We use the \textit{prior} superscript to denote prior parameters and the \textit{post} superscript to denote posterior parameters (encoder model parameters).
The KL divergence between the prior and posterior can be computed using a sum of integrals, where each integral inside the sum corresponds to one constant segment:
{\fontsize{9.75}{11.7}
\begin{align}
& \text{KL} \left [ Q_{\psi}(z | w_1, \dots, w_N) || P_{\theta}(z) \right ] \nonumber \\ & = \int_0^1 Q_{\psi}(z | w_1, \dots, w_N) \log \left ( \dfrac{Q_{\psi}(z|  w_1, \dots, w_N)}{P_{\theta}(z)}\right ) dz
 \\ & = \sum_{i=1}^n \int_{0}^{1/n} \dfrac{a_i^{\text{post}}}{K^{\text{post}}} \log \left ( \dfrac{a_i^{\text{post}}/K^{\text{post}}}{a_i^{\text{prior}}/K^{\text{prior}}} \right ) dz \\ 
 & = \dfrac{1}{n} \sum_{i=1}^n \dfrac{a_i^{\text{post}}}{K^{\text{post}}}
 \log \left ( \dfrac{a_i^{\text{post}}/K^{\text{post}}}{a_i^{\text{prior}}/K^{\text{prior}}} \right )
 \\ & = \dfrac{1}{n} \dfrac{1}{K^{\text{post}}} \sum_{i=1}^n a_i^{\text{post}} \left ( \log (a_i^{\text{post}}) - \log(a_i^{\text{prior}}) \right )  \nonumber \\
 & \quad + \log(K^{\text{prior}}) - \log(K^{\text{post}})
\end{align}
}%

%In order to train the model, we take partial derivatives of the variational bound in eq.\@ \eqref{eq:variatonal_lower_bound} w.r.t.\@ each parameter in $\theta$ and $\psi$.
%These expressions involve derivatives of the indicator functions, which have derivatives zero everywhere except for the changing points where the derivative is undefined.
%However, the probability of sampling $\epsilon$ such that an indicator function is exactly at its changing point is effectively zero.
%Therefore, we fix their derivatives to zero.\footnote{We thank Christian A.\@ Naesseth for pointing out this assumption.}
%A similar approach is used for training neural networks with rectified linear units.

In order to improve training, we further transform the piecewise constant latent variables to lie within the interval $[-1, 1$] after sampling: $z' = 2 z - 1$. This ensures the input to the decoder RNN has mean zero initially.% at the beginning of training.


%in order to integrate eq.\@  \eqref{piece_inv_cdf} and \eqref{kl_piece}.
%into an automatic differentiation framework (the derivations of which can be found in our publicly available code at XXXX).

\section{Appendix: NVDM Implementation}
\label{appendix_c}
%The NVDM framework \citep{mnih2014neural,miao2015neural} collapses the recurrent neural encoder into a simpler bag-of-words model (since no symbol order is taken into account). This is parametrized by a multi-layer perceptron (MLP) for $Enc(c = \emptyset, x) = Enc(x)$. Let $V$ be the vocabulary and $W$ represent a document matrix, where row $w_i$ is the 1-of-$|V|$ binary encoding of the $i$'th word in the document. $Enc(W)$ is trained to compress a document vector into a continuous distributed representation upon which the posterior model is built. % only the posterior depends on $x$ for the NVDM model

%The NVDM parametrization requires only learning the parameters $b_{a}^{\text{prior}} , W_{a}^{\text{post}}, b_{a}^{\text{post}}$ for the piecewise variables, and learning the parameters $b_{\mu}^{\text{prior}}, b_{\sigma}^{\text{prior}},W_{\mu}^{\text{post}}, b_{\mu}^{\text{post}}, W_{\sigma}^{\text{post}}, b_{\sigma}^{\text{post}}$ for the Gaussian variables.

%We initialize the bias parameters to zero, in order for the NVDM to start with a centered Gaussian prior. This prior will be adapted by the parametric encoder as learning progresses, while also learning to turn on/off latent dimensions controlled through the gating mechanism.
%It is important to note that our particular instantiation of the NVDM is different from that of \citep{mnih2014neural} and \citep{miao2015neural}; we jointly learn the prior mean and variance whereas in previous work it has been assumed to be a standard Gaussian. Furthermore, our models learn to interpolate between the generated prior and posterior models to calculate a new posterior. % Alex: I will add a note clarifying some experiments showing the value of the interpolation gate and learning the prior over the vanilla NVDM (and update the table of results)

%Based on preliminary experiments, we choose the encoder to be a 2-hidden layer perceptron, defined by parameters  $\{E^0,b^0,E^1,b^1\}$.
%The decoder is defined by parameters $\{R,c\}$. For example, in the case of the hybrid VAE we use the equations defined in Section \ref{piecewise_param} to generate the distribution parameters. In this case, to sample the Gaussian prior, we draw a standard Gaussian variable and then multiply it by the standard deviation and add the mean of the Gaussian prior.
%To draw a sample from the piecewise prior, we use eq.\@ \eqref{piece_inv_cdf}.
%As such, the complete architecture is:
The complete \textit{NVDM} architecture is defined as:
\begin{align*}
\pi(W) &= f^0(E^0 W + b^0), \\
Enc(W) &= f^1(E^1 \pi(W) + b^1), \\
z_{Gaussian} &= \mu^{\text{post}} + \sqrt{ \sigma^{2,\text{post}} } \otimes \epsilon_0, \\
z_{Piecewise} &= \phi^{-1,post}(\epsilon_1),\\
z &= \langle z_{Gaussian}, z_{Piecewise} \rangle,\\
Dec(w, z) &= g(-w^{\text{T}} R z),
\end{align*}
%Dec(w, z) &= g((E^0 w)^{\text{T}} R z),
where $\otimes$ is the Hadamard product, $\langle \circ, \circ \rangle$ is an operator that combines the Gaussian and the Piecewise variables and $Dec(w, z)$ is the decoder model.\footnote{Operations include vector concatenation, summation, or averaging.}
As a result of using the re-parametrization trick and choice of prior, we calculate the latent variable $z$ through the two samples, $\epsilon_0$ and $\epsilon_1$.
$f(\circ)$ is a non-linear activation function, which was the parametrized linear rectifier (with a learnable ``leak'' parameters) for the 20 News-Groups experiments and the softsign function, or $f(v) = v / (1 + |v|)$, for Reuters and CADE.
The decoder model $Dec(z)$ outputs a probability distribution over words conditioned on $z$. 
In this case, we define $g(\circ)$ as the softmax function (omitting the bias term $c$ for clarity) computed as:
\begin{align*}
Dec(w, z) = P_{\theta}(w|z) = \frac{\exp{( -w^{\text{T}} R z )}}{\sum_{w'} \exp{(-w^{\text{T}} R z)}}, \label{sample_y_given_h1_h2}
\end{align*}
% Alex: I added a negative sign in my decoder during my debugging to more closely match Maio et al paper since I ended up using that: smx(-R h + b)
The decoder's output is used to calculate the first term in the variational lower-bound: $\log P_{\theta}(W|z)$. The prior and posterior distributions are used to compute the KL term in the variational lower-bound.
The lower-bound is:
\begin{align*}
\mathcal{L} = & \text{E}_{Q_{\psi}(z | W) } \Bigg[ \sum^{N}_{i=1} \log P_{\theta}(w_i | z) \Bigg] \nonumber \\ & - \text{KL} \left [ Q_{\psi}(z | W) || P_{\theta}(z) \right ],
\end{align*}
where the KL term is the sum of the Gaussian and piecewise KL-divergence measures:
\begin{align*}
 \text{KL} & \left [ Q(z | W) || P(z) \right ] \\ = & \text{KL}_{Gaussian} \left [ Q(z | W) || P(z) \right ] \\ & + \text{KL}_{Piecewise} \left [ Q(z | W) || P(z) \right ].
\end{align*}
The KL-terms may be interpreted as regularizers of the parameter updates for the encoder model \citep{kingma2013auto}. These terms encourage the posterior distributions to be similar to their corresponding prior distributions, by limiting the amount of information the encoder model transmits regarding the output.
%For example, it encourages the uni-modal Gaussian posterior to move its mean close to the mean of the Gaussian prior, which makes it difficult for the Gaussian posterior to represent different modes conditioned on the observation.
%Similarly, this encourages the piecewise constant posterior to be similar to the piecewise constant prior.
%However, since the piecewise constant posterior is multi-modal, it may be able to shift some of its probability mass towards the prior distribution while keeping other probability mass on one or several modes dependent upon the output observation (e.g.\@ if the prior distribution is a uniform distribution and the true posterior concentrates all its probability mass in several small regions, then the approximate posterior could interpolate between the prior and the true posterior).


\section{Appendix: VHRED Implementation}
\label{appendix_b}
As described in the model section, the probability distribution of the generative model factorizes as:
\begin{align}
& P_\theta(\mathbf{w}_1, \ldots, \mathbf{w}_N) \nonumber \\ &= \prod_{n = 1}^N P_\theta(\mathbf{w}_n | \mathbf{w}_{< n}, z_n) P_{\theta}(z_n | \mathbf{w}_{< n}) \nonumber \\
& = \prod_{n = 1}^N \prod_{m = 1}^{M_n} P_\theta(w_{n, m} | w_{n, < m}, \mathbf{w}_{< n}, z_n) P_{\theta}(z_n | \mathbf{w}_{< n}),
\end{align}
where $\theta$ are the model parameters.
VHRED uses three RNN modules: an \textit{encoder} RNN, a \textit{context} RNN and a \textit{decoder} RNN.
First, each utterance is encoded into a vector by the \textit{encoder} RNN:
\begin{align*}
h^{\text{enc}}_{n, 0} = \mathbf{0}, \ \ h^{\text{enc}}_{n, m} = f^{\text{enc}}_{\theta}(h^{\text{enc}}_{n, m-1}, w_{n, m}) \\  \forall m=1,\dots,M_n,
\end{align*}
where $f^{\text{enc}}_{\theta}$ is either a GRU or a bidirectional GRU function.
The last hidden state of the \textit{encoder} RNN is given as input to the \textit{context} RNN.
The \textit{context} RNN uses this state to updates its internal hidden state:
\begin{align*}
h^{\text{con}}_{0} = \mathbf{0}, \ \ h^{\text{con}}_{n} = f^{\text{con}}_{\theta}(h^{\text{con}}_{n-1}, h^{\text{enc}}_{n, M_n}),
\end{align*}
where $f^{\text{con}}_{\theta}$ is a GRU function taking as input two vectors.
This state conditions the prior distribution over $z_n$:
\begin{align}
P_{\theta}(z_n \mid 
\mathbf{w}_{<n}) = f^{\text{prior}}_\theta (z_n ; h_{n-1}^{con}),
\end{align}
where $f^{\text{prior}}$ is a PDF parametrized by both $\theta$ and $h_{n-1}^{con}$.
Next, a sample is drawn from this distribution: $z_n \sim P_\theta (z_n | \mathbf{w}_{<n})$.
The sample and \textit{context} state are given as input to the \textit{decoder} RNN:
\begin{align*}
h^{\text{dec}}_{n, 0} = \mathbf{0}, \ \ & h^{\text{dec}}_{n, m} = f^{\text{dec}}_{\theta}(h^{\text{dec}}_{n, m-1}, h^{\text{con}}_{n-1}, z_{n}, w_{n, m}) \\ 
& \forall m=1,\dots,M_n,
\end{align*}
where $f^{\text{dec}}_{\theta}$ is the LSTM gating function taking as input four vectors.
The output distribution is computed by passing $h^{\text{dec}}_{n, m}$ through an MLP $f^{\text{mlp}}_{\theta}$, an affine transformation and a softmax function:
\begin{align}
& P_\theta(w_{n, m+1} | w_{n, \leq m}, \mathbf{w}_{< n}, z_n) \nonumber \\ & = \dfrac{e^{(O w_{n, m+1})^{\text{T}}f^{\text{mlp}}_{\theta}(h^{\text{dec}}_{n, m})}}{\sum_{w'}  e^{(O w')^{\text{T}}f^{\text{mlp}}_{\theta}(h^{\text{dec}}_{n, m})}}, \label{eq:hred_decoder_output}
\end{align}
where $O \in \mathbb{R}^{|V| \times d}$ is the word embedding matrix for the output distribution with embedding dimensionality $d \in \mathbb{N}$.

As mentioned in the model section, the approximate posterior is conditioned on the \textit{encoder} RNN state of the next utterance: 
\begin{align}
Q_{\psi}(z_n \mid \mathbf{w}_{\leq n}) = f^{\text{post}}_\psi (z_n ; h_{n-1}^{con}, h_{n, M_n}^{enc}),
\end{align}
where $f^{\text{post}}$ is a PDF parametrized by $\psi$ and $h_{n, M_n}^{enc}$ (i.e.\@ the future state of the \textit{encoder} RNN after processing $\mathbf{w}_n$).

For the Gaussian latent variables, we use the interpolation gating mechanism described in the main text for the approximate posterior. We experimented with other mechanisms for controlling the gating variables, such as defining  $\alpha_{\mu}$ and $\alpha_{\sigma}$ to be a linear function of the encoder. However, this did not improve performance in our preliminary experiments.



\section{Appendix: Training Details}
\label{appendix_d}

\textbf{Piecewise Constant Variable Interpolation}
We conducted initial experiments with the interpolation gating mechanism for the approximate posterior of the piecewise constant latent variables.
However, we found that this did not improve performance.
%As described in Sections \ref{nvdm} and \ref{vhred}, for all models that used piecewise latent variables, we chose to fix $\alpha_{a_i} = 1$, meaning the piecewise prior and posterior models are kept separate --- as opposed to having the posterior be an interpolation between another distribution and the prior --- since we found this to perform better. We believe that if $\alpha_{a_i} = 0$ for a long period of time, then the posterior receives no gradient signal. Without a gradient signal, the estimated posterior becomes increasingly disconnected from the rest of the model and, thus, less effective. This might be due to the choice of non-linearities, which affect the piecewise latent variables more so than the Gaussian latent variables.

\textbf{Dialogue Modeling} We use the Ubuntu Dialogue Corpus v2.0 extracted January, 2016: \url{http://cs.mcgill.ca/~jpineau/datasets/ubuntu-corpus-1.0/}.

For the \textit{HRED} model we found that an additional rectified linear units layer decreased performance on the validation set according to the activity F1 metric. Hence we test \textit{HRED} without the rectified linear units layer.
On the other hand, for all \textit{VHRED} models we found that the additional rectified linear units layer improved performance on the validation set.
For \textit{P-VHRED}, we found that a final weight of one for the KL divergence terms performed best on the validation set.
For \textit{G-VHRED} and \textit{H-VHRED}, reweighing the KL divergence terms with a final value $0.25$ performed best on the validation set.
We conducted preliminary experiments with $n=3$ and $n=5$ pieces, and found that models with $n=3$ were easier to train.
Therefore, we use $n=3$ pieces for both \textit{P-VHRED} and \textit{H-VHRED}.

For all models, we compute the log-likelihood and variational lower-bound costs starting from the second utterance in each dialogue.

\section{Appendix: Additional Document Modeling Experiments}
\label{appendix_e}

\textbf{Iterative Inference}
For the document modeling experiments, our results and conclusions depend on how tight the variational lower-bound is.
As such, it is in theory possible that some of our models are performing much better than reported by the variational lower-bound on the test set.
Therefore, we use a non-parametric iterative inference procedure to tighten the variational lower-bound, which aims to learn a separate approximate posterior for each test example.
The iterative inference procedure consists of simple stochastic gradient descent (no more than 100 steps), with a learning rate of $0.1$ and the same gradient rescaling used in training.
For 20 News-Groups, the iterative inference procedure is stopped on a test example if the bound does not improve over 10 iterations.
For Reuters and CADE, the iterative inference procedure is stopped if the bound does not improve over $5$ iterations.
During iterative inference the parameters of the model, as the well as the generated prior, are all fixed. 
Only the gradients of the variational lower-bound with respect to generated posterior model parameters (i.e.\@ the mean and variance of the Gaussian variables, and the piecewise components, $a_i$) are used to update the posterior model for each document (using a freshly drawn sample for each inference iteration step).

Note, this form of inference is expensive and requires additional meta-parameters (e.g.\@ a step-size and an early-stopping criterion).
We remark that a simpler, and more accurate, approach to inference might perhaps be to use importance sampling.

The results based on iterative inference are reported in Table \ref{ppl_results}. As Section \ref{doc_model}, we find that \textit{H-NVDM} outperforms the \textit{G-NVDM} model. This confirms our previous conclusions.

In our current examples, it appears that the \emph{H-NVDM} with 5 pieces returns more general words. For example, as evidenced in Table \ref{word_query_2}, in the case of ``government'', the baseline seems to value the plural form of the word (which is largely based on morphology) while the hybrid model actually pulls out meaningful terms such as ``federal'', ``policy'', and ``administration''. 

\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.4}do
\caption{Word query similarity test on 20 News-Groups: for the query `government''.} %more general/abstract terms to the query, which may or may not always be what is desired.}
\label{word_query_2}
\centering
\small
\begin{tabular}{lllll}
%\hline\hline
\multicolumn{1}{l}{\begin{tabular}[x]{@{}c@{}}\textbf{G-NVDM}\\\end{tabular}}&\multicolumn{1}{c}{\begin{tabular}[x]{@{}c@{}}\textbf{H-NVDM-3}\\\end{tabular}}&\multicolumn{1}{c}{\begin{tabular}[x]{@{}c@{}}\textbf{H-NVDM-5}\\\end{tabular}}\tabularnewline
\hline
governments & citizens & arms \tabularnewline
citizens & rights & rights \tabularnewline
country & governments & federal \tabularnewline
threat & civil & country \tabularnewline
private & freedom & policy \tabularnewline
rights & legitimate & administration \tabularnewline
individuals & constitution & protect \tabularnewline
military & private & private \tabularnewline
freedom & court & citizens \tabularnewline
foreign & states & military \tabularnewline
\hline
\end{tabular}
\end{table}

\begin{table*}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.4}do
\caption{Comparative test perplexities on various document datasets (50 latent variables). Note that document probabilities were calculated using 10 samples to estimate the variational lower-bound.}
\label{ppl_results}
\centering
\begin{floatrow}
%\resizebox{8.5cm}{!} {%
\begin{tabular}{lrr}
%\hline\hline
\multicolumn{1}{l}{\begin{tabular}[x]{@{}c@{}}\textbf{20-NG}\\\end{tabular}}&\multicolumn{1}{c}{\begin{tabular}[x]{@{}c@{}}\textbf{Sampled}\\\end{tabular}}&\multicolumn{1}{c}{\begin{tabular}[x]{@{}c@{}}\textbf{SGD-Inf}\\\end{tabular}}\tabularnewline
\hline
%\textit{dAE} & $--$ & $--$ & $--$ & $--$\tabularnewline
\textit{LDA} & $1058$ & $--$ \tabularnewline
\textit{RSM} & $953$ & $--$ \tabularnewline
\textit{docNADE} & $896$ & $--$\tabularnewline
\textit{SBN} & $909$ & $--$ \tabularnewline
\textit{fDARN} & $917$ & $--$ \tabularnewline 
\textit{NVDM} & $836$ & $--$ \tabularnewline \hline
\textit{G-NVDM} & $651$ & $588$\tabularnewline 
\textit{H-NVDM-3} & $607$ & $546$ \tabularnewline
\textit{H-NVDM-5} & $\mathbf{566}$ & $\mathbf{496}$ \tabularnewline
%\textit{HC-NVDM-5} & $673$ & $596$\tabularnewline
\hline
\end{tabular}
%}
\begin{tabular}{lrr}
%\hline\hline
\multicolumn{1}{l}{\begin{tabular}[x]{@{}c@{}}\textbf{RCV1}\\\end{tabular}}&\multicolumn{1}{c}{\begin{tabular}[x]{@{}c@{}}\textbf{Sampled}\\\end{tabular}}&\multicolumn{1}{c}{\begin{tabular}[x]{@{}c@{}}\textbf{SGD-Inf}\\\end{tabular}}\tabularnewline
\hline
\textit{G-NVDM} & $905$ & $837$\tabularnewline 
\textit{H-NVDM-3} & $865$ & $807$ \tabularnewline 
%\textit{HC-NVDM-3} & $$ & $$\tabularnewline
\textit{H-NVDM-5} & $\mathbf{833}$ & $\mathbf{781}$ \tabularnewline
%\textit{HC-NVDM-5} & $$ & $$\tabularnewline
\textbf{} &  & \tabularnewline
\hline
\textbf{CADE} & \textbf{Sampled} & \textbf{SGD-Inf} \tabularnewline
\hline % SGD-inf patience of 5
\textit{G-NVDM} & $339$ & $230$\tabularnewline 
\textit{H-NVDM-3} & $\mathbf{258}$ & $\mathbf{193}$ \tabularnewline 
\textit{H-NVDM-5} & $294$ & $209$ \tabularnewline
\hline
\end{tabular}

\end{floatrow}
\end{table*}
% Table 1: Definitions of Sampled and SGD-Inf should appear in the caption. The table should stand alone.

\begin{table*}[t] %[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.4}do
\caption{Approximate posterior word encodings (20-NG). For P-KL, we bold every case where piecewise variables showed greater word sensitivity than Gaussian variables w/in the same hybrid model.}
\label{doc_model_analysis}
\centering
\begin{floatrow}
\resizebox{7cm}{!} {%
\begin{tabular}{cccc}
%\hline\hline
\multicolumn{1}{c}{\begin{tabular}[x]{@{}c@{}}\textbf{Word}\\\end{tabular}}&\multicolumn{1}{c}{\begin{tabular}[x]{@{}c@{}}\textbf{G-NVDM}\\\end{tabular}}&\multicolumn{1}{c}{\begin{tabular}[x]{@{}c@{}}\textbf{H-NVDM-5}\\\end{tabular}}&\multicolumn{1}{c}{\begin{tabular}[x]{@{}c@{}}\textbf{}\\\end{tabular}}\tabularnewline
\textbf{Time-related} & \textbf{G-KL} & \textbf{G-KL} & \textbf{P-KL} \tabularnewline
\hline
months & 23 & 33 & \textbf{40} \tabularnewline
day & 28 & 32 & \textbf{35} \tabularnewline
time & \textbf{55} & 22 & \textbf{40} \tabularnewline
century & \textbf{28} & 13 & \textbf{19} \tabularnewline
past & \textbf{30} & 18 & \textbf{28} \tabularnewline
days & \textbf{37} & 14 & \textbf{19} \tabularnewline
ahead & \textbf{33} & 20 & \textbf{33} \tabularnewline
years & \textbf{44} & 16 & \textbf{38} \tabularnewline
today & 46 & 27 & \textbf{71} \tabularnewline
back & 31 & 30 & \textbf{47} \tabularnewline
future & \textbf{20} & 15 & \textbf{20} \tabularnewline
order & \textbf{42} & 14 & \textbf{26} \tabularnewline
minute & 15 & 34 & \textbf{40} \tabularnewline
began & \textbf{16} & 5 & \textbf{13} \tabularnewline
night & \textbf{49} & 12 & \textbf{18} \tabularnewline
hour & \textbf{18} & \textbf{17} & 16 \tabularnewline
early & 42 & 42 & \textbf{69} \tabularnewline
yesterday & 25 & 26 & \textbf{36} \tabularnewline
year & \textbf{60} & 17 & \textbf{21} \tabularnewline
week & 28 & 54 & \textbf{58} \tabularnewline
hours & 20 & 26 & \textbf{31} \tabularnewline
minutes & \textbf{40} & 34 & \textbf{38} \tabularnewline
months & 23 & 33 & \textbf{40} \tabularnewline
history & \textbf{32} & 18 & \textbf{28} \tabularnewline
late & 41 & \textbf{45} & 31 \tabularnewline
moment & \textbf{23} & \textbf{17} & 16 \tabularnewline
season & \textbf{45} & 29 & \textbf{37} \tabularnewline
summer & 29 & 28 & \textbf{31} \tabularnewline
start & 30 & 14 & \textbf{38} \tabularnewline
continue & 21 & 32 & \textbf{34} \tabularnewline
happened & 22 & 27 & \textbf{35} \tabularnewline
\hline
\end{tabular}
}

\resizebox{7cm}{!} {%
\begin{tabular}{cccc}
%\hline\hline
\multicolumn{1}{c}{\begin{tabular}[x]{@{}c@{}}\textbf{Word}\\\end{tabular}}&\multicolumn{1}{c}{\begin{tabular}[x]{@{}c@{}}\textbf{G-NVDM}\\\end{tabular}}&\multicolumn{1}{c}{\begin{tabular}[x]{@{}c@{}}\textbf{H-NVDM-5}\\\end{tabular}}&\multicolumn{1}{c}{\begin{tabular}[x]{@{}c@{}}\textbf{}\\\end{tabular}}\tabularnewline
\textbf{Names} & \textbf{G-KL} & \textbf{G-KL} & \textbf{P-KL} \tabularnewline
\hline
henry & 33 & \textbf{47} & 39 \tabularnewline
tim & \textbf{32} & 27 & 11 \tabularnewline
%patrick & VAL & 31 & 26 \tabularnewline
%dod & VAL & 171 & 62 \tabularnewline
mary & 26 & \textbf{51} & 30 \tabularnewline
james & 40 & \textbf{72} & 30 \tabularnewline
jesus & 28 & \textbf{87} & 39 \tabularnewline
george & 26 & \textbf{56} & 29 \tabularnewline
keith & 65 & \textbf{94} & 61 \tabularnewline
kent & 51 & \textbf{56} & 15 \tabularnewline
chris & 38 & \textbf{55} & 28 \tabularnewline
thomas & 19 & \textbf{35} & 19 \tabularnewline
hitler & 10 & \textbf{14} & 9 \tabularnewline
paul & 25 & \textbf{52} & 18 \tabularnewline
mike & 38 & \textbf{76} & 40 \tabularnewline
bush & \textbf{21} & 20 & 14 \tabularnewline \tabularnewline
\textbf{Adjectives} & \textbf{G-KL} & \textbf{G-KL} & \textbf{P-KL} \tabularnewline
\hline
american & \textbf{50} & 12 & \textbf{40} \tabularnewline
german & \textbf{25} & 21 & \textbf{22} \tabularnewline
european & 20 & 17 & \textbf{27} \tabularnewline
muslim & 19 & 7 & \textbf{23} \tabularnewline
french & 11 & \textbf{17} & \textbf{17} \tabularnewline
canadian & \textbf{18} & 10 & \textbf{16} \tabularnewline
japanese & 16 & 9 & \textbf{24} \tabularnewline
jewish & \textbf{56} & 37 & \textbf{54} \tabularnewline
%palestinian & 10 & 5 & 6 \tabularnewline
english & 19 & 16 & \textbf{26} \tabularnewline
%armenian & VAL & 3 & 3 \tabularnewline
islamic & 14 & 18 & \textbf{28} \tabularnewline
israeli & \textbf{24} & 14 & \textbf{18} \tabularnewline
british & \textbf{35} & 15 & \textbf{17} \tabularnewline
russian & 14 & 19 & \textbf{20} \tabularnewline
%lebanese & \textbf{15} & 7 & \textbf{10} \tabularnewline
\hline
\end{tabular}
}

\end{floatrow}
\end{table*}

\textbf{Approximate Posterior Analysis} We present an additional analysis of the approximate posterior on 20 News-Groups, in order to understand what the models are capturing. 
For a test example, we calculate the squared norm of the gradient of the KL terms w.r.t.\@ the word embedding inputted to the approximate posterior model.
The higher the squared norm of the gradients of a word is, the more influence it will have on the posterior approximation (encoder model).
For every test example, we count the top $5$ words with highest squared gradients separately for the multivariate Gaussian and piecewise constant latent variables.\footnote{Our approach is equivalent to counting the top $5$ words with the highest L2 gradient norms.}

%needed to formulate word scores, we follow the approach described in Sub-section \ref{dialog_model}, however, conditioning only on the (training) document bag-of-words to compute the latent posterior to then calculate the gradient of the KL-terms with respect to each word in the document.%\footnote{Since the vocabulary for 20-NG is relatively manageable, we opted for fully propagating the error gradients back to the input units.}

The results shown in Table \ref{doc_model_analysis}, illustrate how the piecewise variables capture different aspects of the document data.
The Gaussian variables were originally were sensitive to some of the words in the table.
However, in the hybrid model, nearly all of the temporal words that the Gaussian variables were once more sensitive to now more strongly affect the piecewise variables, which themselves also capture all of the words that were originally missed
This shift in responsibility indicates that the piecewise constant variables are better equipped to handle certain latent factors.
This effect appears to be particularly strong in the case of certain nationality-based adjectives (e.g., ``american'', ``israeli'', etc.).
While the \textit{G-NVDM} could model multi-modality in the data to some degree, this work would be primarily done in the model's decoder. In the \textit{H-NVDM}, the piecewise variables provide an explicit mechanism for capturing modes in the unknown target distribution, so it makes sense that the model would learn to use the piecewise variables instead, thus freeing up the Gaussian variables to capture other aspects of the data, as we found was the case with names (e.g., ``jesus'', ``kent'', etc.).

\section{Appendix: Additional Dialogue Modeling Experiments}
\label{appendix_f}


\begin{table*}[ht]
 \caption{Ubuntu model examples. The $\rightarrow$ token indicates a change of turn.}
 \label{table:ubuntu-examples}
 \scriptsize
 \centering
 \begin{tabular}{p{70mm}p{55mm}}
 \textbf{Dialogue Context (History)} & \textbf{Response} \\ \hline
Hi . I am installing ubuntu now in my new laptop . In " something else " partitioning , what mount point should I set for a drive which is not root or not home ...  $\rightarrow$ It 's up to you , just choose a directory that will remind you of the contents of that partition . E.G. : if it 's the Windows partition , use /windows .  $\rightarrow$ it 's a new harddrive with full free space . I bought it without windows preinstalled .  I want to create drives in which I will only store files .. I mean , not root or not home . What mount point do I set for it ? " /mount " is not shown in drop down menu  sorry . I mean /mount  I mean , in my desktop , extra drives are mounted in /media  do you understand my problem ? Sorry , English is not my native language .  $\rightarrow$ I do :) Just use a directory with a meaningful name and preferably in the root , for example " /files ".  Choose something like " /files ".  $\rightarrow$ / for root , /home for home , but what mount point for a file storage partition .  wait , let me see if there is a " /files " , thank you .  & \textbf{HRED:} No problem . \newline \textbf{G-VHRED:} It should be in the list of the drive . If you want to be able to mount it .  \newline \textbf{P-VHRED:} If you want to mount it , you can use the mount command .  \newline \textbf{H-VHRED:} As far as I know , you can use the " mount " command to mount the partitions .  \\ \hline
I have a problem installing Ubuntu 12.04 desktop . Tried desktop with USB -> hangs . alternate with usb -> hangs . alternate with cd -> hangs ( even faster) . I tried using different usb drives . Could someonle please help me ?  $\rightarrow$ how are you making the usb ? and whats the pc specs .  $\rightarrow$ I used Universal usb installer  $\rightarrow$ what exactly does it do when it ' hangs '  $\rightarrow$ Specs : CPU core i7 @ 3.4 Ghz , Sapphire Radeon HD 7800 , 8GB RAM  $\rightarrow$ ATi video ? tried the NOMODESET option yet ?  $\rightarrow$ nothing at all , I also can't witch to console using Ctrl + alt + F  not yet  Will try that , thanks  & \textbf{HRED:} you could try the alternate install cd .  \newline \textbf{G-VHRED:} it might be a problem with your graphics card . You could also try the alternate install cd and see if it works . \newline \textbf{P-VHRED:} try ' nomodeset ' or ' nomodeset ' \newline \textbf{H-VHRED:} only thing you can do is run the command ' sudo dpkg-reconfigure xserver-xorg ' and see if you get any errors .  \\ \hline
anyone know how to add a route in ubuntu ?  getting an error on adding a route , and I'm not sure why  $\rightarrow$ pastebin your command and pronlem  $\rightarrow$ \textless url \textgreater  & \textbf{HRED:} can you pastebin the output of " route -n " please \newline \textbf{G-VHRED:} if you want to use pastebin to pastebin the output of " sudo route -n " please pastebin the output of " route " \newline \textbf{P-VHRED:} what is the output of : route -n \newline \textbf{H-VHRED:} only if you have a static ip address , you need to specify the ip address of the gateway  \\ \hline
Any idea why empathy 's not playing notification sounds ? Even though I have ' em ticked in preferences  $\rightarrow$ restarted it yet ?  $\rightarrow$ yar  $\rightarrow$ check pulse to see if the application is muted for some reason ?  well Sound settings .  $\rightarrow$ Had sound effects turned off in sound settings , didn't realize that controlled other applications  $\rightarrow$ Ah yea , ive done it a few time it 's annoying  My favorite though is recently pulse has been freezing on my desktop and audio will just not be adjustable for like ... 30 seconds or so  & \textbf{HRED:} I don't think it 's a problem with pulseaudio , but I'm not sure how to fix it \newline \textbf{G-VHRED:} Thank you for your help . I will try that . Thanks for the suggestion . \newline \textbf{P-VHRED:} I think it 's a bug with pulseaudio , but I don't know how to fix it . \newline \textbf{H-VHRED:} as far as I can tell , I have a problem with pulseaudio , but I don't know how to fix it .  \\ \hline
Question about setting up drives : Running 12.04 LTS . I have two drives . An SSD that holds the OS for booting , and a second drive " data " for all the data , home directories , etc . The data drive is encrypted with LUKS . With my current setup , whenever I reboot , I get a password entry screen to decrypt the data drive , where my home directory is . This works fine . The problem is that if I reboot remotely , I'm stuck as I can't enter  password . If I keep my home directory on the SSD , there won't be enough room . If I use a symbolic link , and the encrypted drive isn't mounted , that creates errors and issues . Suggestions ??  $\rightarrow$ yes , you can redirect the  $\rightarrow$ Message was cut off ...  & \textbf{HRED:} if you want to see the data on the drive , you can mount it in /etc/fstab \newline \textbf{G-VHRED:} it should be a good place to do that , but if you want to do a clean install , you will need to reboot\newline \textbf{P-VHRED:} it 's not a problem , it 's just a bug \newline \textbf{H-VHRED:} if you want to see the data on the drive , you can mount it in /etc/fstab  \\ \hline
hey guys , does anyone know why i cannot connect to ssh via my external IP ? (connection refused ) but ssh -v localhost works fine ?  $\rightarrow$ Have you forwarded the port on your router to the machine you're trying to access ?  $\rightarrow$ It is a work machine , so am not sure of the rules & \textbf{HRED:} You need to forward port 22 to your router to forward port 22 to the server . \newline \textbf{G-VHRED:} That 's odd . What are you trying to do ?  Can you pastebin the output of " sudo netstat " to " pastebin " please ? \newline \textbf{P-VHRED:} Can you pastebin the output of " sudo apt-get install openssh-server "? \newline \textbf{H-VHRED:} Even if it 's not working , then you need to set the port forward to your router . \\ \hline
 \end{tabular}
\end{table*}

{\bf Ubuntu Experiments } We present test examples --- dialogue context and model responses generated using beam search --- for the Ubuntu models in Table \ref{table:ubuntu-examples}. The examples qualitatively illustrate the differences between models.
First, we observe that \textit{HRED} tends to generate highly generic responses compared to all the latent variable models. This supports the quantitative results reported in the main text, and suggests that modeling the latent factors through latent variables is critical for this task.
Next, we observe that \textit{H-VHRED} tends to generate relevant entities and commands --- such as \textit{mount command}, \textit{xserver-xorg}, \textit{static ip address} and \textit{pulseaudio} in examples 1-4.
On the other hand, \textit{G-VHRED} tends to be better at generating appropriate verbs --- such as \textit{list}, \textit{install}, \textit{pastebin} and \textit{reboot} in examples   1-3 and example 5.
Qualitatively, \textit{P-VHRED} model appears to perform somewhat worse than both \textit{G-VHRED} and \textit{H-VHRED}. This suggests that the Gaussian latent variables are important for the Ubuntu task, and therefore that the best performance may be obtained by combining both Gaussian and piecewise latent variables together in the \textit{H-VHRED} model.



%{\bf Approximate Posterior Analysis    }
{\bf Twitter Experiments } We also conducted a dialogue modeling experiment on a Twitter corpus, extracted from based on public Twitter conversations \citep{ritter2011data}.
The dataset is split into training, validation, and test sets, containing respectively 749,060, 93,633 and 9,399 dialogues each.
On average, each dialogue contains about $6$ utterances (dialogue turns) and about $94$ words.
We pre-processed the tweets using byte-pair encoding \citep{sennrich2015neural} with a vocabulary consisting of 5000 sub-words.

We trained our models with a learning rate of $0.0002$ and mini-batches of size $40$ or $80$.\footnote{We had to vary the mini-batch size to make the training fit on GPU architectures with low memory.}
As for the Ubuntu experiments, we used a variant of truncated back-propagation and apply gradient clipping.
We experiment with \textit{G-VHRED} and \textit{H-VHRED}. 
Similar to \citep{serban2016hierarchical}, we use a bidirectional GRU RNN \textit{encoder}, where the forward and backward RNNs each have $1000$ hidden units.
We experiment with \textit{context} RNN encoders with $500$ and $1000$ hidden units, and find that that $1000$ hidden units reach better performance w.r.t.\@ the variational lower-bound on the validation set.
The \textit{encoder} and \textit{context} RNNs use layer normalization \citep{ba2016layer}.
We experiment with \textit{decoder} RNNs with $1000$, $2000$ and $4000$ hidden units (LSTM cells), and find that $2000$ hidden units reach better performance.
For the \textit{G-VHRED} model, we experiment with latent multivariate Gaussian variables with $100$ and $300$ dimensions, and find that $100$ dimensions reach better performance.
For the \textit{H-VHRED} model, we experiment with latent multivariate Gaussian and piecewise constant variables each with $100$ and $300$ dimensions, and find that $100$ dimensions reach better performance.
We drop words in the decoder with a fixed drop rate of $25\%$ and multiply the KL terms in the variational lower-bound by a scalar, which starts at zero and linearly increases to $1$ over the first 60,000 training batches.
Note, unlike the Ubuntu experiments, the final weight of the KL divergence is exactly one (hence the bound is tight).

%We also experiment with an \textit{HRED} model. For this model, we use the same \textit{encoder} and \textit{context} RNN architectures as the \textit{G-VHRED} and \textit{H-VHRED} models described above. We set the \textit{encoder} RNN to have $1000$ hidden units.


\newcommand{\heart}{\ensuremath\heartsuit}

\begin{table*}[t]
  \caption{Approximate posterior word encoding on Twitter. The numbers are computed by counting the number of times each word is among the $5$ words with the largest sum of squared gradients of the Gaussian KL divergence (G-KL) and piecewise constant KL divergence (P-KL)}
  \label{tabel:approximate_posterior_word_sensitivity}
  \small
  \centering
    \begin{tabular}{c c c c c c c c }
         
    \textbf{Word} & \textbf{G-VHRED} & \multicolumn{2}{c}{\textbf{H-VHRED}} & \textbf{Word} & \textbf{G-VHRED} & \multicolumn{2}{c}{\textbf{H-VHRED}} \\ 
    \textbf{Time-related} & \textbf{G-KL} & \textbf{G-KL} & \textbf{P-KL} & \textbf{Event-related} & \textbf{G-KL} & \textbf{G-KL} & \textbf{P-KL} \\
    \hline
    monday & 3 & 5 & \textbf{10} & school & 9 & 16 & \textbf{50} \\
    tuesday & 2 & 3 & \textbf{7} & class & 11 & 16 & \textbf{27} \\
    wednesday & 4 & 11 & \textbf{13} & game & 20 & 26 & \textbf{41} \\
    thursday & 2 & 3 & \textbf{9} & movie & 12 & 20 & \textbf{41} \\
    friday & 9 & 18 & \textbf{26} & club & 13 & 22 & \textbf{28} \\
    saturday & 6 & 6 & \textbf{13} & party & 8 & 10 & \textbf{32} \\
    sunday & 2 & 2 & \textbf{9} & wedding & 7 & 13 & \textbf{23} \\
    weekend & 8 & 16 & \textbf{32} & birthday & 12 & 20 & \textbf{23} \\
    today & 18 & 28 & \textbf{56} & easter & 15 & 15 & \textbf{23} \\
    night & 16 & 31 & \textbf{68} & concert & 7 & 16 & \textbf{20} \\
    tonight & 32 & 36 & \textbf{47} & dance & 11 & 12 & \textbf{21} \\
    & & & & & & & \\
    \textbf{Word} & \textbf{G-VHRED} & \multicolumn{2}{c}{\textbf{H-VHRED}} & \textbf{Word} & \textbf{G-VHRED} & \multicolumn{2}{c}{\textbf{H-VHRED}} \\ 
    \parbox[c][2.00em][c]{0.1\textwidth}{\textbf{Sentiment \\ -related}} & \textbf{G-KL} & \textbf{G-KL} & \textbf{P-KL} & \parbox[c][2.00em][c]{0.235\textwidth}{\textbf{Acronyms, Punctuation \\ Marks \& Emoticons}} & \textbf{G-KL} & \textbf{G-KL} & \textbf{P-KL} \\ \hline
  good & \textbf{72} & \textbf{73} & 44 & lol & \textbf{394} & \textbf{358} & 312 \\
  love & \textbf{102} & \textbf{101} & 38 & omg & \textbf{52} & \textbf{45} & 19 \\
  awesome & \textbf{26} & \textbf{44} & 39 & . & 386 & 558 & \textbf{1009} \\
  cool & 14 & 28 & \textbf{29} & ! & \textbf{648} & \textbf{951} & 525 \\
  haha & \textbf{132} & \textbf{101} & 75 & ? & \textbf{507} & \textbf{851} & 221 \\
  hahaha & \textbf{60} & \textbf{48} & 24 & * & \textbf{108} & \textbf{54} & 19 \\
  amazing & \textbf{14} & \textbf{38} & 33 & xd & \textbf{28} & \textbf{42} & 26 \\
  thank & \textbf{137} & \textbf{153} & 29 & \heart & \textbf{56} & \textbf{42} & 24 \\ \hline
    \end{tabular}
\end{table*}

%{\bf Response Evaluation}
%Non-goal-driven dialogue models are typically evaluated by asking humans to rate the quality of different responses.
%We follow the approach by \citet{liu2016not} by conducting an Amazon Mechanical Turk experiment to compare the \textit{G-VHRED} and \textit{H-VHRED} models.
%For each test dialogue, we use TF-IDF to extract 100 candidate responses \citep{lowe2015ubuntu}.
%We then rank the responses according to the \textit{G-VHRED} model and \textit{H-VHRED} model using the variational lower-bound.\footnote{We only use one stochastic sample for evaluating the ranking cost.}
%We ask three human evaluators to rate model responses for $45$ dialogues on a Likert-type scale $1-5$, with $1$ representing an inappropriate response and $5$ representing a highly appropriate response.\footnote{Human evaluators are only given a minimal description of the task, without any examples, before beginning the evaluation.}
%For each dialogue, we show the human evaluators the top two responses ranked %by the \textit{G-VHRED} and \textit{H-VHRED} models.
%We choose to evaluate the re-ranked responses for two reasons.
%First, it reduces variance in the output because it uses the approximate posterior model, compared to using beam search with samples from the high-entropy prior.
%Second, it decreases the number of generic responses, which are extremely common among generative dialogue models for Twitter, and which human evaluators tend to prefer despite not advancing the dialogue \citep{li2015diversity}.

%The results are as follows. The \textit{G-VHRED} model achieves scores $1.88$ and $2.13$ for the first and second ranked responses on average, and the \textit{H-VHRED} model achieves scores $1.93$ and $2.04$ on average. In other words, \textit{H-VHRED} performs nominally better on the first ranked response while \textit{G-VHRED} performs nominally better on the second ranked response. In conclusion, if there exists a difference between the two models, naive human evaluators cannot see it.

Our hypothesis is that the piecewise constant latent variables are able to capture multi-modal aspects of the dialogue.
Therefore, we evaluate the models by analyzing what information they have learned to represent in the latent variables.
For each test dialogue with $n$ utterances, we condition each model on the first $n-1$ utterances and compute the latent posterior distributions using all $n$ utterances.
We then compute the gradients of the KL terms of the multivariate Gaussian and piecewise constant latent variables w.r.t.\@ each word in the dialogue.
Since the words vectors are discrete, we compute the sum of the squared gradients w.r.t.\@ each word embedding.
% Alex: should we mention we normalize to ensure values sum to 1?
% Julian: it doesn't matter for this result. Since we normalize across words, the top 5 most sensitive words are still the same after normalization as before normalization. But I will add a note in the 
The higher the sum of the squared gradients of a word is, the more influence it will have on the posterior approximation (encoder model).
For every test dialogue, we count the top $5$ words with highest squared gradients separately for the multivariate Gaussian and piecewise constant latent variables.\footnote{Our approach is equivalent to counting the top $5$ words with the highest L2 gradient norms. We also did some experiments using L1 gradient norms, which showed similar patterns.}

The results are shown in Table \ref{tabel:approximate_posterior_word_sensitivity}.
The piecewise constant latent variables clearly capture different aspects of the dialogue compared to the Gaussian latent variables.
The piecewise constant variable approximate posterior encodes words related to time (e.g.\@ weekdays and times of day) and events (e.g.\@ parties, concerts, Easter).
On the other hand, the Gaussian variable approximate posterior encodes words related to sentiment (e.g.\@ laughter and appreciation) and acronyms, punctuation marks and emoticons (i.e.\@ smilies). We also conduct a similar analysis on the document models evaluated in Sub-section \ref{doc_model}, the results of which may be found in the Appendix.

\end{document} 


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was
% created by Lise Getoor and Tobias Scheffer, it was slightly modified  
% from the 2010 version by Thorsten Joachims & Johannes Fuernkranz, 
% slightly modified from the 2009 version by Kiri Wagstaff and 
% Sam Roweis's 2008 version, which is slightly modified from 
% Prasad Tadepalli's 2007 version which is a lightly 
% changed version of the previous year's version by Andrew Moore, 
% which was in turn edited from those of Kristian Kersting and 
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.  



\documentclass{article} % For LaTeX2e
\usepackage{iclr2015,times}
\usepackage{hyperref}
\usepackage{url}

% For citations
\usepackage{natbib}

% for images and graphics
\usepackage{graphicx}

% for tables
\usepackage{booktabs}

% for pseudo code
\usepackage{algorithmic}
\usepackage{algorithm}

\title{Training deep neural networks with \\ low precision multiplications}

\author{
Matthieu Courbariaux \& Jean-Pierre David\\
\'{E}cole Polytechnique de Montr\'{e}al \\
\texttt{\{matthieu.courbariaux,jean-pierre.david\}@polymtl.ca} \\
\And
Yoshua Bengio \\
Universit\'{e} de Montr\'{e}al, CIFAR Senior Fellow\\
\texttt{yoshua.bengio@gmail.com} \\
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version

% \iclrconference % Uncomment if submitted as conference paper instead of workshop

\begin{document}

\maketitle

\begin{abstract}

Multipliers are the most space and power-hungry arithmetic operators of the digital implementation of deep neural networks.
We train a set of state-of-the-art neural networks (Maxout networks) on three benchmark datasets:
MNIST, CIFAR-10 and SVHN. They are trained with three distinct formats: floating point, fixed point and dynamic fixed point.
For each of those datasets and for each of those formats,
we assess the impact of the precision of the multiplications on the final error after training.
We find that {\em very low precision is sufficient}
not just for running trained networks but {\em also for training them}.
For example, it is possible to train Maxout networks with {\bf 10} bits multiplications.\

\end{abstract}

\section{Introduction}

% deep learning = f(hardware)
The {\em training} of deep neural networks is very often limited by hardware.
Lots of previous works address the best exploitation of general-purpose hardware,
typically CPU clusters  \citep{Dean-et-al-NIPS2012} and GPUs  \citep{Coates-et-al-2009,Krizhevsky-2012}.
Faster implementations usually lead to state of the art results \citep{Dean-et-al-NIPS2012,Krizhevsky-2012}.

% general purpose vs dedicated hardware
Actually, such approaches always consist in adapting the algorithm to best exploit state of the art general-purpose hardware.
Nevertheless, some dedicated deep learning hardware is appearing as well.
FPGA and ASIC implementations claim a better power efficiency than general-purpose hardware
\citep{Kim-et-al-2009, Farabet-et-al-2011, Pham-et-al-2012, Chen-et-al-ACM2014, Chen-et-al-IEEE2014}.
In contrast with general-purpose hardware, dedicated hardware such as ASIC and FPGA enables to build the hardware from the algorithm.

% Objective and plan
Hardware is mainly made out of memories and arithmetic operators.
Multipliers are the most space and power-hungry arithmetic operators of the digital implementation of deep neural networks.
The objective of this article is to assess the possibility to
reduce the precision of the multipliers for deep learning:
\begin{itemize}

  \item We train deep neural networks with low precision multipliers and high precision accumulators (Section \ref{sec:accumulations}).

  \item We carry out experiments with three distinct formats:
    \begin{enumerate}
      \item Floating point (Section \ref{sec:float})
      \item Fixed point (Section \ref{sec:fixed})
      \item Dynamic fixed point, which we think is a good compromise between floating and fixed points
        (Section \ref{sec:dynamic})
    \end{enumerate}

  \item We use a higher precision for the parameters during the updates
    than during the forward and backward propagations (Section \ref{sec:update}).

  \item Maxout networks  \citep{Goodfeli-et-al-TR2013} are a set of state-of-the-art neural networks (Section \ref{sec:maxout}).
    We train Maxout networks with slightly less capacity than \citet{Goodfeli-et-al-TR2013}
    on three benchmark datasets: MNIST, CIFAR-10 and SVHN (Section \ref{sec:baseline}).

  \item For each of the three datasets and for each of the three formats,
    we assess the impact of the precision of the multiplications on the final error of the training.
    We find that {\em very low precision multiplications are sufficient} not just for running trained networks but
    {\em also for training them} (Section \ref{sec:low}).
    We made our code available
    \footnote{ \url{https://github.com/MatthieuCourbariaux/deep-learning-multipliers} }.

\end{itemize}

\section{Multiplier-accumulators}
\label{sec:accumulations}

\begin{table}[h]
\center
\begin{tabular}{@{}ccc@{}}
\toprule
Multiplier (bits) & Accumulator (bits) & Adaptive Logic Modules (ALMs) \\ \midrule
32                & 32                 & 504                           \\
16                & 32                 & 138                           \\
16                & 16                 & 128                           \\ \bottomrule
\end{tabular}
\caption{Cost of a fixed point multiplier-accumulator on a Stratix V Altera FPGA.}
\label{tab:mac}%
\end{table}%

% in practice
\begin{algorithm}[h]
\begin{algorithmic}
    \FORALL{layers}
    \STATE Reduce the precision of the parameters and the inputs
    \STATE Apply convolution or dot product (with high precision accumulations)
    \STATE Reduce the precision of the weighted sums
    \STATE Apply activation functions
    \ENDFOR
    \STATE Reduce the precision of the outputs
\end{algorithmic}
\caption{Forward propagation with low precision multipliers.}
\label{alg:prop}
\end{algorithm}

% This part is not true for floating point operators... floating point adders cost more than multipliers.
Applying a deep neural network (DNN) mainly consists in convolutions and matrix multiplications.
The key arithmetic operation of DNNs is thus the multiply-accumulate operation.
Artificial neurons are basically multiplier-accumulators computing weighted sums of their inputs.

The cost of a fixed point multiplier varies as the square of the precision (of its operands) for small widths while the cost of adders and accumulators varies as a linear function of the precision \citep{4302704}.
As a result, the cost of a fixed point multiplier-accumulator mainly depends on the precision of the multiplier, 
as shown in table \ref{tab:mac}.
In modern FPGAs, the multiplications can also be implemented with dedicated DSP blocks/slices. One DSP block/slice can implement a single $27 \times 27$ multiplier, a double $18 \times 18$ multiplier or a triple $9 \times 9$ multiplier. Reducing the precision can thus lead to a gain of 3 in the number of available multipliers inside a modern FPGA.

In this article, we train deep neural networks with low precision multipliers and high precision accumulators,
as illustrated in Algorithm \ref{alg:prop}.

\section{Floating point}
\label{sec:float}

\begin{figure}[ht]
\begin{center}
\centerline{\includegraphics[width=.75\textwidth]{exponent}}
\end{center}
\caption{Comparison of the floating point and fixed point formats.}
\label{fig:exponent}
\end{figure}

\begin{table}[h]
\begin{center}
\begin{tabular}{@{}llll@{}}
\toprule
Format                          &  Total bit-width & Exponent bit-width & Mantissa bit-width \\ \midrule
Double precision floating point &  64              & 11                 & 52                 \\
Single precision floating point &  32              & 8                  & 23                 \\
Half precision floating point   &  16              & 5                  & 10                 \\ \bottomrule
\end{tabular}
\end{center}
\caption{Definitions of double, single and half precision floating point formats.
}
\label{tab:float_definitions}
\end{table}

Floating point formats are often used to represent real values.
They consist in a sign, an exponent, and a mantissa, as illustrated in figure \ref{fig:exponent}.
The exponent gives the floating point formats a wide range, and the mantissa gives them a good precision.
One can compute the value of a single floating point number using the following formula:
\[
  value = (-1)^{sign} \times \left(1 + \frac{mantissa}{2^{23}}\right) \times 2^{(exponent-127)}
\]

Table \ref{tab:float_definitions} shows the exponent and mantissa widths associated with each floating point format.
In our experiments, we use single precision floating point format as our reference
because it is the most widely used format in deep learning, especially for GPU computation.
We show that the use of half precision floating point format has little to no impact on the training of neural networks.
At the time of writing this article, no standard exists below the half precision floating point format.

\section{Fixed point}
\label{sec:fixed}

Fixed point formats consist in a signed mantissa and a global scaling factor shared between all fixed point variables.
The scaling factor can be seen as the position of the radix point.
It is usually fixed, hence the name "fixed point".
Reducing the scaling factor reduces the range and augments the precision of the format.
The scaling factor is typically a power of two for computational efficiency (the scaling multiplications are replaced with shifts).
As a result, fixed point format can also be seen as a floating point format with a {\em unique shared fixed exponent}
, as illustrated in figure \ref{fig:exponent}.
Fixed point format is commonly found on embedded systems with no FPU (Floating Point Unit).
It relies on integer operations. It is hardware-wise cheaper than its floating point counterpart,
as the exponent is shared and fixed.

\section{Dynamic fixed point}
\label{sec:dynamic}

% In short, if an overflow rate is too large, the exponent is augmented
\begin{algorithm}[h]
\begin{algorithmic}
    \REQUIRE a matrix $M$, a scaling factor $s_{t}$,
        and a maximum overflow rate $r_{max}$.
    \ENSURE an updated scaling factor $s_{t+1}$.
    \IF{the overflow rate of $M>r_{max}$}
        \STATE $s_{t+1} \leftarrow 2 \times s_{t}$
    \ELSIF{the overflow rate of $2 \times M \le r_{max}$}
        \STATE $s_{t+1} \leftarrow s_{t} /2$
    \ELSE
        \STATE $s_{t+1} \leftarrow s_{t}$
    \ENDIF
\end{algorithmic}
\caption{Policy to update a scaling factor.}
\label{alg:DFXP}
\end{algorithm}

When training deep neural networks,
\begin{enumerate}
  \item activations, gradients and parameters have {\em very different ranges}.
  \item gradients ranges {\em slowly diminish} during the training.
\end{enumerate}
As a result, the fixed point format, with its unique shared fixed exponent, is ill-suited to deep learning.

The dynamic fixed point format \citep{Williamson-1991}
is a variant of the fixed point format in which there are {\em several scaling factors} instead of a single global one.
Those scaling factors are {\em not fixed}. As such, it can be seen as a compromise between floating point format
- where each scalar variable owns its scaling factor which is updated during each operations -
and fixed point format - where there is only one global scaling factor which is never updated.
With dynamic fixed point, a few grouped variables share a scaling factor which is updated from time to time
to reflect the statistics of values in the group.

In practice, we associate each layer's weights, bias, weighted sum, outputs (post-nonlinearity)
and the respective gradients vectors and matrices with a different scaling factor.
Those scaling factors are initialized with a global value.
The initial values can also be found during the training with a higher precision format.
During the training, we update those scaling factors at a given frequency,
following the policy described in Algorithm \ref{alg:DFXP}.

\section{Updates vs. propagations}
\label{sec:update}

% update vs precision
We use a higher precision for the parameters during the updates than during the forward and backward propagations,
respectively called fprop and bprop.
% why it works
The idea behind this is to be able to accumulate small changes in the parameters (which
requires more precision) and while on the other hand sparing a few bits of memory bandwidth
during fprop. This can
be done because of the implicit averaging performed via stochastic gradient descent during training:
\[
  \theta_{t+1} = \theta_t - \epsilon \frac{\partial C_t(\theta_t)}{\partial \theta_t}
\]
where $C_t(\theta_t)$ is the cost to minimize over the minibatch visited at iteration
$t$ using $\theta_t$ as parameters
and $\epsilon$ is the learning rate. We see that the resulting parameter is
the sum
\[
  \theta_T = \theta_0 - \epsilon \sum_{t=1}^{T-1} \frac{\partial C_t(\theta_t)}{\partial \theta_t}.
\]
The terms of this sum are not statistically independent (because the value of $\theta_t$ depends
on the value of $\theta_{t-1}$) but the dominant variations come from the random sample
of examples in the minibatch ($\theta$ moves slowly) so that a strong averaging
effect takes place, and each contribution in the sum is relatively small, hence
the demand for sufficient precision (when adding a small number with a large number).

\section{Maxout networks}
\label{sec:maxout}

A Maxout network is a multi-layer neural network that uses maxout units in its hidden layers.
A maxout unit outputs the maximum of a set of $k$ dot products between $k$ weight
vectors and the input vector of the unit (e.g., the output of the previous layer):
\[
  h^l_i = \max_{j=1}^k ( b^l_{i,j} + w^l_{i,j} \cdot h^{l-1} )
\]
where $h^l$ is the vector of activations at layer $l$ and weight vectors $w^l_{i,j}$
and biases $b^l_{i,j}$ are the parameters of the $j$-th filter of unit $i$ on layer $l$.

A maxout unit can be seen as a generalization of the rectifying units
 \citep{Jarrett-ICCV2009,Nair-2010-small,Glorot+al-AI-2011-small,Krizhevsky-2012-small}
\[
  h^l_i = \max(0, b^l_i + w^l_i \cdot h^{l-1})
\]
which corresponds to a maxout unit when $k=2$ and one of the filters is forced at 0 \citep{Goodfeli-et-al-TR2013}.
Combined with dropout, a very effective regularization method  \citep{Hinton-et-al-arxiv2012},
maxout networks achieved state-of-the-art results on a number of benchmarks  \citep{Goodfeli-et-al-TR2013},
both as part of fully connected feedforward deep nets and as part of deep convolutional nets.
The dropout technique provides a good approximation of model averaging with shared parameters
across an exponentially large number of networks that are formed by subsets of the units
of the original noise-free deep network.

\section{Baseline results}
\label{sec:baseline}

We train Maxout networks with slightly less capacity than \citet{Goodfeli-et-al-TR2013}
on three benchmark datasets: MNIST, CIFAR-10 and SVHN.
In Section \ref{sec:low}, we use the same hyperparameters as in this section
to train Maxout networks with low precision multiplications.

\begin{table}[h]
\begin{center}
\begin{tabular}{@{}lllll@{}}
\toprule
Dataset   & Dimension                       & Labels           & Training set   & Test set  \\ \midrule
MNIST      & 784 (28 $\times$ 28 grayscale)  & 10               & 60K            & 10K       \\
CIFAR-10    & 3072 (32 $\times$ 32 color)     & 10               & 50K            & 10K       \\
SVHN       & 3072 (32 $\times$ 32 color)     & 10               & 604K           & 26K       \\ \bottomrule
\end{tabular}
\end{center}
\caption{
Overview of the datasets used in this paper.
}
\label{tab:datasets}
\end{table}

\begin{table}[h]
\begin{center}
\begin{tabular}{@{}lllllll@{}}
\toprule
Format                          & Prop. & Up. & PI MNIST   & MNIST  & CIFAR-10 & SVHN   \\ \midrule
 \citet{Goodfeli-et-al-TR2013}  & 32    & 32  & 0.94\%     & 0.45\% & 11.68\% & 2.47\% \\
Single precision floating point & 32    & 32  & 1.05\%     & 0.51\% & 14.05\% & 2.71\% \\
Half precision floating point   & 16    & 16  & 1.10\%     & 0.51\% & 14.14\% & 3.02\% \\
Fixed point                     & 20    & 20  & 1.39\%     & 0.57\% & 15.98\% & 2.97\% \\
Dynamic fixed point             & 10    & 12  & 1.28\%     & 0.59\% & 14.82\% & 4.95\% \\ \bottomrule
\end{tabular}
\end{center}
\caption{
Test set error rates of single and half floating point formats, fixed and dynamic fixed point formats on
the permutation invariant (PI) MNIST, MNIST (with convolutions, no distortions),
CIFAR-10 and SVHN datasets.
{\bf Prop.} is the bit-width of the propagations and {\bf Up.}
is the bit-width of the parameters updates.
The single precision floating point line refers to the results of our experiments.
It serves as a baseline to evaluate the degradation brought by lower precision.
}
\label{tab:all_results}
\end{table}

\subsection{MNIST}

The MNIST \citep{LeCun+98} dataset is described in Table \ref{tab:datasets}.
We do not use any data-augmentation (e.g. distortions) nor any unsupervised pre-training.
We simply use minibatch stochastic gradient descent (SGD) with momentum.
We use a linearly decaying learning rate and a linearly saturating momentum.
% TODO: formula and values
We regularize the model with dropout and a constraint on the norm of each weight vector, as in \citep{Srebro05}.

We train two different models on MNIST.
The first is a permutation invariant (PI) model which is unaware of the structure of the data.
It consists in two fully connected maxout layers followed by a softmax layer.
The second model consists in three convolutional maxout hidden layers (with spatial max pooling on top of the maxout layers)
followed by a densely connected softmax layer.

This is the same procedure as in \citet{Goodfeli-et-al-TR2013},
except that we do not train our model on the validation examples.
As a consequence, our test error is slightly larger than the one reported in \citet{Goodfeli-et-al-TR2013}.
The final test error is in Table \ref{tab:all_results}.

\subsection{CIFAR-10}

Some comparative
characteristics of the CIFAR-10 \citep{KrizhevskyHinton2009} dataset are given in Table \ref{tab:datasets}.
We preprocess the data using global contrast normalization and ZCA whitening.
The model consists in three convolutional maxout layers, a fully connected maxout layer, and a fully connected softmax layer.
We follow a similar procedure as with the MNIST dataset.
This is the same procedure as in \citet{Goodfeli-et-al-TR2013},
except that we reduced the number of hidden units and
that we do not train our model on the validation examples.
As a consequence, our test error is slightly larger than the one reported in \citet{Goodfeli-et-al-TR2013}.
The final test error is in Table \ref{tab:all_results}.

\subsection{Street View House Numbers}

The SVHN \citep{Netzer-wkshp-2011} dataset is described in Table \ref{tab:datasets}.
We applied local contrast normalization preprocessing the same way as \citet{Zeiler+et+al-ICLR2013}.
The model consists in three convolutional maxout layers, a fully connected maxout layer, and a fully connected softmax layer.
Otherwise, we followed the same approach as on the MNIST dataset.
This is the same procedure as in \citet{Goodfeli-et-al-TR2013},
except that we reduced the length of the training.
As a consequence, our test error is bigger than the one reported in \citet{Goodfeli-et-al-TR2013}.
The final test error is in Table \ref{tab:all_results}.

\section{Low precision results}
\label{sec:low}

\begin{figure}[ht]
\begin{center}
\centerline{\includegraphics[width=.66\textwidth]{range}}
\end{center}
\caption{
Final test error depending on the radix point position (5 means after the $5$th most significant bit)
and the dataset (permutation invariant MNIST and CIFAR-10).
The final test errors are normalized, that is to say divided by the dataset single float test error.
The propagations and parameter updates bit-widths are both set to 31 bits (32 with the sign).
}
\label{fig:range}
\end{figure}

\begin{figure}[ht]
\begin{center}
\centerline{\includegraphics[width=\textwidth]{propagation}}
\end{center}
\caption{
Final test error depending on the propagations bit-width, the format (dynamic fixed or fixed point)
and the dataset (permutation invariant MNIST, MNIST and CIFAR-10).
The final test errors are normalized, which means that they are divided by the dataset single float test error.
For both formats, the parameter updates bit-width is set to 31 bits (32 with the sign).
For fixed point format, the radix point is set after the fifth bit.
For dynamic fixed point format, the maximum overflow rate is set to 0.01\%.
}
\label{fig:propagations}
\end{figure}

\begin{figure}[ht]
\begin{center}
\centerline{\includegraphics[width=\textwidth]{update}}
\end{center}
\caption{
Final test error depending on the parameter updates bit-width, the format (dynamic fixed or fixed point)
and the dataset (permutation invariant MNIST, MNIST and CIFAR-10).
The final test errors are normalized, which means that they are divided by the dataset single float test error.
For both formats, the propagations bit-width is set to 31 bits (32 with the sign).
For fixed point format, the radix point is set after the fifth bit.
For dynamic fixed point format, the maximum overflow rate is set to 0.01\%.
}
\label{fig:updates}
\end{figure}

\begin{figure}[ht]
\begin{center}
\centerline{\includegraphics[width=.75\textwidth]{overflow}}
\end{center}
\caption{
Final test error depending on the maximum overflow rate and the propagations bit-width.
The final test errors are normalized, which means that they are divided by the dataset single float test error.
The parameter updates bit-width is set to 31 bits (32 with the sign).
}
\label{fig:overflow}
\end{figure}

\subsection{Floating point}

Half precision floating point format has little to no impact on the test set error rate,
as shown in Table \ref{tab:all_results}.
We conjecture that a high-precision fine-tuning could recover the small degradation of the error rate.
% TODO: test it

\subsection{Fixed point}

The optimal radix point position in fixed point is after the fifth (or arguably the sixth) most important bit,
as illustrated in Figure \ref{fig:range}.
The corresponding range is approximately [-32,32].
The corresponding scaling factor depends on the bit-width we are using.
The minimum bit-width for propagations in fixed point is 19 (20 with the sign).
Below this bit-width, the test set error rate rises very sharply,
as illustrated in Figure \ref{fig:propagations}.
The minimum bit-width for parameter updates in fixed point is 19 (20 with the sign).
Below this bit-width, the test set error rate rises very sharply,
as illustrated in Figure \ref{fig:updates}.
Doubling the number of hidden units does not allow any further reduction of the bit-widths on the permutation invariant MNIST.
In the end, using 19 (20 with the sign) bits for both the propagations and the parameter updates
has little impact on the final test error,
as shown in Table \ref{tab:all_results}.

\subsection{Dynamic fixed point}

We find the initial scaling factors by training with a higher precision format.
Once those scaling factors are found, we reinitialize the model parameters.
We update the scaling factors once every 10000 examples.
Augmenting the maximum overflow rate allows us to reduce the propagations bit-width
but it also significantly augments the final test error rate,
as illustrated in Figure \ref{fig:overflow}.
As a consequence, we use a low maximum overflow rate of 0.01\% for the rest of the experiments.
The minimum bit-width for the propagations in dynamic fixed point is 9 (10 with the sign).
Below this bit-width, the test set error rate rises very sharply,
as illustrated in Figure \ref{fig:propagations}.
The minimum bit-width for the parameter updates in dynamic fixed point is 11 (12 with the sign).
Below this bit-width, the test set error rate rises very sharply,
as illustrated in Figure \ref{fig:updates}.
Doubling the number of hidden units does not allow any further reduction of the bit-widths on the permutation invariant MNIST.
In the end, using 9 (10 with the sign) bits for the propagations and 11 (12 with the sign) bits for the parameter updates
has little impact on the final test error, with the exception of the SVHN dataset,
as shown in Table \ref{tab:all_results}.
This is significantly better than fixed point format, which is consistent with our predictions of Section \ref{sec:dynamic}.

\section{Related works}

% training != application
\citet{Vanhoucke-et-al-2011} use 8 bits linear quantization to store activations and weights.
Weights are scaled by taking their maximum magnitude in each layer and normalizing them to fall in the [-128, 127] range.
The total memory footprint of the network is reduced by between 3$\times$ and 4$\times$.
This is very similar to the dynamic fixed point format we use (Section \ref{sec:dynamic}).
However, \citet{Vanhoucke-et-al-2011} only {\em apply} already trained neural networks while we actually {\em train} them.

% Prove originality
Training neural networks with low precision arithmetic has already been done in previous works
\citep{Holt-et-al-1991,Presley-et-al-1994,Simard+Graf-NIPS1994,Wawrzynek-et-al-IEEE1996,Savich-et-al-2007}
\footnote{
A very recent work \citep{Gupta-et-al-2015} also trains neural networks with low precision.
The authors propose to replace round-to-nearest with stochastic rounding,
which allows to reduce the numerical precision to 16 bits while using the fixed point format.
It would be very interesting to combine dynamic fixed point and stochastic rounding.
}.
Our work is nevertheless original in several regards:
\begin{itemize}
    \item We are the first to train deep neural networks with the dynamic fixed point format.
    \item We use a higher precision for the weights during the updates.
    \item We train some of the latest models on some of the latest benchmarks.
\end{itemize}

\section{Conclusion and future works}

% result of our explorations
We have shown that:
\begin{itemize}
  \item Very low precision multipliers are sufficient for training deep neural networks.
  \item Dynamic fixed point seems well suited for training deep neural networks.
  \item Using a higher precision for the parameters during the updates helps.
\end{itemize}

% how to exploit
Our work can be exploited to:
\begin{itemize}
    \item Optimize memory usage on general-purpose hardware \citep{Gray-et-al-2015}.
    \item Design very power-efficient hardware dedicated to deep learning.
\end{itemize}

% where to explore ?
There is plenty of room for extending our work:
\begin{itemize}
  \item Other tasks than image classification.
  \item Other models than Maxout networks.
  \item Other formats than floating point, fixed point and dynamic fixed point.
\end{itemize}

\section{Acknowledgement}

We thank the developers of Theano \citep{bergstra+al:2010-scipy,Bastien-Theano-2012},
a Python library which allowed us to easily develop a fast and optimized code for GPU.
We also thank the developers of Pylearn2 \citep{pylearn2_arxiv_2013},
a Python library built on the top of Theano which allowed us to easily interface the datasets with our Theano code.
We are also grateful for funding from NSERC, the Canada Research Chairs, Compute Canada, and CIFAR.

% {\footnotesize
\bibliography{strings,strings-shorter,aigaion,ml,precision}
% }
\bibliographystyle{natbib}

\end{document}



\documentclass[twoside]{article}
\usepackage{proceed2e}

\usepackage[latin2]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage[
    pdfstartview={FitH},
    bookmarks=true,
    bookmarksnumbered=true,
    bookmarksopen=true,
    bookmarksopenlevel=\maxdimen,
    pdfborder={0 0 0},
    colorlinks=true,
    linkcolor = black,
    citecolor=black,
    urlcolor=black,
    filecolor=black,
    pdfauthor={Ferenc Huszar and Simon Lacoste-Julien},
    pdftitle={A Kernel Approach to Tractable Bayesian Nonparametrics},
    pdfdisplaydoctitle=true
]{hyperref}
\usepackage[pdftex]{graphicx}
\usepackage[numbers,square]{natbib}
\usepackage{tikz}
\usetikzlibrary{arrows,shapes}

% % % % % %
% Definition of macros
%
\newcommand{\ourmethod}{kst}
\newcommand{\figref}[1]{Fig.~\ref{#1}}
\newcommand{\subfigref}[2]{Fig.~\ref{#1}.#2}
\newcommand{\studentt}{student-$\mathcal{T}$\ }
\newcommand{\tr}{\mbox{\,tr\,}}
\newcommand{\trp}[1]{\mbox{\,tr\,}\left(#1\right)}
\newcommand{\transpose}[1]{#1^{\mbox{\sf \scriptsize T}}}
\newcommand{\quadform}[2]{\transpose{#1}#2#1}
\newcommand{\quadformp}[2]{\transpose{\left(#1\right)}#2\left(#1\right)}
\newcommand{\varconc}{\alpha}
\newcommand{\meanconc}{\beta}
\newcommand{\ie}{i.\,e.\ }
\newcommand{\eg}{e.\,g.\ }
\newcommand{\fix}[1]{$^\text{\textcolor{red}{?}}$\marginpar{\parbox[m]{3cm}{\tiny{#1}}}}

% % % % % %
% Title
%
\title{A Kernel Approach to Tractable Bayesian Nonparametrics}

\author{Ferenc Husz\'{a}r\\University of Cambridge \And Simon Lacoste-Julien\\University of Cambridge}

\begin{document}

\maketitle

\begin{abstract}
Inference in popular nonparametric Bayesian models typically relies on sampling or other approximations. This paper presents a general methodology for constructing novel tractable nonparametric Bayesian methods by applying the kernel trick to inference in a parametric Bayesian model. For example, Gaussian process regression can be derived this way from Bayesian linear regression. Despite the success of the Gaussian process framework, the kernel trick is rarely explicitly considered in the Bayesian literature. In this paper, we aim to fill this gap and demonstrate the potential of applying the kernel trick to tractable Bayesian parametric models in a wider context than just regression. As an example, we present an intuitive Bayesian kernel machine for density estimation that is obtained by applying the kernel trick to a Gaussian generative model in feature space.
\end{abstract}

\section{Introduction}

The popularity of nonparametric Bayesian methods has steadily risen in machine learning over the past decade. Bayesian inference in almost all current nonparametric models relies on approximations which typically involve Markov chain Monte Carlo, or more recently variational approximations~\cite{Blei2004}. There is an ever-growing interest in developing nonparametric Bayesian methods in which inference and prediction can be expressed in closed form and no approximations are needed. Such methods would be quite useful and desired in many applications, but are unfortunately rare at present. Perhaps the only known example is Gaussian process (GP) regression. In GP regression~\cite{Rasmussen2006}, the posterior predictive distribution is a tractable Gaussian with parameters that can be computed from data exactly in polynomial time. The method is widely adopted and also has favourable frequentist asymptotic properties~\cite{vanderVaart2008}. But what is the secret behind the remarkable algorithmic clarity of GP regression? What makes closed-form computations possible? We argue that the key is that GP regression is also a \emph{kernel machine}: the method is arrived at by applying the kernel trick in a parametric Bayesian model, namely linear regression (see \eg Chapter~2 in~\cite{Rasmussen2006})

Kernel methods~\cite{Hofman2008,Scholkopf2002} use a simple trick, widely known as the kernel trick, to overcome the limitations of a linear model: the observations ${\bm{x}_i\in\mathcal{X}}$ are first embedded in a feature space $\mathcal{F}$ using a nonlinear mapping ${\varphi:\mathcal{X} \mapsto \mathcal{F}}$. A linear algorithm is then applied on the embedded representations $\bm{\phi}_n = \varphi(\bm{x}_n)$ instead of the observations themselves. If the algorithm only makes use of scalar products $\langle\bm{\phi}_n,\bm{\phi}_m\rangle$, then by replacing all scalar products by tractable \emph{kernel} evaluations ${k(\bm{x}_n,\bm{x}_m) := \langle\bm{\phi}_n,\bm{\phi}_m\rangle}$, the expressive power can be substantially increased with only a minor increase in computational costs. Notably, the kernel trick allows one to construct nonparametric machine learning methods from parametric ones.

Despite its popularity in ``non-Bayesian'' studies, the kernel trick is rarely considered as a construction tool in Bayesian nonparametrics. GP regression is a rare, if not the only example. GPs are therefore often called \emph{Bayesian kernel machines}. In this paper,  we consider finding new examples of Bayesian kernel machines, \ie broadening the intersection between kernel machines and nonparametric Bayesian methods. For Bayesian nonparametrics, the kernel approach offers invaluable closed-form computations and a rigorous analysis framework associated with reproducing kernel Hilbert spaces (RKHS). For kernel machines, a Bayesian formulation offers benefits, such as novel probabilistic approaches to setting kernel hyperparameters and means of incorporating such methods in hierarchical Bayesian models~\cite{Rasmussen2002,Chu2007,Adams2009}.

In this paper, we present a methodology for finding novel Bayesian kernel machines, following the recipe of GP regression:
\begin{enumerate}
\addtolength{\itemsep}{-2mm}
 \item Start with a simple Bayesian model of observations.
 \item Derive exact Bayesian inference in the model.
 \item Express the posterior predictive distribution in terms of dot products and apply the kernel trick.
\end{enumerate}

The crucial point is finding the basic model in step 1, in which both steps 2 and 3 are possible. Fortunately, this search is guided by intuitive orthonormal invariance considerations that will be demonstrated in this paper. We present an example Bayesian kernel machine for density estimation, based on the linear Gaussian generative model underlying principal component analysis (PCA)~\cite{Roweis1999}. We show that the kernel trick can be applied in the Bayesian method by choosing prior distributions over the parameters which preserve invariance to orthonormal transformations.

The rest of the paper is organised as follows. In Sec.~\ref{sec:genmodel}, we review Bayesian inference in a Gaussian generative model and discuss consequences of applying the kernel trick to the predictive density. We consider the infinite dimensional feature spaces case in Subsec.~\ref{sec:infinite_dimensional}. In Sec.~\ref{sec:experiments}, we present experiments on high dimensional density estimation problems comparing our method to other Bayesian and non-Bayesian nonparametric methods.

\section{A Gaussian model in feature space\label{sec:genmodel}}

Assume that we have observations $\bm{x}_i$ in a $d$-dimensional Euclidean space and that our task is to estimate their density. In anticipation of the sequel, we embed the observations $\bm{x}_i$ into a high-dimensional feature space $\mathcal{F}$ with an injective smooth nonlinear mapping ${\varphi:\mathcal{X} \mapsto \mathcal{F}}$. Our density estimation method is based on a simple generative model on the ambient feature space of the embedded observations $\bm{\phi}_i=\varphi(\bm{x}_i)$. For now, think of $\mathcal{F}$ as a $D$-dimensional Euclidean space and $\bm{\phi}_i$ as \emph{arbitrary elements} of $\mathcal{F}$ (\ie they are not necessary constrained to lie on the \emph{observation manifold} $\mathcal{O} = \left\{\varphi(\bm{x}) : \bm{x}\in\mathcal{X}\right\}$). We suppose that $\bm{\phi}_i$ were sampled from a Gaussian distribution with unknown mean $\bm{\mu}$ and covariance $\bm{\Sigma}$:
\begin{equation}
 \bm{\phi}_{1:N}\vert \bm{\mu},\bm{\Sigma} \sim \mathcal{N}\left(\bm{\mu},\bm{\Sigma}\right)\mbox{, i.\,i.\,d.}\label{eqn:genmodel_obs}
\end{equation}
The notation $\bm{\phi}_{1:N}$ is used to denote the set of vectors $\bm{\phi}_{1}$ up to $\bm{\phi}_{N}$.

%Given this model, one can estimate parameters $\bm{\mu}$ and $\bm{\Sigma}$ via a maximum likelihood procedure. When the maximisation is done in the class of finite-rank approximations $\hat{\bm{\Sigma}}=\transpose{\bm{W}}\bm{W}$, the problem is equivalent to finding the eigenvectors of the empirical covariance matrix of $\bm{\phi}_i$'s, also known as principal component analysis~\cite[PCA]{Roweis1999}. Sch\"{o}lkopf \emph{et al.}\~\cite{Scholkopf1998} presented nonlinear principal component analysis as a kernel eigenvalue problem and introduced kernel PCA (kPCA), which has the ability to handle very large, even infinite dimensional feature spaces.
%Instead of maximising the likelihood in eq.\ \eqref{eqn:genmodel_obs},

Now we will consider estimating parameters of this model in a Bayesian way. In Bayesian inference, one defines prior distributions over the parameters and then uses Bayes' rule to compute the posterior over them. Importantly, now we also require that the resulting Bayesian procedure is still amenable to the kernel trick. We therefore start by discussing a necessary condition for kernelisation which can then guide our choice of prior distributions.

The kernel trick requires that the algorithm be expressed solely in terms of scalar products $\langle\bm{\phi}_i,\bm{\phi}_j\rangle$. Scalar products are invariant under orthonormal transformations of the space, \ie if $\mathcal{A}$ is an orthonormal transformation then $\left\langle \bm{u},\bm{v} \right\rangle = \left\langle \mathcal{A}\bm{u},\mathcal{A}\bm{v} \right\rangle$ for all $\bm{u}$, and $\bm{v}$ in the space. Thus, if one wants to express an algorithm in terms of scalar products, it has to be -- at least -- invariant under orthonormal transformations, such as
rotations, reflections and permutations. It is well known that PCA has this property, but, for example, factor analysis (FA) does not~\cite{Roweis1999}, thus one cannot expect a kernel version of FA without any restrictions.

Another desired property of the method is analytical convenience and tractability, which can be ensured by using conjugate priors. The conjugate prior of the Gaussian likelihood is the \emph{Normal-inverse-Wishart}, which in our case has to be restricted to meet the orthonormal invariance condition:
\begin{equation} \label{eqn:genmodel_mean}
\begin{split}
 \bm{\Sigma};\:\sigma_0^{2}, \varconc &\sim \mathcal{W}^{-1}\left(\sigma^{2}_0\bm{I},\varconc\right)  \\ \bm{\mu}\vert\bm{\Sigma},\meanconc &\sim \mathcal{N}\left(\bm{0},\frac{1}{\meanconc}\bm{\Sigma}\right) ,
\end{split}
\end{equation}
where $\mathcal{W}^{-1}$ and $\mathcal{N}$ denote the inverse-Wishart and Gaussian distributions with the usual parametrisation. The general Normal-inverse-Wishart family had to be restricted in two ways: firstly, the mean of $\bm{\mu}$ was set to zero; secondly, the scale matrix of the inverse-Wishart was set to be spherical. These sensible restrictions ensure that the marginal distribution of $\bm{\phi}$ is centered at the origin and is spherically symmetric and therefore orthonormal invariance holds, which is required for kernelisation.

Having defined the hierarchical generative model in eqns.\ \eqref{eqn:genmodel_obs}--\eqref{eqn:genmodel_mean}, our task is to estimate the density of $\bm{\phi}$'s given the previous observations $\bm{\phi}_{1:N}$, which in a Bayesian framework is done by calculating the following posterior predictive distribution:
\begin{multline*}
 p(\bm{\phi}\vert\bm{\phi}_{1:N};\sigma_0^{2},\varconc,\meanconc) = \\
 \int p(\bm{\phi}\vert \bm{\mu},\bm{\Sigma})p(\bm{\mu},\bm{\Sigma}\vert\bm{\phi}_{1:N};\sigma_0^{2},\varconc,\meanconc) d\bm{\mu}d\bm{\Sigma}
\end{multline*}

By straightforward computation, it can be shown that the posterior predictive distribution is a $D$-dimensional \studentt distribution of the form (with the dependence on hyper-parameters made implicit):
\begin{align}
  &p(\bm{\phi}\vert\bm{\phi}_{1:N}) \propto \label{eqn:postpredictive} \\
  &\left( \gamma + \quadform{\tilde{\bm{\phi}}}{\left(\sigma_0^{2}\bm{I} + \tilde{\bm{\Phi}}\left(\bm{I} - \frac{\bm{1}\transpose{\bm{1}}}{N+\meanconc}\right)\transpose{\tilde{\bm{\Phi}}}\right)^{-1}}\right)^{-\frac{1 + N + \varconc}{2}} . \notag
\end{align}
where $\gamma = \frac{1+\meanconc+N}{\meanconc+N}$,  $\tilde{\bm{\phi}} = \bm{\phi} - \frac{N\bar{\bm{\phi}}}{N+\meanconc}$, $\tilde{\bm{\Phi}} = \left[\bm{\phi}_1 - \bar{\bm{\phi}},\ldots,\bm{\phi}_N - \bar{\bm{\phi}}\right]$, $\bar{\bm{\phi}} = \frac{1}{N}\sum_{n=1}^{N}\bm{\phi}_n$ is the empirical mean in feature space and $\bm{1}$ is a $N\times1$ vector of ones.

In order to obtain an expression which only contains scalar products, we invoke Woodbury's matrix inversion formula~\cite{Hager1989}:
\begin{align}
  &p(\bm{\phi}\vert\bm{\phi}_{1:N}) \propto \left( \gamma + \frac{\quadform{\tilde{\bm{\phi}}}{}}{\sigma_0^2} \:- \right. \label{eqn:postpredictive_Woodbury} \\
  &\left. \quadform{\tilde{\bm{\phi}}}{ \tilde{\bm{\Phi}} \left(\sigma_0^{4}\left(\bm{I} + \frac{\bm{1}\transpose{\bm{1}}}{\meanconc}\right) + \sigma_0^2\quadform{\tilde{\bm{\Phi}}}{}\right)^{-1}\transpose{\tilde{\bm{\Phi}}}} \right)^{-\frac{1 + N + \varconc}{2}}. \notag
\end{align}

\subsection{Kernel trick}

Until now, we assumed that $\bm{\phi}$ was an arbitrary point in $D$-dimensional space. In reality, however, we only want to assign probabilities~\eqref{eqn:postpredictive_Woodbury} to points on the so-called observation manifold, $\mathcal{O} = \left\{\varphi(\bm{x}) : \bm{x}\in\mathcal{X}\right\}$, \ie to points that can be realised by mapping an observation $\bm{x}\in\mathcal{X}$ to feature space $\bm{\phi} = \varphi(\bm{x})$. Restricted to $\mathcal{O}$, we can actually make use of the kernel trick, assuming as well that $\bm{\phi}_i=\varphi(\bm{x}_i)$ for the previous observations. Indeed, Eq.~\eqref{eqn:postpredictive_Woodbury} then only depends on $\bm{x}_{1:N}$ and $\bm{x}$ through $\quadform{\tilde{\bm{\phi}}}{}$, $\transpose{\tilde{\bm{\phi}}}\tilde{\bm{\Phi}}$ and $\quadform{\tilde{\bm{\Phi}}}{}$, which can all be expressed in terms of pairwise scalar products $\transpose{\bm{\phi}_n}\bm{\phi}_m = \langle\varphi(\bm{x}_n),\varphi(\bm{x}_m)\rangle = k(\bm{x}_n,\bm{x}_m)$ as follows:

\begin{align}
 \quadform{\tilde{\bm{\phi}}}{} &= k(\bm{x},\bm{x}) - 2\frac{\sum_i k(\bm{x},\bm{x}_i)}{N + \meanconc} + \frac{\sum_{i,j} k(\bm{x}_i,\bm{x}_j)}{\left(N + \meanconc\right)^{2}} \label{eqn:kernelexpressions} \\
 \left[\transpose{\tilde{\bm{\phi}}}\tilde{\bm{\Phi}}\right]_{n} &=
     \begin{aligned}[t]
     k(\bm{x},\bm{x}_n) &- \frac{\sum_i k(\bm{x},\bm{x}_i)}{N} \\
     &+ \frac{\sum_{i,j} k(\bm{x}_i,\bm{x}_j) - N\sum_i k(\bm{x}_n,\bm{x}_i)}{N\left(N+\meanconc\right)} \\
     \end{aligned} \notag \\
 \left[\quadform{\tilde{\bm{\Phi}}}{}\right]_{n,m} &=
    \begin{aligned}[t]
    k(\bm{x}_n,\bm{x}_m) &- \frac{\sum_i k(\bm{x_n},\bm{x}_i) + \sum_i k(\bm{x_m},\bm{x}_i)}{N} \\
    &+ \frac{\sum_{i,j} k(\bm{x}_i,\bm{x}_j)}{N^2}
    \end{aligned} \notag
\end{align}

As said previously, we are only interested in points in the (curved) observation manifold. The restriction of the predictive distribution~\eqref{eqn:postpredictive_Woodbury} to the observation manifold induces the same density function $q(\bm{\varphi}(\bm{x}) \vert \bm{x}_{1:N}) = p(\bm{\varphi}(\bm{x}) \vert\bm{\phi}_{1:N})$ (which is now unnormalised), but with respect to a $d$-dimensional Lebesgue base measure in the geodesic coordinate system of the manifold. Finally, we map the density $q(\bm{\varphi}(\bm{x}) \vert \bm{x}_{1:N})$ back to the original input space by implicitly inverting the mapping $\varphi$, yielding the (unnormalised) predictive density $q_k(\bm{x} \vert \bm{x}_{1:N})$ defined on the input space. By doing so, we need to include a multiplicative Jacobian correction term which relates volume elements in the tangent space of the manifold to volume elements in the input space $\mathcal{X}$:
 %weights the change of variable from the geodesic coordinate system to the $\mathcal{X}$ space:
\begin{multline} \label{corrected_density}
q_k(\bm{x} \vert \bm{x}_{1:N}) = \\
q(\bm{\varphi}(\bm{x})\vert\bm{x}_{1:N}) \cdot \det\left( \left\langle\frac{\partial\varphi(\bm{x})}{\partial x^i} ,  \frac{\partial\varphi(\bm{x})}{\partial x^j}\right\rangle \right)^{\frac{1}{2}}_{i,j=1,\ldots, d} .
\end{multline}
This formula can be be derived from the standard change-of-variable formula for densities, as we show in the Appendix \ref{sec:change_of_variable}.

\begin{figure*}
    \begin{center}
        \begin{tikzpicture}
            \node[anchor = south west, inner sep = 0in,] at (10pt,10pt) {\includegraphics[trim = 1.3in 2.4in 0.93in 2in, clip, width=6.45in,keepaspectratio]{figures/illustration_2D}};
            \def\offsetB{1.70in}
            \def\offsetC{3.40in}
            \def\offsetD{5.11in}

            \node at (0.85in,1.5in) {\textbf{A}};

            \node at (10pt,5pt) {-1};
            \node at (1.02in,0in) {$x$};
            \node at (1.43in,5pt) {1};

            \node at (5pt,15pt) {0};
            \node at (0in,0.82in) [rotate = 90]{$p(x)$};
            \node at (5pt,1.47in) {3};

            \node at (\offsetB+0.85in,1.5in) {\textbf{B}};

            \node at (\offsetB+10pt,5pt) {-1};
            \node at (\offsetB+0.82in,0in) {$\phi^1$};
            \node at (\offsetB+1.43in,5pt) {1};

            \node at (\offsetB+5pt,15pt) {0};
            \node at (\offsetB+0in,0.82in) [rotate = 90]{$\phi^2$};
            \node at (\offsetB+5pt,1.47in) {1};

            \node at (\offsetC+0.85in,1.5in) {\textbf{C}};

            \node at (\offsetC+10pt,5pt) {-1};
            \node at (\offsetC+0.82in,0in) {$x$};
            \node at (\offsetC+1.47in,5pt) {1};

            \node at (\offsetC+5pt,15pt) {0};
            \node at (\offsetC+5pt,1.47in) {3};

            \node at (\offsetD+0.85in,1.5in) {\textbf{D}};

            \node at (\offsetD+10pt,5pt) {-1};
            \node at (\offsetD+0.82in,0in) {$x$};
            \node at (\offsetD+1.43in,5pt) {1};

            \node at (\offsetD+5pt,15pt) {0};
            \node at (\offsetD+0in,0.82in) [rotate = 90]{$p(x)$};
            \node at (\offsetD+5pt,1.47in) {3};
        \end{tikzpicture}
    \end{center}

    \caption{
        \label{fig:illustration_2D}
        Illustration of kernel \studentt density estimation on a toy problem.
        \textbf{A,\ }Fourteen data points $x_{1:14}$ (\emph{crosses}) drawn from from a mixture distribution (\emph{solid curve}).
        \textbf{B,\ }Embedded observations $\bm{\phi}_{1:14}$(\emph{crosses}) in the two dimensional feature space and contours of the predictive density $q(\bm{\phi}_{15}\vert x_{1:14})$. The observation manifold is a parabola (\emph{solid curve}), and we are interested in the magnitude of the predictive distribution to this manifold (\emph{colouring}).
        \textbf{C,\ }The restricted posterior predictive distribution pulled back to the real line (\emph{solid curve}) gets multiplied by the Jacobian term $\sqrt{1 + 4x^2}$ (\emph{dotted curve}) to give
        \textbf{D,\ }the predictive distribution $q_{k}(x_{15}\vert x_{1:14})$ (\emph{solid curve}) in observation space.
         All distributions are scaled so that they normalise to one.}
\end{figure*}

So what exactly did we gain by applying the kernel trick? Despite the fact that the simple density model in the whole feature space is unimodal, in the local coordinate system of the non-linear observation manifold, the predictive density may appear multimodal and hence possess interesting nonlinear features. This is illustrated in Fig.~\ref{fig:illustration_2D}. Assume that we observe draws from a complicated distribution, which cannot be conveniently modelled with a linear Gaussian model (Fig.~\ref{fig:illustration_2D}.A). We map observations to a two dimensional feature space using the mapping $\varphi^1(x)=x,\varphi^2(x)=(x)^2$ hoping that the embedded observations are better fitted by a Gaussian, and carry out inference there (Fig.~\ref{fig:illustration_2D}.B). Note how all possible observations in the original space get mapped to the observation manifold~$\mathcal{O}$, which is now the parabola $\phi^2=(\phi^1)^2$. The outcome of inference is a posterior predictive distribution in feature space, which takes a \studentt form. To obtain the predictive distribution in the observation space we have to look at the magnitude of the predictive distribution along the observation manifold. This is illustrated by the contour lines in panel B. The restricted predictive distribution is then ``pulled back'' to observation space and has to be multiplied by the Jacobian term (Fig.~\ref{fig:illustration_2D}.C) to yield the final estimation of the density of the observed samples (Fig.~\ref{fig:illustration_2D}.D). Remarkably, our method computes the unnormalised density estimate (before the Jacobian term is added, Fig.~\ref{fig:illustration_2D}.C) directly from the data (Fig.~\ref{fig:illustration_2D}.A). All intermediate steps, including the inversion of the mapping, are \emph{implicit} in the problem formulation, and we never have to work with the feature space directly. As the method essentially estimates the density by a \studentt in feature space, we shall call it \emph{kernel \studentt density estimation}.

\subsection{Infinite dimensional feature spaces \label{sec:infinite_dimensional}}

Of course, we constructed the previous toy example so that the simple normal model in this second order polynomial feature space is able to model the bimodal data distribution. Should the data distribution be more complicated, \eg would have three or more modes, the second order features would hardly be enough to pick up all relevant statistical properties. Intuitively, adding more features results in higher potential expressive power, while the Bayesian integration safeguards us from overfitting. In practice, therefore, we will use the method in conjunction with rich, perhaps infinite dimensional feature spaces, which will allow us to relax these parametric restrictions and to apply our method to model a wide class of probability distributions.

However, moving to infinite dimensional feature spaces introduces additional technical difficulties. As for one, Lebesgue measures do not exist in these spaces, therefore our derivations based on densities with respect to Euclidean base measure should be reconsidered. For a fully rigorous description of the method in infinite spaces, one may consider using Gaussian base measures instead. In this paper, we address two specific issues that arise when the feature space is large or infinite dimensional.

Firstly, because of the Jacobian correction term, the predictive density $q_k$ in input space is still not fully kernelised in~\eqref{corrected_density}. We note though that this term is the determinant of a small $d\times d$ matrix of inner products, which can actually be computed efficiently for some kernels. For example, the all-subsets kernel~\cite{Cristianini2004} has a feature space with exponential dimension $D=2^d$, but each inner product of derivatives can be computed in $\mathcal{O}(d)$. But what about nonparametric kernels with infinite $D$? Interestingly, we found that shift invariant kernels -- for which $k(\bm{x},\bm{y}) = k(\bm{x}+\bm{z},\bm{y}+\bm{z})$ for all $\bm{z}$ -- actually yield a \emph{constant Jacobian term} (see proof in the Appendix \ref{sec:Jacobian_shift}) so it can be safely ignored to obtain a fully kernelised -- although unnormalised -- predictive density $q_k(\bm{x} \vert \bm{x}_{1:N}) = q(\bm{\varphi}(\bm{x})\vert\bm{x}_{1:N})$. %, where the RHS can be computed using only kernel evaluations from equations~\eqref{eqn:postpredictive_Woodbury}--\eqref{eqn:kernelexpressions}. 
This result is important, as the most commonly used nonparametric kernels, such as the squared exponential, or the Laplacian fall into this category.

The second issue is normalisation: Eq.~\eqref{corrected_density} only expresses the predictive distribution up to a multiplicative constant. Furthermore, the derivations implicitly assumed that $\alpha>D-1$, where $D$ is the dimensionality of the feature space, otherwise the probability densities involved become improper. This assumption clearly cannot be satisfied when $D$ is infinite.
However, one can argue that even if the densities in the feature space are improper, the predictive distributions that we use may still be normalisable when restricted to the observation manifold for $\alpha>d-1$. Indeed, the two-dimensional \studentt is normalisable when restricted to the parabola as in Fig.~\ref{fig:illustration_2D} for all $\alpha>0$ rather than $\alpha>1$ (to show this, consider $N=0$ so that~\eqref{eqn:postpredictive} takes the simple form $q(\bm{\phi}) = (\textrm{cnst.}+ \transpose{\bm{\phi}}\bm{\phi})^{-\frac{1+\alpha}{2}}$). So although distributions in feature space may be improper, the predictive densities may remain well-defined even when nonparametric kernels are used.

Given these observations, we thus decide to \emph{formally} apply equation~\eqref{corrected_density} for our predictive density estimation algorithm, ignoring the normalisation constant and also the Jacobian term when shift-invariant nonparametric kernels are applied.

%Moving to infinite dimensional feature spaces introduces additional technical difficulties, as for example there is no analog to the Lebesgue measure in infinite dimensions, and so one often has to resort to Gaussian process measures~\cite{Bogachev1991} instead. We conjecture that the change of variable expression~\eqref{corrected_density} still holds, but it is unclear whether the full Bayesian procedure outlined from~\eqref{eqn:genmodel_obs}--\eqref{eqn:postpredictive_Woodbury} can be made rigorous. Orbanz~\cite{Orbanz2010} recently brought the difficulties of Bayesian conditioning in infinite dimensions to light and made the construction of several nonparametric conjugate priors rigorous. We leave it for future work to develop a similar construction for our generative model. In this paper, our algorithm applies equation~\eqref{corrected_density} formally and uses the Bayesian model to help interpretation.  In particular, consider the case of the squared exponential (SE) kernel with length scale $l$, $k_{SE}(\bm{x},\bm{x}')=\exp(-\frac{1}{2l}\|\bm{x} - \bm{x}'\|^2)$. Embeddings $\bm{\phi}_i$ will now be functions, and not finite dimensional vectors. In the SE case, each observation $\bm{x}_i$ gets mapped to a bell-shaped `bump' $\bm{\phi}_i(\cdot) = \varphi_{SE}(\bm{x}_i) = \exp(-\frac{1}{2l}\|\cdot-\bm{x}_i\|^2)$. The observation manifold is the set of SE bumps positioned all over the observation space. The infinite dimensional analogue to our generative model~ \eqref{eqn:genmodel_mean}--\eqref{eqn:genmodel_obs} is that these bumps were sampled from a Gaussian process of unknown mean function $m(\cdot)$ and covariance function $c(\cdot,\cdot)$. The method implicitly estimates $m$ and $c$ from the observations in a Bayesian way, using a nonparametric conjugate prior analogous to the Normal-inverse-Wishart one in~\eqref{eqn:genmodel_mean}\footnote{The ``inverse-Wishart process'' appears in \eg~\cite{Li2009} as priors over covariance or kernel functions. However it is yet to be shown that they indeed constitute valid nonparametric priors~\cite{Orbanz2010}.}. The posterior predictive distribution will be a \studentt process, which is restricted to the observation manifold.

\begin{figure*}[t]
 \begin{center}
  \begin{tikzpicture}
    \node[anchor = south west, inner sep = 0in] at (0in,4pt) {\includegraphics[width=2.1in,keepaspectratio]{figures/fantasy_lowalpha}};
    \draw[ultra thick,draw = red] (0in,0pt) -- (0.15in,0pt);
    \node[inner sep = 0in] at (1.05in,2.25in) {\textbf{A}: $\alpha = 0.01,\beta = 0.01$};
    \node[anchor = south west, inner sep = 0in] at (2.15in,4pt) {\includegraphics[width=2.1in,keepaspectratio]{figures/fantasy_medalpha}};
    \draw[ultra thick,draw = red] (2.15in,0pt) -- (2.85in,0pt);
    \node[inner sep = 0in] at (3.2in,2.25in) {\textbf{B}: $\alpha = 3,\beta = 0.01$};
    \node[anchor = south west, inner sep = 0in] at (4.3in,4pt) {\includegraphics[width=2.1in,keepaspectratio]{figures/fantasy_largealpha}};
    \draw[ultra thick,draw = red] (4.3in,0pt) -- (6in,0pt);
    \node[inner sep = 0in] at (5.35in,2.25in) {\textbf{C}: $\alpha = 10, \beta = 0.01$};
  \end{tikzpicture}
 \end{center}
 \caption{\label{fig:fantasy}Fantasy data drawn from a two dimensional kernel \studentt model with SE kernel. The  length-scale $\ell=1$ (\emph{bars below panels}) and $\meanconc=0.01$ are fixed, $\varconc$ varies between $0.01$ and $10$ (\textbf{A} to \textbf{C}). Panels show 50 points (\emph{blue dots}) drawn from the generative model and the predictive density (\emph{gray level}) of the $51^{\mbox{st}}$ draw.}
\end{figure*}

Finally, we want to emphasize that the method does not imply nor does it require that arbitrary distributions, when embedded in rich Hilbert spaces, will look exactly like Gaussian or \studentt measures. Instead, the method uses the potentially infinitely flexible family of Gaussian measures to approximately model distributions in the feature space, and uses Bayesian integration to address model-misspecification via explicit representation of uncertainty. As demonstrated in Fig.~\ref{fig:illustration_2D}, the richness and flexibility of the density model stems from projecting the distribution onto a nonlinear observation manifold, allowing us to overcome limitations of Gaussian measures, such as unimodality.

\section{Related work}

Our method is closely related to kernel principal component analysis~\cite[kPCA]{Scholkopf1998} as both of them essentially use the same underlying generative model (Eq.~\ref{eqn:genmodel_obs}). While our method is based on Bayesian inference, kPCA performs constrained maximum likelihood estimation~\cite{Rosipal2001} of parameters $\bm{\mu}$ and $\bm{\Sigma}$. kPCA is a very effective and popular method for nonlinear feature extraction. Its pitfall from a density estimation perspective is that it does not generally induce a sensible probability distribution in observation space. To understand this, consider performing kPCA of order one (\ie recover just a single nonlinear component) on the toy data in~\subfigref{fig:illustration_2D}{A}. The solution defines a degenerate Gaussian distribution in feature space which is concentrated~ along a line, roughly the long axes of the equiprobability ellipses in \subfigref{fig:illustration_2D}{B}. As such a distribution can intersect the observation manifold at most twice, the predictive distribution in observation space will be a mixture of two delta distributions. This degeneracy can be ruled out by recovering at least as many nonlinear components as the dimensionality of feature space, but this is clearly not a viable option in infinite or large dimensional feature spaces. The Bayesian approach sidesteps the degeneracy problem by integrating out the mean and covariance, thereby averaging many, possibly degenerate distributions to obtain a non-degenerate, smooth \studentt distribution in feature space, which results in a smooth density estimate in observation space.

Another particularly interesting related topic is that of \emph{characteristic kernels}~\cite{Sriperumbudur2008}. A positive definite kernel $k(x,x') = \langle\varphi(x),\varphi(x')\rangle$ is called characteristic if the mean element $\mu_\pi = \mathbb{E}_{\mathbb{X} \sim \pi}[\varphi(\mathbb{X})]$ uniquely characterises any probability measure $\pi$. From a density estimation perspective, this suggests that estimating a distribution in observation space boils down to estimating the mean in a characteristic kernel space. The kernel moment matching framework~\cite{Song2008} tries to exploit characteristic kernels in a very direct way: parameters of an approximating distribution are chosen by minimising maximum mean discrepancy in feature space. Our method can be interpreted as performing Bayesian estimation of first and second moments in feature space, therefore one may hope that work on characteristic kernels will help us further study and understand properties of the algorithm.

The Gaussian process has been used as a nonparametric prior over functions for unsupervised learning tasks in several studies, such as in Gaussian process latent variable models~\cite[GPLVMs]{Lawrence2004} or the Gaussian process density sampler~\cite[GPDS]{Adams2009}. What sets the present approach apart from these is that while they incorporated Gaussian processes as a building block in an unsupervised model, we re-used the construction of GP regression and explicitly applied the kernel trick in an parametric unsupervised method. The GPDS method relies on Markov chain Monte Carlo (MCMC) methods for so called doubly-intractable distributions~\cite{Murray2006}. These methods can be used to sample from the posterior over hyper-parameters of probabilistic methods where normalisation is intractable. In particular, they could be used to integrate out the hyper-parameters $\varconc,\meanconc$ and $\sigma_0$, of our method. We emphasise that while MCMC is a crucial component of the GPDS, inference in our model is essentially closed-form.

\section{Experiments\label{sec:experiments}}

A common way to examine properties of unsupervised learning models is to draw \emph{fantasy datasets} from the generative model. The kernel \studentt model can be simulated as follows: first, we fix $\bm{x}_1=\bm{0}$ for convenience. Our generative model with shift-invariant kernels define an improper prior over the whole space when there is no observation (as it should since only difference between points is meaningful for shift invariant kernels). Then we draw each subsequent $\bm{x}_n$ from the kernel \studentt predictive distribution $q_{k}(\bm{x}_n\vert\bm{x}_{1:n-1};\sigma_o,\varconc,\meanconc)$ conditioned on previous draws. Sampling from the predictive distribution is possible via hybrid Monte Carlo~\cite{Neal2010}.

Fig.~\ref{fig:fantasy} shows example fantasy datasets for different settings of parameters. We observe that the generative process has an approximate clustering property, in which it behaves similarly to the Chinese restaurant process. We also note that the ``checkerboard - like'' eigenfunctions of the squared exponential kernel induce a tree like refinement of the space, in which the method is similar to Dirichlet diffusion trees.

\begin{figure}[t]
	\begin{center}
	\resizebox{\columnwidth}{!}{
	\begin{tikzpicture}
		\node[anchor = south west, inner sep = 0in] at (18pt,16pt) {\includegraphics[width=0.88\columnwidth,trim = 1.2in 0.8in 0.9in 0.6in, clip]{figures/AUC_and_Confusion}};
		\node[rotate=90] at (-3pt,0.76in) {confusion$[\%]$};
		\node[rotate=90] at (-3pt,2.15in) {AuC$[\%]$};
		\node at (1.63in,0pt) {number of training data};
		\node[anchor = east] at (20pt,0.28in) {0};
		\node[anchor = east] at (20pt,0.60in) {25};
		\node[anchor = east] at (20pt,0.92in) {50};
		\node[anchor = east] at (20pt,1.24in) {75};
		\node[anchor = west] at (2.5in,1.18in) {\emph{\ourmethod}};
		\node[anchor = west] at (2.5in,1.01in) {\emph{kde}};
		\node at (1.63in,1.4in) {label reconstruction};

		\node[anchor = east] at (20pt,1.65in) {50};
		\node[anchor = east] at (20pt,2.15in) {75};
		\node[anchor = east] at (20pt,2.65in) {100};
		\node[anchor = west] at (2.5in,1.94in) {\emph{\ourmethod}};
		\node[anchor = west] at (2.5in,1.77in) {\emph{kde}};
		\node at (1.63in,2.8in) {novelty detection};
	
		\node at (20pt,10pt) {0};
		\node at (0.94in,10pt) {100};
		\node at (1.63in,10pt) {200};
		\node at (2.32in,10pt) {300};
		\node at (3.01in,10pt) {400};
	\end{tikzpicture}
	}
	\end{center}
	\caption{\label{fig:AUC_and_Confusion} Performance of \emph{\ourmethod} and \emph{kde} as a function of the number of training examples  on USPS data. Errorbars show $\pm$ one standard deviation.}
\end{figure}

\begin{table}[t]
 \begin{center}
 \resizebox{\columnwidth}{!}{
  \begin{tabular}{|l||c|c|}
	\hline \textsc{method}& \textsc{novelty}(AuC[\%]) & \textsc{recons.}(conf[\%])\\
	\hline\hline \emph{\ourmethod}	& $\bm{96.82}\pm0.8$ & $\bm{2.15}\pm{1.0}$\\
	\hline	\emph{kde}	& $94.56\pm0.6$ & $9.24\pm1.9$\\
	\hline	\emph{dpm}	& $73.56\pm4.3$ & $23.85\pm8.2$\\
	\hline
  \end{tabular}
 }
 \end{center}
 \caption{\label{tab:digits_results}Performance of \emph{\ourmethod}, \emph{kde} and \emph{dpm} in novelty detection (\emph{novelty, left}) and label reconstruction (\emph{recons., right}) on the USPS data. The table shows average AuC and confusion values $\pm$ one standard deviation measured over 10 randomised experiments.}
\end{table}

The performance of density models where the normalisation constants are not available -- similar examples in the past included Markov random fields~\cite{Haluk1987} and deep belief nets -- is hard to evaluate directly. These density models are typically used in unsupervised tasks, such as reconstruction of missing data, novelty detection and image denoising, where the important quantity is the shape of the distribution, rather than the absolute density values.  In the following, we consider three such unsupervised tasks that are indicative of the model's performance: novelty detection, assessment of reconstruction performance and the recently introduced task of relative novelty detection~\cite{Smola2009}. The purpose of these experiments is to demonstrate, as proof of concept, that the method is able to model relevant statistical properties in high dimensional data.

\subsection{Novelty detection}

Novelty detection, recognition of patterns in test data that are unlikely under the distribution of the training data, is a common unsupervised task for which density estimation is used. Here, we considered the problem of detecting mislabelled images of handwritten digits given a training set of correctly labelled images from the USPS dataset. The dataset consists of 7291 training and 2007 test images, each of which is a labelled, $16\times 16$ gray-scale image of a handwritten digit (0-9).

We modelled the joint distribution of the image (a 256 dimensional vector) and the label (a ten dimensional sparse vector, where the $l^{\mbox{th}}$ element is $1$ and the rest are $0$'s if the label is $l$) by a kernel \studentt density (\emph{\ourmethod}). We used a squared exponential kernel with length-scale chosen to be the median distance between data-points along digit dimensions and length-scale $1$ along the label dimensions. We trained the algorithm on a randomly selected subset of available training data and calculated predictive probabilities on a test dataset composed of 100 correctly labelled and 100 mislabelled test points. Digit-label pairs with predictive probability under a threshold were considered \emph{mislabelled}. We compared the performance of our method to two baseline methods, kernel density estimation (\emph{kde}, also called Parzen window estimate) and Dirichlet process mixtures of Gaussians (\emph{dpm}), based on the area under the receiver operating characteristic curve (AuC) metric. Hyper-parameters of all three methods were chosen by grid search so that the average AuC was maximised on a separate validation set. Table \ref{tab:digits_results} summarises the results of this comparison based on ten randomised iteration of the experiment with 2000 training and 200 test samples. We found that \emph{\ourmethod} outperformed both competing methods significantly ($p<0.01$, two sample T-test). The performance of \emph{dpm} was substantially worse than that of the other two, which demonstrates that general mixture models are very inefficient in modelling high dimensional complex densities. To investigate the difference between the performance of \emph{kde} and \emph{\ourmethod}, we carried out a second sequence of experiments where the size of the training set ranged from 10 to 400. Fig.~\ref{fig:AUC_and_Confusion} shows average AuC values as a function of training set size. We observed that the two algorithms perform similarly for small amounts of training data, the difference in performance becomes significant ($p<0.05$) for 60 training points or more.

\subsection{Label reconstruction\label{sec:classification}}

In these experiments, we considered learning the classification of handwritten digits. In order to assess the density modelling capabilities of our method, we posed this problem as an image reconstruction problem \footnote{We could have considered more general image reconstruction tasks, but reconstructing labels equips us with intuitive measures of loss, such as confusion and AuC.}. Again, we augmented the gray-scale image with 10 additional `pixels' representing a sparse coding of the labels 0-9. We first trained our density model on the augmented images on randomly selected training subsets of various sizes. Then, on a separate subset of test images we computed the probability of each label 0-9 conditioned on the image by computing the joint probability of the image with that label and renormalising. For each image we assigned the label for which the conditional predictive probability was the highest (maximum \emph{a posteriori} reconstruction). Results of comparison to \emph{kde} and \emph{DPM} with 2000 training and 400 test points are shown in \figref{fig:AUC_and_Confusion}. As measure of performance, we used average confusion, \ie the fraction of misclassified test images. We again found that \emph{kst} outperformed both other methods, and now the advantage of our method compared to \emph{kde} is more substantial. Fig.~\ref{fig:AUC_and_Confusion} shows confusion of \emph{\ourmethod} and \emph{kde} as a function of the number of training examples. The difference between the two methods becomes significant ($p<0.01$) for 60 training examples or more.

% We note that we did not test our method against dedicated methods for classification such as SVMs or GPs, and we expect that such methods would substantially outperform all density estimation methods considered here.

\subsection{Relative novelty detection}

\begin{figure*}
 \begin{center}
  \resizebox{\textwidth}{!}{
  \begin{tikzpicture}
    \node[anchor = south west, inner sep = 0in] at (0in,8pt) {\includegraphics[width=2.1in,keepaspectratio]{figures/satellite_background}};
    \node[inner sep = 0in] at (1.05in,2.3in) {\textbf{A}: Background};
    \node[anchor = south west, inner sep = 0in] at (2.15in,8pt) {\includegraphics[width=2.1in,keepaspectratio]{figures/satellite_target}};
    \node[inner sep = 0in] at (3.2in,2.3in) {\textbf{B}: Target};
    \node[anchor = south west, inner sep = 0in] at (4.3in,8pt) {\includegraphics[width=2.1in,keepaspectratio]{figures/satellite_novelty}};
    \node[inner sep = 0in] at (5.35in,2.3in) {\textbf{C}: Relative novelty};
  \end{tikzpicture}
    }
 \end{center}

 \caption{\textbf{A,\ }Background and \textbf{B,\ }target images for relative novelty detection. \textbf{C,\ }Top 5\% novel patches in the target image are highlighted in \emph{red}. The results are qualitatively similar to those presented in~\cite[Fig. 2]{Smola2009}\label{fig:satellite}.}
\end{figure*}

Finally, we considered the problem of relative novelty detection~\cite{Smola2009}, for which we re-used the experimental setup in~\cite{Smola2009}. The data consists of two $200\times 200$ color satellite images, the \emph{background} and the \emph{target} (Fig.~\ref{fig:satellite}.A--B). The task is to detect novel objects on the target image relative to the background image. Treating RGB values in each non-overlapping $2\times 2$ patches as data points, we estimated the density over such patches in both the background ($\hat{q}(\bm{x})$) and in the target ($\hat{p}(\bm{x})$) images. A patch $\bm{x}$ was identified as relatively novel if the ratio $\hat{p}(\bm{x})/\hat{q}(\bm{x})$ was above a certain threshold. The results produced by our algorithm are similar to that obtained by using state of the art novelty detection algorithms (c.\,f.\ Fig.~2 in~\cite{Smola2009}), even though here we only used a 3\% random subsample of available image patches to estimate target and background densities.

\section{Discussion}

\paragraph{Summary} In this paper, we considered applying the kernel trick as a tool for constructing nonparametric Bayesian methods, and suggested a general recipe for obtaining novel analytically tractable \emph{Bayesian kernel machines}. The starting point is a suitably simple parametric Bayesian model in which inference is tractable. We have demonstrated how arguments based on orthonormal invariance can be used to guide our choice of models that are amenable to kernel trick. Having defined the basic model, one needs to express posterior predictive distributions in terms of kernel evaluations, then apply the kernel trick in those equations. By using kernels with infinite dimensional feature spaces, novel tractable nonparametric Bayesian methods may be obtained in this way.

To illustrate our general approach, we studied the problem of density estimation and presented kernel \studentt density estimation, a novel Bayesian kernel machine for density modelling. We followed our general methodology to arrive at a closed form expression that is exact up to a multiplicative constant. We also carried out numerical experiments to investigate the performance of the method. The results demonstrate that the method is capable of estimating the distribution of high dimensional real-world data, and can be used in various applications such as novelty detection and reconstruction. The most apparent limitation of the kernel \studentt density model is the intractability of its normalisation constant. Nevertheless, the unnormalised density can still be used in a variety of unsupervised learning tasks, such as image reconstruction or novelty detection. As other kernel methods, our algorithm has cubic complexity in the number of training points, though sparse approximations~\cite{Fine2001} can be used to construct faster algorithms for large-scale applications. Another advantage of the kernel approach is that the expressions can be formally extended to define non-trivial discrete distributions over \eg graphs, permutations or text.

\paragraph{Conclusions and future work} We believe that the present paper opens the door for developing a novel class of Bayesian kernel machines. The methodology presented in this paper can be applied in further problems to obtain nonparametric methods based on parametric Bayesian models. Investigating merits and limitations of this general approach in various statistical estimation and decision problems is certainly an interesting theme for future research. One particularly exciting direction that we plan to pursue is a Bayesian two-sample hypothesis test analogous to the frequentist test based on characteristic kernels~\cite{Sriperumbudur2008}. For hypothesis testing, one needs to apply the kernel trick to a \emph{ratio} of marginal likelihoods, rather than to a predictive density, which enables cancellations which simplify the issues with normalisation or Jacobians. Other potentially fruitful directions include semi-supervised regression and Bayesian numerical analysis~\cite{OHagan1992}.

\paragraph{Acknowledgements} We would like to thank Zoubin Ghahramani, Carl Rasmussen, Arthur Gretton, Peter Orbanz
and Le Song for helpful discussions. In particular, the proof in
Appendix \ref{sec:Jacobian_shift} is mainly due to Le Song. Part of this work was supported
under the EPSRC grant EP/F026641/1. Ferenc Husz\'{a}r is supported by Trinity College Cambridge.

\bibliographystyle{nature3}
\bibliography{bibliography/Bayesian_kernel}

\appendix

\twocolumn[
\section{Change-of-variable formula\label{sec:change_of_variable}}

\paragraph{Theorem} Let $\mathcal{X}$ and $\mathcal{F}$ be $d$ and $D$ dimensional Euclidean spaces respectively ($D>d$), ${\varphi:\mathcal{X} \mapsto \mathcal{F}}$ be a smooth injective mapping and let $q(\bm{\phi})$ define a density function with respect to the Lebesgue measure on $\mathcal{F}$. Then the restriction of distribution $q$ to the observation manifold $\varphi(\mathcal{X})$ (\ie conditioning on the event of being in the observation manifold) induces the following density in the input space $\mathcal{X}$:
\begin{equation} \label{eq:densityChange}
p(\bm{x}) \propto q(\bm{\varphi}(\bm{x})) \cdot \det\left( \left\langle\frac{\partial\varphi(\bm{x})}{\partial x^i} ,  \frac{\partial\varphi(\bm{x})}{\partial x^j}\right\rangle \right)^{\frac{1}{2}}_{i,j=1,\ldots, d}\mbox{.}
\end{equation}

We suspect this result may have appeared in several forms in the literature, but we didn't succeed to find a citation, and so we provide a proof here.

\paragraph{Proof} Note that we cannot apply the standard change-of-variable formula directly as the two spaces have different dimensions. So we augment the space $\mathcal{X}$ with an Euclidean space $\Xi$ of dimension $D-d$. We then define the following mapping $F:\mathcal{X}\times\Xi \mapsto \mathcal{F}$:
$$
F(\bm{x},\bm{\xi}) = \varphi(\bm{x}) + V_{\bm{x}} \bm{\xi},
$$
where $V_{\bm{x}}$ is a $D\times(D-d)$ matrix of mutually orthonormal vectors to the tangent space of the manifold at $\bm{x}$ (\ie a basis for the orthogonal space to the tangent space at $\bm{x}$). For small enough $\bm{\xi}$'s (which could depend on $\bm{x}$), $F$ will map small neighborhoods of $(\bm{x},\bm{0})$ to neighborhoods of $\varphi(\bm{x})$ in an injective manner. We thus apply the standard change of variable formula to map the density $q(\bm{\phi})$ to a density on an open set around $\mathcal{X}\times\{\bm{0}\}$:
$$
    p(\bm{x},\bm{\xi}) = q(F(\bm{x},\bm{\xi})) \left\vert\det J_F(\bm{x},\bm{\xi}) \right\vert ,
$$
where $J_F(\bm{x},\bm{\xi})$ is the Jacobian matrix of the $F$ mapping. To condition on being in the observation manifold, we simply need to condition on $\bm{\xi}=0$, and so the density we want to compute is:
$$
    p(\bm{x}) \propto p(\bm{x},\bm{0}) = q(F(\bm{x},\bm{0})) \left\vert\det J_F(\bm{x},\bm{0}) \right\vert .
$$
Now, $J_F(\bm{x},\bm{0})=(J_\varphi(\bm{x}), V_{\bm{x}})$ where $J_\varphi(\bm{x})=(\frac{\partial\varphi(\bm{x})}{\partial x^1}, \ldots, \frac{\partial\varphi(\bm{x})}{\partial x^d})$ is the Jacobian matrix of $\varphi$. Finally, we use the fact that $\vert\det(J_F)\vert$ = $(\det(\transpose{J_F} J_F))^{1/2}$ for any squared matrix $J_F$ to get that:
$$
\left\vert\det\left(J_F (\bm{x},\bm{0})\right)\right\vert = \det\left(\begin{array}{cc}
                        \transpose{J_\varphi(\bm{x})}J_\varphi(\bm{x}) & \bm{0} \\
                        \bm{0} & \bm{I}
                      \end{array} \right)^{\frac{1}{2}} = \det\left( \transpose{J_\varphi(\bm{x})}J_\varphi(\bm{x}) \right)^{\frac{1}{2}},
$$
where we used that $\transpose{J_\varphi(\bm{x})} V_{\bm{x}}=0$ and $\transpose{V_{\bm{x}}} V_{\bm{x}}=\bm{I}$ by orthonormality of the $V_{\bm{x}}$ with the tangent space spanned by the columns of $J_\varphi(\bm{x})$. Finally, we use the fact that:
$$
 \transpose{J_\varphi(\bm{x})}J_\varphi(\bm{x}) = \left( \left\langle\frac{\partial\varphi(\bm{x})}{\partial x^i} ,  \frac{\partial\varphi(\bm{x})}{\partial x^j}\right\rangle \right)_{i,j=1,\ldots, d}
$$
to obtain~\eqref{eq:densityChange}. $\square$
]

\twocolumn[
\section{Jacobian term for translation invariant kernel\label{sec:Jacobian_shift}}

We provide the sketch of a proof that $\left\langle\frac{\partial\varphi(\bm{x})}{\partial x^i} ,  \frac{\partial\varphi(\bm{x})}{\partial x^j}\right\rangle$ doesn't depend on $\bm{x}$ for a translation invariant kernel under suitable regularity conditions.

\paragraph{}By Bochner's theorem (\eg theorem 4.1 in~\cite{Rasmussen2006}), a translation invariant kernel can be expressed as the inverse Fourier transform of a positive finite measure. If moreover we assume that the Fourier transform of the kernel exists, this measure will have a density $p(\bm{s})$ with respect to the Lebesgue measure, and so we can write:
\begin{align*}
 k(\bm{x}-\bm{y}) & =  \int_{\mathbb{R}^d} p(\bm{s}) e^{2\pi i \transpose{\bm{s}} (\bm{x}-\bm{y})} d\bm{s} \\
 &= \int p(\bm{s}) \left( \cos(2\pi \transpose{\bm{s}} (\bm{x}-\bm{y}))+i
 \sin(2\pi \transpose{\bm{s}} (\bm{x}-\bm{y})) \right) d\bm{s} \\
 &= \int p(\bm{s}) \left( \cos(2\pi \transpose{\bm{s}} (\bm{x}-\bm{y})) \right) d\bm{s}
\end{align*}
as $\sin(\transpose{\bm{s}}\bm{\delta})$ is an odd function of $\bm{s}$ for any $\bm{\delta}$ and $p(\bm{s})$ is a symmetric function given that it is the inverse Fourier transform of the symmetric function $k$, so the Cauchy principal value of the second term vanishes. Now using a trigonometric identity, we get:
\begin{align*}
 k(\bm{x}-\bm{y}) & = \int p(\bm{s}) \left( \left( \cos(2\pi \transpose{\bm{s}}\bm{x})  \cos(2\pi \transpose{\bm{s}}\bm{y}) \right) +
 \left( \sin(2\pi \transpose{\bm{s}}\bm{x})  \sin(2\pi \transpose{\bm{s}}\bm{y}) \right) \right) d\bm{s} \\
  &= \int \left\langle \sqrt{p(\bm{s})} \left( {\cos(2\pi \transpose{\bm{s}}\bm{x}) \atop \sin(2\pi \transpose{\bm{s}}\bm{x})} \right) , \sqrt{p(\bm{s})} \left( {\sin(2\pi \transpose{\bm{s}}\bm{y}) \atop \sin(2\pi \transpose{\bm{s}}\bm{y})} \right) \right\rangle d\bm{s}
\end{align*}
So we can obtain an explicit feature mapping by defining:
$$
\varphi(\bm{x}) = \left( \sqrt{p(\bm{s})} \left( {\cos(2\pi \transpose{\bm{s}}\bm{x}) \atop \sin(2\pi \transpose{\bm{s}}\bm{x})} \right) \right)_{\bm{s} \in \mathbb{R}^d}.
$$
Assuming that $p(\bm{s})$ goes to zero sufficiently quickly, we have that:
$$
\frac{\partial \varphi(\bm{x})}{\partial x_k} = \left( 2\pi s_k \sqrt{p(\bm{s})} \left( {-\sin(2\pi \transpose{\bm{s}}\bm{x}) \atop \cos(2\pi \transpose{\bm{s}}\bm{x})} \right) \right)_{\bm{s} \in \mathbb{R}^d}
$$
and using a similar derivation as we used above, we get that:
\begin{align*}
\left\langle\frac{\partial\varphi(\bm{x})}{\partial x_k} ,  \frac{\partial\varphi(\bm{x})}{\partial x_l}\right\rangle &= 4\pi^2 \int
s_k s_l p(\bm{s}) e^{2\pi i \transpose{\bm{s}} (\bm{x}-\bm{x})} d\bm{s} \\
&=  4\pi^2 \int
s_k s_l p(\bm{s}) e^{0} d\bm{s}
\end{align*}
which doesn't depend on $\bm{x}$ as we wanted to show. $\square$
]
\end{document}



\def\year{2018}\relax %and enjoy the writing

\documentclass[letterpaper]{article} %DO NOT CHANGE THIS
\usepackage{aaai18}  %Required
\usepackage{times}  %Required
\usepackage{helvet}  %Required
\usepackage{courier}  %Required
\usepackage{url}  %Required
\usepackage{graphicx}  %Required
\frenchspacing  %Required
\setlength{\pdfpagewidth}{8.5in}  %Required
\setlength{\pdfpageheight}{11in}  %Required
 
\usepackage{xcolor}
\usepackage{mdframed}
\usepackage{amssymb,amsmath,bm}
\usepackage{textcomp}
\usepackage{amsfonts}
\usepackage{booktabs}
\usepackage{color}
\usepackage{siunitx}
\usepackage{subcaption}
\usepackage{afterpage}
\usepackage [english]{babel}
\usepackage [autostyle, english = american]{csquotes}
\usepackage{multirow}
\usepackage{makecell}
\MakeOuterQuote{"}

%dashed line
\usepackage{array}
\usepackage{arydshln}
% \setlength\dashlinedash{0.2pt}
% \setlength\dashlinegap{1.5pt}
% \setlength\arrayrulewidth{0.3pt}

\def\vec#1{\ensuremath{\bm{{#1}}}}
\def\mat#1{\vec{#1}}
\newcommand{\etal}{\textit{et al.}}

\newcolumntype{x}[1]{>{\centering\arraybackslash\hspace{0pt}}p{#1}}

%PDF Info Is Required: Camera-ready version: fill this in!
\pdfinfo{
/Title (FiLM: Visual Reasoning with a General Conditioning Layer)
/Author (Ethan Perez, Florian Strub, Harm de Vries, Vincent Dumoulin, Aaron Courville)
/Keywords (Deep Learning, Language and Vision)
}
\setcounter{secnumdepth}{2}
\title{FiLM: Visual Reasoning with a General Conditioning Layer}% @Harm
\author{Ethan Perez$^1$$^2$, Florian Strub$^3$, Harm de Vries$^1$,Vincent Dumoulin$^1$, Aaron Courville$^1$$^4$\\
$^1$MILA, Universit\'e of Montr\'eal, Canada; $^2$Rice University, U.S.A.;\\
$^3$Univ. Lille, CNRS, Centrale Lille, Inria, UMR 9189 CRIStAL France; $^4$CIFAR Fellow, Canada\\
ethanperez@rice.edu, florian.strub@inria.fr, mail@harmdevries.com,\\dumouliv@iro.umontreal.ca, courvila@iro.umontreal.ca
}

% Comment out nocopyright for camera-ready version
\nocopyright
% @Aaron: Any suggestions for overall organization / figure+table placement / etc.?
\begin{document}
\maketitle
\begin{abstract}% @Aaron: Any fixes for the new abstract? 
    	We introduce a general-purpose conditioning method for neural networks called \textbf{FiLM}: \textbf{F}eature-w\textbf{i}se \textbf{L}inear \textbf{M}odulation. FiLM layers influence neural network computation via a simple, feature-wise affine transformation based on conditioning information. We show that FiLM layers are highly effective for visual reasoning --- answering image-related questions which require a multi-step, high-level process --- a task which has proven difficult for standard deep learning methods that do not explicitly model reasoning. Specifically, we show on visual reasoning tasks that FiLM layers 1) halve state-of-the-art error for the CLEVR benchmark, 2) modulate features in a coherent manner, 3) are robust to ablations and architectural modifications, and 4) generalize well to challenging, new data from few examples or even zero-shot.
\end{abstract}

\noindent{\bf Index Terms}: Deep Learning, Language and Vision

\section{Introduction} \label{introduction}
    The ability to reason about everyday visual input is a fundamental building block of human intelligence. Some have argued that for artificial agents to learn this complex, structured process, it is necessary to build in aspects of reasoning, such as compositionality~\cite{N2NMN,IEP} or relational computation~\cite{RN}. However, if a model made from general-purpose components could learn to visually reason, such an architecture would likely be more widely applicable across domains.

	% If space: "emerged for visual question answering...": \cite{malinowski2015ask}
    To understand if such a general-purpose architecture exists, we take advantage of the recently proposed CLEVR dataset~\cite{CLEVR} that tests visual reasoning via question answering. Examples from CLEVR are shown in Figure~\ref{fig:CLEVR}. Visual question answering itself has its own line of datasets~\cite{malinowski2014multi,geman2015visual,antol2015} which focus on asking a diverse set of simpler questions on images, often answerable in a single glance. From these datasets, a number of effective, general-purpose deep learning models have emerged for visual question answering~\cite{SANs,lu2016hierarchical,anderson2017bottom}. However, tests on CLEVR show that these general deep learning approaches struggle to learn structured, multi-step reasoning~\cite{CLEVR}. In particular, these methods tend to exploit biases in the data rather than capture complex underlying structure behind reasoning~\cite{goyal2016making}.

    In this work, we show that a general model architecture can achieve strong visual reasoning with a method we introduce as \textbf{FiLM}: \textbf{F}eature-w\textbf{i}se \textbf{L}inear \textbf{M}odulation. A FiLM layer carries out a simple, feature-wise affine transformation on a neural network's intermediate features, conditioned on an arbitrary input. In the case of visual reasoning, FiLM layers enable a Recurrent Neural Network (RNN) over an input question to influence Convolutional Neural Network (CNN) computation over an image. This process adaptively and radically alters the CNN's behavior as a function of the input question, allowing the overall model to carry out a variety of reasoning tasks, ranging from counting to comparing, for example. FiLM can be thought of as a generalization of Conditional Normalization, which has proven highly successful for image stylization~\cite{CIN,CIN2,AIN}, speech recognition~\cite{DynamicLayerNorm}, and visual question answering~\cite{modulating_vision}, demonstrating FiLM's broad applicability.
    % NB: Clean up and use longer/tougher questions
    \begin{figure}[t]
	\centering
    \begin{subfigure}[t]{.2\textwidth}
        \centering
        \includegraphics[height=0.8in]{CLEVR_val_000008.png}
        \caption{\bf{Q:} \it{What number of cylinders are small purple things or yellow rubber things?
        \bf{A:} \textit{2}}}
    \end{subfigure}
    ~ 
    \begin{subfigure}[t]{.2\textwidth}
		\centering
        \includegraphics[height=0.8in]{CLEVR_val_000007.png}
        \caption{\bf{Q:} \it{What color is the other object that is the same shape as the large brown matte thing?
        \bf{A:} \textit{Brown}}}
    \end{subfigure}
    \caption{CLEVR examples and FiLM model answers.}
    \label{fig:CLEVR}
\end{figure}

    In this paper, which expands upon a shorter report~\cite{LVRWSP}, our key contribution is that we show FiLM is a strong conditioning method by showing the following on visual reasoning tasks:
    \begin{enumerate}
    	\item FiLM models achieve state-of-the-art across a variety of visual reasoning tasks, often by significant margins.
        \item FiLM operates in a coherent manner. It learns a complex, underlying structure and manipulates the conditioned network's features in a selective manner. It also enables the CNN to properly localize question-referenced objects.
        \item FiLM is robust; many FiLM model ablations still outperform prior state-of-the-art. Notably, we find there is no close link between normalization and the success of a conditioned affine transformation, a previously untouched assumption. Thus, we relax the conditions under which this method can be applied.
        \item FiLM models learn from little data to generalize to more complex and/or substantially different data than seen during training. We also introduce a novel FiLM-based zero-shot generalization method that further improves and validates FiLM's generalization capabilities.
    \end{enumerate}

\section{Method} \label{method}

    Our model processes the question-image input using FiLM, illustrated in Figure~\ref{fig:FiLM}. We start by explaining FiLM and then describe our particular model for visual reasoning.

	\subsection{Feature-wise Linear Modulation}
		\label{FiLM}
FiLM learns to adaptively influence the output of a neural network by applying an affine transformation, or FiLM, to the network's intermediate features, based on some input. More formally, FiLM learns
functions $f$ and $h$ which output $\gamma_{i,c}$ and $\beta_{i,c}$ as a function of input $\bm{x_i}$:
  \begin{align}
  	\gamma_{i,c} = f_c(\bm{x}_i) \qquad ~ \qquad
  	\beta_{i,c} = h_c(\bm{x}_i),
  \end{align}
where $\gamma_{i,c}$ and $\beta_{i,c}$ modulate a neural network's activations $\bm{F}_{i,c}$, whose subscripts refer to the $i^{th}$ input's $c^{th}$ feature or feature map, via a feature-wise affine transformation:
  \begin{equation}
  	FiLM(\bm{F}_{i,c} | \gamma_{i,c}, \beta_{i,c}) = \gamma_{i,c}\bm{F}_{i,c} + \beta_{i,c}.
  \end{equation}
$f$ and $h$ can be arbitrary functions such as neural networks. Modulation of a target neural network's processing can be based on the same input to that neural network or some other input, as in the case of multi-modal or conditional tasks. For CNNs, $f$ and $h$ thus modulate the per-feature-map distribution of activations based on $\bm{x_i}$, agnostic to spatial location.

	In practice, it is easier to refer to $f$ and $h$ as a single function that outputs a single $(\bm{\gamma}, \bm{\beta})$ vector, since, for example, it is often beneficial to share parameters across $f$ and $h$ for more efficient learning. We refer to this single function as the FiLM generator. We also refer to the network to which FiLM layers are applied as the Feature-wise Linearly Modulated network, the FiLM-ed network.

	FiLM layers empower the FiLM generator to manipulate feature maps of a target, FiLM-ed network by scaling them up or down, negating them, shutting them off, selectively thresholding them (when followed by a ReLU), and more. Each feature map is conditioned independently, giving the FiLM generator moderately fine-grained control over activations at each FiLM layer.
    
    As FiLM only requires two parameters per modulated feature map, it is a scalable and computationally efficient conditioning method. In particular, FiLM has a computational cost that does not scale with the image resolution.

	\subsection{Model}
		\label{model}

        \begin{figure}[t]
            \centering
            \includegraphics[width=0.45\linewidth]{FiLM.png}
	            \caption{A single FiLM layer for a CNN. The dot signifies a Hadamard product. Various combinations of $\gamma$ and $\beta$ can modulate individual feature maps in a variety of ways.}
            \label{fig:FiLM}
        \end{figure}

        \begin{figure}[t]
            \centering
            \includegraphics[width=0.9\linewidth]{model_AAAI.png}
	            \caption{The FiLM generator (left), FiLM-ed network (middle), and residual block architecture (right) of our model.}
            \label{fig:model}
        \end{figure}

		Our FiLM model consists of a FiLM-generating linguistic pipeline and a FiLM-ed visual pipeline as depicted in Figure~\ref{fig:model}. The FiLM generator processes a question $\bm{x_i}$ using a Gated Recurrent Unit (GRU) network~\cite{GRU} with 4096 hidden units that takes in learned, 200-dimensional word embeddings. The final GRU hidden state is a question embedding, from which the model predicts $(\bm{\gamma}^{n}_{i,\cdot}, \bm{\beta}^{n}_{i,\cdot})$ for each $n^{th}$ residual block via affine projection.

        The visual pipeline extracts 128 $14\times14$ image feature maps from a resized, $224\times224$ image input using either a CNN trained from scratch or a fixed, pre-trained feature extractor with a learned layer of $3\times3$ convolutions. The CNN trained from scratch consists of 4 layers with 128 $4\times4$ kernels each, ReLU activations, and batch normalization, similar to prior work on CLEVR~\cite{RN}. The fixed feature extractor outputs the \textit{conv4} layer of a ResNet-101~\cite{ResNet} pre-trained on ImageNet \cite{ImageNet} to match prior work on CLEVR~\cite{CLEVR,IEP}. Image features are processed by several --- 4 for our model --- FiLM-ed residual blocks (ResBlocks) with 128 feature maps and a final classifier. The classifier consists of a $1\times1$ convolution to 512 feature maps, global max-pooling, and a two-layer MLP with 1024 hidden units that outputs a softmax distribution over final answers.

        Each FiLM-ed ResBlock starts with a $1\times1$ convolution followed by one $3\times3$ convolution with an architecture as depicted in Figure~\ref{fig:model}. We turn the parameters of batch normalization layers that immediately precede FiLM layers off. Drawing from prior work on CLEVR~\cite{N2NMN,RN}, we concatenate two coordinate feature maps indicating relative x and y spatial position (scaled from $-1$ to $1$) with the image features, each ResBlock's input, and the classifier's input to facilitate spatial reasoning.
        
        % If space: ~\cite{Adam}
        We train our model end-to-end from scratch with Adam (learning rate $3e^{-4}$), weight decay ($1e^{-5}$), batch size 64, and batch normalization and ReLU throughout FiLM-ed network. Our model uses only image-question-answer triplets from the training set without data augmentation. We employ early stopping based on validation set accuracy, training for a maximum of 80 epochs. Further model details are in the appendix.
        
        We stress that our model relies \textit{solely} on feature-wise affine conditioning to use question information influence the visual pipeline behavior to answer questions. This approach differs from classical visual question answering pipelines which fuse image and language information into a single embedding via element-wise product, concatenation, attention, and/or more advanced methods~\cite{SANs,lu2016hierarchical,anderson2017bottom}.
        
\section{Related Work}\label{relatedwork}

	FiLM can be viewed as a generalization of Conditional Normalization (CN) methods. CN replaces the parameters of the feature-wise affine transformation typical in normalization layers, as introduced originally~\cite{BN}, with a learned function of some conditioning information. Various forms of CN have proven highly effective across a number of domains: Conditional Instance Norm~\cite{CIN,CIN2} and Adaptive Instance Norm~\cite{AIN} for image stylization, Dynamic Layer Norm for speech recognition~\cite{DynamicLayerNorm}, and Conditional Batch Norm for general visual question answering on complex scenes such as VQA and GuessWhat?!~\cite{modulating_vision}. This work complements our own, as we seek to show that feature-wise affine conditioning is effective for multi-step reasoning and understand the underlying mechanism behind its success.
    
    Notably, prior work in CN has not examined whether the affine transformation must be placed directly after normalization. Rather, prior work includes normalization in the method name for instructive purposes or due to application-specific implementation details. We investigate the relationship between FiLM and normalization and find that it is not strictly necessary for the affine transformation to be placed directly after normalization. Thus, we provide a unified framework for all of these methods through FiLM, as well as a normalization-free relaxation of this approach which can be more broadly applied.

	Beyond CN, there are many connections between FiLM and other conditioning methods. A common approach, used for example in Conditional DCGANs~\cite{DCGAN}, is to concatenate constant feature maps of conditioning information with convolutional layer input. This method, though not as parameter efficient, amounts to simply adding a post-layer, feature-wise conditional bias. Likewise, concatenating conditioning information with a fully-connected layer input amounts to a feature-wise conditional bias. Other approaches such as Conditional PixelCNN~\cite{van2016conditional} and WaveNet~\cite{WaveNet} directly add a conditional feature-wise bias. These approaches are equivalent to FiLM with $\bm{\gamma}=\bm{1}$, which we compare FiLM to in the Experiments section.
    
    Other methods gate an input's features as a function of that same input, rather than a separate conditioning input. These methods include LSTMs for sequence modeling~\cite{LSTM}, Convolutional Sequence to Sequence for machine translation~\cite{pmlr-v70-gehring17a}, and even the ImageNet 2017 winning model, Squeeze and Excitation Networks~\cite{hu2017}. This approach amounts to a feature-wise, conditional scaling, restricted to between 0 and 1, while FiLM consists of both scaling and shifting, each unrestricted. In the Experiments section, we show the effect of restricting FiLM's scaling to between 0 and 1 for visual reasoning. We find it noteworthy that this general approach of feature modulation is effective across a variety of settings and architectures.
    
        \begin{table*}[t]
        \centering
        
        {\small
        \begin{tabular}{l|c|cccccc}
        \toprule
        {Model} & {\textbf{Overall}} & {Count} & {Exist} & \begin{tabular}{@{}c@{}}Compare \\ Numbers\end{tabular} & \begin{tabular}{@{}c@{}}Query \\ Attribute\end{tabular} & \begin{tabular}{@{}c@{}}Compare \\ Attribute\end{tabular}\\
        \midrule
        Human~\cite{IEP}                   & 92.6 &86.7 &96.6 &86.5 &95.0 &96.0\\
        \midrule
        Q-type baseline~\cite{IEP}         &41.8 &34.6 &50.2 &51.0 &36.0 &51.3\\
        LSTM~\cite{IEP}                    &46.8 &41.7 &61.1 &69.8 &36.8 &51.8\\
        CNN+LSTM~\cite{IEP}                &52.3 &43.7 &65.2 &67.1 &49.3 &53.0\\
        CNN+LSTM+SA~\cite{RN}              &76.6 &64.4 &82.7 &77.4 &82.6 &75.4\\
        N2NMN*~\cite{N2NMN}                &83.7 &68.5 &85.7 &84.9 &90.0 &88.7\\
        PG+EE (9K prog.)*~\cite{IEP}       &88.6 &79.7 &89.7 &79.1 &92.6 &96.0\\
        PG+EE (700K prog.)*~\cite{IEP}     &96.9 &92.7 &97.1 &\bf{98.7} &98.1 &98.9\\
        CNN+LSTM+RN\textdagger$\ddagger$~\cite{RN}  &95.5 &90.1 &97.8 & 93.6 &97.9 &97.1\\
        \midrule
%         CNN+GRU+FiLM &\bf{97.6} &\bf{94.5} &\bf{99.2} &93.8 &\bf{99.2} &\bf{99.0}\\
        CNN+GRU+FiLM &\bf{97.7} &\bf{94.3} &99.1 &96.8 &99.1 &99.1\\
        CNN+GRU+FiLM$\ddagger$ &97.6 &\bf{94.3} &\bf{99.3} &93.4 &\bf{99.3} &\bf{99.3}\\
        \bottomrule
        \end{tabular}
        }
        \caption{\label{tab:results}
        CLEVR accuracy by baseline methods, competing methods, and our method (FiLM). (*) denotes use of extra supervisory information through program labels. (\textdagger) denotes use of data augmentation. ($\ddagger$) denotes training from raw pixels.}
        \end{table*}
    
        % NB: Cite/explain Overcoming Catastrophic forgetting?
        % @Aaron: Any fixes for this new paragraph?
        There are even broader links between FiLM and other methods. For example, FiLM can be viewed as using one network to generate parameters of another network, making it a form of hypernetwork~\cite{Hypernets}. Also, FiLM has potential ties with conditional computation and mixture of experts methods, where specialized network sub-parts are active on a per-example basis~\cite{HME,DeepMoE,shazeer2017outrageously}; we later provide evidence that FiLM learns to selectively highlight or suppress feature maps based on conditioning information. Those methods select at a sub-network level while FiLM selects at a feature map level.

	In the domain of visual reasoning, one leading method is the Program Generator + Execution Engine model~\cite{IEP}. This approach consists of a sequence-to-sequence Program Generator, which takes in a question and outputs a sequence corresponding to a tree of composable neural modules, each of which is a two or three layer residual block. This tree of neural modules is assembled to form the Execution Engine that then predicts an answer from the image. This modular approach is part of a line of neural module network methods~\cite{NMNQA,NMN,N2NMN}, of which End-to-End Module Networks~\cite{N2NMN} have also been tested on visual reasoning. These models use strong priors by explicitly modeling the compositional nature of reasoning and by training with additional program labels, \textit{i.e.} ground-truth step-by-step instructions on how to correctly answer a question. End-to-End Module Networks further build in model biases via per-module, hand-crafted neural architectures for specific functions. Our approach learns directly from visual and textual input without additional cues or a specialized architecture.

	Relation Networks (RNs) are another leading approach for visual reasoning~\cite{RN}. RNs succeed by explicitly building in a comparison-based prior. RNs use an MLP to carry out pairwise comparisons over each location of extracted convolutional features over an image, including LSTM-extracted question features as input to this MLP. RNs then element-wise sum over the resulting comparison vectors to form another vector from which a final classifier predicts the answer. We note that RNs have a computational cost that scales quadratically in spatial resolution, while FiLM's cost is independent of spatial resolution. Notably, since RNs concatenate question features with MLP input, a form of feature-wise conditional biasing as explained earlier, their conditioning approach is related to FiLM.

\section{Experiments} \label{experiments}

	% NB: Make website / include link
    First, we test our model on visual reasoning with the CLEVR task and use trained FiLM models to analyze what FiLM learns. Second, we explore how well our model generalizes to more challenging questions with the CLEVR-Humans task. Finally, we examine how FiLM performs in few-shot and zero-shot generalization settings using the CLEVR Compositional Generalization Test. In the appendix, we provide an error analysis of our model. Our code is available at~\url{https://github.com/ethanjperez/film}.

	\subsection{CLEVR Task} \label{CLEVR}
    	CLEVR is a synthetic dataset of 700K (image, question, answer, program) tuples~\cite{CLEVR}. Images contain 3D-rendered objects of various shapes, materials, colors, and sizes. Questions are multi-step and compositional in nature, as shown in Figure~\ref{fig:CLEVR}. They range from counting questions (\textit{"How many green objects have the same size as the green metallic block?"}) to comparison questions (\textit{"Are there fewer tiny yellow cylinders than yellow metal cubes?"}) and can be 40+ words long. Answers are each one word from a set of $28$ possible answers. Programs are an additional supervisory signal consisting of step-by-step instructions, such as \texttt{filter\_shape[cube]}, \texttt{relate[right]}, and \texttt{count}, on how to answer the question.
        
\begin{figure*}[ht!]
   		\small{
       \begin{tabular}{m{3.2cm}m{3.2cm}m{3.2cm}m{3.2cm}m{3.2cm}}
  	  \textbf{Q:} \textit{What shape is the...}
  	  &
      \textit{...purple thing?} 
      \textbf{A:} \textit{cube}
       &
       \textit{...blue thing?} 
       \textbf{A:} \textit{sphere}
       &
       \textit{...red thing right of the blue thing?} 
       \textbf{A:} \textit{sphere}
       &
       \textit{...red thing left of the blue thing?} 
       \textbf{A:} \textit{cube}
       \\
      \includegraphics[width=\linewidth]{Pool-Feature-Locations/CLEVR_val_000013.png}
	  & 	
  	  \includegraphics[width=\linewidth]{Pool-Feature-Locations/b-PFL13-Q1.png}
	  &
  	  \includegraphics[width=\linewidth]{Pool-Feature-Locations/b-PFL13-Q2.png}
	  &
\includegraphics[width=\linewidth]{Pool-Feature-Locations/b-PFL13-Q3.png}
	  &
  	  \includegraphics[width=\linewidth]{Pool-Feature-Locations/b-PFL13-Q4.png}
    \\
      	  \includegraphics[width=\linewidth]{Pool-Feature-Locations/CLEVR_val_000020.png}
&
  	  \includegraphics[width=\linewidth]{Pool-Feature-Locations/b-PFL20-Q1.png}
&
  	  \includegraphics[width=\linewidth]{Pool-Feature-Locations/b-PFL20-Q2.png}
&
  	  \includegraphics[width=\linewidth]{Pool-Feature-Locations/b-PFL20-Q3.png}
&
  	  \includegraphics[width=\linewidth]{Pool-Feature-Locations/b-PFL20-Q4.png}
\cr
	\\
    \textbf{Q:} \textit{How many cyan things are...}
    &
    \textit{...right of the gray cube?}
    \textbf{A:} \textit{3}
    &
    \textit{...left of the small cube?}
    \textbf{A:} \textit{2}
    &
    \textit{...right of the gray cube and left of the small cube?}
    \textbf{A:} \textit{1}
    &
    \textit{...right of the gray cube or left of the small cube? }     
    \textbf{A:} \textit{4} (\textbf{P:} \textit{3})
	\end{tabular}
    }
    \caption{Visualizations of the distribution of locations which the model uses for its globally max-pooled features which its final MLP predicts from. FiLM correctly localizes the answer-referenced object (top) or all question-referenced objects (bottom), but not as accurately when it answers incorrectly (rightmost bottom). Questions and images used match~\cite{IEP}.}
      	\label{fig:pool-feature-locations}
  \end{figure*}
        
    \subsubsection{Baselines} \label{baselines}
    	We compare against the following methods:
        \begin{itemize}
        	\item \textbf{Q-type baseline}: Predicts based on a question's category.
            \item \textbf{LSTM}: Predicts using only the question. 
            \item \textbf{CNN+LSTM}: MLP prediction over CNN-extracted image features and LSTM-extracted question features.
        	\item \textbf{Stacked Attention Networks (CNN+LSTM+SA)}: Linear prediction over CNN-extracted image feature and LSTM-extracted question features combined via two rounds of soft spatial attention~\cite{SANs}.
             \item \textbf{End-to-End Module Networks (N2NMN) and Program Generator + Execution Engine (PG+EE)}: Methods in which separate neural networks learn separate sub-functions and are assembled into a question-dependent structure~\cite{N2NMN,IEP}.
             \item \textbf{Relation Networks (CNN+LSTM+RN)}: An approach which builds in pairwise comparisons over spatial locations to explicitly model reasoning's relational nature~\cite{RN}.
        \end{itemize}

	\subsubsection{Results} \label{results}

		FiLM achieves a new overall state-of-the-art on CLEVR, as shown in Table~\ref{tab:results}, outperforming humans and previous methods, including those using explicit models of reasoning, program supervision, and/or data augmentation. For methods not using extra supervision, FiLM roughly halves state-of-the-art error (from $4.5\%$ to $2.3\%$). Note that using pre-trained image features as input can be viewed as a form of data augmentation in itself but that FiLM performs equally well using raw pixel inputs. Interestingly, the raw pixel model seems to perform better on lower-level questions (\textit{i.e.} querying and comparing attributes) while the image features model seems to perform better on higher-level questions (\textit{i.e.} compare numbers of objects).

	\subsection{What Do FiLM Layers Learn?} \label{what-is-learned}
        
        To understand how FiLM visually reasons, we visualize activations to observe the net result of FiLM layers. We also use histograms and t-SNE~\cite{maaten2008visualizing} to find patterns in the learned FiLM $\gamma$ and $\beta$ parameters themselves. In Figures~\ref{fig:film-effect-1} and~\ref{fig:film-effect-2} in the appendix, we visualize the effect of FiLM at the single feature map level.
        
        \subsubsection{Activation Visualizations}
        Figure~\ref{fig:pool-feature-locations} visualizes the distribution of locations responsible for the globally-pooled features which the MLP in the model's final classifier uses to predict answers. These images reveal that the FiLM model predicts using features of areas near answer-related or question-related objects, as the high CLEVR accuracy also suggests. This finding highlights that appropriate feature modulation indirectly results in spatial modulation, as regions with question-relevant features will have large activations while other regions will not. This observation might explain why FiLM outperforms Stacked Attention, the next best method not explicitly built for reasoning, so significantly ($21\%$); FiLM appears to carry many of spatial attention's benefits, while also influencing feature representation.
        
		Figure~\ref{fig:pool-feature-locations} also suggests that the FiLM-ed network carries out reasoning throughout its pipeline. In the top example, the FiLM-ed network has localized the answer-referenced object alone before the MLP classifier. In the bottom example, the FiLM-ed network retains, for the MLP classifier, features on objects that are not referred to by the answer but are referred to by the question. The latter example provides evidence that the final MLP itself carries out some reasoning, using FiLM to extract relevant features for its reasoning.
        
        \begin{figure}
          \begin{subfigure}[b]{0.23\textwidth}
              \includegraphics[width=\linewidth]{Statsv2/Gammas}
          \end{subfigure}%
          \begin{subfigure}[b]{0.23\textwidth}
              \includegraphics[width=\linewidth]{Statsv2/Betas}
          \end{subfigure}%
        \caption{Histograms of $\gamma_{i,c}$ (left) and $\beta_{i,c}$ (right) values over all FiLM layers, calculated over the validation set.}
        \label{fig:gammas-betas}
        \end{figure}
        
        \begin{figure*}[t]
        \centering
        \includegraphics[width=0.9\linewidth]{tsne.png}
        \caption{t-SNE plots of $(\bm{\gamma}$, $\bm{\beta})$ of the first (left) and last (right) FiLM layers of a 6-FiLM layer Network. FiLM parameters cluster by low-level reasoning functions in the first layer and by high-level reasoning functions in the last layer.}
        \label{fig:tsne}
        \end{figure*}
        
        \subsubsection{FiLM Parameter Histograms}
		To analyze at a lower level how FiLM uses the question to condition the visual pipeline, we plot $\gamma$ and $\beta$ values predicted over the validation set, as shown in Figure~\ref{fig:gammas-betas} and in more detail in the appendix (Figures~\ref{fig:gammas-layerwise} to~\ref{fig:film-stats}). $\gamma$ and $\beta$ values take advantage of a sizable range, varying from -15 to 19 and from -9 to 16, respectively. $\gamma$ values show a sharp peak at 0, showing that FiLM learns to use the question to shut off or significantly suppress whole feature maps. Simultaneously, FiLM learns to upregulate a much more selective set of other feature maps with high magnitude $\gamma$ values. Furthermore, a large fraction ($36\%$) of $\gamma$ values are negative; since our model uses a ReLU after FiLM, $\gamma<0$ can cause a significantly different set of activations to pass the ReLU to downstream layers than $\gamma>0$. Also, $76\%$ of $\beta$ values are negative, suggesting that FiLM also uses $\beta$ to be selective about which activations pass the ReLU. We show later that FiLM's success is largely architecture-agnostic, but examining a particular model gives insight the type of influence FiLM learns to exert in a specific case. Together, these findings suggest that FiLM learns to selectively upregulate, downregulate, and shut off feature maps based on conditioning information.
        
        \subsubsection{FiLM Parameters t-SNE Plot}
        In Figure~\ref{fig:tsne}, we visualize FiLM parameter vectors $(\bm{\gamma}, \bm{\beta})$ for 3,000 random validation points with t-SNE. We analyze the deeper, 6-ResBlock version of our model, which has a similar validation accuracy as our 4-ResBlock model, to better examine how FiLM layers in different layers of a hierarchy behave. First and last layer FiLM $(\bm{\gamma}, \bm{\beta})$ are grouped by the low-level and high-level reasoning functions necessary to answer CLEVR questions, respectively. For example, FiLM parameters for \texttt{equal\_color} and \texttt{query\_color} are close for the first layer but apart for the last layer. The same is true for shape, size and material questions. Conversely, \texttt{equal\_shape}, \texttt{equal\_size}, and \texttt{equal\_material} FiLM parameters are grouped in the last layer but split in the first layer --- likewise for other high level groupings such as integer comparison and querying. This finding suggests that FiLM layers learn a sort of function-based modularity without an architectural prior. Simply with end-to-end training, FiLM learns to handle not only different types of questions differently, but also different types of question sub-parts differently; the FiLM model works from low-level to high-level processes as is the proper approach. For models with fewer FiLM layers, such patterns also appear, but less clearly; these models must begin higher level reasoning sooner.

	\subsection{Ablations} \label{ablations}
    
    	Using the validation set, we conduct an ablation study on our best model to understand how FiLM learns visual reasoning. We show results for test time ablations in Figure~\ref{fig:noisy-ablations}, for architectural ablations in Table~\ref{tab:ablations}, and for varying model depths in Table~\ref{tab:num-resblocks}. Without hyperparameter tuning, most architectural ablations and model depths outperform prior state-of-the-art on training from image-question-answer triplets alone, supporting the overall robustness of FiLM.
    
        \begin{figure}[ht]
        \centering
        \includegraphics[width=0.8\linewidth]{noisy_film.png}
        \caption{\label{fig:noisy-ablations} An analysis of how robust FiLM parameters are to noise at test time. The horizontal lines correspond to setting $\gamma$ or $\beta$ to their respective training set mean values.}
        \end{figure}
    
\begin{table}[ht!]
        {\small
       \begin{tabular}{lm{4.9cm}|x{1.5cm}}
        \toprule
        {Model}   && {\textbf{Overall}} \\
        \end{tabular}
        \\	
        \begin{tabular}{lm{5.7cm}|x{1.5cm}}
        \midrule
        \multicolumn{2}{l|}{\textbf{Restricted $\bm{\gamma}$ or $\bm{\beta}$}}&\\
        & FiLM with $\bm{\beta}:=\bm{0}$ &96.9\\%96.89
        & FiLM with $\bm{\gamma}:=\bm{1}$ &95.9\\%95.89
        & FiLM with $\bm{\gamma}:=\sigma(\bm{\gamma})$ &95.9\\%95.92
        & FiLM with $\bm{\gamma}:=tanh(\bm{\gamma})$ &96.3\\%96.34
        & FiLM with $\bm{\gamma}:=exp(\bm{\gamma})$ &96.3\\%96.28
        \end{tabular}
        \\	
        \begin{tabular}{lm{5.7cm}|x{1.5cm}}
        \midrule
        \multicolumn{2}{l|}{\textbf{Moving FiLM within ResBlock}}&\\
        &FiLM after residual connection &96.6\\%96.55
        &FiLM after ResBlock ReLU-2 &97.7\\%97.72
        &FiLM after ResBlock Conv-2 &97.1\\%97.08
        &FiLM before ResBlock Conv-1 &95.0\\%95.01
        \end{tabular}
        \\	
        \begin{tabular}{lm{5.7cm}|x{1.5cm}}
        \midrule
        \multicolumn{2}{l|}{\textbf{Removing FiLM from ResBlocks}}&\\
        &No FiLM in ResBlock 4 &96.8\\%96.83
        &No FiLM in ResBlock 3-4 &96.5\\%96.51
        &No FiLM in ResBlock 2-4 &97.3\\%97.31
        &No FiLM in ResBlock 1-4 &21.4\\%21.40
        \end{tabular}
        \\	
        \begin{tabular}{lm{5.7cm}|x{1.5cm}}
        \midrule
        \multicolumn{2}{l|}{\textbf{Miscellaneous}}&\\
        % &Replacing FiLM with pre-Conv-2 Concat Cond &95.31\\
        &$1\times1$ conv only, with no coord. maps &95.3\\%95.26
        &No residual connection &94.0\\%94.01
        &No batch normalization &93.7\\%93.73
        &Replace image features with raw pixels & 97.6\\%97.62
        \end{tabular}
        \\
        \begin{tabular}{lm{5.7cm}|x{1.5cm}}
        \midrule
        & Best Architecture &97.4$\pm$.4\\ %97.37
        \bottomrule
        
        \end{tabular}
        }
        \caption{\label{tab:ablations}
        CLEVR val accuracy for ablations, trained with the best architecture with only specified changes. We report the standard deviation of the best model accuracy over 5 runs.}
        \end{table}
        
        \subsubsection{Effect of $\gamma$ and $\beta$}
        
        To test the effect of $\bm{\gamma}$ and $\bm{\beta}$ separately, we trained one model with a constant $\bm{\gamma}=\bm{1}$ and another with $\bm{\beta}=\bm{0}$. With these models, we find a 1.5\% and .5\% accuracy drop, respectively; FiLM can learn to condition the CNN for visual reasoning through either biasing or scaling alone, albeit not as well as conditioning both together. This result also suggests that $\bm{\gamma}$ is more important than $\bm{\beta}$.
        
        To further compare the importance of $\bm{\gamma}$ and $\bm{\beta}$, we run a series of test time ablations (Figure~\ref{fig:noisy-ablations}) on our best, fully-trained model. First, we replace $\bm{\beta}$ with the mean $\bm{\beta}$ across the training set. This ablation in effect removes all conditioning information from $\bm{\beta}$ parameters during test time, from a model trained to use both $\bm{\gamma}$ and $\bm{\beta}$. Here, we find that accuracy only drops by 1.0\%, while the same procedure on $\bm{\gamma}$ results in a 65.4\% drop. This large difference suggests that, in practice, FiLM largely conditions through $\bm{\gamma}$ rather than $\bm{\beta}$. Next, we analyze performance as we add increasingly more Gaussian noise to the best model's FiLM parameters at test time. Noise in gamma hurts performance significantly more, showing FiLM's higher sensitivity to changes in $\bm{\gamma}$ than in $\bm{\beta}$ and corroborating the relatively greater importance of $\bm{\gamma}$.
        
        \subsubsection{Restricting $\gamma$}% @Aaron: Any fixes for this new paragraph?
        To understand what aspect of $\bm{\gamma}$ is most effective, we train a model that limits $\bm{\gamma}$ to $(0,1)$ using sigmoid, as many models which use feature-wise, multiplicative gating do. Likewise, we also limit $\bm{\gamma}$ to $(-1,1)$ using $tanh$. Both restrictions hurt performance, roughly as much as removing conditioning from $\bm{\gamma}$ entirely by training with $\bm{\gamma}=\bm{1}$. Thus, FiLM's ability to scale features by large magnitudes appears to contribute to its success. Limiting $\bm{\gamma}$ to $(0,\infty)$ with $exp$ also hurts performance, validating the value of FiLM's capacity to negate and zero out feature maps.
        
        \subsubsection{Conditional Normalization}
        We perform an ablation study on the placement of FiLM to evaluate the relationship between normalization and FiLM that Conditional Normalization approaches assume. Unfortunately, it is difficult to accurately decouple the effect of FiLM from normalization by simply training our corresponding model without normalization, as normalization significantly accelerates, regularizes, and improves neural network learning~\cite{BN}, but we include these results for completeness. However, we find no substantial performance drop when moving FiLM layers to different parts of our model's ResBlocks; we even reach the upper end of the best model's performance range when placing FiLM after the post-normalization ReLU in the ResBlocks. Thus, we decouple the name from normalization for clarity regarding where the fundamental effectiveness of the method comes from. By demonstrating this conditioning mechanism is not closely connected to normalization, we open the doors to applications other settings in which normalization is less common, such as RNN's and reinforcement learning, which are promising directions for future work with FiLM.
        
        \subsubsection{Repetitive Conditioning}
        To understand the contribution of repetitive conditioning towards FiLM model success, we train FiLM models with successively fewer FiLM layers. Models with fewer FiLM layers, even a single FiLM layer, do not deviate far from the best model's performance, revealing that the model can reason and answer diverse questions successfully by modulating features even just once. This observation highlights the capacity of even one FiLM layer. Perhaps one FiLM layer can pass enough question information to the CNN to enable it to carry out reasoning later in the network, in place of the more hierarchical conditioning deeper FiLM models appear to use. We leave more in-depth investigation of this matter for future work.
        
        \subsubsection{Spatial Reasoning}
        To examine how FiLM model approach spatial reasoning, we train a version of our best model architecture, on pre-trained ResNet features, with only $1\times1$ convolutions and without feeding coordinate feature maps indicating relative spatial position to the model. Due to the global max-pooling near the end of the model, this model cannot transfer information across spatial positions. Notably, this ablation model still achieves a high 95.3\% accuracy, indicating that FiLM models are able to reason about space simply from the spatial information contained in a single location of fixed image features.
        
        \subsubsection{Residual Connection}
		Removing the residual connection causes one of the larger accuracy drops. Since there is a global max-pooling operation near the end of the network, this finding suggests that the best model learns to primarily use features of locations that are repeatedly important throughout lower and higher levels of reasoning to make its final decision. The higher accuracies for models with FiLM modulating features inside residual connections rather than outside residual connections supports this hypothesis.
        
\begin{table}[t]
        \centering
        \begin{tabular}{l|c||l|cc|}
        \toprule
        {Model} & {\textbf{Overall}} & {Model} & {\textbf{Overall}} \\
        \midrule
         1 ResBlock &93.5 & 6 ResBlocks &97.7\\%93.53,97.65
         2 ResBlocks &97.1 &7 ResBlocks &97.4\\%97.06,97.42
         3 ResBlocks &96.7 &8 ResBlocks &97.6\\%96.66,97.63
         4 ResBlocks &97.4$\pm$.4 &12 ResBlocks &96.9\\%97.84,96.91
         5 ResBlocks &97.4  &  & \\%97.43
\end{tabular}
\caption{\label{tab:num-resblocks} CLEVR val accuracy by FiLM model depth.}
\end{table}

		\subsubsection{Model Depth}% @Aaron: Any fixes for the new paragraph and associated table?
        Table~\ref{tab:num-resblocks} shows model performance by the number of ResBlocks. FiLM is robust to varying depth but less so with only 1 ResBlock, backing the earlier theory that the FiLM-ed network reasons throughout its pipeline.

\begin{figure*}[t!]
		\centering
   		\small{
       \begin{tabular}{m{3.2cm}m{3.2cm}m{3.2cm}m{3.2cm}m{3.2cm}}
    \\
      	  \includegraphics[width=\linewidth]{CLEVR-Humans/CLEVR_val_004711.png}
&
  	  \includegraphics[width=\linewidth]{CLEVR-Humans/CLEVR_val_005923.png}
&
  	  \includegraphics[width=\linewidth]{CLEVR-Humans/CLEVR_val_002850.png}
&
  	  \includegraphics[width=\linewidth]{CLEVR-Humans/CLEVR_val_006560.png}
&
  	  \includegraphics[width=\linewidth]{CLEVR-Humans/CLEVR_val_001398.png}
\cr
	\\
    \textbf{Q:} \textit{What object is the color of \underline{grass}?}
    \textbf{A:} \textit{Cylinder}
    &
    \textbf{Q:} \textit{\underline{Which} shape objects are \underline{partially} \underline{obscured} \underline{from} \underline{view}?}
    \textbf{A:} \textit{Sphere}
    &
    \textbf{Q:} \textit{What color is the matte object \underline{farthest} to the right?}
    \textbf{A:} \textit{Brown}
    &
    \textbf{Q:} \textit{What shape is \underline{reflecting} in the large cube?}
    \textbf{A:} \textit{Cylinder}
    &
    \textbf{Q:} \textit{If \underline{all} \underline{cubical} objects were \underline{removed} what \underline{shaped} objects \underline{would} there \underline{be} the \underline{most} of?}     
    \textbf{A:} \textit{Sphere} (\textbf{P:} \textit{Rubber})
	\end{tabular}
    }
    \caption{Examples from CLEVR-Humans, which introduces new words (underlined) and concepts. After fine-tuning on CLEVR-Humans, a CLEVR-trained model can now reason about obstruction, superlatives, and reflections but still struggles with hypothetical scenarios (rightmost). It also has learned human preference to primarily identify objects by shape (leftmost).}
    \label{fig:CLEVR-Humans-examples}
  \end{figure*}

	\subsection{CLEVR-Humans: Human-Posed Questions} \label{CLEVR-Humans}
    
    	To assess how well visual reasoning models generalize to more realistic, complex, and free-form questions, the CLEVR-Humans dataset was introduced~\cite{IEP}. This dataset contains human-posed questions on CLEVR images along with their corresponding answers. The number of samples is limited --- 18K for training, 7K for validation, and 7K for testing. The questions were collected from Amazon Mechanical Turk workers prompted to ask questions that were likely \textit{hard for a smart robot to answer}. As a result, CLEVR-Humans questions use more diverse vocabulary and complex concepts.
        
        \subsubsection{Method}
		To test FiLM on CLEVR-Humans, we take our best CLEVR-trained FiLM model and fine-tune its FiLM-generating linguistic pipeline alone on CLEVR-Humans. Similar to prior work~\cite{IEP}, we do not update the visual pipeline on CLEVR-Humans to mitigate overfitting on the small training set.% We use a slightly higher learning rate ($7e^{-4}$) to fine-tune on CLEVR-Humans.% If space / If alignment okay
        
        \subsubsection{Results}% @Aaron: I modified this paragraph significantly and updated the CLEVR-Humans figure with a new (dope) one. Any fixes to suggest?
		Our model achieves state-of-the-art generalization to CLEVR-Humans, both before and after fine-tuning, as shown in Table~\ref{tab:CLEVR-Humans}, indicating that FiLM is well-suited to handle more complex and diverse questions. Figure~\ref{fig:CLEVR-Humans-examples} shows examples from CLEVR-Humans with FiLM model answers. Before fine-tuning, FiLM outperforms prior methods by a smaller margin. After fine-tuning, FiLM reaches a considerably improved final accuracy. In particular, the \textit{gain} in accuracy made by FiLM upon fine-tuning is more than 50\% greater than those made by other models; FiLM adapts data-efficiently using the small CLEVR-Humans dataset.
        
        Notably, FiLM surpasses the prior state-of-the-art method, Program Generator + Execution Engine (PG+EE), after fine-tuning by \textit{$9.3\%$}. Prior work on PG+EEs explains that this neural module network method struggles on questions which cannot be well approximated with the model's module inventory~\cite{IEP}. In contrast, FiLM has the freedom to modulate existing feature maps, a fairly flexible and fine-grained operation, in novel ways to reason about new concepts. These results thus provide some evidence for the benefits of FiLM's general nature.

    	\begin{table}[t]
          {\small
          \begin{tabular}{l|c|cccccc}
          \toprule
          {}      & {Train} & {Train CLEVR,} \\
          {Model} & {CLEVR} & {fine-tune human} \\
          \midrule
          LSTM               & 27.5 & 36.5 \\
          CNN+LSTM           & 37.7 & 43.2 \\
          CNN+LSTM+SA+MLP    & 50.4 & 57.6 \\
          PG+EE (18K prog.)  & 54.0 & 66.6 \\
          \midrule
          CNN+GRU+FiLM & \textbf{56.6} & \textbf{75.9} \\
          \bottomrule
          \end{tabular}
          }
          \caption{\label{tab:CLEVR-Humans} CLEVR-Humans test accuracy, before (left) and after (right) fine-tuning on CLEVR-Humans data}
        \end{table}
    
    \subsection{CLEVR Compositional Generalization Test} 
    	\label{CLEVR-CoGenT}

        To test how well models learn compositional concepts that generalize, CLEVR-CoGenT was introduced~\cite{CLEVR}. This dataset is synthesized in the same way as CLEVR but contains two conditions: in Condition A, all cubes are gray, blue, brown, or yellow and all cylinders are red, green, purple, or cyan; in Condition B, cubes and cylinders swap color palettes. Both conditions contain spheres of all colors. CLEVR-CoGenT thus indicates how a model answers CLEVR questions: by memorizing combinations of traits or by learning disentangled or general representations.
        
\begin{figure}[t]
\begin{center}
        \includegraphics[width=0.75\linewidth]{acc_samples.png}
        \small
  \begin{tabular}{l| c c | c c }
    \toprule
    &  \multicolumn{2}{m{1.8cm}|}{\makecell{\textbf{Train A}}}& \multicolumn{2}{m{1.8cm}}{\makecell{\textbf{Fine-tune B}}}  \\ 
    Method & A & B & A & B \\ \midrule 
    CNN+LSTM+SA & 80.3 & 68.7 & 75.7 & 75.8 \\ 
    PG+EE (18K prog.) & 96.6 & 73.7 & 76.1 & 92.7 \\  \hline
    CNN+GRU+FiLM & \textbf{98.3} & 75.6 & 80.8 & \textbf{96.9} \\ 
    CNN+GRU+FiLM 0-Shot & \textbf{98.3} & \textbf{78.8} & \textbf{81.1} & \textbf{96.9} \\
    \bottomrule 
  \end{tabular}
  \caption{CoGenT results. FiLM ValB accuracy reported on ValB without the 30K fine-tuning samples (Figure). Accuracy before and after fine-tuning on 30K of ValB (Table).}
\label{fig:CoGenT}
\end{center}
\end{figure}
        
        \subsubsection{Results}
          We train our best model architecture on Condition A and report accuracies on Conditions A and B, before and after fine-tuning on B, in Figure~\ref{fig:CoGenT}. Our results indicate FiLM surpasses other visual reasoning models at learning general concepts. FiLM learns better compositional generalization even than PG+EE, which explicitly models compositionality and is trained with program-level supervision that specifically includes filtering colors and filtering shapes.
        % NB: Does FiLM actually generalize better compositionally? Perhaps all of the gains are due to improvements on questions that are in the training set?

		\subsubsection{Sample Efficiency and Catastrophic Forgetting} We show sample efficiency and forgetting curves in Figure~\ref{fig:CoGenT}. FiLM achieves prior state-of-the-art accuracy with $1/3$ as much fine-tuning data. However, our FiLM model still suffers from catastrophic forgetting after fine-tuning.
        
		\subsubsection{Zero-Shot Generalization}
\begin{figure}[th!]
\begin{center}
  \includegraphics[width=0.45\linewidth]{CLEVR_val_000031.png}
        \scriptsize
  \begin{tabular}{|l| l |}
    \hline
    Question & What is the blue big cylinder made of? \\ \hline
    (1) Swap shape & What is the blue big \textbf{sphere} made of? \\
	(2) Swap color & What is the \textbf{green} big cylinder made of? \\
	(3) Swap shape/color & What is the \textbf{green} big \textbf{sphere} made of?  \\ \hline
  \end{tabular}
  \caption{A CLEVR-CoGenT example. The combination of concepts \textit{"blue"} and \textit{"cylinder"} is not in the training set. Our zero-shot method computes the original question's FiLM parameters via linear combination of three other questions' FiLM parameters: (1) + (2) - (3). This method corrects our model's answer from \textit{"rubber"} to \textit{"metal"}.}
  \label{fig:zero-shot-generalization}
\end{center}
\end{figure}
        
		FiLM's accuracy on Condition A is much higher than on B, suggesting FiLM has memorized attribute combinations to an extent. For example, the model learns a bias that cubes are not cyan, as learning this training set bias helps minimize training loss.

    	To overcome this bias, we develop a novel FiLM-based zero-shot generalization method. Inspired by word embedding manipulations, \textit{e.g. "King" - "Man" + "Woman" = "Queen"}~\cite{mikolov2013distributed}, we test if linear manipulation extends to reasoning with FiLM. We compute $(\bm{\gamma},\bm{\beta})$ for \textit{"How many cyan cubes are there?"} via the linear combination of questions in the FiLM parameter space: \textit{"How many cyan spheres are there?"} $+$ \textit{"How many brown cubes are there?"} $-$ \textit{"How many brown spheres are there?"}. With this $(\bm{\gamma},\bm{\beta})$, our model can correctly count cyan cubes. We show another example of this method in Figure~\ref{fig:zero-shot-generalization}.
    
    We evaluate this method on validation B, using a parser to automatically generate the right combination of questions. We test previously reported CLEVR-CoGenT FiLM models with this method and show results in Figure~\ref{fig:CoGenT}. With this method, there is a $3.2\%$ overall accuracy gain when training on A and testing for zero-shot generalization on B. Yet this method could only be applied to $1/3$ of questions in B. For these questions, model accuracy starts at $71.5\%$ and jumps to $80.7\%$. Before fine-tuning on B, the accuracy between zero-shot and original approaches on A is identical, likewise for B after fine-tuning. We note that difference in the predicted FiLM parameters between these two methods is negligible, likely causing the similar performance.
    
    We achieve these improvements without specifically training our model for zero-shot generalization. Our method simply allows FiLM to take advantage of any concept disentanglement in the CNN after training. We also observe that convex combinations of the FiLM parameters -- \textit{i.e.} between \textit{"How many cyan things are there?"} and \textit{"How many brown things are there?"} -- often monotonically interpolates the predicted answer between the answers to endpoint questions. These results highlight, to a limited extent, the flexibility of FiLM parameters for meaningful manipulations.

	% @Aaron: Any fixes for the CLEVR-CoGenT example figure (moved from appendix earlier)?
	% @Aaron: Any fixes for the below new paragraph?
    As implemented, this method has many limitations. However, approaches from word embeddings, representation learning, and zero-shot learning can be applied to directly optimize $(\bm{\gamma},\bm{\beta})$ for analogy-making~\cite{TransE,gu2015traversing,46127}. The FiLM-ed network could directly train with this procedure via backpropagation. A learned model could also replace the parser. We find such avenues promising for future work.

\section{Conclusion} \label{conclusion}
	We show that a model can achieve strong visual reasoning using general-purpose Feature-wise Linear Modulation layers. By efficiently manipulating a neural network's intermediate features in a selective and meaningful manner using FiLM layers, a RNN can effectively use language to modulate a CNN to carry out diverse and multi-step reasoning tasks over an image.
    Our ablation study suggests that FiLM is resilient to architectural modifications, test time ablations, and even restrictions on FiLM layers themselves.
    Notably, we provide evidence that FiLM's success is not closely connected with normalization as previously assumed.
    Thus, we open the door for applications of this approach to settings where normalization is less common, such as RNNs and reinforcement learning.
    Our findings also suggest that FiLM models can generalize better, more sample efficiently, and even zero-shot to foreign or more challenging data.
    Overall, the results of our investigation of FiLM in the case of visual reasoning complement broader literature that demonstrates the success of FiLM-like techniques across many domains, supporting the case for FiLM's strength not simply within a single domain but as a general, versatile approach.

  \section{Acknowledgements}
    We thank the developers of PyTorch (\url{pytorch.org}) and~\cite{IEP} for open-source code which our implementation was based off. We thank Mohammad Pezeshki, Dzmitry Bahdanau, Yoshua Bengio, Nando de Freitas, Hugo Larochelle, Laurens van der Maaten, Joseph Cohen, Joelle Pineau, Olivier Pietquin, J\'er\'emie Mary, C\'esar Laurent, Chin-Wei Huang, Layla Asri, Max Smith, and James Ough for helpful discussions and Justin Johnson for CLEVR test evaluations. We thank NVIDIA for donating a DGX-1 computer used in this work. We also acknowledge FRQNT through the CHIST-ERA IGLU project and CPER Nord-Pas de Calais/FEDER DATA Advanced data science and technologies 2015-2020 for funding our work. Lastly, we thank \url{acronymcreator.net} for the acronym FiLM.

% Comment out below for camera-ready
% \clearpage

\bibliographystyle{aaai}
\bibliography{mybib}

\clearpage
  \section{Appendix}% @Aaron: I don't think the appendix has been reviewed much since it was just cleaned up recently - if you have any tips, let me know.
  
	\subsection{Error Analysis} \label{errors}

        We examine the errors our model makes to understand where our model fails and how it acts when it does. Examples of these errors are shown in Figures~\ref{fig:errors} and~\ref{fig:logic-error}.
        
        \subsubsection{Occlusion}
        Many model errors are due to partial occlusion. These errors may likely be fixed using a CNN that operates at a higher resolution, which is feasible since FiLM has a computational cost that is independent of resolution.
        
        \subsubsection{Counting}
		$96.1\%$ of counting mistakes are off-by-one errors, showing FiLM has learned underlying concepts behind counting such as close relationships between close numbers.

		\subsubsection{Logical Consistency}
    	The model sometimes makes curious reasoning mistakes a human would not. For example, we find a case where our model correctly counts one gray object and two cyan objects but simultaneously answers that there are the same number of gray and cyan objects. In fact, it answers that the number of gray objects is both less than \textit{and} equal to the number of yellow blocks. These errors could be prevented by directly minimizing logical inconsistency, an interesting avenue for future work orthogonal to FiLM.
    
    \subsection{Model Details}
    Rather than output $\gamma_{i,c}$ directly, we output $\Delta\gamma_{i,c}$, where:
\begin{equation}
	\gamma_{i,c}=1+\Delta\gamma_{i,c},
\end{equation}
since initially zero-centered $\gamma_{i,c}$ can zero out CNN feature map activations and thus gradients. In our implementation, we opt to output $\Delta\gamma_{i,c}$ rather than $\gamma_{i,c}$, but for simplicity, throughout our paper, we explain FiLM using $\gamma_{i,c}$. However, this modification does not seem to affect our model's performance on CLEVR statistically significantly.

	We present training and validation curves for best model trained from image features in Figure~\ref{fig:training-curves}. We observe fast accuracy gains initially, followed by slow, steady increases to a best validation accuracy of $97.84\%$, at which point training accuracy is $99.53\%$. We train on CLEVR for 80 epochs, which takes 4 days using 1 NVIDIA TITAN Xp GPU when learning from image features. For practical reasons, we stop training on CLEVR after 80 epochs, but we observe that accuracy continues to increase slowly even afterwards.
    
%     As an observational note, we find that FiLM has a large capacity; FiLM models easily and consistently closely fit the CLEVR training data for a variety of FiLM generator and FiLM-ed network configurations we tried. Many of the architectural details we choose for our best model, such as the GRU with linear decoder, max-pooling, and batch normalization, were selected to regularize the model so that it would generalize well.
    % @Vincent: Should we include the above commented out paragraph? Might be helpful for others who want to use FiLM elsewhere?
    
\begin{figure}[h]
	\centering
	\includegraphics[width=0.75\linewidth]{best-model-curves.png}
	\caption{Best model training and validation curves.}
    \label{fig:training-curves}
\end{figure}
    
\begin{figure}[th!]
   	\small{
      \begin{tabular}{m{4cm}m{4cm}}
  	  \textbf{Q:} \textit{Is there a big brown object of the same shape as the green thing?} \textbf{A:} \textit{Yes} (\textbf{P:} \textit{No})
  	  & 
      \textbf{Q:} \textit{What number of other things are the same material as the big gray cylinder?} \textbf{A:} \textit{6} (\textbf{P:} \textit{5})
      \\
      \includegraphics[width=\linewidth]{CLEVR_val_000000.png}
	  & 	
  	  \includegraphics[width=\linewidth]{CLEVR_val_000001.png}
      \\
      \includegraphics[width=\linewidth]{CLEVR_val_000018.png}
	  &
  	  \includegraphics[width=\linewidth]{CLEVR_val_000095.png}
	  \cr
	  \\
      \textbf{Q:} \textit{What shape is the big metal thing that is the same color as the small cylinder?} \textbf{A:} \textit{Cylinder} (\textbf{P:} \textit{Sphere})
      &
      \textbf{Q:} \textit{How many other things are the same material as the tiny sphere?} \textbf{A:} \textit{3} (\textbf{P:} \textit{2})
	  \end{tabular}
    }
    \caption{Some image-question pairs where our model predicts incorrectly. Most errors we observe are due to partially occluded objects, as highlighted in the three first examples.}
    \label{fig:errors}
\end{figure}

\begin{figure}[th!]
    \centering
    \includegraphics[width=0.7\linewidth]{CLEVR_val_000001.png}
    \scriptsize
    \begin{tabular}{|l|l|}
    \hline
    Question & Answer\\
    \hline
    How many gray things are there? & 1\\
    How many cyan things are there? & 2\\
    Are there as many gray things as cyan things? & \bf{Yes}\\
    Are there more gray things than cyan things? & No\\
    Are there fewer gray things than cyan things? & Yes\\
    \hline
    \end{tabular}
    \caption{An interesting failure example where our model counts correctly but compares counts erroneously. Its third answer is incorrect and inconsistent with its other answers.}
    \label{fig:logic-error}
\end{figure}

\subsection{What Do FiLM Layers Learn?} \label{appendix-what-is-learned}
	We visualize FiLM's effect on a single arbitrary feature map in Figures~\ref{fig:film-effect-1} and~\ref{fig:film-effect-2}. We also show histograms of per-layer $\gamma_{i,c}$ values, per-layer $\beta_{i,c}$ values, and per-channel FiLM parameter statistics in Figures~\ref{fig:gammas-layerwise},~\ref{fig:betas-layerwise}, and~\ref{fig:film-stats}, respectively.

\begin{figure*}[th]
   \small{
           \begin{tabular}{ccm{3cm}m{3cm}m{3cm}m{3cm}}
         \multirow{1}{*}{
         \makecell{
 		\includegraphics[width=0.2\linewidth]{Img25/CLEVR_val_000025.png} \\ Feature 14 - Block 1
         }}
         & 
         \rotatebox[origin=c]{90}{Before FiLM}
         &
         \includegraphics[width=3cm]{Img25/b-1-0-14-before.png}
         & 
         \includegraphics[width=3cm]{Img25/b-2-0-14-before.png}
         & 
   		\includegraphics[width=3cm]{Img25/b-3-0-14-before.png}
         & 
         \includegraphics[width=3cm]{Img25/b-4-0-14-before.png}
         \\
         &        
         \rotatebox[origin=c]{90}{After FiLM}
         &
         \includegraphics[width=3cm]{Img25/b-1-0-14-after.png}
         & 
         \includegraphics[width=3cm]{Img25/b-2-0-14-after.png}
         & 
   		\includegraphics[width=3cm]{Img25/b-3-0-14-after.png}
         &
         \includegraphics[width=3cm]{Img25/b-4-0-14-after.png}
         \\
         &
         &
         \textbf{Q:} \textit{What is the color of the large rubber cylinder?} \textbf{A:} \textit{Cyan}
         & 
         \textbf{Q:} \textit{What is the color of the large rubber sphere?} \textbf{A:} \textit{Gray}
         & 
   		\textbf{Q:} \textit{What is the color of the cube?} \textbf{A:} \textit{Yellow}
         &
         \textbf{Q:} \textit{How many cylinders are there?}  \textbf{A:} \textit{4}        
\\
&&&&&
\\
         \multirow{1}{*}{
         \makecell{
 		\includegraphics[width=0.2\linewidth]{Img32/CLEVR_val_000032.png}\\ Feature 14 - Block 1
         }}
         &
         \rotatebox[origin=c]{90}{Before FiLM}
         &
         \includegraphics[width=3cm]{Img32/b-1-0-14-before.png}
         & 
         \includegraphics[width=3cm]{Img32/b-2-0-14-before.png}
         & 
   		\includegraphics[width=3cm]{Img32/b-3-0-14-before.png}
         & 
         \includegraphics[width=3cm]{Img32/b-4-0-14-before.png}
         \\
         &        
         \rotatebox[origin=c]{90}{After FiLM}
         &
         \includegraphics[width=3cm]{Img32/b-1-0-14-after.png}
         & 
         \includegraphics[width=3cm]{Img32/b-2-0-14-after.png}
         &  
   		\includegraphics[width=3cm]{Img32/b-3-0-14-after.png}
         &
         \includegraphics[width=3cm]{Img32/b-4-0-14-after.png}
         \\
         &
         &
         \textbf{Q:} \textit{What is the color of the large rubber cylinder?} \textbf{A:} \textit{Yellow}
         & 
         \textbf{Q:} \textit{What is the color of the large rubber sphere?} \textbf{A:} \textit{Gray}
         & 
   	 	\textbf{Q:} \textit{What is the color of the cube?} \textbf{A:} \textit{Yellow}
         &
         \textbf{Q:} \textit{How many cylinders are there?}  \textbf{A:} \textit{4}        
 	\end{tabular}
    }
      \caption{Visualizations of feature map activations (scaled from 0 to 1) before and after FiLM for a \textit{single} arbitrary feature map from the first ResBlock. This particular feature map seems to detect gray and brown colors. Interestingly, FiLM modifies activations for specifically colored objects for color-specific questions but leaves activations alone for color-agnostic questions. Note that since this is the first FiLM layer, pre-FiLM activations (Rows 1 and 3) for all questions are identical, and differences in post-FiLM activations (Rows 2 and 4) are solely due FiLM's use of question information.}
      \label{fig:film-effect-1}
\end{figure*}
  
\begin{figure*}[th]
     	\small{
           \begin{tabular}{ccm{3cm}m{3cm}m{3cm}m{3cm}}
         \multirow{1}{*}{
         \makecell{
 		\includegraphics[width=0.2\linewidth]{Img25/CLEVR_val_000025.png} \\ Feature 79 - Block 4
         }}
         & 
         \rotatebox[origin=c]{90}{Before FiLM}
         &
         \includegraphics[width=3cm]{Img25/b-1-1-96-before.png}
         & 
         \includegraphics[width=3cm]{Img25/b-2-1-96-before.png}
         & 
   		\includegraphics[width=3cm]{Img25/b-3-1-96-before.png}
         & 
         \includegraphics[width=3cm]{Img25/b-4-1-96-before.png}
         \\
         &        
         \rotatebox[origin=c]{90}{After FiLM}
         &
         \includegraphics[width=3cm]{Img25/b-1-0-14-after.png}
         & 
         \includegraphics[width=3cm]{Img25/b-2-0-14-after.png}
         & 
   		\includegraphics[width=3cm]{Img25/b-3-0-14-after.png}
         &
         \includegraphics[width=3cm]{Img25/b-4-0-14-after.png}
         \\
         &
         &
         \textbf{Q:} \textit{How many cyan objects are behind the gray sphere?} \textbf{A:} \textit{2}
         & 
         \textbf{Q:} \textit{How many cyan objects are in front of the gray sphere?} \textbf{A:} \textit{1}
         & 
   		\textbf{Q:} \textit{How many cyan objects are left of the gray sphere?} \textbf{A:} \textit{2}
         &
         \textbf{Q:} \textit{How many cyan objects are right of the gray sphere?}  \textbf{A:} \textit{1}        
 	\end{tabular}
    }
      \caption{Visualization of the impact of FiLM for a \textit{single} arbitrary feature map from the last ResBlock. This particular feature map seems to focus on spatial features (\textit{i.e.} front/back or left/right) Note that since this is the last FiLM layer, the top row activations have already been influenced by question information via several FiLM layers.}
      \label{fig:film-effect-2}
\end{figure*}

\begin{figure*}[h]
        \begin{subfigure}[b]{0.25\textwidth}
                \includegraphics[width=\linewidth]{Statsv2/Gammas__Layer_1}
        \end{subfigure}%
        \begin{subfigure}[b]{0.25\textwidth}
                \includegraphics[width=\linewidth]{Statsv2/Gammas__Layer_2}
        \end{subfigure}%
        \begin{subfigure}[b]{0.25\textwidth}
                \includegraphics[width=\linewidth]{Statsv2/Gammas__Layer_3}
        \end{subfigure}%
        \begin{subfigure}[b]{0.25\textwidth}
                \includegraphics[width=\linewidth]{Statsv2/Gammas__Layer_4}
        \end{subfigure}
        \caption{Histograms of $\gamma_{i,c}$ values for each FiLM layer (layers 1-4 from left to right), computed on CLEVR's validation set. Plots are scaled identically. FiLM layers appear gradually more selective and higher variance.}
        \label{fig:gammas-layerwise}
\end{figure*}
  
\begin{figure*}[h]
        \begin{subfigure}[b]{0.25\textwidth}
                \includegraphics[width=\linewidth]{Statsv2/Betas__Layer_1}
        \end{subfigure}%
        \begin{subfigure}[b]{0.25\textwidth}
                \includegraphics[width=\linewidth]{Statsv2/Betas__Layer_2}
        \end{subfigure}%
        \begin{subfigure}[b]{0.25\textwidth}
                \includegraphics[width=\linewidth]{Statsv2/Betas__Layer_3}
        \end{subfigure}%
        \begin{subfigure}[b]{0.25\textwidth}
                \includegraphics[width=\linewidth]{Statsv2/Betas__Layer_4}
        \end{subfigure}
        \caption{Histograms of $\beta_{i,c}$ values for each FiLM layer (layers 1-4 from left to right) computed on CLEVR's validation set. Plots are scaled identically. $\beta_{i,c}$ values take a different, higher variance distribution in the first layer than in later layers.}
        \label{fig:betas-layerwise}
\end{figure*}

\begin{figure*}[h]
        \begin{subfigure}[b]{0.25\textwidth}
                \includegraphics[width=\linewidth]{Statsv2/Gamma_Means}
        \end{subfigure}%
        \begin{subfigure}[b]{0.25\textwidth}
                \includegraphics[width=\linewidth]{Statsv2/Gamma_SDs}
        \end{subfigure}%
        \begin{subfigure}[b]{0.25\textwidth}
                \includegraphics[width=\linewidth]{Statsv2/Beta_Means}
        \end{subfigure}%
        \begin{subfigure}[b]{0.25\textwidth}
                \includegraphics[width=\linewidth]{Statsv2/Beta_SDs}
        \end{subfigure}
        \caption{Histograms of per-channel $\gamma_{c}$ and $\beta_{c}$ statistics (mean and standard deviation) computed on CLEVR's validation set. From left to right: $\gamma_{c}$ means, $\gamma_{c}$ standard deviations, $\beta_{c}$ means, $\beta_{c}$ standard deviations. Different feature maps are modulated by FiLM in different patterns; some are often zero-ed out while other rarely are, some are consistently scaled or shifted by similar values while others by high variance values, etc.}\label{fig:film-stats}
\end{figure*}

% 	\subsection{CLEVR-Humans}
%         \begin{figure}[ht]
%         \small{
%         \centering
%         \includegraphics[width=0.7\linewidth]{CLEVR_val_000003.png}
%         \scriptsize
%         \begin{tabular}{|l|l|l|}
%         \hline
%         \textit{How many}      & Train CLEVR & Train CLEVR, \\
%         \textit{things are...} &             & fine-tune human, \\
%         \hline
%         ...yellow? & 6 & 1 \\
%         ...blue? & 4 & 1 \\
%         ...purple? & 6 & 2 \\
%         ...brown? & 6 & 2 \\
%         ...gray? & 6 & 1 \\
% %         ...there? & 7 & 7 \\
%         \hline
%         \end{tabular}
%         \caption{An example of what the model learns upon fine-tuning on the more diverse CLEVR-Humans questions. For fair comparison, all questions use original CLEVR vocabulary but sentence phrasing not seen in CLEVR.}
%         \label{fig:CLEVR-Humans-improvement}
%         }
%         \end{figure}

% NB: Plot the following numbers for sample efficiency (20 epoch runs with (num_examples, percent accuracy) best validation result pairs):
% [(1367,42.63),(2734,43.98),(5469,45.24),(10938,46.64),(21875,47.8),(43750,49.62),(87500,61.6),(1312500,73.41),(175000,77.31),(262500,84.3),(350000,91.62),(525000,95.53),(700000,96.59)]

% Conversion to CMYK command: gs -dSAFER -dBATCH -dNOPAUSE -dNOCACHE -sDEVICE=pdfwrite -sColorConversionStrategy=CMYK -dProcessColorModel=/DeviceCMYK  -sOutputFile=output.pdf input.pdf

\end{document}



% !TEX root =  cikm_main.tex

% Nowadays, web search is used by million of users to fulfil their information need.
% Search engines return a list of web pages that are likely to be relevant to the user need, expressed by the means of a textual query. Predicting relevance is a complex task involving a degree of semantic understanding of documents and queries.
% In spite of this, retrieval algorithms that rely on exact word or phrase matching historically exhibited strong performance~\cite{zhai_statistical_2008}.
% The path towards more complex semantic matching methods has been slow and generally proved to yield performance in-par or inferior to traditional models, if semantic matching is used alone~\cite{clinchant2013aggregating,mitra2016}.

% The current trend in semantic retrieval models is to map documents and queries to vectors and correlate relevance to a distance measure in the vector space~\cite{shen2014latent}.
% Given that a typical web document may contain several thousands of words, a single vector seems insufficient to obtain a fine-grained representation of its content.
% Some works address this problem by encoding only the title part of the web document,~i.e. a hand-crafted summary tempering the information overflow occurring for long documents,
% and matching queries to the document titles~\cite{huang2013learning,shen2014latent}.
% However, a title conveys only broad information about the document which may not be sufficient to assess relevance, especially for rare or more focused queries asking for very specific details described in the document.

% When asked to assess relevance, a human judge is likely to skim through the document by identifying possible relevant passages and by aggregating the evidence gathered from those passages.
% Bag-of-words matching methods primitively emulate this behaviour by solely considering the presence of query words in the document while discarding the rest of the information content.
% More recent exact matching passage retrieval models explicitly segment a document into passages of fixed-size and compute the score of a document given each passage score~\cite{bendersky2008utilizing}. However, the passages are often scored independently of each other and the overall document score is obtained by simple aggregation functions, for example by taking the maximum scoring passage. 
% Our view is that semantic matching methods should also learn how to pay attention only to the parts of a document that may concur in having an impact on relevance.

Recently, the idea of training machine comprehension models that can read, understand, and answer questions about a text has come closer to reality principally through two factors.  The first is the advent of deep learning techniques~\cite{bengio_book}, which allow manipulation of natural language beyond its surface forms and generalize beyond relatively small amounts of labeled data.  The second factor is the formulation of standard machine comprehension benchmarks based on Cloze-style queries~\cite{hill2015goldilocks,hermann2015teaching}, which permit fast integration loops between model conception and experimental evaluation.

Cloze-style queries~\cite{taylor1953} are created by deleting a particular word in a natural-language statement. The task is to guess which word was deleted. In a pragmatic approach, recent work~\cite{hill2015goldilocks} formed such questions by extracting a sentence from a larger document. In contrast to considering a stand-alone statement, the system is now required to handle a larger amount of information that may possibly influence the prediction of the missing word. Such contextual dependencies may also be injected by removing a word from a short human-crafted summary of a larger body of text. The abstractive nature of the summary is likely to demand a higher level of comprehension of the original text~\cite{hermann2015teaching}. In both cases, the machine comprehension system is presented with an ablated query and the document to which the original query refers. The missing word is assumed to appear in the document.

Encouraged by the recent success of deep learning attention architectures~\cite{bahdanau2014neural,sukhbaatar2015end}, we propose a novel neural attention-based inference model designed to perform machine reading comprehension tasks.
The model first reads the document and the query using a recurrent neural network~\cite{bengio_book}.
%This associates each word of the input text to a semantic representation,~{\it i.e.}, a vector in a low-dimensional space that encodes its semantic content in the context of the preceding and following tokens.
Then, it deploys an iterative inference process to uncover the inferential links that exist between the missing query word, the query, and the document. This phase involves a novel alternating attention mechanism; it first attends to some parts of the query, then finds their corresponding matches by attending to the document. 
The result of this alternating search is fed back into the iterative inference process to seed the next search step. This permits our model to reason about different parts of the query in a sequential way, based on the information that has been gathered previously from the document. After a fixed number of iterations, the model uses a summary of its inference process to predict the answer.
%Here, we use the Attentive-Search mechanism to ``point out'' the part of the document that answers the query.
%Attentive-Search first decomposes the document and the query into a set of sub-passages that constitute the targets of the attention mechanism.
%Each passage is associated to a semantic representation,~{\it i.e.}, a vector in a low-dimensional space that encodes its information content.
%Document passages are assumed to be atomic units of information, while query passages are considered as different aspects of the information that must be matched in the document.
%The model estimates the probability of relevance by means of an iterative process.
%At each iteration, the model applies an attentive search module that first selects some of the query passages as search keys, then retrieves the document passages that are relevant to the selected query passages.
%Mathematically, this iterative attentive process is a discrete-time dynamical system.

This paper makes the following contributions. We present a novel iterative, alternating attention mechanism that, unlike existing models~\cite{hill2015goldilocks,watson}, does not compress the query to a single representation, but instead alternates its attention between the query and the document to obtain a fine-grained query representation within a fixed computation time. Our architecture tightly integrates previous ideas related to bidirectional readers~\cite{watson} and iterative attention processes~\cite{hill2015goldilocks,sukhbaatar2015end}. It obtains state-of-the-art results on two machine comprehension datasets and shows promise for application to a broad range of natural language processing tasks.


% is related to the degree to which different aspects of the information need have been satisfied. (Intuitively, the particular information I am seeking now depends on the information I have already obtained.)

% Unique from existing attention models, our query representation changes at each step of the search process.
% This enforces the idea that relevance is related to the degree to which different aspects of the information need have been satisfied. (Intuitively, the particular information I am seeking now depends on the information I have already obtained.)
% After a fixed number of iterations, the model uses a summary of its search process to predict the relevance score for each document passage. The passage with the highest score is considered the answer to the query.

%The entire Attentive-Search model, from passage encoding through iterative attention to answer scoring, consists of neural networks that can be trained end-to-end by backpropagation.

%A number of similar deep neural models has been developed recently for the task of machine comprehension~\citep{hermann2015teaching, Weston-etal:2015, hill2015, kadlec2016}, most of which employ an attention mechanism.
%Simultaneously, several large-scale corpora suited to the data-hungry deep learning process have been released.
%In this paper, we focus on the Children's Book Test (CBT)~\citep{hill2015} and the CNN corpus~\citep{hermann2015teaching}.
%In the first, the documents are 20-sentence excerpts from well-known childen's books; in the second, the documents are news articles from the CNN website.
%We train and evaluate the Attentive-Search model on these MC datasets and achieve state-of-the-art performance.

%This paper is organized as follows,...

% \begin{figure}[t]
% \centering
% \includegraphics[scale=0.5]{prob-of-rel.pdf}
% \caption{\label{fig:overview}
% Attentive-Search architecture:
% 1) The document $d$ and the query $q$ are converted to a set of vectorial passage representations using a convolutional encoder.
% 2) A recurrent neural network controls the search process by iteratively applying an attentive search module.
% 3) After a fixed number of steps, the last state of the recurrent network is used to estimate the probability of relevance.}
% \end{figure}

% We train and test our models on publicly available TREC collections and queries.
% We deal with the scarce number of relevance assessments available in the public domain by pre-training the model using a surrogate task and then fine-tuning on a small labeled dataset.
% Our model compares favourably to classical bag-of-words models and existing semantic models.
% We obtain results on par with a strong learning-to-rank baseline employing a large number of features.
% Additional improvements are obtained by integrating our model as an additional feature into the learning-to-rank approach. To our knowledge, this is the first work applying neural attention to IR and to demonstrate the feasibility of training such models on publicly available corpora.




\label{sec:task}
One of the advantages of using Cloze-style questions to evaluate machine comprehension systems is that a sufficient amount of training and test data can be obtained without human intervention. The CBT~\cite{hill2015goldilocks} and CNN~\cite{hermann2015teaching} corpora are two such datasets.

The CBT\footnote{Available at \url{http://www.thespermwhale.com/jaseweston/babi/CBTest.tgz}.} corpus was generated from well-known children's books available through Project Gutenberg. Documents consist of 20-sentence excerpts from these books. The related query is formed from an excerpt's 21st sentence by replacing a single word with an anonymous placeholder token. The dataset is divided into four subsets depending on the type of the word replaced.
The subsets are named entity, common noun, verb, and preposition. We will focus our evaluation solely on the first two subsets,~i.e. CBT-NE (named entity) and CBT-CN (common nouns), since the latter two are relatively simple as demonstrated by~\cite{hill2015goldilocks}.

The CNN\footnote{Available at \url{https://github.com/deepmind/rc-data}.} corpus was generated from news articles available through the CNN website. The documents are given by the full articles themselves, which are accompanied by short, bullet-point summary statements. Instead of extracting a query from the articles themselves, the authors replace a named entity within each article summary with an anonymous placeholder token.
\begin{table}[t]
\centering
\vspace{3mm}
\begin{tabular}{lrrrr}
\toprule
&  & CBT-NE & CBT-CN & CNN \\
\midrule
\# Train &  & 108,719 & 120,769 & 380,298      \\
\# Valid &  & 2,000    & 2,000      & 3,924    \\
\# Test  &  & 2,500    & 2,500      & 3,198    \\
\# Cand. ($|\mathcal{A}|$) & & 10 & 10        & $\sim$26 \\
Avg. $|\mathcal{D}|$ && $\sim$430 & $\sim$460 & $\sim$750 \\
\bottomrule
\end{tabular}
\caption{\label{tab:stats}Statistics of CBT-NE, CBT-CN and CNN.}
\end{table}

For both datasets, the training and evaluation data consist of tuples $(\mathcal{Q}, \mathcal{D}, \mathcal{A}, a)$, where $\mathcal{Q}$ is the query (represented as a sequence of words), $\mathcal{D}$ is the document, $\mathcal{A}$ is the set of possible answers, and $a \in \mathcal{A}$ is the correct answer. All words come from a vocabulary $V$, and, by construction, $\mathcal{A} \subset \mathcal{D}$. For each query, a placeholder token is substituted for the real answer $a$. Statistics on the datasets are reported in Table~\ref{tab:stats}.





Our model is represented in Fig.~\ref{fig:model}. Its workflow has three steps.
First is the \emph{encoding} phase, in which we compute a set of vector representations, acting as a memory of the content of the input document and query.
Next, the \emph{inference} phase aims to untangle the complex semantic relationships linking the document and the query in order to provide sufficiently strong evidence for the answer prediction to be successful.
To accomplish this, we use an iterative process that, at each iteration, alternates attentive memory accesses to the query and the document.
Finally, the \emph{prediction} phase uses the information gathered from the repeated attentions through the query and the document to maximize the probability of the correct answer.
We describe each of the phases in the following sections.

% In TWIA, we opt for a two-way, iterative search phase: at each iteration, the model first chooses some query passages that would be  in the document and stores the resulting relevant document passages.
% The iterative process allows the model to focus on different aspects of the query and the document on the basis of the information that has been obtained in the previous time steps. 
% The information gathered from the document is summarized by a recurrent neural network~\cite{bengio_book}, described in Section~\ref{sec:iterative}.

\begin{figure*}[t]
    \centering
    \includegraphics[scale=1.2]{bidir_full.pdf}
    \caption{Our model first encodes the query and the document by means of bidirectional GRU networks. Then, it deploys an iterative inference mechanism that alternates between attending query encodings (1) and document encodings (2) given the query attended state. The results of the alternating attention is gated and fed back into the inference GRU. Even if the encodings are computed only once, the query representation is dynamic and changes throughout the inference process. After a fixed number of steps $T$, the weights of the document attention are used to estimate the probability of the answer $P(a|\mathcal{Q}, \mathcal{D})$.}
    \label{fig:model}
\end{figure*}

\subsection{Bidirectional Encoding}
The input to the encoding phase is a sequence of words $\mathcal{X} =  (x_1, \ldots, x_{|\mathcal{X}|})$, such as a document or a query, drawn from a vocabulary $V$.
Each word is represented by a continuous word embedding $\mathbf{x} \in \mathbb{R}^d$ stored in a word embedding matrix $\mathbf{X} \in \mathbb{R}^{|V| \times d}$.
The sequence $\mathcal{X}$ is processed using a recurrent neural network encoder~\cite{bengio_book} with gated recurrent units (GRU)~\cite{GRU}. For each position $i$ in the input sequence, the GRU takes as input the word embedding $\mathbf{x}_i$ and updates a hidden state $\mathbf{h}_{i-1}$ to $\mathbf{h}_i = f(\mathbf{x}_i, \mathbf{h}_{i-1})$, where $f$ is defined by:
\begin{equation}
\label{eq:gru}
\begin{split}
& \b r_{i} = \sigma ( \b I_r\, \b x_i + \b H_r\b  h_{i-1}),\\
& \b u_{i} = \sigma ( \b I_u\, \b x_i + \b H_u\b  h_{i-1} ), \\
& \b{\bar h}_i = \text{tanh} (\b I_h\, \b x_i +\b  H_h (\b r_i \cdot \b h_{i-1})), \\
& \b h_i = (1 - \b u_i) \cdot \b h_{i-1} + \b u_i \cdot \b{\bar h}_i , \\
\end{split}
\end{equation}
where $\b h_i, \b r_i$ and $\b u_i \in \mathbb{R}^h$ are the recurrent state, the reset gate and update gate respectively, $\b I_{\{r,u,h\}} \in \mathbb{R}^{h \times d}$, $\b H_{\{r,u,h\}} \in \mathbb{R}^{h \times h}$ are the parameters of the GRU, $\sigma$ is the sigmoid function and $\cdot$ is the element-wise multiplication.
The hidden state $\mathbf{h}_i$ acts as a representation of the word $x_i$ in the context of the preceding sequence inputs $x_{<i}$. In order to incorporate information from the future tokens $x_{>i}$, we choose to process the sequence in reverse with an additional GRU~\cite{watson}.
Therefore, the encoding phase maps each token $x_i$ to a contextual representation given by the concatenation of the forward and backward GRU hidden states $\mathbf{\tilde{x}}_i = [\overrightarrow{\mathbf{h}_i}, \overleftarrow{\mathbf{h}_i}]$. We denote by $\mathbf{\tilde{q}}_i \in \mathbb{R}^{2h}$ and $\mathbf{\tilde{d}}_i \in \mathbb{R}^{2h}$ the contextual encodings for word $i$ in the query $\mathcal{Q}$ and the document $\mathcal{D}$ respectively.


\subsection{Iterative Alternating Attention}
\label{sec:iterative}
This phase can be considered a means to uncover a possible inference chain that starts at the query and the document and leads to the answer.
The inference is modelled by an additional recurrent GRU network. The recurrent network iteratively performs an alternating search step to gather information that may be useful to predict the answer.
In particular, at each time step: (1) it performs an attentive read on the query encodings, resulting in a query glimpse, $\mathbf{q}_t$, and (2) given the current query glimpse, it extracts a conditional document glimpse, $\mathbf{d}_t$, representing the parts of the document that are relevant to the current query glimpse. In turn, both attentive reads are conditioned on the previous hidden state of the inference GRU $\mathbf{s}_{t-1}$, summarizing the information that has been gathered from the query and the document up to time $t$. The inference GRU uses both glimpses to update its recurrent state and thus decides which information needs to be gathered to complete the inference process.

\paragraph{Query Attentive Read} Given the query encodings $\{\mathbf{\tilde{q}}_i\}$, we formulate a query glimpse $\mathbf{q}_t$ at timestep $t$ by:
\[
q_{i,\, t} = \underset{i=1,\ldots,\mathcal{|Q|}}{\text{softmax}}\; \mathbf{\tilde{q}}_i^\top(\mathbf{A}_q \, \mathbf{s}_{t-1} + \mathbf{a}_q),
\]
\[
\mathbf{q}_t = \sum_i q_{i,\, t}\; \mathbf{\tilde{q}}_i
\]
where $q_{i,\, t}$ are the query attention weights and $\mathbf{A}_q \in \mathbb{R}^{2h \times s}$, where $s$ is the dimensionality of the inference GRU state, and $\mathbf{a}_q \in \mathbb{R}^{2h}$.
The attention we use here is similar to the formulation used in~\cite{hill2015goldilocks,sukhbaatar2015end}, but with two differences. First, we use a bilinear term instead of a simple dot product in order to compute the importance of each query term in the current time step. This simple bilinear attention has been successfully used in~\cite{luong-pham-manning:2015:EMNLP}. Second, we add a term $\mathbf{a}_q$ that allows to bias the attention mechanism towards words which tend to be important across the questions independently of the search key $\mathbf{s}_{t-1}$. % Intuitively, $\mathbf{a}_q$ may be considered as asking ``what are the important words in the question?'' and
This is similar to what is achieved by the original attention mechanism proposed in~\cite{bahdanau2014neural} without the burden of the additional $\tanh$ layer.

\paragraph{Document Attentive Read} The alternating attention continues by probing the document given the current query glimpse $\mathbf{q}_t$.
In particular, the document attention weights are computed based on both the previous search state and the currently selected query glimpse $\mathbf{q}_t$:
\[
d_{i,\, t} = \underset{i = 1, \ldots, \mathcal{|D|}}{\text{softmax}}\; \mathbf{\tilde{d}}_i^\top(\mathbf{A}_d \, [\mathbf{s}_{t-1}, \mathbf{q}_t] + \mathbf{a}_d),
\]
\[
\mathbf{d}_t = \sum_i d_{i,\, t}\; \mathbf{\tilde{d}}_i,
\]
where $d_{i,\, t}$ are the attention weights for each word in the document and $\mathbf{A}_d \in \mathbb{R}^{2h \times (s + 2h)}$ and $\mathbf{a}_d \in \mathbb{R}^{2h}$.
Note that the document attention is also conditioned on $\mathbf{s}_{t - 1}$. This allows the model to perform transitive reasoning on the document side,~i.e.~to use previously obtained document information to bias future attended locations, which is particularly important for natural language inference tasks~\cite{sukhbaatar2015end}.

\paragraph{Gating Search Results} In order to update its recurrent state, the inference GRU may evolve on the basis of the information gathered from the current inference step,~i.e. $\mathbf{s}_t = f([\mathbf{q}_t, \mathbf{d}_t], \mathbf{s}_{t-1})$, where $f$ is defined in Eq.~\ref{eq:gru}. However, the current query glimpse may be too general or the document may not contain the information specified in the query glimpse,~i.e.~the query or the document attention weights may be nearly uniform. We include a gating mechanism that is designed to reset the current query and document glimpses in the case that the current search is not fruitful. Formally, we implement a gating mechanism $\mathbf{r} = g([\mathbf{s}_{t-1}, \mathbf{q}_t, \mathbf{d}_t, \mathbf{q}_t \cdot \mathbf{d}_t])$, where $\cdot$ is the element-wise multiplication and $g : \mathbb{R}^{s + 6h} \rightarrow \mathbb{R}^{2h}$. The gate $g$ takes the form of a 2-layer feed-forward network with sigmoid output unit activation. The fourth argument of the gate takes into account multiplicative interactions between query and document glimpses, making it easier to determine the degree of matching between them. Given a query gate $g_q$, producing $\mathbf{r}_q$, and a document gate $g_d$, producing $\mathbf{r}_d$, the inputs of the inference GRU are given by the reset version of the query and document glimpses,~i.e., $\mathbf{s}_t = f([\mathbf{r}_q \cdot \mathbf{q}_t, \mathbf{r}_d \cdot \mathbf{d}_t], \mathbf{s}_{t-1})$. Intuitively, the model reviews the query glimpse with respect to the contents of the document glimpse and {\it vice versa}.

\subsection{Answer Prediction}
After a fixed number of time-steps $T$, the document attention weights obtained in the last search step $d_{i, T}$ are used to predict the probability of the answer given the document and the query $P(a | \mathcal{Q}, \mathcal{D})$. Formally, we follow~\cite{watson} and apply the ``pointer-sum'' loss:
\begin{equation}
\label{eq:answer_prob}
P(a | \mathcal{Q}, \mathcal{D}) = \sum_{i \in I(a, \mathcal{D})} d_{i, T},
\end{equation}
where $I(a, \mathcal{D})$ is a set of positions where $a$ occurs in the document. The model is trained to maximize $\log P(a | \mathcal{Q}, \mathcal{D})$ over the training corpus.




\begin{table*}[t]
\small
\def\arraystretch{1.1}
\centering
\begin{tabular}{llccccc}
\toprule
\# & Model & \multicolumn{2}{c}{CBT-NE} & \phantom{ab} & \multicolumn{2}{c}{CBT-CN} \\
&& Valid & Test & & Valid & Test \\
\midrule
\small{1}&Humans (query) $^{1}$   &  - & 52.0 &  & - & 64.4 \\
\small 2&Humans (context+query) $^{1}$   & - & 81.6 &  & - & 81.6 \\
\midrule
\small3&LSTMs (context+query) $^{1}$    & 51.2  & 41.8 &  & 62.6 & 56.0 \\
\midrule
\small4&MemNNs (window memory + self-sup.) $^{1}$   & 70.4 & 66.6 &  & 64.2 & 63.0 \\
\small5&AS Reader $^{2}$                            & 73.8 & 
\textbf{68.6} &  & 68.8 & 63.4 \\
\midrule
\small6&Ours (fixed query attention) & 73.3 & 66.0 &  & 69.9 & 64.3 \\
\small7&Ours & \textbf{75.2} & \textbf{68.6} &  & \textbf{72.1} & \textbf{69.2} \\
\midrule
\midrule
\small8&AS Reader (Ensemble) $^2$ & 74.5 & 70.6 && 71.1 & 68.9 \\
\small9&Ours (Ensemble) & \textbf{76.9} & \textbf{72.0} && \textbf{74.1} & \textbf{71.0} \\
\bottomrule
\end{tabular}
\caption{\label{tab:cbt}Results on the CBT-NE (named entity) and CBT-CN (common noun) datasets. Results marked with $^1$ are from \protect\cite{hill2015goldilocks} and those marked with $^2$ are from \protect\cite{watson}.}
\end{table*}

To train our model, we used stochastic gradient descent with the ADAM optimizer~\cite{adam}, with an initial learning rate of 0.001. We set the batch size to 32 and we decay the learning rate by 0.8 if the accuracy on the validation set does not increase after a half-epoch,~i.e.~2000 batches (for CBT) and 5000 batches for (CNN).
We initialize all weights of our model by sampling from the normal distribution $\mathcal{N}$(0, 0.05). Following~\cite{saxe2013exact}, the GRU recurrent weights are initialized to be orthogonal and biases are initialized to zero.
In order to stabilize the learning, we clip the gradients if their norm is greater than 5~\cite{Pascanu13}.
We performed a hyperparameter search with embedding regularization in $\{0.001, 0.0001\}$, inference steps $T \in \{3, 5, 8\}$, embedding size $d \in \{256, 384\}$, encoder size $h \in \{128, 256\}$ and the inference GRU size $s \in \{256, 512\}$. We regularize our model by applying a dropout~\cite{srivastava2014dropout} rate of 0.2 on the input embeddings, on the search gates and on the inputs to both the query and the document attention mechanisms. We found that setting embedding regularization to $0.0001$, $T=8$, $d=384$, $h=128$, $s=512$ worked robustly across the datasets. Our model is implemented in Theano~\cite{Theano12}, using the Keras~\cite{chollet2015keras} library.

\paragraph{Computational Complexity} Similar to previous state-of-the-art models~\cite{watson,danqi} which use a bidirectional encoder, the major bottleneck of our method is computing the document and query encodings. The alternating attention mechanism runs only for a fixed number of steps ($T = 8$ in our tests), which is orders of magnitude smaller than a typical document or query in our datasets (see Table~\ref{tab:stats}).
The repeated attentions each require a softmax over ${\sim}$1000 locations which is typically fast on recent GPU architectures. Thus, our computation cost is comparable to~\cite{watson,danqi}, but we outperform the latter models on the datasets tested.



We report the results of our model on the CBT-CN, CBT-NE and CNN datasets, previously described in Section~\ref{sec:task}.

%\begin{table}
%\centering
%\caption{Proportion of CBT validation and test examples in which the answer has already been predicted in an example of the training set.}
%\vspace{2mm}
%\begin{tabular}{lccc}
%\toprule
%Category &\phantom{abcd}& Valid  & Test      \\
%\midrule
%CBT-NE   && 72.4\% & 70.4\%    \\
%CBT-CN   && 97.4\% & 95.4\%    \\
%\bottomrule
%\end{tabular}
%\end{table}

\begin{table*}[t]
\small
\def\arraystretch{1.1}
\centering
\begin{tabular}{llccc}
\toprule
\# & Model & \phantom{abcd} & \multicolumn{2}{c}{CNN} \\
&& & Valid & Test \\
\midrule
\small{1}&Word distance model $^{1}$ && 50.5 & 50.9 \\
\small{2}&Deep LSTM Reader $^{1}$  && 55.0 & 57.0 \\
\small{3}&Attentive Reader $^{1}$  && 61.6 & 63.0 \\
\small{4}&Impatient Reader $^{1}$                              && 61.8 & 63.8 \\
\midrule
\small{5}&MemNNs (window memory) $^{2}$                        && 50.8 & 60.6 \\
\small{6}&MemNNs (window memory + self sup.) $^{2}$            && 63.4 & 66.8 \\
\small{7}&AS Reader $^{3}$                                     && 68.6 & 69.9 \\
\small{8}&Ours                                                 && \textbf{72.6} & \textbf{73.3} \\
\midrule
\small{9}&Stanford AR (with GloVe) $^{4}$                      && 72.4 & 72.4 \\
\midrule
\midrule
\small{10}&MemNNs (Ensemble) $^{2}$                            && 66.2 & 69.4 \\
\small{11}&AS Reader (Ensemble) $^3$                           && 73.9 & 75.4 \\
\small{13}&Ours (Ensemble)                                     && \textbf{75.2} & \textbf{76.1} \\
\bottomrule
\end{tabular}
\caption{\label{tab:cnn}Results on the CNN datasets. Results marked with $^1$ are from \protect\cite{hermann2015teaching}, $^2$ from \protect\cite{hill2015goldilocks}, $^{3}$ from \protect\cite{watson} and $^4$ from~\protect\cite{danqi}.}
\end{table*}

\subsection{CBT}
Table~\ref{tab:cbt} reports our results on the CBT-CN and CBT-NE dataset. The Humans, LSTMs and Memory Networks (MemNNs) results are taken from~\cite{hill2015goldilocks} and the Attention-Sum Reader (AS Reader) is a state-of-the-art result recently obtained by~\cite{watson}.

\paragraph{Main result} Our model (line 7) sets a new state-of-the-art on the common noun category by gaining 3.6 and 5.6 points in validation and test over the best baseline AS Reader (line 5). This performance gap is only partially reflected on the CBT-NE dataset. We observe that the 1.4 accuracy points on the validation set do not reflect better performance on the test set, which sits on par with the best baseline. In CBT-NE, the missing word is a named entity appearing in the story which is likely to be less frequent than a common noun. We found that approximatively 27.5\% of validation examples and 29.6\% of test examples contain an answer that has never been predicted in the training set. These numbers are considerably lower for the CBT-CN, for which only 2.5\% and 4.6\% of validation and test examples respectively contain an answer that has not been previously seen.

\paragraph{Ensembles} Fusing multiple models generally achieves better generalization.
In order to investigate whether this could help achieving better held-out performance on CBT-NE, we adopt a simple strategy and average the predictions of 5 models trained with different random seeds (line 9). In this case, our ensemble outperforms the AS Reader ensemble both on CBT-CN and CBT-NE setting new state-of-the-art for this task. On CBT-NE, it achieves a validation and test performance of 76.9 and 72.0 accuracy points respectively (line 9). On CBT-CN it shows additional improvements over the single model and sits at 74.1 on validation and 71.0 on test.

\paragraph{Fixed query attention} In order to measure the impact of the query attention step in our model, we constrain the query attention weights $q_{i, t}$ to be uniform,~i.e. $q_{i, t} = 1/|\mathcal{Q}|$, for all $t = 1,\ldots,T$ (line 6). This corresponds to fixing the query representation to the average pooling over the bidirectional query encodings and is similar in spirit to previous work~\cite{watson,danqi}. By comparing line 6 and line 7, we see that the query attention mechanism allows improvements up to 2.3 points in validation and 4.9 points in test with respect to fixing the query representation throughout the search process. A similar scenario was observed on the CNN dataset.

\subsection{CNN}
Table~\ref{tab:cnn} reports our results on the CNN dataset. We compare our model with a simple word distance model, the three neural approaches from~\cite{hermann2015teaching} (Deep LSTM Reader, Attentive Reader and Impatient Reader), and with the AS reader~\cite{watson}.

\paragraph{Main result} The results show that our model (line 8) improves state-of-the-art accuracy by 4 percent absolute on validation and 3.4 on test with respect to the most recent published result (AS Reader) (line 7). We also report the very recent results of the Stanford AR system that came to our attention during the write-up of this article~\cite{danqi} (line 9).
Our model slightly improves over this strong baseline by 0.2 percent on validation and 0.9 percent on test. We note that the latter comparison may be influenced by different training and initialization strategies. First, Stanford AS uses GloVe embeddings~\cite{pennington2014glove}, pre-trained from a large external corpus.
% Due to the simultaneous nature of our work, we didn't manage to train our models by pretraining word embeddings.
Second, the system normalizes the output probabilities only over the candidate answers in the document.
%We hypothesize that this may advance the baseline, whose model capacity can be focused in discriminating only between those entities (and not between all the words in the document).

\paragraph{Ensembles} We also report the results using ensembled models. Similarly to the single model case, our ensembles achieve state-of-the-art test performance of 75.2 and 76.1 on validation and test respectively, outperforming previously published results.

\begin{table}[t]
    \centering
    \vspace{2mm}
    \begin{tabular}{llrlr}
    \toprule
    Category & \multicolumn{2}{c}{Stanford AR} & \multicolumn{2}{c}{Ours} \\
    \midrule
    
    Exact Match        & 13 & (100.0\%) & 13 & (100.0\%) \\
    Paraphrasing       & 39 & (95.1\%)  & 39 & (95.1\%)  \\
    Partial Clue        & 17 & (89.5\%)  & 16 & (84.2\%)  \\
    \midrule
    Multiple Sent.     & 1  & (50.0\%)  & 1  & (50.0\%)  \\
    Ambig. / Hard   & 1  & (5.9\%)   & 5  & (29.4\%)   \\
    \midrule
    Coref. Errors & 3  & (37.5\%)  & 3  & (37.5\%)  \\
    \midrule
    All                & 74 &   & 77 &      \\
    \bottomrule
    \end{tabular}
    \label{tab:cnn-category}
    \caption{Per-category performance of the Stanford AR and our system. The first three categories require local context matching, the next two global context matching and coreference errors are unanswerable questions~\protect\cite{danqi}.}
\end{table}

\paragraph{Category analysis}~\cite{danqi} classified a sample of 100 CNN stories based on the type of inference required to guess the answer. Categories that only require local context matching around the placeholder and the answer in the text are Exact Match, Paraphrasing, and Partial Clue, while those which require higher reasoning skills are Multiple Sentences and Ambiguous. For example, in Exact Match examples, the question placeholder and the answer in the document share several neighboring exact words.
%In contrast, in Multiple Sentences examples, one needs to account for information in multiple sentences to infer the answer.

Category-specific results are reported in Table~\ref{tab:cnn-category}. Local context categories generally seem to be easily tackled by the neural models, which perform similarly. It seems that the iterative alternating attention inference is better able to solve more difficult examples such as Ambiguous/Hard. One hypothesis is that, in contrast to Stanford AR, which uses only one fixed-query attention step, our iterative attention may better explore the documents and queries. Finally, Coreference Errors ($\sim$25\% of the corpus) includes examples with critical coreference resolution errors which may make the questions ``unanswerable''. This is a barrier to achieving accuracies considerably above 75\%~\cite{danqi}. If this estimate is accurate, our ensemble model (76.1\%) may be approaching near-optimal performance on this dataset.

% Intuitively, local and global context examples should require different number of inference steps. A possible avenue to improve our model would be to adapt the number of steps on the basis of the type of inference required.

\begin{figure}
    \centering            
    \includegraphics[width=\linewidth]{{192.pkl_plot-crop}.pdf}
    \label{fig:attention-plot}
    \vspace{-3mm}
    \caption{Visualization of the alternated attention mechanism for an article in CNN, treating about the decline of the Italian language in schools. The title of the plot is the query. Each row correspond to a timestep. The target is @entity3 which corresponds to the word ``Italian''.}
\end{figure}


\subsection{Discussion}

We inspect the query and document attention weights for an example article from the CNN dataset. The title of the article is ``Dante turns in his grave as Italian language declines'', and it discusses the decline of Italian language in schools. The plot is shown in Figure~\ref{fig:attention-plot}, where locations attended to in the query and document are in the left and right column respectively. Each row corresponds to an inference timestep $1 \leq t \leq 8$. At the first step, the query attention focuses on the placeholder token, as its local context is generally important to discriminate the answer. The model first focuses on @entity148, which corresponds to ``Greek'' in this article. At this point, the model is still uncertain about other possible locations in the document (we can observe small weights across document locations). At $t = 2$, the query attention moves towards ``schools'' and the model hesitates between ``Italian'' and ``European Union'' (@entity28, see step 3), both of which may satisfy the query.
At step 3, the most likely candidates are ``European Union'' and ``Rome'' (@entity159). As the timesteps unfold, the model learns that ``needs'' may be important to infer the correct entity,~i.e. ``Italian''. The query sits on the same attended location, while the document attention evolves to become more confident about the answer.

We find that, across CBT and CNN examples, the query attention wanders near or focuses on the placeholder location, attempting to discriminate its identity using only local context. For these particular datasets, the majority of questions can be answered after attending only to the words directly neighbouring the placeholder. This aligns with the findings of~\cite{danqi} concerning CNN, which state that the required reasoning and inference levels for this dataset are quite simple. It would be worthwhile to formulate a dataset in which the placeholder is harder to infer using only local neighboring words, and thereby necessitates deeper query exploration.

Finally, across this work we fixed the number of inference steps $T$. We found that using $8$ timesteps works well consistently across the tested datasets. However, we hypothesize that more (fewer) time-steps would benefit harder (easier) examples. A straight-forward extension of the model would be to dynamically select the number of inference steps conditioned on each example.



% !TEX root = acl_main.tex

Neural attention models have been applied recently to a sm\"org\aa sbord of machine learning and natural language processing problems. These include, but are not limited to, handwriting recognition~\cite{Graves13}, digit classification~\cite{mnih2014recurrent}, machine translation~\cite{bahdanau2014neural}, question answering~\cite{sukhbaatar2015end,hermann2015teaching} and caption generation~\cite{xu2015show}.
In general, attention models keep a memory of states that can be accessed at will by learned attention policies.
In our case, the memory is represented by the set of document and query contextual encodings.
% ~\cite{mnih2014recurrent} apply a neural attention mechanism to cluttered image classification, a task in which it is particularly important to detect the salient regions in the input images.
% ~\cite{bahdanau2014neural} deploy attention for machine translation in order to align target words with source words.
% At each step of the translation, the attention mechanism learns to select the states in the source encoder RNN that are the most useful to predict the next word in the target sentence.
% ~\cite{xu2015show} use a similar model to automatically generate a caption given an input image. The image is segmented into different patches using a convolutional architecture and the attention model picks a patch to generate each caption word.
% In our work, the memory is represented by the passage vectors extracted from the document and the query.

Our model is closely related to~\cite{sukhbaatar2015end,kumar2015ask,hermann2015teaching,watson,hill2015goldilocks}, which were also applied to question answering.
The pointer-style attention mechanism that we use to perform the final answer prediction has been proposed by~\cite{watson}, which in turn was based on the earlier Pointer Networks of~\cite{vinyals2015pointer}.  However, differently from our work,~\cite{watson} perform only one attention step and embed the query into a single vector representation, corresponding to the concatenation of the last state of the forward and backward GRU networks. To our knowledge, embedding the query into a single vector representation is a choice that is shared by most machine reading comprehension models. In our model, the repeated, tight integration between query attention and document attention allows the model to explore dynamically which parts of the query are most important to predict the answer, and then to focus on the parts of the document that are most salient to the currently-attended query components.

A similar attempt in attending different components of the query may be found in~\cite{hermann2015teaching}. In that model, the document is processed once for each query word. This can be computationally intractable for large documents, since it involves unrolling a bidirectional recurrent neural network over the entire document multiple times. In contrast, our model only estimates query and document encodings once and can learn how to attend different parts of those encodings in a fixed number of steps. The inference network is responsible for making sense of the current attention step with respect to what has been gathered before. In addition to achieving state-of-the-art performance, this technique may also prove to be more scalable than alternative query attention models.

Finally, our iterative inference process shares similarities to the iterative \emph{hops} in Memory Networks~\cite{sukhbaatar2015end,hill2015goldilocks}. In that model, the query representation is updated iteratively from hop to hop, although its different components are not attended to separately. Moreover, we substitute the simple linear update with a GRU network. The gating mechanism of the GRU network made it possible to use multiple steps of attention and to propagate the learning signal effectively back through to the first timestep.


\documentclass[11pt]{article}
\usepackage{emnlp2016}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{xspace}
\usepackage{booktabs}

\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{microtype}
\usepackage{multirow}
\usepackage{url}
\usepackage{verbatim}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{array}
\usepackage{ifthen}
\usepackage{courier}
% \usepackage[linesnumbered,vlined,ruled]{algorithm2e}
\emnlpfinalcopy

\newcommand{\E}{\mathbb{E}}
\renewcommand{\b}[1]{\mathbf{#1}}
%\renewcommand{\b}[1]{#1}

%%% <tightens up paper>
\usepackage{amsfonts}
\usepackage{microtype}
\usepackage[scaled=0.86]{helvet}

\newcommand{\captionfonts}{\small}
%
\makeatletter  % Allow the use of @ in command names
\long\def\@makecaption#1#2{%
  \vskip\abovecaptionskip
  \sbox\@tempboxa{{\captionfonts #1: #2}}%
  \ifdim \wd\@tempboxa >\hsize
    {\captionfonts #1: #2\par}
  \else
    \hbox to\hsize{\hfil\box\@tempboxa\hfil}%
  \fi
  \vskip\belowcaptionskip}
\makeatother   % Cancel the effect of \makeatletter

\renewcommand\floatpagefraction{.9}
\renewcommand\topfraction{.9}
\renewcommand\bottomfraction{.9}
\renewcommand\textfraction{.1}
\setcounter{totalnumber}{50}
\setcounter{topnumber}{50}
\setcounter{bottomnumber}{50}
\abovedisplayskip 2.0pt plus2pt minus2pt%

\belowdisplayskip \abovedisplayskip
\renewcommand{\baselinestretch}{0.99}
% make itemize stuff smaller:
\newcommand{\reduceVerticalSpace}{
\setlength{\topsep}{0pt}
\setlength{\partopsep}{0pt}
\setlength{\parskip}{0pt}
\setlength{\parsep}{0pt}
\setlength{\itemsep}{0pt}
\setlength{\itemindent}{0pt}
\setlength{\listparindent}{0pt}
}
\newcommand\itm[1]{\begin{itemize} \reduceVerticalSpace #1 \end{itemize}}
%%% </tightens up paper>

%\setlength\titlebox{6.5cm}    % Expanding the titlebox
\setlength\titlebox{6.1cm}

\usepackage{latexsym}

\author{Alessandro Sordoni$^f$ \and Philip Bachman$^f$ \and Adam Trischler$^f$ \and Yoshua Bengio$^g$ \\
$^f$Maluuba Research, Montr\'eal, Qu\'ebec \\
$^g$University of Montr\'eal, Montr\'eal, Qu\'ebec \\
\small \tt \{alessandro.sordoni, phil.bachman, adam.trischler\}@maluuba.com \\
\small \tt bengioy@iro.umontreal.ca}

\title{Iterative Alternating Neural Attention for Machine Reading}
\begin{document}

\maketitle
\begin{abstract}
We propose a novel neural attention architecture to tackle machine comprehension tasks, such as answering Cloze-style queries with respect to a document. Unlike previous models, we do not collapse the query into a single vector, instead we deploy an iterative alternating attention mechanism that allows a fine-grained exploration of both the query and the document. Our model outperforms state-of-the-art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children's Book Test (CBT) dataset.

\end{abstract}

\section{Introduction}
\input{10-intro}

\section{Task Description}
\input{20-task}

\section{Alternating Iterative Attention}
\input{30-model}

\section{Training Details}
\input{40-training}

\section{Results}
\input{50-results}

\section{Related Works}
\input{60-related}

\section{Conclusion}
We presented an iterative neural attention model and applied it to machine comprehension tasks. Our architecture deploys a novel alternating attention mechanism, and tightly integrates successful ideas from past works in machine reading comprehension to obtain state-of-the-art results on three datasets. The iterative alternating attention mechanism continually refines its view of the query and document while aggregating the information required to answer a query.

Multiple future research directions may be envisioned. We plan to dynamically select the optimal number of inference steps required for each example. Moreover, we suspect that shifting towards stochastic attention should permit us to learn more interesting search policies. Finally, we believe that our model is fully general and may be applied in a straightforward way to other tasks such as information retrieval.


\bibliographystyle{emnlp2016}
\bibliography{twoway}

\end{document}



\documentclass{article} % For LaTeX2e
\usepackage{nips14submit_e,times}
\usepackage{hyperref}
\usepackage{url}
%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{xspace}
\usepackage{tabularx}
\usepackage{algorithm}
\usepackage{algorithmic}
%\newcommand{\theHalgorithm}{\arabic{algorithm}}

\newtheorem{proposition}{Proposition}

\newif\ifarxiv
\arxivtrue

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\newcommand{\obs}{\text{obs}}
\newcommand{\mis}{\text{mis}}

\newcommand{\qt}[1]{\left<#1\right>}
\newcommand{\ql}[1]{\left[#1\right]}
\newcommand{\hess}{\mathbf{H}}
\newcommand{\jacob}{\mathbf{J}}
\newcommand{\hl}{HL}
%\newcommand{\params}{\boldsymbol{\theta}}
\newcommand{\cost}{\mathcal{L}}
\newcommand{\lout}{\mathbf{r}}
\newcommand{\louti}{r}
\newcommand{\outi}{y}
\newcommand{\out}{\mathbf{y}}
\newcommand{\gauss}{\mathbf{G_N}}
\newcommand{\eye}{\mathbf{I}}
\newcommand{\softmax}{\phi}
\newcommand{\targ}{\mathbf{t}}
\newcommand{\metric}{\mathbf{G}}
\newcommand{\sample}{\mathbf{z}}
\newcommand{\floor}[1]{\lfloor #1 \rfloor}
\newcommand{\bmx}[0]{\begin{bmatrix}}
\newcommand{\emx}[0]{\end{bmatrix}}
\newcommand{\qexp}[1]{\left<#1\right>}
\newcommand{\vect}[1]{\mathbf{#1}}
\newcommand{\vects}[1]{\boldsymbol{#1}}
\newcommand{\matr}[1]{\mathbf{#1}}
\newcommand{\var}[0]{\operatorname{Var}}
\newcommand{\std}[0]{\operatorname{std}}
\newcommand{\cov}[0]{\operatorname{Cov}}
\newcommand{\diag}[0]{\operatorname{diag}}
\newcommand{\matrs}[1]{\boldsymbol{#1}}
\newcommand{\va}[0]{\vect{a}}
\newcommand{\vb}[0]{\vect{b}}
\newcommand{\vc}[0]{\vect{c}}
\newcommand{\vh}[0]{\vect{h}}
\newcommand{\vv}[0]{\vect{v}}
\newcommand{\vx}[0]{\vect{x}}
\newcommand{\vw}[0]{\vect{w}}
\newcommand{\vs}[0]{\vect{s}}
\newcommand{\vf}[0]{\vect{f}}
\newcommand{\vy}[0]{\vect{y}}
\newcommand{\vg}[0]{\vect{g}}
\newcommand{\vm}[0]{\vect{m}}
\newcommand{\vu}[0]{\vect{u}}
\newcommand{\vL}[0]{\vect{L}}
\newcommand{\mW}[0]{\matr{W}}
\newcommand{\mG}[0]{\matr{G}}
\newcommand{\mX}[0]{\matr{X}}
\newcommand{\mQ}[0]{\matr{Q}}
\newcommand{\mU}[0]{\matr{U}}
\newcommand{\mV}[0]{\matr{V}}
\newcommand{\mA}{\matr{A}}
\newcommand{\mD}{\matr{D}}
\newcommand{\mS}{\matr{S}}
\newcommand{\mI}{\matr{I}}
\newcommand{\td}[0]{\text{d}}
\newcommand{\vsig}[0]{\vects{\sigma}}
\newcommand{\valpha}[0]{\vects{\alpha}}
\newcommand{\vmu}[0]{\vects{\mu}}
\newcommand{\vzero}[0]{\vect{0}}
%\newcommand{\tf}[0]{\text{f}}
\newcommand{\tf}[0]{\text{m}}
\newcommand{\tdf}[0]{\text{dm}}
\newcommand{\grad}[0]{\nabla}
\newcommand{\alert}[1]{\textcolor{red}{#1}}
\newcommand{\N}[0]{\mathcal{N}}
\newcommand{\LL}[0]{\mathcal{L}}
\newcommand{\HH}[0]{\mathcal{H}}
\newcommand{\RR}[0]{\mathbb{R}}
\newcommand{\II}[0]{\mathbb{I}}
\newcommand{\Scal}[0]{\mathcal{S}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\E}[0]{\mathbb{E}}
\newcommand{\enabla}[0]{\ensuremath{%
    \overset{\raisebox{-0.3ex}[0.5ex][0ex]{%
    \ensuremath{\scriptscriptstyle e}}}{\nabla}}}
\newcommand{\enhnabla}[0]{\nabla_{\hspace{-0.5mm}e}\,}
\newcommand{\tred}[1]{\textcolor{red}{#1}}
\newcommand{\todo}[1]{{\Large\textcolor{red}{#1}}}
\newcommand{\done}[1]{{\Large\textcolor{green}{#1}}}
\newcommand{\dd}[1]{\ensuremath{\mbox{d}#1}}

\DeclareMathOperator*{\argmax}{\arg \max}
\DeclareMathOperator*{\argmin}{\arg \min}
\newcommand{\newln}{\\&\quad\quad{}}

\nipsfinalcopy % Uncomment for camera-ready version

\title{Deep Directed Generative Autoencoders}

\author{Sherjil Ozair \\
Indian Institute of Technology Delhi \\
\And Yoshua Bengio\\
Universit\'e de Montr\'eal \\
CIFAR Fellow \\
}

\begin{document}

\maketitle

\begin{abstract}
For discrete data, the likelihood $P(x)$ can be rewritten exactly and
parametrized into $P(X=x)=P(X=x|H=f(x)) P(H=f(x))$ if $P(X|H)$ has enough
capacity to put no probability mass on any $x'$ for which $f(x')\neq f(x)$, 
where $f(\cdot)$ is a deterministic discrete function. The log of the
first factor gives rise to the log-likelihood reconstruction error of an
autoencoder with $f(\cdot)$ as the encoder and $P(X|H)$ as the
(probabilistic) decoder. The log of the second term can be seen as a
regularizer on the encoded activations $h=f(x)$, e.g., as in sparse
autoencoders. Both encoder and decoder can be represented by a deep neural
network and trained to maximize the average of the optimal log-likelihood $\log p(x)$. The objective
is to learn an encoder $f(\cdot)$ that maps $X$ to $f(X)$
that has a much simpler distribution than $X$ itself, estimated by $P(H)$. This ``flattens
the manifold'' or concentrates probability mass in a smaller number of
(relevant) dimensions over which the distribution factorizes. 
Generating samples from the model is straightforward using ancestral
sampling. One challenge
is that regular back-propagation cannot be used to obtain the
gradient on the parameters of the encoder, but we find that using
the straight-through estimator works well here.  We also find that although
optimizing a single level of such architecture may be difficult, much better
results can be obtained by pre-training and
stacking them, gradually transforming the data
distribution into one that is more easily captured by a simple parametric
model.
\end{abstract}

\section{Introduction}

Deep learning is an aspect of machine learning that regards the question of
learning multiple levels of representation, associated with different
levels of abstraction~\citep{Bengio-2009-book}.  These representations are
distributed~\citep{Hinton89b}, meaning that at each level there are many
variables or features, which together can take a very large number of
configurations. 

An important conceptual challenge of deep learning is the following
question: {\em what is a good representation}? The question is most
challenging in the unsupervised learning setup. Whereas we understand
that features of an input $x$ that are predictive of some target $y$
constitute a good representation in a supervised learning setting,
the question is less obvious for unsupervised learning. 

\subsection{Manifold Unfolding}

In this paper we explore
this question by following the geometrical inspiration introduced
by~\citet{Bengio-arxiv2014}, based on the notion of {\em manifold unfolding},
illustrated in Figure~\ref{fig:ddga}. It was already observed
by~\citet{Bengio-et-al-ICML2013} that representations obtained
by stacking denoising autoencoders or RBMs appear to yield ``flatter'' or
``unfolded'' manifolds:
if $x_1$ and $x_2$ are examples from the data generating distribution $Q(X)$ and $f$ is
the encoding function and $g$ the decoding function, 
then points on the line $h_\alpha = \alpha f(x_1) + (1-\alpha) f(x_2)$ ($\alpha \in [0,1]$)
were experimentally found to correspond to probable input configurations, i.e., $g(h_\alpha)$ looks like
training examples (and quantitatively often comes close to one). This property
is not at all observed for $f$ and $g$ being the identity function: interpolating
in input space typically gives rise to non-natural looking inputs (we can immediately
recognize such inputs as the simple addition of two plausible examples).
This is illustrated in Figure~\ref{fig:flattening-manifold}.
It means that the input manifold (near which the distribution concentrates)
is highly twisted and curved and occupies a small volume in input space.
Instead, when mapped in the representation space of stacked autoencoders (the output of $f$), 
we find that
the convex between high probability points (i.e., training examples) is often
also part of the high-probability manifold, i.e., the transformed manifold
is flatter, it has become closer to a convex set.

\begin{figure}[H]
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{flattening-manifold.pdf}}
\caption{Illustration of the flattening effect observed in~\citet{Bengio-et-al-ICML2013}
by stacks of denoising autoencoders or RBMs, trained on MNIST digit images.
Whereas interpolating in pixel space ($X$-space)
between dataset examples (such as the 9 on the bottom
left and the 3 on the bottom right) gives rise to images unlike those in
the training set (on the bottom interpolation line), interpolating at the
first and second level of the stack of autoencoders ($H$-space) gives rise to images
(when projected back in input space, see text) that look like dataset examples.
These experiments suggest that the manifolds near which data concentrate, which are very
twisted and occupy a very small volume in pixel space, become flatter
and occupy more of the available volume in representation-space. Note
in particular how the two class manifolds have been brought closer to each other
(but interestingly there are also easier to separate, in representation space).
The manifolds associated with each class have become closer to a convex set, i.e., flatter.
}
\label{fig:flattening-manifold}
\end{center}
\end{figure} 

\subsection{From Manifold Unfolding to Probability Modeling}

If it is possible to unfold the data manifold into a nearly
flat and convex manifold (or set of manifolds), then estimating
the probability distribution of the data becomes much easier.
Consider the situation illustrated in Figure~\ref{fig:ddga}: a highly
curved 1-dimensional low-dimensional manifold is unfolded so that it occupies
exactly one dimension in the transformed representation (the ``signal dimension''
in the figure). Moving on the manifold corresponds to changing the hidden unit corresponding to
that signal dimension in representation space. On the other hand, moving orthogonal
to the manifold in input space corresponds to changing the ``noise dimension''.
There is a value of the noise dimension that corresponds to being on the
manifold, while the other values correspond to the volume filled by
unlikely input configurations. With the manifold learning mental picture,
estimating the probability distribution of the data basically amounts
to distinguishing between ``off-manifold'' configurations, which should have
low probability, from ``on-manifold'' configurations, which should have
high probability. Once the manifold is unfolded, answering that question
becomes very easy.

\begin{figure}[ht]
\begin{center}
\centerline{\includegraphics[width=0.35\columnwidth]{flattening.pdf}}
\caption{Illustration of the work that the composed encoder $f = f_L \circ \ldots f_2 \ldots f_1$
should do: flatten the manifold (more generally the region where probability mass is
concentrated) in such a way that a simple (e.g. factorized) prior distribution $P(h)$ can well approximate
the distribution of the transformed data $Q(h)$ obtained by applying $f$ to the data $x\sim Q(x)$.
The red curve indicates the manifold, or region near which $Q(x)$ concentrates. The first
encoder $f_1$ is not powerful and non-linear enough to untwist the manifold and flatten
it, so applying a factorized prior at that level would yield poor samples, because most
generated samples from the prior (in orange) would fall far from the transformed data
manifold (in red). At the top level, if training is successful, the transformed data distribution $Q(h)$
concentrates in a small number of directions (the ``signal'' directions, in the figure),
making it easy to distinguish the inside of the manifold (signal) from the outside (noise),
i.e. to concentrate probability mass in the right places.
At that point, $Q(h)$ and $P(h)$ can match better, and less probability mass is wasted
outside of the manifold.
}
\label{fig:ddga}
\end{center}
\end{figure} 

If probability mass is concentrated in a predictible and regular way
in representation space ($H$), it becomes easy to capture the data distribution $Q(X)$.
Let $H=f(X)$ be the random variable associated with the transformed data $X \sim Q(X)$,
with $Q(H)$ its marginal distribution.
Now, consider for example the extreme case where $Q(H)$ is almost factorized 
and can be well approximated by a factorized distribution model $P(H)$.
It means that the probability distribution in $H$-space can be captured
with few parameters and generalization is easy. Instead, when
modeling data in $X$-space, it is very difficult to find a family of
probability distributions that will put mass where the examples are
but not elsewhere. What typically happens is that we end up choosing
parameters of our probability model such that it puts a lot more
probability mass outside of the manifold than we would like. The
probability function is thus ``smeared''. We clearly get that effect
with non-parametric kernel density estimation, where we see that
the kernel bandwidth controls the amount of smearing, or smoothing.
Unfortunately, in a high-dimensional space, this puts a lot more
probability mass outside of the manifold than inside. Hence what
we see the real challenge as finding an encoder $f$ that transforms
the data distribution $Q(X)$ from a complex and twisted one into
a flat one, $Q(H)$, where the elements $H_i$ of $H$ are easy
to model, e.g. they are independent.


\section{Directed Generative Autoencoder (DGA) for Discrete Data}

We mainly consider in this paper the case of a discrete variable, which is simpler 
to handle.
In that case, a Directed Generative Autoencoder (DGA) is a model over the random variable $X$
whose training criterion is as follows
\begin{equation}
\label{eq:dga}
 \log P(X=x | H=f(x)) + \log P(H=f(X))
\end{equation}
where the deterministic function $f(\cdot)$ is called the encoder
and the conditional distribution $P(x|h)$ is called the decoder 
or decoding distribution. As shown below (Proposition~\ref{prop:exact-dga}), 
this decomposition becomes
exact as the capacity of the decoder $P(x|h)$ increases sufficiently to capture
a conditional distribution that puts zero probability on any $x'$ for which $f(x')\neq h$. 
The DGA is parametrized from two components:
\begin{enumerate}
\item $P(X=x | H=f(x))$ is an autoencoder with a {\em discrete}
representation $h=f(x)$. Its role in the log-likelihood
training objective is to make sure that $f$ preserves as much
information about $x$ as possible.
\item $P(H=h)$ is a probability model of the samples $H=f(X)$
obtained by sending $X$ through $f$. Its role in the log-likelihood
training objective is to make sure that $f$ transforms $X$
in a representation that has a distribution that can be well
modeled by the family of distributions $P(H)$.
\end{enumerate}

What can we say about this training criterion?

\begin{proposition}
\label{prop:exact-dga}
There exists a decomposition of the likelihood into \mbox{$P(X=x) = P(X=x | H=f(x)) P(H=f(X))$}
that is exact for discrete $X$ and $H$, and it is achieved when $P(x |h)$ is zero
unless $h=f(x)$. When $P(x|h)$ is trained with pairs $(X=x,H=f(x))$, it estimates
and converges (with enough capacity) to a conditional probability distribution which
satisfies this contraint. When $P(x|h)$ does not satisfy the condition, then
the unnormalized estimator \mbox{$P^*(X=x) = P(X=x|H=f(x))P(H=f(X))$} underestimates the true probability
\mbox{$P(X=x) = \sum_h P(x|H=h) P(H=h)$}.
\end{proposition}
\begin{proof}
We start by observing that because $f(\cdot)$ is deterministic, its value
$f(x)$ is perfectly predictible from the knowledge of $x$, i.e.,
\begin{equation}
  P(H=f(x) | X=x) = 1.
\end{equation}
Therefore, we can multiply this value of 1 by $P(X)$ and obtain the joint $P(X,H)$:
\begin{equation}
 P(X=x) = P(H=f(x) | X=x) P(X=x) = P(X=x, H=f(x))
\end{equation}
for any value of $x$.
Now it means that there exists a parametrization of the joint $P(X,H)$
into $P(H)P(X|H)$ that achieves
\begin{equation}
 P(X=x) = P(X=x | H=f(X)) P(H=f(X))
\end{equation}
which is the first part of the claimed result. Furthermore, for exact relationship
with the likelihood is achieved
when $P(x|h)=0$ unless $h=f(x)$, since with that condition,
\[
  P(X=x) = \sum_h P(X=x|H=h) P(H=h) = P(X=x|H=f(x)) P(H=f(x))
\]
because all the other terms of the sum vanish. Now, consider the case (which
is true for the DGA criterion) where the parameters
of $P(x|h)$ are only estimated to maximize the expected value of the
conditional likelihood $\log P(X=x | H=f(x))$. Because the maximum
likelihood estimator is consistent, if the family of distributions used
to estimate $P(x|h)$ has enough capacity to contain a solution
for which the condition of $P(x|h)=0$ for $h \neq f(x)$ is 
satisfied, then we can see that with enough capacity (which may also
mean enough training time), $P(x|h)$ converges to a solution that satisfies
the condition, i.e., for which $P(x) = P(x|H=f(x))P(H=f(x))$.
In general, a learned decoder $P(x|h)$ will not achieve
the guarantee that $P(x|h)=0$ when $h \neq f(x)$. However, the correct 
$P(x)$, for given $P(x|h)$ and $P(h)$ can always be written
\[
  P(x) = \sum_h P(x|h) P(h) \geq P(x|h=f(x)) P(H=f(x)) = P^*(x)
\]
which proves the claim that $P^*(x)$ underestimates the true likelihood.
\end{proof}

What is particularly interesting about this bound is that {\em as training
progresses and capacity increases} the bound becomes tight.
However, if $P(x|h)$ is a parametric distribution (e.g. factorized Binomial,
in our experiments) whose parameters (e.g. the Binomial probabilities)
are the output of a neural net, increasing the capacity of the neural net may not be sufficient
to obtain a $P(x|h)$ that satisfies the desired condition and makes the bound tight.
However, if $f(x)$ does not lose information about $x$, then $P(x|f(x))$ should
be unimodal and in fact be just 1. In other words, we should penalize the
reconstruction error term strongly enough to make the reconstruction error
nearly zero. That will guarantee that $f(x)$ keeps all the information about $x$
(at least for $x$ in the training set) and that the optimal $P(x|h)$ fits our
parametric family.

\iffalse
We clearly do not have access that ${\cal P}$, but we can use the estimated $P(x|h)$
in at least two ways to recover a normalized estimator for $P(x)$. One option is
to consider
\[
 P^*(x) = P(x|H=f(x)) P(H=f(x))
\]
as an unnormalized probability function, i.e., define
\[
  P(x) = \frac{P^*(x)}{Z}.
\]
Then one has to estimate the associated
normalization constant or partition function $Z$, which could be done in various ways,
e.g. by importance sampling with a proposal distribution $\pi(x)$:
\[
  Z = E_{\pi(x)}[ \frac{P*(x)}{\pi(x)} ].
\]
Another option is to take advantage of the knowledge of $f(x)$ to construct
a proposal distribution $q(h|x)$ which basically adds noise around $f(x)$
and estimates $P(h|x)$, and then use
another importance sampling estimator, this time for $P(x)$:
\[
  P(x) = \sum_h P(x|h)P(h) = E_{h\sim q(h|x)}[ \frac{P(x|h)P(h)}{q(h|x)} ].
\]
\fi

\subsection{Parameters and Training}

The training objective is thus a lower bound on the true log-likelihood:
\begin{equation}
\label{eq:log-likelihood}
 \log P(x) \geq \log P^*(x) = \log P(x|H=f(x)) + \log P(H=f(x))
\end{equation}
which can be seen to have three kinds of parameters:
\begin{enumerate}
\item The encoder parameters associated with $f$.
\item The decoder parameters associated with $P(x|h)$.
\item The prior parameters associated with $P(h)$.
\end{enumerate}

Let us consider how these three sets of parameters could be optimized
with respect to the log-likelihood bound (Eq.~\ref{eq:log-likelihood}).
The parameters of $P(h)$ can be learned by maximum likelihood (or any proxy for it),
with training examples that are the $h=f(x)$ when $x \sim Q(X)$
is from the given dataset. For example, if $P(H)$ is a factorized
Binomial, then the parameters are the probabilities \mbox{$p_i=P(H_i=1)$}
which can be learned by simple frequency counting of these events.
The parameters for $P(x|h)$ can be learned by maximizing the conditional
likelihood, like any neural network with a probabilistic output.
For example, if $X|H$ is a factorized Binomial, then we just have
a regular neural network taking $h=f(x)$ as input and $x$ as target,
with the cross-entropy loss function.

\subsection{Gradient Estimator for $f$}

One challenge is to deal with the training of the parameters of the
encoder $f$, because $f$ is discrete. We need to estimate a gradient
on these parameters in order to both optimize $f$ with respect to $\log P(h=f(x))$
(we want $f$ to produce outputs that can easily be modeled by the 
family of distributions $P(H)$) and with respect to $\log P(x|h=f(x))$
(we want $f$ to keep all the information about $x$, so as to be able
to reconstruct $x$ from $h=f(x)$). Although the true gradient
is zero, we need to obtain an update direction (a pseudo-gradient)
to optimize the parameters of the encoder with respect to these
two costs. A similar question was raised in a different context
in~\citet{Bengio-arxiv2013,bengio2013estimating}, and a number
of possible update directions were compared. In our experiments
we considered the special case where
\[
 f_i(x) = 1_{a_i(x)>0}
\]
where $a_i$ is the activation of the $i$-th output unit of the encoder before
the discretizing non-linearity is applied.
The actual encoder output is discretized, but we are interested in obtaining
a ``pseudo-gradient'' for $a_i$, which we will back-propagate inside
the encoder to update the encoder parameters.
What we did in the experiments is to compute the derivative of the reconstruction loss 
and prior loss \begin{equation}
\label{eq:loss}
{\cal L}=-\log P(x|h=f(x)) - \log P(h=f(x)),
\end{equation}
with respect to $f(x)$,
as if $f(x)$ had been continuous-valued.
We then used the {\bf Straight-Through Pseudo-Gradient}:
the update direction (or pseudo-gradient)
for $a$ is just set to be equal to the gradient with respect to $f(x)$:
\[
  \Delta a = \frac{\partial {\cal L}}{\partial f(x)}.
\]
The idea for this technique was proposed by~\citet{Hinton-Coursera2012} and
was used very succesfully in~\citet{bengio2013estimating}. It clearly has
the right sign (per value of $a_i(x)$, but not necessarily overall) but does not
take into account the magnitude of $a_i(x)$ explicitly.

Let us see how the prior negative log-likelihood bound can be written as a function of $f(x)$
in which we can pretend that $f(x)$ is continuous. For example, if
$P(H)$ is a factorized Binomial, we write this negative log-likelihood as the
usual cross-entropy:
\[
 - \sum_i f_i(x) \log P(h_i=1) + (1-f_i(x)) \log (1-P(h_i=1))
\]
so that if $P(x|h)$ is also a factorized Binomial, the overall loss is
\begin{align}
\label{eq:binomial-loss}
{\cal L} = & - \sum_i f_i(x) \log P(h_i=1) + (1-f_i(x)) \log (1-P(h_i=1)) \nonumber \\
           & - \sum_j x_j \log P(x_j=1|h=f(x)) + (1-x_j) \log (1-P(x_j=1|h=f(x)))
\end{align}
and we can compute $\frac{\partial {\cal L}}{\partial f_i(x)}$ as if $f_i(x)$ had been
a continuous-valued variable. All this is summarized in Algorithm~\ref{alg:dga}.

\begin{algorithm}[ht]
\caption{Training procedure for Directed Generative Autoencoder (DGA).}
\label{alg:dga}
\begin{algorithmic}
\STATE $\bullet$ Sample $x$ from training set.
\STATE $\bullet$ Encode it via feedforward network $a(x)$ and $h_i=f_i(x)=1_{a_i(x)>0}$.
\STATE $\bullet$ Update $P(h)$ with respect to training example $h$.
\STATE $\bullet$ Decode via decoder network estimating $P(x|h)$.
\STATE $\bullet$ Compute (and average) loss ${\cal L}$, as per Eq.~\ref{eq:loss},
e.g., in the case of factorized Binomials as per Eq.~\ref{eq:binomial-loss}.
\STATE $\bullet$ Update decoder in direction of gradient of $-\log P(x|h)$ w.r.t. the decoder parameters.
\STATE $\bullet$ Compute gradient $\frac{\partial \cal L}{\partial f(x)}$ as if $f(x)$ had been continuous.
\STATE $\bullet$ Compute pseudo-gradient w.r.t. $a$ as $\Delta a =  \frac{\partial \cal L}{\partial f(x)}$.
\STATE $\bullet$ Back-propagate the above pseudo-gradients (as if they were true gradients of the loss on $a(x)$)
inside encoder and update encoder parameters accordingly.
\end{algorithmic}
\end{algorithm}


\section{Greedy Annealed Pre-Training for a Deep DGA}

For the encoder to twist the data into a form that fits the prior $P(H)$,
we expect that a very strongly non-linear transformation will be required. As deep
autoencoders are notoriously difficult to train~\citep{martens2010hessian}, adding the extra constraint
of making the output of the encoder fit $P(H)$, e.g., factorial, was found
experimentally (and without surprise) to be difficult.

What we propose here is to use a an annealing (continuation method) and a greedy pre-training strategy,
similar to that previously proposed to train Deep Belief Networks 
(from a stack of RBMs)~\citep{Hinton06-small} or deep autoencoders (from a stack of shallow
autoencoders)~\citep{Bengio-nips-2006-small,Hinton+Salakhutdinov-2006}.

\subsection{Annealed Training}

Since the loss function of Eq.\ref{eq:dga} is hard to optimize directly, we consider a generalization
of the loss function by adding trade-off parameters to the two terms in the loss function corresponding
to the reconstruction and prior cost. A zero weight for the prior cost makes the loss function
same as that of a standard autoencoder, which is a considerably easier optimization problem.

\begin{align}
\label{eq:relaxed-binomial-loss}
{\cal L} = & - \beta \sum_i f_i(x) \log P(h_i=1) + (1-f_i(x)) \log (1-P(h_i=1)) \nonumber \\
           & - \sum_j x_j \log P(x_j=1|h=f(x)) + (1-x_j) \log (1-P(x_j=1|h=f(x))).
\end{align}

\subsubsection{Gradient Descent on Annealed Loss Function}

Training DGAs with fixed trade-off parameters is sometimes difficult because it is
much easier for gradient descent to perfectly optimize the prior cost by making $f$ map
all $x$ to a constant $h$. This may be a local optimum or a saddle point, escaping from which is 
difficult by gradient descent.
Thus, we use the tradeoff paramters to make the model first learn perfect reconstruction,
by setting zero weight for the prior cost. $\beta$ is then gradually increased to 1.
The gradual increasing schedule for $\beta$ is also important, as any rapid growth in $\beta$'s value
causes the system to `forget' the reconstruction and prioritize only the prior cost.

A slow schedule thus ensures that the model learns to reconstruct as well as to fit the prior.

\subsubsection{Annealing in Deep DGA}

Above, we describe the usefulness of annealed training of a shallow DGA. A similar trick
is also useful when pretraining a deep DGA. We use different values for these tradeoff parameters
to control the degree of difficulty for each
 pretraining stage. Initial stages have a high weight for reconstruction, and a low weight for 
 prior fitting, while the final stage has $\beta$ set to unity, giving back the 
 original loss function. Note that the lower-level DGAs can sacrifice on prior
 fitting, but must make sure that the reconstruction is near-perfect, so that no information is lost.
 Otherwise, the upper-level DGAs can never recover from that loss, which will show up in
 the high entropy of $P(x|h)$ (both for the low-level decoder and for the global decoder).

\section{Relation to the Variational Autoencoder (VAE) and Reweighted Wake-Sleep (RWS)}

The DGA can be seen as a special case of the Variational Autoencoder
(VAE), with various versions introduced
by~\citet{Kingma+Welling-ICLR2014,Gregor-et-al-ICML2014,Mnih+Gregor-ICML2014,Rezende-et-al-arxiv2014},
and of the Reweighted Wake-Sleep (RWS)
algorithm~\citep{Bornschein+Bengio-arxiv2014-small}.

The main difference between the DGA and these models is that with the latter the
encoder is stochastic, i.e., outputs a sample from an encoding (or approximate inference)
distribution $Q(h|x)$ instead of $h=f(x)$. This basically gives rise to a training
criterion that is not the log-likelihood but a variational lower bound on it,
\begin{equation}
\label{eq:vae-criterion}
  \log p(x) \geq E_{Q(h|x)}[ \log P(h) + \log P(x|h) - \log Q(h|x)].
\end{equation}
Besides the fact that $h$ is now sampled, we observe that the training criterion
has exactly the same first two terms as the DGA log-likelihood, but it also has
an extra term that attempts to maximize the conditional entropy of the encoder output,
i.e., encouraging the encoder to introduce noise, to the extent that it does not
hurt the two other terms too much. It will hurt them, but it will also help
the marginal distribution $Q(H)$ (averaged over the data distribution $Q(X)$)
to be closer to the prior $P(H)$, thus encouraging the decoder to contract
the ``noisy'' samples that could equally arise from the injected noise in $Q(h|x)$
or from the broad (generally factorized) $P(H)$ distribution.

\section{Experiments}
In this section, we provide empirical evidence for the feasibility of the proposed model, 
and analyze the influence of various techniques on the performance of the model.

We used the binarized MNIST handwritten digits dataset. We used the same binarized
version of MNIST as \cite{Uria+al-ICML2014}, and also used the same training-validation-test
split.

We trained shallow DGAs with 1, 2 and 3 hidden layers, and deep DGAs composed of
2 and 3 shallow DGAs. The dimension of $H$ was chosen to be 500, as this is 
considered sufficient for coding binarized MNIST. $P(H)$ is modeled as 
a factorized Binomial distribution.

Parameters of the model were learnt using minibatch gradient descent, with minibatch 
size of 100. Learning rates were chosen from {10.0, 1.0, 0.1}, and halved whenever
the average cost over an epoch increased. We did not use momentum or L1, L2 regularizers.
We used tanh activation functions in the hidden layers and sigmoid outputs.

While training, a small salt-and-pepper noise is added to the decoder input to make it
robust to inevitable mismatch between the encoder output $f(X)$ and samples from the prior, $h \sim P(H)$. Each bit of decoder input is selected with 1\% probability and changed to $0$ or $1$ randomly.

\begin{figure}[t]
\begin{center}
\begin{tabular}{c c}

\includegraphics[width=6.5cm]{samples8x15.png} & \includegraphics[width=6.5cm]{shallowsamples8x15.png} \\
(a) & (b)
\end{tabular}

\caption{(a) Samples generated from a 5-layer deep DGA, composed of 3 shallow DGAs 
of 3 layers (1 hidden layer) each. The model was trained greedily by training the 
first shallow DGA on the raw data and training each subsequent shallow DGA
on the output code of the previous DGA. (b) Sample generated from a 3-layer shallow DGA.}
\label{fig:samples}
\end{center}
\end{figure}

\subsection{Loglikelihood Estimator}
When the autoencoder is not perfect, i.e. the encoder and decoder are not
perfect inverses of each other, the loglikelihood estimates using
Eq. \ref{eq:binomial-loss} are biased estimates of the true loglikehood. We
treat these estimates as an unnormalized probability distribution, where
the partition function would be one when the autoencoder is perfectly
trained. In practice, we found that the partition function is less than
one. Thus, to compute the loglikehood for comparison, we estimate the
partition function of the model, which allows us to compute normalized
loglikelihood estimates. If $P^*(X)$ is the unnormalized probability
distribution, and $\pi(X)$ is another tractable distribution on $X$ from
which we can sample, the
partition function can be estimated by importance sampling:
$$Z = \sum_x P^*(x) = \sum_x \pi(x) \frac{P^*(x)}{\pi(x)} = E_{x \sim
  \pi(x)}[\frac{P^*(x)}{\pi(x)}]$$

As proposal distribution $\pi(x)$, We took $N$ expected values $\mu_j = E[X | H_j]$ under the decoder distribution,
for $H_j \sim P(H)$, and use them as centroids for a mixture model,

   $$\pi(x) = \frac{1}{N} \sum_{j=1}^N FactorizedBinomial(x; \mu_j).$$

Therefore,
$$log(P(X)) = log(P^*(X)) - log(Z)$$ gives us the estimated normalized loglikelihood.

\subsection{Performance of Shallow vs Deep DGA }
The 1-hidden-layer shallow DGA gave a loglikelihood estimate of -118.12 on the test set. The 5-layer deep DGA, 
composed of two 3-layer shallow DGAs trained greedily, gave a test set loglikelihood estimate of -114.29. We observed
that the shallow DGA had better reconstructions than the deep DGA. The deep DGA sacrificed on the reconstructibility, but 
was more successful in fitting to the factorized Binomial prior.

Qualitatively, we can observe in Figure~\ref{fig:samples} that samples from the deep DGA are much better than those from the shallow DGA.
The samples from the shallow DGA can be described as a mixture of incoherent MNIST features: although all the decoder units
have the necessary MNIST features, the output of the encoder does not match well the factorized Binomial of $P(H)$,
and so the decoder is not correctly mapping these unusual inputs $H$ from the Binomial prior to data-like samples.

The samples from the deep DGA are of much better quality. However, we can see that some of the samples are non-digits. This is due to the fact that the autoencoder had to sacrifice some reconstructibility to fit $H$ to the prior. The reconstructions
also have a small fraction of samples which are non-digits.

%We conjecture that this less-than-ideal performance could be overcome by using higher capacity neural networks, advanced 
%optimization techniques, and better hyperparamter search.

\subsection{Entropy, Sparsity and Factorizability}

We also compared the entropy of the encoder output and the raw data, under a factorized
Binomial model. The entropy, reported in Table~\ref{tab:entropy},
is measured with the logarithm base 2, so it counts
the number of bits necessary to encode the data under the simple factorized Binomial distribution.
A lower entropy under the factorized distribution means that fewer independent units are necessary to 
encode each sample. It means that the probability mass has been moved from a highly complex
manifold which is hard to capture under a factorized model and thus requires many dimensions
to characterize to a manifold that is aligned with a smaller set of dimensions, 
as in the cartoon of Figure~\ref{fig:ddga}. Practically this happens when many
of the hidden units take on a nearly constant value, i.e., the representation
becomes extremely ``sparse'' (there is no explicit preference for 0 or 1 for $h_i$ but one could
easily flip the sign of the weights of some $h_i$ in the output layer to make sure that 0 is
the frequent value and 1 the rare one). Table~\ref{tab:entropy} also contains a measure of sparsity of the 
representations based on such a bit flip (so as to make 0 the most frequent value).
This flipping allows to count the average number of 1's (of rare bits) necessary to
represent each example, in average (third column of the table).


We can see from Table~\ref{tab:entropy}
that only one autoencoder is not sufficient to reduce the entropy down
to the lowest possible value. Addition of the second autoencoder reduces
the entropy by a significant amount. 

Since the prior distribution is factorized, the encoder has to map data
samples with highly correlated dimensions, to a code with independent
dimensions. To measure this, we computed the Frobenius norm of the
off-diagonal entries of the correlation matrix of the data represented in its
raw form (Data) or at the outputs of the different encoders. See the 3rd columns
of Table~\ref{tab:entropy}. We see
that each autoencoder removes correlations from the data representation,
making it easier for a factorized distribution to model.


\begin{table}[H]
\begin{center}
\begin{tabular}{|c| c| c|c|}
\hline
Samples & Entropy & Avg \# active bits & $||Corr-diag(Corr)||_{F}$ \\
\hline
Data ($X$) & 297.6 & 102.1 & 63.5 \\
Output of $1^{st}$ encoder ($f_1(X)$) & 56.9  & 20.1 & 11.2\\
Output of $2^{nd}$ encoder ($f_2(f_1(X))$) & 47.6 & 17.4 & 9.4\\ 
\hline
\end{tabular}
\end{center}
\caption{Entropy, average number of active
bits (the number of rarely active bits, or 1's if 0 is the most frequent bit value, i.e. a measure of non-sparsity) 
and inter-dimension correlations, 
decrease as we encode into higher levels of representation,
making it easier to model it.}
\label{tab:entropy}
\end{table}

\iffalse
\begin{figure}[t]
\begin{center}
\begin{tabular}{|c| c|}
\hline
Model & Lowerbound of Loglikelihood \\
DGA-1hl & -117 \\
DDGA-5hl & -111 \\
\hline
\end{tabular}
\end{center}
\caption{}
\end{figure}
\fi

\iffalse
\section{NOT-CLEAR-ENOUGH-YET - Extension to Continuous Random Variables}

How could the DGA likelihood criterion be extended to continuous-valued data?

To avoid the hard constraint that $f$ be a bijection (that is perfectly invertible), 
we consider here a formulation of maximum likelihood that stays in the world
of probabilities instead of going to densities.
We consider a small ball $B_\epsilon(x)$ of radius $\epsilon$ around $x$ and the probability
of falling into it. Following Eq.~\ref{eq:dga},
\begin{equation}
\label{eq:continuous-dga}
  \log P(x \in B_\epsilon(x)) = \log P(x \in B_\epsilon | H \in f(B_\epsilon(x))) 
   + \log P(H \in f(B_\epsilon(x)))
\end{equation}
When $\epsilon$ is small with respect to the variations of $f$...

\fi

\section{Conclusion}

We have introduced a novel probabilistic interpretation for autoencoders as generative models,
for which the training criterion is similar to that of regularized (e.g. sparse) autoencoders
and the sampling procedure is very simple (ancestral sampling, no MCMC). We showed that this
training criterin is a lower bound on the likelihood and that the bound becomes tight as
the decoder capacity (as an estimator of the conditional probability $P(x|h)$) increases.
Furthermore, if the encoder keeps all the information about the input $x$, then the optimal
$P(x|h=f(x))$ is unimodal, i.e., a simple neural network with a factorial output suffices.
Our experiments showed that minimizing the proposed criterion yielded good generated
samples, and that even better samples could be obtained by pre-training a stack of
such autoencoders, so long as the lower ones are constrained to have very low reconstruction
error. We also found that a continuation method in which the weight of the prior term
is only gradually increased yielded better results. 

These experiments are most interesting because they reveal a picture of the representation
learning process that is in line with the idea of manifold unfolding and transformation from
a complex twisted region of high probability into one that is more regular (factorized)
and occupies a much smaller volume (small number of active dimensions). They also help
to understand the respective roles of reconstruction error and representation prior
in the training criterion and training process of such regularized auto-encoders.

\subsection*{Acknowledgments}

The authors would like to thank Laurent Dinh, Guillaume Alain and Kush Bhatia for fruitful discussions, and also the developers of Theano~\citep{bergstra+al:2010-scipy,Bastien-Theano-2012}. We acknowledge the support of the following agencies for
research funding and computing support: NSERC, Calcul Qu\'{e}bec, Compute Canada,
the Canada Research Chairs and CIFAR.

\bibliography{strings,strings-shorter,ml,aigaion}
\bibliographystyle{natbib}

\end{document}







\subsection{CIFAR-10}
We chose the CIFAR-10 dataset (\cite{krizhevsky2009learning}) 
to study the ability of various models to learn from high dimensional input data.  
It contains color images of size $32\times32$ pixels that are assigned 
to $10$ different classes.
The number of samples for training is $50,000$ and for testing is $10,000$. 
We consider the permutation invariant recognition task where the method is 
unaware of the 2D spatial structure of the input. We evaluated several other models 
along with ours, namely contractive autoencoder, 
standout autoencoder (\cite{ba2013adaptive}) and K-means. 
The evaluation is based on classification performance.  

The input data of size $3\times32\times32$ is contrast normalized and dimensionally reduced 
using PCA whitening retaining $99\%$ variance. We also evaluated a second method of
dimensionality reduction using PCA without whitening (denoted NW below). By whitening we mean
normalizing the variances, i.e., dividing each dimension by the square-root of the eigenvalues after PCA projection.
The number of features for each of the model 
is set to {$200,500,1000,1500,2000,2500,3000 ,3500,4000$}. 
All models are trained with stochastic gradient descent. 
For all the experiments in this section we chose a learning rate of $0.0001$ for 
a few (e.g. $3$) initial training epochs, and then increased it to $0.001$. 
This is to ensure that scaling issues in the initializing are dealt with at the outset, 
and to help avoid any blow-ups during training. 
Each model is trained for $1000$ epochs in total with a fixed momentum of $0.9$.
For inference, we use rectified linear units \emph{without bias} for all the models. 
We classify the resulting representation using logistic regression with weight decay 
for classification, with weight cost parameter estimated using cross-validation on 
a subset of the training samples of size $10000$. 

The threshold  parameter $\theta$ is fixed to $1.0$ for both the TRec and TLin 
autoencoder. % based experiments. 
For the cAE we tried the regularization strengths $1.0,2.0,3.0,-3.0$; the latter 
being ``uncontraction''.
In the case of the Standout AE we set $\alpha=1$,$\beta=-1$. 
The results are reported in the plots of Figure \ref{perfplot_NWWW}. 
Learned filters are shown in Figure~\ref{allfilters}.

\begin{figure*}
\begin{center}
  \subfigure[TLin AE]{\includegraphics[width=0.2\linewidth]{TLin_1500_filters.pdf}} \hspace{3mm}
  \subfigure[TRec AE]{\includegraphics[width=0.2\linewidth]{TRec_1500_filters.pdf}} \hspace{3mm}
  \subfigure[cAE RS=3]{\includegraphics[width=0.2\linewidth]{cAE_RS3_1500_filters.pdf}} \hspace{3mm}
  \subfigure[cAE RS=-3]{\includegraphics[width=0.2\linewidth]{cAE_RS-3_1500_filters.pdf}} \\

  \subfigure[TLin AE NW]{\includegraphics[width=0.2\linewidth]{TLin_1000_NW_filters.pdf}} \hspace{3mm}
  \subfigure[TRec AE NW]{\includegraphics[width=0.2\linewidth]{TRec_1000_NW_filters.pdf}} \hspace{3mm}
  \subfigure[cAE RS=3 NW]{\includegraphics[width=0.2\linewidth]{cAE_RS3_1000_NW_filters.pdf}} \hspace{3mm}
  \subfigure[cAE RS=-3 NW]{\includegraphics[width=0.2\linewidth]{cAE_RS-3_1000_NW_filters.pdf}} \\
  \caption{Features of different models trained on CIFAR-10 data. Top: PCA with whitening as preprocessing. Bottom: PCA with no whitening as preprocessing. RS denotes regularization strength.}
  \label{allfilters}
\end{center}
\end{figure*}


From the plots in Figure \ref{perfplot_NWWW} it is observable that the results are in line 
with our discussions in the earlier sections. 
Note, in particular that the TRec and TLin autoencoders perform well even with 
very few hidden units. 
As the number of hidden units increases, the performance of the models 
which tend to ``tile'' the input space tends to improve.  


\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\begin{tabular}{cc}
\includegraphics[width=0.45\columnwidth]{perf_plot_no_whitening.pdf}&
\includegraphics[width=0.45\columnwidth]{perf_plot_with_whitening.pdf}\\
\includegraphics[width=0.45\columnwidth]{patch_plot_500hids_with_whitening.pdf} &
\includegraphics[width=0.45\columnwidth]{patch_plot_1000hids_with_whitening.pdf}\\
\end{tabular}
\caption{Top row: Classification accuracy on permutation invariant CIFAR-10 data as a function of number of features. PCA with whitening (left) and without whitening (right) is used for preproceesing.
Bottom row: 
Classification accuracy on CIFAR-10 data for $500$ features (left) $1000$ features (right) as a function of input patch size. PCA with whitening is used for preprocessing.
}
\label{perfplot_NWWW}
\end{center}
\vskip -0.2in
\end{figure} 


In a second experiment we evaluate the impact of different input sizes on a fixed 
number of features.  
For this experiment the training data is given by image patches of 
size $P$ cropped from the center of each training image from the CIFAR-10 dataset. 
This yields for each patch size $P$ a training set of $50000$ samples 
and a test set of $10000$ samples. 
The different patch sizes that we evaluated are $10, 15, 20, 25$ as well as the original 
image size of $32$. The number of features is set to $500$ and $1000$. 
The same preprocessing (whitening/no whitening) and classification procedure as above 
are used to report performance. The results are shown in Figure~\ref{perfplot_NWWW}.%\ref{patchplot_WW_500_1000}.


%\begin{figure}[ht]
%\vskip 0.2in
%\begin{center}
%\centerline{\includegraphics[width=0.7\columnwidth]{patch_plot_500hids_with_whitening.pdf}}
%\centerline{\includegraphics[width=0.7\columnwidth]{patch_plot_1000hids_with_whitening.pdf}}
%\caption{Classification accuracy on CIFAR-10 dataset for multiple models with $500$ (top) and $1000$ (bottom) features and different input patch sizes. PCA with whitening is used for preprocessing.}
%\label{patchplot_WW_500_1000}
%\end{center}
%\vskip -0.2in
%\end{figure} 

When using preprocessed input data directly for classification, the performance 
increased with increasing patch size $P$, as 
one would expect. 
Figure~\ref{perfplot_NWWW} shows that 
for smaller patch sizes, all the models perform equally well. 
The performance of the TLin based model improves monotonically as the patch size is increased. 
All other model's performances suffer when the patch size gets too large. 
Among these, the ZAE model using TRec activation suffers the least, as expected. 

We also experimented by initializing a neural network with features from the trained models. 
We use a single hidden layer MLP with ReLU units where the input to hidden weights are 
initialized with features from the trained models and the hidden to output weights
from the logistic regression models (following \cite{krizhevsky2009learning}). 
A hyperparameter search yielding $0.7$ as the optimal threshold, along with 
supervised fine tuning helps increase the best performance in the case of the TRec 
%AE to $63.2$. 
AE to $63.8$. 
The same was not observed in the case of the cAE where the performance went slightly down. 
Thus using the TRec AE followed by supervised fine-tuning with dropout regularization yields $64.1\%$ accuracy and 
the cAE with regularization strength of $3.0$ yields $63.9\%$. 
To the best of our knowledge both results beat the current state-of-the-art performance on 
the permutation invariant CIFAR-10 recognition task (cf., for example, \cite{fastfood}), 
with the TRec slightly outperforming the cAE.
In both cases PCA without whitening was used as preprocessing.
In contrast to \cite{krizhevsky2009learning} we do not train on any extra data, so 
none of these models is provided with any knowledge of the task beyond the preprocessed 
training set. 



\subsection{Video data}
An dataset with very high intrinsic dimensionality are videos that show transforming 
random dots, as used in \cite{factoredGBM} and subsequent work: 
each data example is a vectorized video, whose first frame is a random image and whose 
subsequent frames show transformations of the first frame. 
Each video is represented by concatenating the vectorized frames into a large vector. 
This data has an intrinsic dimensionality which is at least as high as the dimensionality 
of the first frame. So it is very high if the first frame is a random image. 

It is widely assumed that only bi-linear models, such as \cite{factoredGBM} and related models, 
should be able to learn useful representations of this data. 
The interpretation of this data in terms of high intrinsic dimensionality suggests that a
simple autoencoder may be able to learn reasonable features, as long as it uses a 
linear activation function so hidden units can span larger regions. 

We found that this is indeed the case by training the ZAE on rotating 
random dots as proposed in \cite{factoredGBM}. The ZAE model with $100$ hiddens is trained 
on vectorized $10$-frame random dot videos with $13\times13$ being the
size of each frame. Figure~\ref{figure:rotationfilters} depicts filters learned and
shows that the model learns to represent the structure in this data by developing phase-shifted 
rotational Fourier components as discussed in the context of bi-linear models. 
We were not able to learn features that were distinguishable from noise with the cAE, which 
is in line with existing results (eg. \cite{factoredGBM}). 


\begin{figure}[h!]
\begin{center}
\begin{tabular}{cc}
\hspace{-7pt}
  \includegraphics[width=0.40\linewidth]{rotationfeatures_trelu_100hiddens_frame2.png} &
  \includegraphics[width=0.40\linewidth]{rotationfeatures_trelu_100hiddens_frame4.png} 
\end{tabular}
\begin{tabular}{lc}
\hline
%\abovespace\belowspace
  Model			&Average precision \\
  \hline
  %\abovespace
  TRec AE 		&50.4 \\
  TLin AE		&49.8 \\
  covAE	\cite{gated}	&43.3 \\
  GRBM \cite{convGBM}	&46.6 \\
  K-means		&41.0 \\
  %\belowspace
  contractive AE	&45.2\\
\hline
\end{tabular}
\end{center}
\caption{Top: Subset of filters learned from rotating random dot movies (frame 2 on the left, frame 4 on the right).
Bottom: Average precision on Hollywood2.}
\label{figure:rotationfilters}
\end{figure}

%\begin{table}[t]
%\caption{Average precision on Hollywood2.}
%\label{activityresults}
%\vskip 0.15in
%\begin{center}
%\begin{small}
%\begin{sc}
%\begin{tabular}{lc}
%\hline
%%\abovespace\belowspace
%  Model			&Average precision \\
%  \hline
%  %\abovespace
%  TRec AE 		&50.4 \\
%  TLin AE		&49.8 \\
%  covAE	\cite{gated}	&43.3 \\
%  GRBM \cite{convGBM}	&46.6 \\
%  K-means		&41.0 \\
%  %\belowspace
%  contractive AE	&45.2\\
%\hline
%\end{tabular}
%\end{sc}
%\end{small}
%\end{center}
%\vskip -0.1in
%\end{table}



We then chose activity recognition to perform a quantitative evaluation of this observation. 
The intrinsic dimensionality of real world movies is probably lower than that of random 
dot movies, but higher than that of still images. 
We used the recognition pipeline proposed in \cite{QuocISA,KondaMM13} and 
evaluated it on the Hollywood2 dataset \cite{marszalek09}.
The dataset consists of $823$ training videos and $884$ test videos with $12$ classes 
of human actions.
The models were trained on PCA-whitened input patches of size $10\times16\times16$ cropped randomly
from training videos. 
The number of training patches is $500,000$. 
The number of features is set to $600$ for all models.

In the recognition pipeline, sub blocks of the same size as the patch size are cropped 
from $14\times20\times20$ “super-blocks”, using
a stride of $4$. Each super block results in $8$ sub blocks. 
The concatenation of sub block filter responses is dimensionally reduced by 
performing PCA to get a super block descriptor, 
on which a second layer of K-means learns a vocabulary of spatio-temporal words, 
that get classified with an SVM (for details, see \cite{QuocISA,KondaMM13}). 

In our experiments we plug the features learned with the different models 
into this pipeline.  
The performances of the models are reported in Figure \ref{figure:rotationfilters} (right). 
They show that the TRec and TLin autoencoders clearly outperform the more 
localized models. Surprisingly, they also outperform more sophisticated gating models, 
such as \cite{factoredGBM}. %This may suggest viewing gating itself as a way to obtain a linear encoding as we discuss in the next section.






%\subsection{Orthogonality of learned features}
%To evaluate the orthogonality of the features learned by the TRec autoencoder  
%we computed the matrix shown in Figure \ref{TRecdotmat}. 
%Each cell in the matrix is dot product of a weight vector with the rest in the set. 
%The plot shows the dot products of $70$ weight vectors from a TRec ZAE 
%model with $1000$ hidden units trained on CIFAR-10. 
%It can be observed that the norm of the weight vectors (diagonal of the matrix) 
%is predominantly $1$ which, together with the fact that dot-products are otherwise 
%mostly $0$, shows that
%the learned weights form an approximately orthonormal basis. 
%However, one can also observe some values are close to $-1$, which shows that 
%some features are polar opposites of one another. 
%This suggests that the image data is distributed densely within \emph{subspaces}, 
%which the TRec model, due to rectification, needs to span using multiple weight vector.
%This also explains the success of the TLin model for few hidden units on large 
%patchsizes (Figure~\ref{patchplot_WW_500_1000}). 
%For comparison, the dot product matrix for feature 
%vectors from a cAE model is depicted in Figure \ref{cAEdotmat}.
%
%\begin{figure}[ht]
%\vskip 0.2in
%\begin{center}
%\subfigure[TRec AE]{\label{TRecdotmat}\includegraphics[width=0.4\columnwidth]{dotmat_of_weightsubset_NW1000_TRec.pdf}} \hspace{3mm}
%\subfigure[cAE]{\label{cAEdotmat}\includegraphics[width=0.4\columnwidth]{dotmat_of_weightsubset_NW1000_cAE_RS3.pdf}}
%\caption{Dot product matrix of random subset of learned weight vectors.}
%\label{dotmats}
%\end{center}
%\vskip -0.2in
%\end{figure} 
%




\subsection{Rectified linear inference}
\label{sec:RectlinInf}
In previous sections we discussed the importance of (unbiased) rectified linear inference. Here we experimentally show that using 
rectified linear inference yields the best 
performance among different inference schemes. 
We use a cAE model with a fixed number of hiddens trained on CIFAR-10 images, and 
evaluate the performance of 
%\begin{enumerate}
\begin{enumerate} \itemsep1pt \parskip0pt \parsep0pt
%\itemsep0em
\item Rectified linear inference with bias (the natural preactivation for the unit): $[W^TX+b]_+$ 
\item Rectified linear inference without bias: $[W^TX]_+$ 
\item natural inference: $\mathrm{sigmoid}(W^TX+b)$ 
\end{enumerate}
The performances are shown in Figure \ref{figure:negbiases} (right), 
confirming and extending the results presented in \cite{coatessinglelayer,ICML2011Saxe_551}. 



%\begin{figure}[ht]
%\vskip 0.2in
%\begin{center}
%\centerline{\includegraphics[width=0.3\columnwidth]{perf_plot_cAE_inf_schms.pdf}}
%\caption{Classification accuracy on CIFAR-10 dataset for cAE model with different inference schemes.}
%\label{inf_compare}
%\end{center}
%\vskip -0.2in
%\end{figure} 





\documentclass{article} % For LaTeX2e
\usepackage{iclr2015,times}
\usepackage{hyperref}
\usepackage{url}

\usepackage{bm}
\usepackage{color}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{subfigure} 
\usepackage{adjustbox}

%\usepackage[normaltitle, normalsections, normalmargins, normalindent, normalbib]{savetrees}

\title{Zero-bias autoencoders and the benefits of co-adapting features}


\author{
Kishore Konda\\
Goethe University Frankfurt\\
Germany\\
\texttt{konda.kishorereddy@gmail.com}\\
\And
Roland Memisevic\\
University of Montreal\\
Canada\\
\texttt{roland.memisevic@umontreal.ca} \\
\And
David Krueger\\
University of Montreal\\
Canada\\
\texttt{david.krueger@umontreal.ca}\\
}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version

\iclrconference % Uncomment if submitted as conference paper instead of workshop

\begin{document}


\maketitle

\begin{abstract}
Regularized training of an autoencoder typically results in hidden unit biases that take on large negative values. We show that negative biases are a natural result of using a hidden layer whose responsibility is to both represent the input data and act as a selection mechanism that ensures sparsity of the representation. We then show that negative biases impede the learning of data distributions whose intrinsic dimensionality is high. We also propose a new activation function that decouples the two roles of the hidden layer and that allows us to learn representations on data with very high intrinsic dimensionality, where standard autoencoders typically fail. Since the decoupled activation function acts like an implicit regularizer, the model can be trained by minimizing the reconstruction error of training data, without requiring any additional regularization. 
%Since our approach allows us to absorb the regularization of the model in the hidden activation function it makes it possible to train autoencoders by simply minimizing squared error without using any additional regularization. 
%We show that training common regularized autoencoders resembles clustering, 
%because it amounts to fitting a density model whose mass is concentrated in the directions of the individual weight vectors. We then propose a new activation function based 
%on thresholding a linear function with zero bias (so it is truly linear not affine), 
%and argue that this allows hidden units to ``collaborate'' in order to define larger 
%regions of uniform density. 
%We show that the new activation function makes it possible to train autoencoders without an 
%explicit regularization penalty, such as sparsification, contraction or denoising,  
%by simply minimizing reconstruction error. 
%Experiments in a variety of recognition tasks show that zero-bias autoencoders 
%perform about on par with common regularized autoencoders on low dimensional 
%data and outperform these by an increasing margin as the dimensionality of the data increases. 
\end{abstract}


\section{Introduction}
Autoencoders are popular models used for learning features and pretraining deep networks.  
In their simplest form, they are based on minimizing the squared error between 
an observation, $\bm{x}$, and a non-linear reconstruction defined as 
\begin{equation}
\bm{r}({\bm{x}}) = \sum_k h\big( \bm{w}_{k}^\mathrm{T}\bm{x} + b_k \big) \bm{w}_k + \bm{c}
\label{eq:AE}
\end{equation}
where $\bm{w}_k$ and $b_k$ are weight vector and bias for hidden unit $k$, 
$\bm{c}$ is a vector of visible biases, and $h(\cdot)$ is a hidden unit activation 
function. 
Popular choices of activation function are the sigmoid $h(a)=\big(1+\exp(-a)\big)^{-1}$, 
or the rectified linear (ReLU) $h(a)=\max(0,a)$. 
Various regularization schemes can be used to prevent trivial solutions when 
using a large number of hidden units. These include 
corrupting inputs during learning \cite{denoisingAE}, 
adding a ``contraction'' penalty which forces derivatives of hidden unit activations 
to be small \cite{contractiveAE}, or using sparsity penalties \cite{coatessinglelayer}. 


This work is motivated by the empirical observation that across a wide range of applications, 
hidden biases, $b_k$, tend to take on large negative values when training an autoencoder 
with one of the mentioned regularization schemes. 

In this work, we show that negative hidden unit biases are at odds with some 
desirable properties of the representations learned by the autoencoder.  
%We show that for contrast normalized images, negative biases give rise to an energy landscape that is non-uniform 
%and concentrated around the weight vectors, which lets the autoencoder approximate a clustering method. 
%This is in stark contrast to methods like PCA, in which hidden variables ``collaborate'' 
%to assign a single energy (or probability density) value to large homogeneous regions 
%in the input space. 
%Our analysis applies approximately to Restricted Boltzmann Machines with Gaussian visibles, 
%because the unnormalized log-probability is identical to the energy function defined by 
%the autoencoder \cite{aescoring}.
%It may also help explain recent work by \cite{coatessinglelayer,ICML2011Saxe_551} who show 
%that clustering performs surprisingly well in comparison to autoencoders and RBMs in recognition tasks 
%(as long as a linear activation function \emph{without} bias is used at test time). 
%We then argue that the localization of active regions may be attributed to the fact that in conventional autoencoders, 
We also show that negative biases are a simple consequence of the fact that hidden units  
in the autoencoder have the dual function of (1) selecting which weight vectors take part in 
reconstructing a given training point, 
and (2) representing the coefficients with which the selected weight vectors get combined 
to reconstruct the input (cf., Eq.~\ref{eq:AE}).

To overcome the detrimental effects of negative biases, 
we then propose a new activation function that allows us to disentangle these roles. 
%The activation function can be motivated by spike-and-slab. 
We show that this yields features that increasingly outperform regularized autoencoders 
in recognition tasks of increasingly high dimensionality. 
Since the regularization is ``built'' into the activation function, it allows us 
to train the autoencoder without additional regularization, like contraction or 
denoising, by simply minimizing reconstruction error. 
We also show that using an encoding without negative biases at test-time in both 
this model and a contractive autoencoder achieves state-of-the-art performance on 
the permutation-invariant CIFAR-10 
dataset.\footnote{An example implementation of the zero-bias autoencoder in python is 
available at \url{http://www.iro.umontreal.ca/~memisevr/code/zae/}.} 


\subsection{Related work}
Our analysis may help explain why in a network with linear hidden units, the optimal 
number of units tends to be relatively small \cite{ba2013adaptive,ksparse}. 
Training via thresholding, which we introduce in Section~\ref{section:thresholding}, 
is loosely related to dropout \cite{dropout}, in that it forces features to align with 
high-density regions. In contrast to dropout, our thresholding scheme is not stochastic. 
Hidden activations and reconstructions are a deterministic function of the input. 
Other related work is the work by \cite{goroshin-lecun-iclr-13} who introduce a variety of new 
activation functions for training autoencoders and argue for 
shrinking non-linearities (see also \cite{hyvarinen2004independent}), 
which set small activations to zero. 
In contrast to that work, we show that it is possible 
to train autoencoders without additional regularization,  
when using the right type of shrinkage function. 
Our work is also loosely related to \cite{MartensRBMefficiency} who discuss limitations of 
RBMs with \emph{binary} observations. 




\section{Negative bias autoencoders}
\label{section:negbias}
This work is motivated by the observation that regularized training of most common 
autoencoder models tends to yield hidden unit biases which are negative. 
Figure \ref{figure:negbiases} shows an experimental demonstration of this effect using 
whitened $6\times6$-CIFAR-10 color patches \cite{krizhevsky2009learning}.  
%Both models used 100 hidden units (cAE sigmoid, dAE ReLU) and were trained on corresponding histograms over hidden unit bias terms after training a contractive and a denoising autoencoder on this data. 
Negative biases and sparse hidden units have also been shown to be important for obtaining 
good features with an RBM 
\cite{LeeSparseRBM,HintonPracticalGuideRBM}. 
%, and sparsifying responses by 
%initializing hidden biases with large negative values is a fairly common trick to learn better features \cite{HintonPracticalGuideRBM}. 

\begin{figure}[t]
\begin{tabular}{cc}
\begin{adjustbox}{valign=t}
\begin{tabular}{cc}
  \includegraphics[width=0.20\linewidth]{./2014_01_02_features_sigmoid_cifarminiwhitened_numhid100contraction1_0.png}
& \includegraphics[width=0.20\linewidth]{./2014_01_31_features_sigmoid_cifarminiwhitened_numhid100corruption0_5.png} \\
  \includegraphics[width=0.20\linewidth]{./2014_01_02_biashistogram_sigmoid_cifarminiwhitened_numhid100contraction1_0.png}
& \includegraphics[width=0.22\linewidth]{./2014_01_31_biashistogram_sigmoid_cifarminiwhitened_numhid100corruption0_5.png}
%\caption{Classification accuracy on CIFAR-10 dataset for cAE model with different inference schemes.}
\end{tabular}
\end{adjustbox}
&
\begin{adjustbox}{valign=t}
\includegraphics[width=0.5\columnwidth]{perf_plot_cAE_inf_schms.pdf}
\end{adjustbox}
\end{tabular}
\caption{Left: Filters learned by a sigmoid contractive autoencoder \cite{contractiveAE}
(contraction strength $1.0$; left) and 
a ReLU denoising autoencoder \cite{denoisingAE} (zeromask-noise 0.5; right) 
from CIFAR-10 patches, and resulting histograms 
over learned hidden unit biases. 
Right: Classiﬁcation accuracy on permutation invariant CIFAR-10 data using cAE 
with multiple different inference schemes.
All plots in this paper are best viewed in color.
}
\label{figure:negbiases}
\end{figure}


\subsection{Negative biases are required for learning and bad in the encoding}
Negative biases are arguably important for training autoencoders, especially 
overcomplete ones, because they help constrain capacity and localize features. 
But they can have several undesirable consequences on the encoding as we shall discuss. 

Consider the effect of a negative bias on a hidden 
unit with ``one-sided activation functions'', such as ReLU or sigmoid 
(i.e. activations which asymptote at zero for increasingly negative preactivation): 
%With negative bias, $b_k$, 
On contrast-normalized data, 
it will act like a selection function, zeroing 
out the activities for points whose inner product with the weight vector $\bm{w}_k$ is small. 
As a result, the region on the hypersphere that activates a hidden unit (ie. that yields a value 
that is significantly different from $0$) will be a spherical cap, whose size is determined 
by the size of the weight vector and the bias. 
When activations are defined by spherical caps, the model 
effectively defines a radial basis function network on the hypersphere. 
(For data that is not normalized, it will still have the effect of limiting the number of training examples for which the activation function gets active.)

%During training, this is a desirable effect as it prevents co-adaptation and allows for learning an overcomplete representation.  In fact, fixing the bias term at zero would enable a small ensemble of hidden units to represent every point in the data-space and obviate the need for an overcomplete representation. 

%The reason that they work well in applications suggests that tiling the input space 
%into small cells is a good way to learn representations in those applications. 
%It also suggests that one role of sparsity and overcompleteness is to support locality. 
%Clearly, tiling spaces is a way to eliminate ``collaborations'' 
%between hidden units and thereby to yield a disentangled representation \cite{Bengio-2009}. 
%Indeed, clustering may be viewed as the ultimate disentangling method, because given 
%any sufficiently fine-grained quantization and enough training data,  
%a subsequent linear layer (such as a classifier) can learn almost any arbitrary non-linear function. 
%Obviously, this will work only as long as the intrinsic dimensionality of the data manifold is not too high, and there is enough training data.  Since it is common to train autoencoders on small image patches, it may not be surprising that tiling works reasonably well in those cases.  We study the success of autoencoders on data with varying intrinsic dimensionality empirically in Section~\ref{section:experiments}. 

As long as the regions where weight vectors become active do not overlap this will 
be equivalent to clustering. In contrast to clustering, regions may of course overlap 
for the autoencoder. However, as we show in the following on the basis of an autoencoder 
with ReLU hidden units and negative biases, even where active regions merge, the model 
will resemble clustering, in that it will learn a point attractor to represent that region. 
In other words, the model will not be able to let multiple hidden units ``collaborate'' to 
define a multidimensional region of constant density. 

%If the intrinsic dimensionality of the data distribution is high, a perfect tiling of space will not be possible as the number of prototypes required would need to grow exponentially with the intrinsic dimensionality. In this case, the active regions of different hidden units will have to merge, such that their joint responses can be steered to represent any point within the common active region.  We show in the following that negative biases will be detrimental in that they disallow such steering to define large homogenous regions of high intrinsic dimensionality. 


%\subsubsection{localization of active region for contrast-normalized data}
%1) ? 
%Autoencoders and RBMs are typically applied to DC-centered and contrast-normalized 
%image patches. Other types of data, like word representations, are also often normalized.  
%We assume in the following that data is normalized to have fixed norm, so that all data-points 
%are located on the hypersphere.
%But most of the analysis in this paper should apply approximately to mean-centered data without 
%fixed norm by replacing regions on the sphere with convex cones. 




\subsubsection{Modes of the density learned by a ReLU autoencoder}
%An autoencoder can be viewed as a dynamical system as it defines a map $r(\bm{x}):\mathbb{R}^n\rightarrow\mathbb{R}^n$. Similar to probabilistic models, they thereby define a data density whose modes are the fixed point of the dynamics \cite{}. 
%A mode of the autoencoder density (or energy function) satisfies: $r(\bm{x}) = \bm{x}$. 
We shall focus on autoencoders with ReLU activation function in the following. We add an approximate argument about sigmoid autoencoders in Section~\ref{subsubsection:sigmoid} below. 

Consider points $\bm{x}$ with $r(\bm{x}) = \bm{x}$ which can be reconstructed perfectly 
by the autoencoder. 
The set of such points may be viewed as the mode of the true data generating density, 
or the true ``manifold'' the autoencoder has to find. 

For an input $\bm{x}$, define the \emph{active set} (see also \cite{numberofregions}) 
as the set of hidden units which yield a positive response: 
$
S(\bm{x})=\{k:\bm{w}_k^\mathrm{T}\bm{x}+b_k>0\}.
$
Let $W_{S(\bm{x})}$ denote the weight matrix restricted to the active units. 
That is, $W_{S(\bm{x})}$ contains in its columns the weight vectors 
associated with active hidden units for data point $\bm{x}$.
The fixed point condition $r(\bm{x}) = \bm{x}$ for the ReLU autoencoder can now be written
\begin{equation}
W_{S(\bm{x})}(W_{S(\bm{x})}^\mathrm{T}\bm{x} + \bm{b}) = \bm{x}, 
\label{eq:perfectreconstruction}
\end{equation}
or equivalently,  
\begin{equation}
(W_{S(\bm{x})}W_{S(\bm{x})}^\mathrm{T}-I) \bm{x} = - W_{S(\bm{x})} \bm{b} 
\end{equation}
This is a set of inhomogeneous linear equations, whose solutions are given by a 
specific solution plus the null-space of $M=(W_{S(\bm{x})}W_{S(\bm{x})}^\mathrm{T}-I)$. 
The null-space is given by the eigenvectors corresponding to the unit eigenvalues of 
$W_{S(\bm{x})}W_{S(\bm{x})}^\mathrm{T}$. The 
%The null-space is given by the vanishing singular values of $M$
%These in turn correspond to the unit singular values of $W_{S(\bm{x})}W_{S(\bm{x})}^\mathrm{T}$ (because they will cancel with the diagonal entries in the identity matrix).  Thus, the dimensionality of the manifold defined by the active set is given by the number of unit singular values of $W_{S(\bm{x})}W_{S(\bm{x})}^\mathrm{T}$.  A unit singular value of $W_{S(\bm{x})}W_{S(\bm{x})}^\mathrm{T}$ will be a unit singular value of $W_{S(\bm{x})}$, 
%So the dimensionality of the manifold can equivalently be defined as the maximal number of orthonormal weight vectors in the active set. 
number of unit eigenvalues is equal to the number of orthonormal weight vectors in $W_{S(\bm{x})}$. 

Although minimizing the reconstruction error without bias, 
$
\|\bm{x} - W_{S(\bm{x})}W^\mathrm{T}_{S(\bm{x})} \bm{x} \|^2
$, 
%Now consider the task of learning $W_{S(\bm{x})}$, assuming a fixed, non-zero $\bm{b}$ in Eq.~\ref{eq:perfectreconstruction}.  Learning amounts to minimizing the reconstruction error for a set of training cases (those for which $S(\bm{x})$ is the active set).  If biases were zero, Eq.~\ref{eq:perfectreconstruction} would be optimal exactly for orthonormal $W_{S(\bm{x})}$.  This can be seen, for example, by noting that it amounts to learning parameters  under the constraint that the encoding is given by the inner product of data-points with active weight vectors: 
would enforce orthogonality of $W_{S(\bm{x})}$ for those hidden units that are active together, 
%The solution is given by $W_{S(\bm{x})}$ containing orthonormal columns (which define an orthogonal projection onto space spanned by the columns). %of $W_{S(\bm{x})}$. 
%In this case, the optimal $W_{S(\bm{x})}^\mathrm{T}W_{S(\bm{x})}$ defines the orthogonal projection onto the subspace spanned by its columns\footnote{overcomplete}. 
learning with a fixed, non-zero $\bm{b}$ will not: 
it amounts to minimizing the reconstruction error between $\bm{x}$ and a shifted projection, 
$
\|\bm{x} - W_{S(\bm{x})} W^\mathrm{T}_{S(\bm{x})} \bm{x} + W_{S(\bm{x})} \bm{b}\|^2 
$, 
for which the orthonormal solution is no longer optimal (it has to account for the 
non-zero translation $W_{S(\bm{x})} \bm{b}$). 
%\begin{equation}
%r(\bm{x}) - \bm{x} = W_{S(\bm{x})} \big(W^\mathrm{T}_{S(\bm{x})} \bm{x} +\bm{b}\big) - \bm{x} = 
%W_{S(\bm{x})} W^\mathrm{T}_{S(\bm{x})} \bm{x} + W_{S(\bm{x})} \bm{b} - \bm{x}
%\end{equation}
%consider the task of learning the weight s$W_S(\bm{x})$
%However, during learning nothing will encourage the columns $W_{S(\bm{x})}$ to orthonormalize, however, since an orthonormal weight matrix would be optimal for the homogeneous system ($b=\bm{0}$). 


%\subsubsection{Orthonormalization}
%Assume mean-centered data and consequently zero output-biases. 
%Given a vector $\bm{h}(\bm{x})$ of hidden unit activations, the   
%reconstruction of $\bm{x}$ may be written
%\begin{equation}
%    \bm{r}(\bm{x}) = W^\mathrm{T}\bm{h} = \sum_k h_k \bm{w}_k
%\end{equation}
%where $h_k$ is the activation of hidden unit $k$. 
%As this is simply a linear combination, the optimal coefficients 
%(those which minimize squared reconstruction error) are given by 
%\begin{equation}
%    \bm{h} = \big( W_{S(\bm{x})}^{\mathrm{T}} W_{S(\bm{x})}\big)^{-1} W_{S(\bm{x})}^\mathrm{T} \bm{x}
%\end{equation}
%For orthonormal weight vectors, this becomes 
%\begin{equation}
%h_k = \bm{w}_k^\mathrm{T} \bm{x}
%\label{equation:linearencoding}
%\end{equation}
%and minimizing reconstruction error using Eq.~\ref{equation:linearencoding} will be the 
%only way to 
%
%%Any other encoding will be suboptimal in the least-squares sense. Minimizing reconstruction error will not tend to orthonormalize weights 
%With 
%\begin{equation}
%h_k = \bm{w}_k^\mathrm{T} \bm{x} + b_k
%\end{equation}
%so they were optimal in the least-squares sense if all $\beta_k$ were exactly zero. (But they 
%cannot be due to the sparsity reasons mentioned above).

%(although this would be desirable cite ganguli). 

%there is no reason for the autoencoder to orthonormalize the rows (columns?) of $W_{S(\bm{x})}$ and thereby to learn a multidimensional mode of the density. 

%[[argue that same is true for contraction/denoising because these are spherical?]]

%Even if, the solution would be an affine subspace, whose offset is determined by the sparsity criterion (in particular, it can learn affine subspaces with negative offset). 

%[[intuition: W acts like the identity matrix within a subspace]]



%A desirable property of pretraining a 



%\subsubsection{}
%The energy function is non-uniform 


%These regions are visualized for an autoencoder with ReLU activation in Figure \ref{figure:energyfunctions} (top left plot): the output of each hidden peaks at the input which is perfectly aligned with the weight vector, $\bm{w}_k$, of that hidden unit, and falls off symmetrically around that point. 

%Similarly, the output of a hidden unit with sigmoid activation function will be localized around the weight vector as well, but can contain a spherical plateau of constant activity in the center.\footnote{The output of a hidden unit with tanh activation function will not be local, but rather have two peaks on the sphere, pointing in roughly opposite direction. Curiously, there are hardly any results in the literature, that demonstrate the successful application of tanh autoencoders on real data.}  

%\subsubsection{Gating and subspace models amount to using zero-bias encodings}
%Gating networks give rise linear, zero-bias encodings when trained as autoencoder. 


%Another empirical observation that supports the predominant role of localization 

%Spherical contraction 

%%in learning representations with autoencoders is the following: 
%In Section~\ref{} we show that if we train a 
%contractive autoencoder \cite{contractiveAE} with \emph{negative} contraction 
%cost, we also learn Gabor-like features similar to those shown in 
%Figure~\ref{figure:negbiases}. And the model performs about as well as a cAE 
%with (positive) contraction (see Section~\ref{section:experiments} for details). 


\subsubsection{Sigmoid activations}
\label{subsubsection:sigmoid}
The case of a sigmoid activation function is harder to analyze because the sigmoid 
is never exactly zero, and so the notion of an active set cannot be used. 
But we can characterize the manifold learned by a sigmoid autoencoder (and thereby 
an RBM, which learns the same density model (\cite{aescoring})
approximately using the binary activation 
$h_k(\bm{x})=\big((\bm{w}^\mathrm{T}_k \bm{x}) + b_k \ge 0 \big)$. The reconstruction 
function in this case would be 
$$
r(\bm{x})=\sum_{k: \big((\bm{w}^\mathrm{T}_k \bm{x}) + b_k \ge 0 \big)} w_k
$$
which is simply the superposition of active weight vectors 
(and hence not a multidimensional manifold either). 
%%\bm{x} = \sum_{k\in S(\bm{x})} w_k
%\bm{x} = \sum_{k\in S(\bm{x})} w_k
%$(0$
%That is, the mode is simply given by the superposition of active weight vectors. 



\subsubsection{Zero-bias activations at test-time}
This analysis suggests that even though negative biases are required to achieve sparsity, 
they may have a detrimental effect in that they make it difficult to learn a non-trivial manifold. 
%In the previous section we showed that non-zero hidden biases will force an autoencoder to learn a density model which is non-uniform within a non-trivial region defined by active weight vectors. 
%This analysis suggests that the only role of non-zero biases in an autoencoder is to avoid trivial representations during \emph{learning}, by setting them to large negative values as a result of regularization. 
%A potential solution to this problem would be to set all biases to zero at test-time, where there is no reason to constrain the model capacity in any way.
The observation that sparsity can be detrimental 
is not new, and has already been discussed, for example, 
in \cite{ranzato2007unsupervised,kavukcuoglu2010fast}, where
the authors give up on sparsity at test-time and show that this improves recognition performance. 
%However, non-zero biases may have an effect on the performance at \emph{test-time}, too. 
Similarly, \cite{coatessinglelayer,ICML2011Saxe_551} showed that 
very good classification performance can be achieved using a linear classifier applied to 
a bag of features, using ReLU activation without bias. 
They also showed how this classification scheme is 
robust wrt. the choice of learning method used for obtaining features (in fact,
it even works with random training points as features, or using K-means as the feature 
%how a clustering method, such as $K$-means, or even choosing random datapoints as weight vectors, can perform as well and often better.  They also showed that, if the representations are used by a subsequent linear classifier,  a piecewise linear 
learning method).\footnote{In \cite{coatessinglelayer} the so-called ``triangle activation'' 
was used instead of a ReLU as the inference method for $K$-means. 
This amounts to setting activations below the mean activation to zero, and 
it is almost identical to a zero-bias ReLU since the mean linear preactivation 
is very close to zero on average.} %(across the training set, the inner product of a weight with all training cases will be about as often negative as it will be positive).}

In Figure~\ref{figure:negbiases} we confirm this finding, and we show that it 
is still true when 
features represent whole CIFAR-10 images (rather than a bag of features).
The figure shows the classification performance of a standard contractive autoencoder 
with sigmoid hidden units trained on the permutation-invariant CIFAR-10 training 
dataset (ie. using the whole images not patches for training), 
using a linear classifier applied to the hidden activations. 
It shows that much better classification performance (in fact better than the previous 
state-of-the-art in the permutation invariant task) is achieved when replacing the sigmoid activations 
used during training with a zero-bias ReLU activation at test-time 
(see Section~\ref{sec:RectlinInf} for more details). 





%\subsection{Energy function of autoencoders with negative bias}
%We now study the shape of the ``density model'' defined by an autoencoder in more detail,
%using their associated energy function \cite{aescoring}.
%A real-valued autoencoder 
%(same for RBM) is associated with an energy function 
%\begin{equation} 
%\mathcal{F}_{\mathrm{b}}(\bm{x}) = \sum_k H\big(\bm{w}_{k}^\mathrm{T}\bm{x} + b_k\big) - \frac{1}{2} \| \bm{x} - \bm{c} \|^2
%\label{energy_simple}
%\end{equation}
%where $H(\cdot)$ is the anti-derivative of the hidden unit activation function \cite{aescoring}. 
%The negative of Eq.~\ref{energy_simple} is also know as free energy in the RBM. 
%The energy function is defined as the function whose derivative with respect to $\bm{x}$ yields 
%the autoencoder reconstruction function 
%\begin{equation}
%\bm{r}({\bm{x}}) - \bm{x} = \sum_k h\big( \bm{w}_{k}^\mathrm{T}\bm{x} + b_k \big) \bm{w}_k + \bm{c} - \bm{x}
%\end{equation}
%The significance of the energy for studying autoencoders is that it can viewed as 
%unnormalized log-probability of the data. Learning has the effect of making the 
%energy surface large at training examples and small elsewhere \cite{alain_iclr}. 
%
%
%By defining the \emph{active set} of hidden units with non-negative 
%response\footnote{A similar approach of restricting attention to 
%the active set was used recently by \cite{numberofregions} to study the number of 
%regions defined in a deep ReLU network}: 
%$
%S(\bm{x})=\{k:\bm{w}_k^\mathrm{T}\bm{x}+b_k>0\}, 
%$
%we may rewrite the energy for a ReLU autoencoder as follows: 
%\begin{small}
%\begin{equation} 
%\label{eq:energy_with_bias_activeset}
%\begin{split}
%&\mathcal{F}_{\mathrm{b}}(\bm{x})=\frac{1}{2}\sum_{k\in S(\bm{x})} (\bm{w}_k^\mathrm{T}\bm{x} + b_{k} \big)^2 - \frac{1}{2} \| \bm{x} - \bm{c} \|^2\\
%%&=\frac{1}{2}\sum_{k\in S(\bm{x})} (\bm{w}_k^\mathrm{T}\bm{x})^2 + \frac{1}{2}\sum_{k\in S(\bm{x})} b_{k} \big(\bm{w}_k^\mathrm{T}\bm{x}\big) - \frac{1}{2} \| \bm{x} - \bm{c} \|^2 + \Omega 
%&=\frac{1}{2}\sum_{k\in S(\bm{x})} (\bm{w}_k^\mathrm{T}\bm{x})^2 + \sum_{k\in S(\bm{x})} b_{k} \big(\bm{w}_k^\mathrm{T}\bm{x}\big)  - \frac{1}{2} \| \bm{x} - \bm{c} \|^2 + \Omega 
%\end{split}
%\end{equation} 
%\end{small}
%where $\Omega$ denotes terms that do not depend on $\bm{x}$.
%Defining $W_{\bm{x}}$ as matrix containing the active weight vectors, 
%this simplifies further to 
%\begin{small}
%\begin{equation} 
%\label{eq:energy_with_bias_activeset}
%\begin{split}
%\mathcal{F}_{\mathrm{b}}(\bm{x})&=\frac{1}{2} \big(W_{\bm{x}}\bm{x} + b_{\bm{x}}\big)^\mathrm{T} \big(W_{\bm{x}}\bm{x} + b_{\bm{x}}\big) - \frac{1}{2} \| \bm{x} - \bm{c} \|^2 + \Omega \\
%&=\frac{1}{2}\bm{x}^\mathrm{T} W_{\bm{x}}^\mathrm{T}W_{\bm{x}} \bm{x} + \bm{x}^\mathrm{T}W_{\bm{x}}^\mathrm{T} b_{\bm{x}} - \frac{1}{2} \| \bm{x} - \bm{c} \|^2 + \Omega\\
%&=\frac{1}{2}(\bm{x} + A)^\mathrm{T} W_{\bm{x}}^\mathrm{T}W_{\bm{x}}(\bm{x} + A) - \frac{1}{2} \| \bm{x} - \bm{c} \|^2 + \Omega\\
%\end{split}
%\end{equation} 
%\end{small}
%with $A =  \frac{1}{2}(W_{\bm{x}}^\mathrm{T}W_{\bm{x}})^{-1} W_{\bm{x}}^\mathrm{T} b_{\bm{x}}$.
%
%This shows that the energy function, up to constant terms,
%is the sum of two quadratic forms in $\bm{x}$: 
%\begin{equation}
%Q_p(\bm{x}) - Q_n(\bm{x})
%\end{equation}
%with 
%\begin{small}
%\begin{equation}
%%\hspace{-16pt}
%\begin{split}
%&Q_p(\bm{x})=\frac{1}{2}(\bm{x} + A)^\mathrm{T} W_{\bm{x}}^\mathrm{T}W_{\bm{x}}(\bm{x} + A)
%\end{split}
%\end{equation}
%\end{small}
%and
%\begin{small}
%\begin{equation}
%Q_n(\bm{x}) = \frac{1}{2} \| \bm{x} - \bm{c} \|^2 
%\end{equation}
%\end{small}
%
%$Q_p$ has its global minimum at 
%\begin{equation}
%\label{eq:optimumQp}
%\bm{x}^\mathrm{opt} = -A = -\frac{1}{2} \big(W_{\bm{x}}^\mathrm{T} W_{\bm{x}}\big)^{-1} W_{\bm{x}}^\mathrm{T} b_{\bm{x}} 
%\end{equation}
%
%
%Equation ~\ref{eq:optimumQp} defines a weighted average of the active weight vectors with positive coefficients when the corresponding biases are negative (experimentally, biases are almost always negative, as discussed 
%in Section~\ref{section:negbias}).  Note also that $W_{\bm{x}}^\mathrm{T}W_{\bm{x}}$ will be positive definite 
%(and typically is close to the identity). 
%
%For mean-centered data, the form $Q_n$ has its global minimum at the origin, 
%because the visible biases will be close to $0$ 
%(this is generally assumed to be the case, but we also confirmed this to be the case 
%in our experiments). 
%Thus, with $Q_n$ constant on the hypersphere, the shape of the density defined by 
%the autoencoder will be determined by the values that $Q_p$ takes on in the active region. 
%Because the global minimum of $Q_p$ is in the cone defined by the active weight vectors, 
%the density will be non-uniform in this region. More specifically, 
%$Q_p$ will achieve its maximum in the corners of the cone, given by the active 
%weight vectors.  
%And it will achieve its minimum in the interior of the convex hull 
%defined by the weight vectors. 
%
%In other words, the density model defined by an autoencoder with negative biases 
%is concentrated in the direction of the weight vectors and decays towards their center of mass. 
%The concentration will be more pronounced in high dimensions, as the quadratic function 
%will grow faster as a function of distance from its center. 
%Any learning algorithm will try to fit the density model to the empirical data density. 
%Since the density function is concentrated in the directions of the active weight vectors,  
%learning will have a similar effect to running a clustering algorithm. 
%
%Empirically, RBMs and autoencoders were shown to work well in many applications. 
%However, \cite{coatessinglelayer,ICML2011Saxe_551} also showed that a clustering 
%method proper, such as $K$-means, or even choosing random datapoints as weight vectors, 
%can perform as well and often better.  
%They also showed that, if the representations are used by a subsequent linear classifier,  
%a piecewise linear 
%activation function\footnote{In \cite{coatessinglelayer} the so-called ``triangle activation'' was 
%used instead of a ReLU as the inference method for $K$-means. 
%This amounts to setting activations below the mean activation to zero, and 
%it is almost identical to a ReLU when the mean preactivation 
%of hiddens is approximately zero.}, like the ReLU \emph{without} bias term, 
%works best at test-time. 
%This suggests that the bias terms acts like a regularizer for training. 
%But obviously, there is no need to regularize at test time.  
%We will discuss the energy function of an autoencoder with linear activation 
%and zero bias below. 
%
%A similar argument applies to sigmoid hiddens, 
%because these quickly tend to zero for large negative inputs. 
%Since the sigmoid does not take on exact zero values, there is not 
%as straightforward an argument analytically.
%Autoencoders with sigmoid hidden units have the same energy function as a 
%Restricted Boltzmann Machines with Gaussian observables \cite{aescoring}. 
%
%
%
%\subsection{Energy function of autoencoders without bias}
%The difference in the energy function, between a ReLU with and 
%without bias term, is that the linear term disappears for the latter: 
%\begin{small}
%\begin{equation} 
%\label{eq:energy_without_bias_activeset}
%\mathcal{F}(\bm{x})=\frac{1}{2}\sum_{k\in S(\bm{x})} (\bm{w}_k^\mathrm{T}\bm{x}\big)^2 - \frac{1}{2} \| \bm{x} - \bm{c} \|^2\\
%\end{equation} 
%\end{small}
%This is simply the sum of squared responses of the active hiddens. 
%In analogy to Eq.~\ref{eq:energy_with_bias_activeset}, the energy can still be 
%written as the difference of two quadratic functions, but both have the same center. 
%More specifically, we now have
%\begin{small}
%\begin{equation}
%Q_p(\bm{x})= \frac{1}{2} \bm{x}^\mathrm{T} W_{\bm{x}}^\mathrm{T}W_{\bm{x}} \bm{x}
%\end{equation}
%\end{small}
%This defines a region of uniform density on the unit hypersphere, 
%if $W_{\bm{x}}^\mathrm{T}W_{\bm{x}}$ is the identity matrix. 
%It will deviate from being a uniform region to the degree that 
%$W_{\bm{x}}^\mathrm{T}W_{\bm{x}}$ is not the identity matrix, but 
%in high dimensions it will do so less drastically than a quadratic with a linear term, 
%on the unit hypersphere.  
%As we show empirically in Section~\ref{section:thresholding}
%training autoencoders with zero bias tends to exactly orthonormalize the weights. 
%
%The active region is equivalent to the energy function of a linear autoencoder 
%or of PCA \cite{aescoring}, but \emph{it is confined to the active region}. 
%Thus, given a point from the active region, hidden units define a basis which spans 
%a subspace. 
%So the autoencoder without bias models the data density using (potentially very large) 
%patches of constant density. 
%Illustrations of the energy function defined by an ReLU autoencoder with zero bias 
%are shown in Figure~\ref{figure:energyfunctions} (for two hiddens on the bottom left; 
%for three hiddens on the bottom right). 
%
%
%\begin{figure}[h]
%\begin{center}
%\begin{tabular}{cccc}
%  \includegraphics[width=0.22\linewidth]{./EnergyFunction.png} &
%  \includegraphics[width=0.22\linewidth]{./EnergyFunctionWithBias.png} &
%  \includegraphics[width=0.22\linewidth]{./EnergyFunctionCone.png} &
%  \includegraphics[width=0.22\linewidth]{./EnergyFunctionCone3Vectors.png} 
%  %\includegraphics[width=0.5\linewidth]{./EnergyFunctionSubspace.png} 
%\end{tabular}
%\end{center}
%\caption{
%Activation of hidden units in autoencoder with ReLU hiddens with negative bias (top left); 
%Energy functions of autoencoders with 
%2 ReLU hiddens with negative bias (top right); 
%2 ReLU hiddens without bias (bottom left). 
%3 ReLU hiddens without bias (bottom left). 
%Green means high, blue means low.
%%linear activation without bias (bottom right). 
%}
%\label{figure:energyfunctions}
%\end{figure}
%



\section{Learning with threshold-gated activations}
\label{section:thresholding}
%It is interesting to contrast the case of a zero-bias and negative-bias autoencoder 
%in the case of a density that is concentrated along some high-density region, such as 
%a subspace or part of it.  
%When modeling this density with convex blobs,  
%there is a trade-off between the fidelity with which one can model this region and the 
%number of hidden units needed to do so. Decreasing the number of hidden units will 
%require their active regions to be larger, but since the regions are convex,  
%``subspace leakage'' will occur which will cause probability mass to be wasted in 
%directions orthogonal to the region of high density. 
%
%
%The same is not true for linear responses without bias (as long as the high density 
%regions can be approximated using piecewise linear parts). 
%It is well-known that a set of orthogonal weight vectors, $\bm{w}_1, \ldots, \bm{w}_K$, 
%can represent any point $\bm{x}$ in its span by using 
%the {linear coefficients $\bm{w}_i^\mathrm{T}\bm{x}$, in other words by setting 
%$
%\bm{r}(\bm{x}) = \sum_{k\in S(\bm{x})} \big( \bm{w}_k^\mathrm{T}\bm{x} \big) \bm{w}_k 
%\label{eq:linearreconstruction}
%$




In light of the preceding analysis, hidden units should promote sparsity during learning, 
by becoming active in only a small region of the input space, but once a hidden unit is 
active it should use a \emph{linear} not affine encoding. 
Furthermore, any sparsity-promoting process should be removed at test time. 

To satisfy these criteria 
we suggest separating the \emph{selection} function, which sparsifies hiddens, from 
the \emph{encoding}, which defines the representation, and should be linear. 
To this end, we define the autoencoder reconstruction as the product of the selection 
function and a linear representation: 
\begin{equation}
\bm{r}(\bm{x}) = \sum_k h\big( \bm{w}_k^\mathrm{T}\bm{x} \big) \big( \bm{w}_k^\mathrm{T}\bm{x} \big) \bm{w}_k 
\label{eq:trec}
\end{equation}
The selection function, $h(\cdot)$, may use 
a negative bias to achieve sparsity, but once active, a hidden unit uses 
a linear activation to define the coefficients in the reconstruction.
This activation function is reminiscent of spike-and-slab models 
(for example, \cite{courville2011spike}), 
which define probability distributions over hidden variables as the product of a 
binary spike variable and a real-valued code. In our case, the product does not come with 
a probabilistic interpretation and it only serves to define a deterministic 
activation function which supports a linear encoding. 
The activation function is differentiable almost everywhere, so 
one can back-propagate through it for learning. 
The activation function is also related to adaptive dropout (\cite{ba2013adaptive}), 
which however is not differentiable and thus cannot be trained with back-prop. 

\subsection{Thresholding linear responses}
\label{subsection:thresholdgating}
In this work, we propose as a specific choice for $h(\cdot)$ the boolean selection function
\begin{equation}
h\big( \bm{w}_k^\mathrm{T}\bm{x} \big)=\big( \bm{w}_k^\mathrm{T}\bm{x} > \theta \big)
\end{equation}
With this choice, the overall activation function is 
$\big(\bm{w}_k^\mathrm{T}\bm{x}>\theta)\bm{w}_k^\mathrm{T}\bm{x}$.  
It is shown in Figure~\ref{figure:thresholdedactivations} (left). 
From the product rule, and the fact that the derivative of the boolean expression 
$\big(\bm{w}_k^\mathrm{T}\bm{x}>\theta)$ is zero, it follows that the derivative 
of the activation function wrt. to the unit's net 
input, $\bm{w}_k^\mathrm{T}\bm{x}$, is 
$\big(\bm{w}_k^\mathrm{T}\bm{x}>\theta) \cdot 1 + 0 \cdot \bm{w}_k^\mathrm{T}\bm{x}
=\big(\bm{w}_k^\mathrm{T}\bm{x}>\theta) $.
%$\big(\bm{w}_k^\mathrm{T}\bm{x}>\theta)\bm{w}_k^\mathrm{T}\bm{x}$.
Unlike for ReLU, the non-differentiability of the activation function at $\theta$ 
is also a non-continuity. As common with ReLU activations, we train with (minibatch) 
stochastic gradient descent and ignore the non-differentiability during the optimization.  

We will refer to this activation function as Truncated Rectified (TRec) in the following. 
We set $\theta$ to $1.0$ in most of our experiments (and all hiddens have the same threshold). 
While this is unlikely to be optimal, we found it to work well and 
often on par with, or better than, traditional regularized autoencoders like the 
denoising or contractive autoencoder.
Truncation, in contrast to the negative-bias ReLU, can also be viewed as a 
hard-thresholding operator, the inversion of which is fairly 
well-understood \cite{RecoveryFromThresholded}. 

Note that the TRec activation function is simply a peculiar activation function that we 
use for training. So training amounts to minimizing squared error without any kind 
of regularization. 
We drop the thresholding for testing, where we use simply the rectified linear response. 

\begin{figure}[t]
\begin{center}
\begin{tabular}{cc}
\resizebox{6.0cm}{!}{\input truncatedrelu.pdf_t} 
&
\resizebox{6.0cm}{!}{\input truncatedlinear.pdf_t} 
\end{tabular}
\caption{Activation functions for training autoencoders: 
thresholded rectified (left); thresholded linear (right).}
\label{figure:thresholdedactivations}
\end{center}
\end{figure}

%Square pooling models define active regions to be subspaces rather than cones. 
%Inspired by this, 
We also experiment with autoencoders that use a ``subspace'' variant of the TRec 
activation function (\cite{rozell2008sparse}), given by 
\begin{equation}
\bm{r}(\bm{x}) = \sum_k h\big( (\bm{w}_k^\mathrm{T}\bm{x})^2 \big) \big( \bm{w}_k^\mathrm{T}\bm{x} \big) \bm{w}_k 
\label{eq:tlin}
\end{equation}
It performs a linear reconstruction when the preactivation is either very large or 
very negative, so 
%Thus, the energy function defined by this model is a sum of squares within the active region, too,
the active region is a subspace rather than a convex cone. 
To contrast it with the rectified version, we refer to this activation function as 
thresholded linear (TLin) below, but it is also known as hard-thresholding in the 
literature \cite{rozell2008sparse}. 
See Figure~\ref{figure:thresholdedactivations} (right plot) for an illustration. 

Both the TRec and TLin activation functions allow hidden units to use a linear rather 
than affine encoding. We shall refer to autoencoders with these activation functions 
as zero-bias autoencoder (ZAE) in the following. 




\subsection{Parseval autoencoders}
For overcomplete representations orthonormality can no longer hold. 
However, if the weight vectors span the data space, 
they form a \emph{frame} (eg. \cite{kovacevic2008introduction}), 
so analysis weights ${\bm{\tilde{w}}}_i$ exist, 
such that an exact reconstruction can be written as 
\begin{equation}
\bm{r}(\bm{x}) = \sum_{k\in S(\bm{x})} \big( {\bm{\tilde{w}}}_k^\mathrm{T}\bm{x} \big) \bm{w}_k 
\label{eq:framereconstruction}
\end{equation}
The vectors ${\bm{\tilde{w}}}_i$ and $\bm{w}_i$ are in general not identical, 
but they are related through a matrix multiplication: 
$
\bm{w}_{k} = S {\bm{\tilde{w}}}_{k}. 
$
The matrix $S$ is known as frame operator for the frame $\{\bm{w}_k\}_k$ given by the 
weight vectors $\bm{w}_k$, and the set $\{\bm{\tilde{w}}_k\}_k$ is the dual frame
associated with $S$ (\cite{kovacevic2008introduction}). 
The frame operator may be the identity in which case $\bm{w}_k=\bm{\tilde{w}}_k$
(which is the case in an autoencoder with tied weights.)

Minimizing reconstruction error will make the 
frames $\{\bm{w}_k\}_k$ and $\{\bm{\tilde{w}}_k\}_k$ 
approximately duals of one another, so that Eq.~\ref{eq:framereconstruction} will 
approximately hold.
More interestingly, for an autoencoder with tied weights ($\bm{w}_k=\bm{\tilde{w}}_k$), 
minimizing reconstruction error would let the frame approximate a 
Parseval frame (\cite{kovacevic2008introduction}), such that Parseval's identity holds 
$
\sum_{k \in S(\bm{x})} \big(\bm{w}_k^\mathrm{T}\bm{x}\big)^2 = \|\bm{x}\|^2
$.
%In this case, we would obtain an exactly piecewise constant density in the active region as shown in Figure~\ref{figure:energyfunctions} (bottom). 

%This suggests defining an autoencoder with piecewise linear, not affine, activation functions.  But it leaves open the question of how to regularize such a model, since training with piece-wise linear functions will learn the identity with any reasonably large number of hiddens.  The solution that we propose below %(Section~\ref{section:thresholding}) is based on \emph{thresholding} the hidden responses in order to reduce the size of their active regions. 













\section{Experiments}
\label{section:experiments}

\input{experiments.tex}




\section{Discussion}
Quantizing the input space with tiles proportional in quantity to the 
data density is arguably the best way to represent data given enough training data and 
enough tiles, because it allows us to approximate any function reasonably well 
using only a subsequent linear layer. 
However, for data with high intrinsic dimensionality and a limited number of hidden 
units, we have no other choice than to summarize regions using responses that are invariant 
to some changes in the input. 
Invariance, from this perspective, is a necessary evil and not a goal in itself. But it is increasingly important for increasingly high dimensional inputs. 

We showed that linear not affine hidden responses allow us to get invariance,  
because the density defined by a linear autoencoder is a superposition 
of (possibly very large) regions or subspaces. 

%Our work also suggests viewing subspace (AKA square-pooling) models as a way to 
%define larger regions, so as to cope with dimensionality. 
%This is quite different from saying it allows us to learn 
%``covariance structure'' \cite{mcrbm} or to ``represent relations'' \cite{factoredGBM}. 
%It does seem likely, however, that the two tasks of encoding relations and 
%of representing linear regions are intimately related, given that the same type 
%of learning mechanisms seem to apply to both. 

%An interesting direction for future work will be the study of thresholding in feed-forward networks, especially considering that higher layers are often high-dimensional and their units less correlated than the input data, making them likely amenable to more ``collaborative'' hidden units, too.  
%\subsection{Relation to square-pooling and gating}
After a selection is made as to which hidden units are active 
for a given data example, linear coefficients are used in the reconstruction. 
This is very similar to the way in which gating and square pooling models
%(eg., \cite{OlshausenBilinear,Memisevic07,factoredGBM,mcrbm,QuocISA,gated,KondaMM13,LearningToRelateImages}) define 
(eg., \cite{OlshausenBilinear,Memisevic07,factoredGBM,mcrbm,QuocISA,courville2011spike}) define 
their reconstruction: 
The response of a hidden unit in these models is defined by multiplying the filter 
response or squaring it, followed by a non-linearity. 
To reconstruct the input, the output of the hidden unit 
is then multiplied by the filter response itself, making the model bi-linear. 
As a result, reconstructions are defined as the sum of feature vectors, weighted by 
\emph{linear} coefficients of the active hiddens. 
This may suggest interpreting the fact that these models work well on videos 
and other high-dimensional data as a result of using linear, zero-bias hidden units, too. 
%\footnote{In factored bi-linear models (eg. \cite{factoredGBM}), a hidden unit is connected to multiple filters, and the decision whether to take part in the reconstruction is thus made jointly for all filters connected to that unit.  In our model, the decision is made separately for each filter.  It is the learning that has to figure out how to group filters. This is analogous to how it is the learning that has to determine the selection of hidden units in undirected models like RBMs in the absence of lateral connections.  From a practical point of view it seems like a much better idea to move difficult decisions like this into the learning rather than the inference phase.} 




\section*{Acknowledgments} This work was supported by an NSERC Discovery grant, a Google faculty research award, and the German Federal Ministry of Education and Research (BMBF) in the project 01GQ0841 (BFNT Frankfurt). 


{\small
\bibliography{zae}
\bibliographystyle{iclr2015}
}

\end{document}





\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2016
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2016}

%\usepackage{nips_2016}

% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage[final]{nips_2016}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{framed}
\usepackage[pdftex]{graphicx}
\usepackage[usenames, dvipsnames]{color}
\usepackage{subcaption}
\usepackage{tabularx}

\usepackage{slashbox}
\usepackage{colortbl}
\usepackage{array}


\title{Generative Deep Neural Networks for Dialogue: \\
A Short Review}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  Iulian Vlad Serban \\ Department of Computer Science \\ and Operations Research, \\ University of Montreal \\ 
  \And
  Ryan Lowe \\ School of Computer Science, \\ McGill University
  \And
  Laurent Charlin  \\ School of Computer Science, \\ McGill University
  \And  
  Joelle Pineau \\  School of Computer Science, \\ McGill University
  %% examples of more authors
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
Researchers have recently started investigating deep neural networks for dialogue applications.
In particular, generative sequence-to-sequence (Seq2Seq) models have shown promising results for unstructured tasks, such as word-level dialogue response generation.
The hope is that such models will be able to leverage massive amounts of data to learn meaningful natural language representations and response generation strategies, while requiring a minimum amount of domain knowledge and hand-crafting.
An important challenge is to develop models that can effectively incorporate dialogue context and generate meaningful and diverse responses.
In support of this goal, we review recently proposed models based on generative encoder-decoder neural network architectures, and show that these models have better ability to incorporate long-term dialogue history, to model uncertainty and ambiguity in dialogue, and to generate responses with high-level compositional structure.
\end{abstract}


\vspace*{-0.5\baselineskip}
\section{Introduction}
\vspace*{-0.5\baselineskip}

Researchers have recently started investigating sequence-to-sequence (Seq2Seq) models for dialogue applications.
These models typically use neural networks to both represent dialogue histories and to generate or select appropriate responses. %, which are trained to mimic dialogues occurring in a dialogue corpus.
Such models are able to leverage large amounts of data in order to learn meaningful natural language representations and generation strategies, while requiring a minimum amount of domain knowledge and hand-crafting.
Although the Seq2Seq framework is different from the well-established goal-oriented setting~\citep{gorin1997may,young2000probabilistic,singh2002optimizing},
these models have already been applied to several real-world applications, with Microsoft's system Xiaoice~\citep{markoff2015forsymp} and Google's Smart Reply system~\citep{kannan2016smart} as two prominent examples.

Researchers have mainly explored two types of Seq2Seq models.
The first are generative models,
which are usually trained with cross-entropy to generate responses word-by-word conditioned on a dialogue context~\citep{ritter2011data,vinyals2015neural,sordoni2015aneural,shang2015neural,li2015diversity,DBLP:conf/aaai/SerbanSBCP16}.
The second are discriminative models, which are trained to select an appropriate response from a set of candidate responses \citep{lowe2015ubuntu,bordes2016learning,inaba2016neural,yu2016strategy}.
In a related strand of work, researchers have also investigated applying neural networks to the different components of a standard dialogue system, including natural language understanding, natural language generation, dialogue state tracking and evaluation \citep{wen2016network,wen-EtAl:2015:EMNLP,henderson2013deep,mrkvsic2015multi,su2015learning}. %wen2015stochastic
In this paper, we focus on generative models trained with cross-entropy.
%Our work focuses on generative model trained with the cross-entropy training criterion.

One weakness of current generative models is their limited ability to incorporate rich dialogue context and to generate meaningful and diverse responses \citep{DBLP:conf/aaai/SerbanSBCP16,li2015diversity}.
To overcome this challenge, we propose new generative models that are better able to incorporate long-term dialogue history, to model uncertainty and ambiguity in dialogue, and to generate responses with high-level compositional structure.
Our experiments demonstrate the importance of the model architecture and the related inductive biases in achieving this improved performance.

% focus on the generative models, because they can easily be scaled to large datasets and open domains.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.8\textwidth]{Images/GraphicalModels_v4.png}
  \caption{Probabilistic graphical models for dialogue response generation.
  Variables $\mathbf{w}$ represent natural language utterances.
  Variables $\mathbf{z}$ represent discrete or continuous stochastic latent variables.
  (A): Classic LSTM model, which uses a shallow generation process. This is problematic because it has no mechanism for incorporating uncertainty and ambiguity and because it forces the model to generate compositional and long-term structure incrementally on a word-by-word basis.
  (B): VHRED expands the generation process by adding one latent variable for each utterance, which helps incorporate uncertainty and ambiguity in the representations and generate meaningful, diverse responses.
  (C): MrRNN expands the generation process by adding a sequence of discrete stochastic variables for each utterance, which helps generate responses with high-level compositional structure.}
  \label{fig:MultiresolutionHRED}
\end{figure}

\vspace*{-0.5\baselineskip}
\section{Models}
\vspace*{-0.5\baselineskip}

% , each with its own inductive biases motivated by linguistic observations:
\textbf{HRED}: The Hierarchical Recurrent Encoder-Decoder model (HRED) \citep{DBLP:conf/aaai/SerbanSBCP16} is a type of Seq2Seq model that decomposes a dialogue into a two-level hierarchy: a sequence of utterances, each of which is a sequence of words.
HRED consists of three recurrent neural networks (RNNs): an \textit{encoder} RNN, a \textit{context} RNN and a \textit{decoder} RNN.
Each utterance is encoded into a real-valued vector representation by the \textit{encoder} RNN.
These utterance representations are given as input to the \textit{context} RNN, which computes a real-valued vector representation summarizing the dialogue at every turn.
This summary is given as input to the \textit{decoder} RNN, which generates a response word-by-word.
Unlike the RNN encoders in previous Seq2Seq models, the \textit{context} RNN is only updated once every dialogue turn and uses the same parameters for each update.
This gives HRED an inductive bias that helps incorporate long-term context and learn invariant representations.

\textbf{VHRED}: The Latent Variable Hierarchical Recurrent Encoder-Decoder model (VHRED) \citep{serban2016hierarchical} is an HRED model with an additional component: a high-dimensional stochastic latent variable at every dialogue turn.
As in HRED, the dialogue context is encoded into a vector representation using \textit{encoder} and \textit{context} RNNs.
%each utterance is encoded into a vector representation by an \textit{encoder} RNN.
%These utterance representations are given as input to the \textit{context} RNN, which computes a vector representation summarizing the dialogue at every turn.
Conditioned on the summary vector at each dialogue turn, VHRED samples a multivariate Gaussian variable, which is given along with the summary vector as input to the \textit{decoder} RNN.
%which generates the response word-by-word.
The multivariate Gaussian latent variable allows modelling ambiguity and uncertainty in the dialogue through the latent variable distribution parameters (mean and variance parameters).
This provides a useful inductive bias, which helps VHRED encode the dialogue context into a real-valued embedding space even when the dialogue context is ambiguous or uncertain, and it helps VHRED generate more diverse responses.


\textbf{MrRNN}: The Multiresolution RNN (MrRNN) \citep{serban2016multiresolution} models dialogue as two parallel stochastic sequences:
a sequence of high-level coarse tokens (coarse sequences), and a sequence of low-level natural language words (utterances).
The coarse sequences follow a latent stochastic process---analogous to hidden Markov models---which conditions the utterances through a hierarchical generation process.
The hierarchical generation process first generates the coarse sequence, and conditioned on this generates the natural language utterance.
In our experiments, the coarse sequences are defined as either noun sequences or activity-entity pairs (predicate-argument pairs) extracted from the natural language utterances.
The coarse sequences and utterances are modelled by two separate HRED models.
The hierarchical generation provides an important inductive bias, because it helps MrRNN model high-level, compositional structure and generate meaningful and on-topic responses.


%We implement all models in Theano.
%~\cite{2016arXiv160502688short}.
%We train all models w.r.t.\@ the log-likelihood or joint log-likelihood on the training set using Adam~\cite{kingma2014adampublished}. 
%The models are trained using early stopping with patience based on the validation set log-likelihood.
%W%e choose model hyperparameters -- such as the number of hidden units, word embedding dimensionality, and learning rate -- based on the validation set log-likelihood.
%For further details on the models, see %\cite{serban2016multiresolution,serban2016hierarchical,DBLP:journals/corr/SerbanLCP15}.

\vspace*{-0.5\baselineskip}
\section{Experiments}
\vspace*{-0.5\baselineskip}


We apply our generative models to dialogue response generation on the Ubuntu Dialogue Corpus~\citep{lowe2015ubuntu}.
For each example, given a dialogue context, the model must generate an appropriate response.
We also present results on Twitter in the Appendix.
This task has been studied extensively in the recent literature \citep{ritter2011data,sordoni2015aneural,li2015diversity}.
% DBLP:conf/aaai/SerbanSBCP16
% lowe2015ubuntu


\newcolumntype{L}[1]{>{\raggedright\arraybackslash}p{#1}}

\begin{table*}[t]
 \caption{Model response examples. The arrows indicate change of turn.}
 \label{table:ubuntu-examples-small}
 \scriptsize
 \centering
 \begin{tabular}{L{80mm}L{50mm} }
 \textbf{Dialogue Context} & \textbf{Model Response}  \\ \hline
        Hello I tried to install ubuntu studio but I get W : Failed to fetch \textless url \textgreater \ 404 Not Found when apt-get update $\rightarrow$ the ppa you added is not online & \textbf{MrRNN Act.\@ -Ent.\@:} try a different mirror \newline \textbf{MrRNN Noun:} something \newline \textbf{VHRED:} but I dont know how to add it \newline \textbf{HRED:} how do I do that ? \\ \hline 
        hi community.. is there any difference between " ~/.bashrc " " ~/.bashrc. local " $\rightarrow$ yes, they're different files :-) the default ~/.bashrc sources ~/.bashrc. local $\rightarrow$ sorry I could undersatnd... If I write on terminal " gedit ~/.bashrc " it opens a file but how I open " ~/.bashrc. local " $\rightarrow$ " gedit ~/.bashrc. local "... $\rightarrow$ but it just open blank file nothing inside it & \textbf{MrRNN Act.\@-Ent.\@:} open up gedit ~/.bashrc, there is a file called " something " \newline \textbf{MrRNN Noun:} the file is empty.. \newline \textbf{VHRED:} it's in ~/.bashrc or ~/.bashrc  \newline \textbf{HRED:} what are you trying to do ? \\ \hline
         Hey guys what do you in general use for irc something ubuntu xchat or xchat-gnome ? $\rightarrow$ without -gnome. that is just cut down $\rightarrow$ you mean drop xchat-gnome and go with xchat ? & \textbf{MrRNN Act.\@ -Ent.\@:} im using xchat right now \newline \textbf{MrRNN Noun:} what is xchat-gnome ? \newline \textbf{VHRED:} correct \quad \textbf{HRED:} yes \\ \hline
        when setting up rules with iptables command only writes changes this file " \/etc\/iptables. rules "? i ask this so i can backup before messing anything $\rightarrow$ sudo iptables-save something . dat to backup your rules restore with sudo iptables-restore \textless \ something . dat & \textbf{MrRNN Act.\@ -Ent.\@:} I don't see any reason why you need iptables to do that \newline \textbf{MrRNN Noun:} are you using ubuntu ? \newline \textbf{VHRED:} thx \quad \textbf{HRED:} thanks \\ 
 \bottomrule
 \end{tabular}
\end{table*}

\textbf{Corpus}: 
% developed by Lowe et al.
%\footnote{We use the Ubuntu Dialogue Corpus v2.0 extracted Jamuary, 2016: \url{http://cs.mcgill.ca/~jpineau/datasets/ubuntu-corpus-1.0/}}\footnote{For the context and response pairs in the official Ubuntu Dialogue Corpus, there is no distinction between the users having technical problems and other users helping them resolve their problems. Therefore the model must learn to act as both a user in need of technical support and as a user providing technical support.}
The Ubuntu Dialogue Corpus consists of about half a million dialogues extracted from the \textit{\#Ubuntu} Internet Relayed Chat (IRC) channel.
Users entering this chat channel usually have a specific technical problem. 
Typically, users first describe their problem, and other users try to help them resolve it.
The technical problems range from software-related and hardware-related issues (e.g.\@ installing packages, fixing broken drivers) to informational needs (e.g.\@ finding software).




\textbf{Evaluation}: 
%Accurate evaluation of dialogue system responses is difficult~\cite{schatzmann2005quantitative}.
%Liu et al.~\cite{liu2016not} have recently shown that all automatic evaluation metrics adopted for such evaluation, including word overlap-based metrics such as BLEU and METEOR, have either very low or no correlation with human judgement of system performance.
We carry out an in-lab human study to evaluate the model responses.
We recruit 5 human evaluators. We show each evaluator between 30 and 40 dialogue contexts with the ground truth response, and 4 candidate model responses.
% (HRED, HRED + Activity-Entity Features, MrRNN Noun, and MrRNN Activity-Entity).
For each example, we ask the evaluators to compare the candidate responses to the ground truth response and dialogue context,
and rate them for fluency and relevancy on a scale 0--4, where 0 means incomprehensible or no relevancy and 4 means flawless English or all relevant.
In addition to the human evaluation, we also evaluate dialogue responses w.r.t.\@ the activity-entity metrics proposed by ~\cite{serban2016multiresolution}. These metrics measure whether the model response contains the same activities (e.g.\@ download, install) and entities (e.g.\@ ubuntu, firefox) as the ground truth responses.
Models that generate responses with the same activities and entities as the ground truth responses---including expert responses, which often lead to solving the user's problem---are given higher scores.  Sample responses from each model are shown in Table~\ref{table:ubuntu-examples-small}.



\begin{table}[ht]
  \caption{Ubuntu evaluation using F1 metrics w.r.t.\@ activities and entities (mean scores $\pm \ 90\%$ confidence intervals), and human fluency and human relevancy scores given on a scale 0-4 {\small ($^*$ indicates scores significantly different from baseline models at $90\%$ confidence)}} \label{tabel:ubuntu_results}
  \small
  \centering
    \begin{tabular}{lccccccc}
    \toprule
%     & \textbf{Activity} & \textbf{Entity} & \multicolumn{2}{c}{\textbf{Human Eval.\@}} \\ \midrule
    \textbf{Model} & \textbf{F1 Activity} & \textbf{F1 Entity} & \textbf{Human Fluency} & \textbf{Human Relevancy} \\
    \midrule
        LSTM & \small $1.18$ \footnotesize $\pm 0.18$ & \small $0.87$ \footnotesize $\pm 0.15$ & - & - \\
        HRED & \small $4.34$ \footnotesize $\pm 0.34$ & \small $2.22$ \footnotesize $\pm 0.25$ & 2.98 & 1.01\\
        VHRED & \small $4.63$ \footnotesize $\pm 0.34$ & \small $2.53$ \footnotesize $\pm 0.26$ & - & - \\       
        %\parbox[c][2.65em][c]{0.070\textwidth}{HRED + \\ Act.\@-Ent.\@} & \small $5.46$ \footnotesize $\pm 0.35$ & \small $2.44$ \footnotesize $\pm 0.26$ & 2.96 & 0.75 \\
%        \parbox[c][2.5em][c]{0.11\textwidth}{MrRNN \\ Stem.\@} & \small $1.66$ & \small $3.18$ & - & - \\
       MrRNN Noun & \small $4.04$ \footnotesize $\pm 0.33$ & \small $\mathbf{6.31}$ \footnotesize $\mathbf{\pm 0.42}$ & \small $\mathbf{3.48}^*$ & \small $\mathbf{1.32}^*$ \\
        MrRNN Act.\@-Ent.\@ & \small $\mathbf{11.43}$ \footnotesize $\pm \mathbf{0.54}$ & \small $3.72$ \footnotesize $\pm 0.33$ & \small $\mathbf{3.42}^*$ & \small $1.04$ \\ \bottomrule
    \end{tabular}
\end{table}
% \parbox[c][2.65em][c]{0.070\textwidth}{VHRED}


\textbf{Results}:
The results are given in Table~\ref{tabel:ubuntu_results}.
The MrRNNs perform substantially better than the other models w.r.t.\@ both the human evaluation study and the evaluation metrics based on activities and entities.
MrRNN with noun representations obtains an F1 entity score at $6.31$, while all other models obtain less than half F1 scores between $0.87 - 2.53$, and human evaluators consistently rate its fluency and relevancy significantly higher than all the baseline models.
MrRNN with activity representations obtains an F1 activity score at $11.43$, while all other models obtain less than half F1 activity scores between $1.18-4.63$, and performs substantially better than the baseline models w.r.t. the\@ F1 entity score.
This indicates that the MrRNNs have learned to model high-level, goal-oriented sequential structure in the Ubuntu domain.
Followed by these, VHRED performs better than the HRED and LSTM models w.r.t.\@ both activities and entities.
This shows that VHRED generates more appropriate responses, which suggests that the latent variables are useful for modeling uncertainty and ambiguity.
Finally, HRED performs better than the LSTM baseline w.r.t.\@ both activities and entities,
which underlines the importance of representing longer-term context.
These conclusions are confirmed by additional experiments on response generation for the Twitter domain (see Appendix).
%, as well as manual inspections of the responses.

%the results indicate that the MrRNNs have learned to model high-level, goal-oriented sequential structure in the Ubuntu domain.
%In addition to this, the results indicate that modelling uncertainty and ambiguity through a latent variable has also helped the VHRED model generate appropriate responses.

%Furthermore, the MrRNNs outperform the HRED baseline augmented activity-entity features across all metrics.
%This indicates that the hierarchical generation process helps MrRNNs generate responses by adding high-level, compositional structure.

%Followed by the MrRNN models, the VHRED model performs better than both the HRED and LSTM models w.r.t.\@ both activities and entities.
%This indicates that the latent variable in VHRED helps generate meaningful responses.

% induces an inductive bias,



\vspace*{-0.5\baselineskip}
\section{Discussion}
\vspace*{-0.5\baselineskip}

% added a quick discussion, feel free to expand, remove or re-write

We have presented generative models for dialogue response generation.
%that are variants on the traditional Seq2Seq framework.
We have proposed architectural modifications with inductive biases towards 1) incorporating longer-term context, 2) handling uncertainty and ambiguity, and 3) generating diverse and on-topic responses with high-level compositional structure.
Our experiments show the advantage of the architectural modifications quantitatively through human experiments and qualitatively through manual inspections.
These experiments demonstrate the need for further research into generative model architectures.
Although we have focused on three generative models, other model architectures such as memory-based models \citep{bordes2016learning,weston2014memory} and attention-based models \citep{shang2015neural} have also demonstrated promising results and therefore deserve the attention of future research.

In another line of work, researchers have started proposing alternative training and response selection criteria~\citep{weston2016dialog}. \cite{li2015diversity} propose ranking candidate responses according to a mutual information criterion, in order to incorporate dialogue context efficiently and retrieve on-topic responses.
\cite{li2016deep} further propose a model trained using reinforcement learning to optimize a hand-crafted reward function. Both these models are motivated by the lack of \textit{diversity} observed in the generative model responses.
Similarly, \cite{yu2016strategy} propose a hybrid model---combining retrieval models, neural networks and hand-crafted rules---trained using reinforcement learning to optimize a hand-crafted reward function.
In contrast to these approaches, without combining several models or having to modify the training or response selection criterion, VHRED generates more diverse responses than previous models.
Similarly, by optimizing the joint log-likelihood over sequences, MrRNNs generate more appropriate and on-topic responses with compositional structure.
Thus, improving generative model architectures has the potential to compensate --- or even remove the need --- for hand-crafted reward functions. 

%override counter-act the need for 
%The need for further research into developing generative model architectures is 
%This underlines our previous conclusion, that much research is still required in developing generative model architectures.

At the same time, the models we propose are not necessarily better language models, which are more efficient at compressing dialogue data as measured by word perplexity.
Although these models produce responses that are preferred by humans, they often result in higher test set perplexity than traditional LSTM language models.
This suggests maximizing log-likelihood (i.e.\@ minimizing perplexity) is not a sufficient training objective for these models.
An important line of future work therefore lies in improving the objective functions for training and response selection, as well as learning directly from interactions with real users.

%may require external rewards from interacting with real users.
%This discrepancy between perplexity and human judgement of model responses 

%This discrepancy is because perplexity is reduced most significantly by the efficient compression of frequent (generic) utterances; since our models have inductive biases towards producing more diverse responses, they place lower probabilities on generic utterances, which results in a higher perplexity. 
%This suggests that maximizing log-likelihood (i.e.\@ minimizing perplexity) is not a sufficient training objective for these models, and we may require external rewards from interacting with real users.
%% insert paragraph about important future directions?



\newpage
\bibliographystyle{abbrvnat}
\begingroup
    \vspace{-2.0mm}
%    \scriptsize
    \small
    \setlength{\bibsep}{2pt}
    \bibliography{references}
\endgroup


\newpage
\section*{Appendix}

\subsection*{Twitter Results}


\textbf{Corpus}: We experiment on a Twitter Dialogue Corpus~\citep{ritter2011data} containing about one million dialogues.
The task is to generate utterances to append to existing Twitter conversations.
This task is typically categorized as a non-goal-driven task, because any fluent and on-topic response may be adequate.
%The dataset is extracted using a procedure similar to Ritter et al.~\shortcite{ritter2011data}, and is split into training, validation and test sets, containing respectively 749,060, 93,633 and 10,000 dialogues each.

\textbf{Evaluation}: We carry out a human study on Amazon Mechanical Turk (AMT).
We show human evaluators a dialogue context along with two potential responses:
one response generated from each model conditioned on the dialogue context.
We ask evaluators to choose the response most appropriate to the dialogue context.
If the evaluators are indifferent, they can choose neither response.
For each pair of models we conduct two experiments: one where the example contexts contain at least $80$ unique tokens (\emph{long context}),
and one where they contain at least $20$ (not necessarily unique) tokens (\emph{short context}).
We experiment with the LSTM, HRED and VHRED models, as well as a TF-IDF retrieval-based baseline model.
We do not experiment with the MrRNN models, because we do not have appropriate coarse representations for this domain.

\textbf{Results}:
The results given in Table~\ref{table:human-study-twitter} show that VHRED is strongly preferred in the majority of the experiments.
In particular, VHRED is strongly preferred over the HRED and TF-IDF baseline models for both short and long context settings. VHRED is also strongly preferred over the LSTM baseline model for long contexts, although the LSTM model is preferred over VHRED for short contexts.For short contexts, the LSTM model is often preferred over VHRED because the LSTM model tends to generate very \textit{generic} responses. Such \textit{generic} or \textit{safe} responses are reasonable for a wide range of contexts, but are not useful when applied through-out a dialogue, because the user would loose interest in the conversation.
%; however, the LSTM is preferred over VHRED for short contexts. %\footnote{The difference in preferences for VHRED vs. LSTM with long contexts is significant with $95\%$ confidence.}

In conclusion, VHRED performs substantially better overall than competing models,
which suggests that the high-dimensional latent variables help model uncertainty and ambiguity in the dialogue context and help generate meaningful responses.

\begin{table}[!htb]
  \caption{Wins, losses and ties (in \%) of VHRED against baselines based on the human study (mean preferences $\pm \ 90\%$ confidence intervals, where $*$ indicates significant differences at $90\%$ confidence) }
  \label{table:human-study-twitter}
  \def\arraystretch{1}
  \small
  \centering
  \begin{tabular}{lccc}
    \toprule
    \textbf{Opponent} & \textbf{Wins} & \textbf{Losses} & \textbf{Ties} \\ \midrule
    \multicolumn{4}{l}{\textbf{Short Contexts}}  \\ \midrule
    VHRED vs LSTM & \small $32.3$ \footnotesize $\pm 2.4$ & \small $\mathbf{42.5}$ \footnotesize $\mathbf{\pm 2.6}^{*}$ & \small $25.2$ \footnotesize $\pm 2.3$ \\
    VHRED vs HRED & \small $\mathbf{42.0}$ \footnotesize $\mathbf{\pm 2.8}^{*}$ & \small $31.9$ \footnotesize $\pm 2.6$ & \small $26.2$ \footnotesize $\pm 2.5$ \\
    VHRED vs TF-IDF & \small $\mathbf{51.6}$ \footnotesize $\mathbf{\pm 3.3}^{*}$ & \small $17.9$ \footnotesize $\pm 2.5$ & \small $30.4$ \footnotesize $\pm 3.0$ \\ \midrule
    \multicolumn{4}{l}{\textbf{Long Contexts}}  \\ \midrule
    VHRED vs LSTM & \small $\mathbf{41.9}$ \footnotesize $\mathbf{\pm 2.2}^{*}$ & \small $36.8$ \footnotesize $\pm 2.2$ & \small $21.3$ \footnotesize $\pm 1.9$ \\
    VHRED vs HRED & \small $\mathbf{41.5}$ \footnotesize $\mathbf{\pm 2.8}^{*}$ & \small $29.4$ \footnotesize $\pm 2.6$ & \small $29.1$ \footnotesize $\pm 2.6$ \\
    VHRED vs TF-IDF & \small $\mathbf{47.9}$ \footnotesize $\mathbf{\pm 3.4}^{*}$ & \small $11.7$ \footnotesize $\pm 2.2$ & \small $40.3$ \footnotesize $\pm 3.4$ \\ \bottomrule
  \end{tabular}
 \end{table}



\end{document}



\documentclass{article} % For LaTeX2e
\usepackage{nips13submit_e,times}
%\usepackage{hyperref}
\usepackage{url}
\usepackage{natbib}
\usepackage[utf8]{inputenc}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{algorithm}
\usepackage[noend]{algorithmic}
\usepackage{graphicx}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{theorem}{Theorem}
\newtheorem*{thmnonumber}{Theorem}
\nipsfinaltrue

\def\R{{\mathbf R}}
\newcommand{\vs}[1]{\vspace*{-#1mm}}
\newcommand{\one}{\mathbf{1}}
\newcommand{\sigm}{{\rm sigm}}
\newcommand{\splus}{{\rm s}_+}
\newcommand*{\defeq}{\stackrel{\text{def}}{=}}

\title{Estimating or Propagating Gradients Through Stochastic Neurons
for Conditional Computation}

\newif\ifLong
\Longfalse
\newif\ifConvergence
\Convergencefalse

\author{
Yoshua Bengio, Nicholas Léonard and Aaron Courville\\
Département d'informatique et recherche opérationnelle\\
Université de Montréal\\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}


\maketitle
\vs{3}
\begin{abstract}
  Stochastic neurons and hard non-linearities can be useful for a number of
  reasons in deep learning models, but in many cases they pose a
  challenging problem: how to estimate the gradient of a loss function with
  respect to the input of such stochastic or non-smooth neurons? I.e., can
  we ``back-propagate'' through these stochastic neurons?  We examine this
  question, existing approaches, and compare four families of
  solutions, applicable in different settings.  One of them is the
  minimum variance unbiased gradient estimator for stochatic binary neurons
  (a special case of the REINFORCE algorithm).
  A second approach, introduced here,
  decomposes the operation of a binary stochastic neuron into a
  stochastic binary part and a smooth differentiable part, which
  approximates the expected effect of the pure stochatic binary neuron to
  first order. A third approach involves the 
  injection of additive or multiplicative noise in a computational graph that 
is otherwise differentiable. A fourth approach heuristically copies the
gradient with respect to the stochastic output directly as an estimator of the gradient
with respect to the sigmoid argument (we call this the straight-through estimator).
To explore a context where these estimators are useful, 
we consider a small-scale version of 
{\em conditional computation}, where sparse stochastic units form a
distributed representation of gaters that can turn off in combinatorially
many ways large chunks of the computation performed in the rest of the
neural network. In this case, it is important that the gating units produce
an actual 0 most of the time. The resulting sparsity can be potentially be exploited to greatly reduce the computational cost of large deep networks for which conditional computation would be useful.
\end{abstract}

\vs{3}
\section{Introduction and Background}
\vs{2}

Many learning algorithms and in particular those based on neural networks 
or deep learning rely on gradient-based learning. To compute exact gradients,
it is better if the relationship between parameters and the training objective
is continuous and generally smooth. If it is only constant by parts, i.e., mostly flat, then
gradient-based learning is impractical. This was what motivated the
move from neural networks based on so-called formal neurons, with a hard
threshold output, to neural networks whose units are based on a
sigmoidal non-linearity, and the well-known back-propagation 
algorithm to compute the gradients~\citep{Rumelhart86b}.

We call the {\em computational graph} or {\em flow graph} the graph that
relates inputs and parameters to outputs and training criterion.  Although
it had been taken for granted by most researchers that smoothness of this
graph was a necessary condition for exact gradient-based training methods
to work well, recent successes of deep networks with rectifiers and other
``non-smooth''
non-linearities~\citep{Glorot+al-AI-2011-small,Krizhevsky-2012-small,Goodfellow+al-ICML2013-small}
clearly question that belief: see Section~\ref{sec:semi-hard} for a deeper
discussion.

In principle, even if there are hard decisions (such as the
treshold function typically found in formal neurons) in the computational
graph, it is possible to obtain {\em estimated gradients} by introducing
{\em perturbations} in the system and observing the effects. Although
finite-difference approximations of the gradient 
appear hopelessly inefficient (because independently perturbing
each of $N$ parameters to estimate its gradient would be $N$ times
more expensive than ordinary back-propagation), another option
is to introduce {\em random perturbations}, and this idea has
been pushed far (and experimented on neural networks for control)
by \citet{Spall-1992} with the
Simultaneous Perturbation Stochastic Approximation (SPSA) algorithm.

As discussed here (Section~\ref{sec:semi-hard}), non-smooth non-linearities
and stochastic perturbations can be combined to obtain reasonably
low-variance estimators of the gradient, and a good example of that success
is with the recent advances with {\em
  dropout}~\citep{Hinton-et-al-arxiv2012,Krizhevsky-2012,Goodfellow+al-ICML2013-small}. The
idea is to multiply the output of a non-linear unit by independent binomial
noise.  This noise injection is useful as a regularizer and it does slow
down training a bit, but not apparently by a lot (maybe 2-fold), which is very
encouraging. The symmetry-breaking and induced sparsity may also compensate
for the extra variance and possibly help to reduce ill-conditioning, as
hypothesized by~\citet{Bengio-arxiv-2013}.

However, it is appealing to consider noise whose amplitude can be modulated
by the signals computed in the computational graph, such as with
{\em stochastic binary neurons}, which output a 1 or a 0 according
to a sigmoid probability. Short of computing an average over an
exponential number of configurations, it would seem that computing the
exact gradient (with respect to the average of the loss over all possible
binary samplings of all the stochastic neurons in the neural network)
is impossible in such neural networks. The question is whether good
estimators (which might have bias and variance) or similar alternatives
can be computed and yield effective training. We discuss and compare here
four reasonable solutions to this problem, present theoretical
results about them, and small-scale experiments to validate that training
can be effective, in the context where one wants to use such stochastic
units to gate computation. The motivation, described further below, is
to exploit such sparse stochastic gating units for {\em conditional
computation}, i.e., avoiding to visit every parameter for every example,
thus allowing to train potentially much larger models for the same
computational cost.

\iffalse
We
show in Section~\ref{sec:unbiased} that one can indeed produce
an unbiased estimator of the gradient, and that this gradient is
even cheaper to compute than with the back-propagation algorithm, 
since it does not require a backwards pass. 
To compensate its possibly
high variance, we also propose in Section~\ref{sec:biased} a biased
but lower-variance estimator that is based on {\em learning a correction
factor to deterministically transform a biased but low-variance estimator 
into a less biased one, with the same variance}.

In a first attempt to study the question of the efficiency of such
estimators, in Section~\ref{sec:correlation}, we first show that the proposed
unbiased estimator can be seen as basically estimating the correlation
between the perturbation and the training loss. We then show that the
Boltzmann machine gradient can also be interpreted as a form of correlation-based
estimator, but missing an additive normalization. 
This is encouraging, since various Boltzmann machines (in
particular the restricted Boltzmann machine) have been quite successful in
recent years~\citep{Hinton06-small,Bengio-2009-book}.
\fi
\vs{2}
\subsection{More Motivations and Conditional Computation}
\vs{2}

One motivation for studying stochastic neurons is that stochastic behavior
may be a required ingredient in {\em modeling biological neurons}. The
apparent noise in neuronal spike trains could come from an actual noise
source or simply from the hard to reproduce changes in the set of input
spikes entering a neuron's dendrites. Until this question is resolved by
biological observations, it is interesting to study how such noise -- which
has motivated the Boltzmann machine~\citep{Hinton84} -- may impact
computation and learning in neural networks.

Stochastic neurons with binary outputs are also interesting because
they can easily give rise to {\em sparse representations} (that have many zeros), 
a form of regularization that has been used in many representation
learning algorithms~\citep{Bengio-Courville-Vincent-TPAMI2013}.
Sparsity of the representation corresponds to the prior that, for a given
input scene, most of the explanatory factors are irrelevant (and that
would be represented by many zeros in the representation). 

\iffalse
Non-smooth stochastic neurons such as those studied in Section~\ref{sec:semi-hard}
also give rise to {\em sparse gradients}, i.e., such that for most
examples, the gradient vector (with respect to parameters) has many zeros.
Indeed, for weights into units that are shut off or are in a flat
saturation region (e.g., at 0 or 1), the derivative will be zero.
As argued in \citet{Bengio-arxiv-2013}, sparse gradients may be useful
to reduce the optimization difficulty due to ill-conditioning, by
reducing the number of interactions between parameters to those parameters
that are simultaneously ``active'' (with a non-zero gradient) for a particular
example.
\fi

As argued by~\citet{Bengio-arxiv-2013}, sparse representations may
be a useful ingredient of {\em conditional computation}, by which
only a small subset of the model parameters are ``activated'' (and need
to be visited) for any particular example, thereby greatly reducing the
number of computations needed per example. Sparse gating units may
be trained to select which part of the model actually need to be computed
for a given example.

Binary representations are also useful as keys for a hash table, as in the
semantic hashing algorithm~\citep{Salakhutdinov+Geoff-2009}.  
\iffalse
In the
latter, zero-mean Gaussian noise is added to the pre-sigmoid activation of
the code neuron (whose output will be used as hash keys). As the amount of
noise is increased, this forces the weights and biases to also increase to
avoid losing too much information in the sigmoid output, thereby gradually
forcing these units into saturation, i.e., producing a nearly 0 or nearly 1
output. Hence one option would be to train hard-decision neurons by
gradually annealing the magnitude of the parameters, turning soft-output
easy to train units into hard-decision binary-output units. However, it
means that during training, we would not be able to take advantage of the hard zeros 
(only available at the end of training). Since both conditional computation
and sparse gradients are motivated by speeding up training, this would not
be an interesting solution.
\fi
Trainable stochastic neurons would also be useful inside recurrent networks
to take hard stochastic decisions about temporal events at different
time scales. This would be useful to train multi-scale temporal hierarchies
\footnote{Daan Wierstra, personal communication} such that back-propagated
gradients could quickly flow through the slower time scales. Multi-scale
temporal hierarchies for recurrent nets have already been proposed and
involved exponential moving averages of different time
constants~\citep{ElHihi+Bengio-nips8}, where each unit is still updated
after each time step.  Instead, identifying key events at a high level of
abstraction would allow these high-level units to only be updated when
needed (asynchronously), creating a fast track (short path) for gradient
propagation through time.

\vs{2}
\subsection{Prior Work}
\vs{2}

The idea of having stochastic neuron models is of course very old, with one
of the major family of algorithms relying on such neurons being the
Boltzmann machine~\citep{Hinton84}.  Another biologically motivated
proposal for synaptic strength learning was proposed by
\citet{Fiete+Seung-2006}. It is based on small zero-mean {\em i.i.d.}
perturbations applied at each stochastic neuron potential (prior to a
non-linearity) and a Taylor expansion of the expected reward as a function
of these variations. \citet{Fiete+Seung-2006} end up proposing a gradient
estimator that looks like a {\em correlation between the reward and the
  perturbation}, very similar to that presented in
Section~\ref{sec:unbiased}, and which is a special case of the
REINFORCE~\citep{Williams-1992} algorithm. However, unlike REINFORCE, their
estimator is only unbiased in the limit of small perturbations.

Gradient estimators based on stochastic perturbations have long been shown
to be much more efficient than standard finite-difference
approximations~\citep{Spall-1992}. Consider $N$ quantities $u_i$ to 
be adjusted in order to minimize an expected loss $L(u)$. A finite difference
approximation is based on measuring separately the effect of changing each
one of the parameters, e.g., through $\frac{L(u) - L(u - \epsilon e_i)}{\epsilon}$,
or even better, through $\frac{L(u + \epsilon e_i) - L(u - \epsilon e_i)}{2 \epsilon}$,
where $e_i = (0,0,\cdots,1,0,0,\cdots,0)$ where the 1 is at position $i$. With $N$
quantities (and typically $O(N)$ computations to calculate $L(u)$), the
computational cost of the gradient estimator is $O(N^2)$. Instead, a perturbation-based
estimator such as found in 
Simultaneous Perturbation Stochastic Approximation (SPSA) \citep{Spall-1992}
chooses a random perturbation vector $z$ (e.g., isotropic Gaussian noise
of variance $\sigma^2$) and estimates the gradient of the expected loss with
respect to $u_i$ through $\frac{L(u + z) - L(u - z)}{2 z_i}$.
So long as the perturbation does not put too much probability around 0,
this estimator is as efficient as the finite-difference estimator but
requires $O(N)$ less computation. However, like the algorithm proposed
by \citet{Fiete+Seung-2006} this estimator becomes unbiased only
as the perturbations go towards 0. When we want to consider all-or-none
perturbations (like a neuron sending a spike or not), it is not clear
if these assumptions are appropriate. An advantage of the approaches
proposed here is that they do not require that the perturbations
be small to be valid.


\vs{3}
\section{Non-Smooth Stochastic Neurons}
\label{sec:semi-hard}
\vs{2}

One way to achieve gradient-based learning in networks of stochastic neurons 
is to build an architecture in which noise is injected so that gradients
can sometimes flow into the neuron and can then adjust it (and its predecessors
in the computational graph) appropriately.

In general, we can consider the output $h_i$ of a stochastic neuron as the
application of a deterministic function that depends on some noise source $z_i$
and on some differentiable
transformation $a_i$ of its inputs
(typically, a vector containing the outputs of other neurons) and internal parameters
(typically the bias and incoming weights of the neuron):
\begin{equation}
\label{eq:noisy-output}
  h_i = f(a_i,z_i) 
\end{equation}
So long as $f(a_i,z_i)$ in the above equation has a non-zero
gradient with respect to $a_i$, gradient-based learning 
(with back-propagation to compute gradients) can proceed.
In this paper, we consider the usual affine transformation 
$a_i = b_i + \sum_j W_{ij} x_{ij}$, where $x_i$ is the vector
of inputs into neuron $i$.

For example, if the noise $z_i$ is added or multiplied somewhere in the
computation of $h_i$, gradients can be computed as usual. Dropout
noise~\citep{Hinton-et-al-arxiv2012} and masking noise (in denoising
auto-encoders~\citep{VincentPLarochelleH2008-small}) is multiplied (just
after the neuron non-linearity), while in semantic
hashing~\citep{Salakhutdinov+Geoff-2009} noise is added (just before the
non-linearity). For example, that noise can be binomial (dropout, masking
noise) or Gaussian (semantic hashing).

However, if we want $h_i$ to be binary (like in stochastic binary neurons), 
then $f$ will have derivatives that are 0 almost everywhere (and infinite
at the threshold), so that gradients can never flow. 

There is an intermediate option that we put forward here: choose $f$ so
that it has two main kinds of behavior, with zero derivatives in some
regions, and with significantly non-zero derivatives in other regions.  
We call these two states of the neuron respectively the insensitive state
and the sensitive state. 

\vs{3}
\subsection{Noisy Rectifier}
\vs{2}

A special case is when the insensitive state corresponds to $h_i=0$ and we
have sparsely activated neurons. The prototypical example of that situation
is the rectifier unit~\citep{Hinton2010,Glorot+al-AI-2011-small}, whose
non-linearity is simply $\max(0,{\rm arg})$, i.e.,
\[
  h_i = \max(0, z_i + a_i)
\]
where $z_i$ is a zero-mean noise. Although in practice Gaussian noise is
appropriate, interesting properties can be shown for $z_i$ sampled from
the logistic density $p(z)=\sigm(z)(1-\sigm(z))$, where $\sigm(\cdot)$
represents the logistic sigmoid function.

In that case, we can prove nice properties of the noisy rectifier.
\begin{proposition}
\label{thm:noisy-rectifier}
The noisy rectifier with logistic noise has the following properties: (a) $P(h_i>0)=\sigm(a_i)$,
(b) $E[h_i]=\splus(a_i)$, where $\splus(x)=\log(1+\exp(x))$ is the softplus function
and the random variable is $z_i$ (given a fixed $a_i$).
\end{proposition}
\begin{proof}
\begin{eqnarray*}
 P(h_i>0) &=& P(z_i>-a_i) = 1-\sigm(-a_i)\\
  %&=& \frac{1 + e^{a_i} - 1}{1+e^{-(-a_i)}} = \frac{1}{1+e^{-a_i}} = \sigm(a_i)
  &=& 1 - \frac{1}{1 + e^{a_i}} = \frac{1}{1+e^{-a_i}} = \sigm(a_i)
\end{eqnarray*}
which proves (a). For (b), we use the facts $\frac{d \,\sigm(x)}{dx}=\sigm(x)(1-\sigm(x))$,
$\splus(u) \rightarrow u$ as $u\rightarrow\infty$,
change of variable $x=a_i+z_i$ and integrate by parts:
\begin{eqnarray*}
 E[h_i] &=& \lim_{t\rightarrow\infty} \int_{z_i>-a_i}^t (a_i+z_i) \sigm(z_i)(1-\sigm(z_i) dz_i \\
    &=& \lim_{t\rightarrow\infty} \left(\left[x \,\sigm(x-a_i)\right]_0^t - \int_0^t \sigm(x-a)dx \right)\\
    &=& \lim_{t\rightarrow\infty} \left[x \,\sigm(x-a_i)-\splus(x-a_i)\right]_0^t \\
    &=& a_i + \splus(-a_i) = \splus(a_i)
\end{eqnarray*}
\end{proof}

Let us now consider two cases:
\begin{enumerate}
\item If $f(a_i,0)>0$, the basic state is {\em active},
the unit is generally sensitive and non-zero,
but sometimes it is shut off (e.g., when $z_i$ is sufficiently negative to push 
the argument of the rectifier below 0). In that case gradients will flow
in most cases (samples of $z_i$). If the rest of the system sends the signal 
that $h_i$ should have been smaller, then gradients will push it towards
being more often in the insensitive state.
\item If $f(a_i,0)=0$, the basic state is {\em inactive},
  the unit is generally insensitive and zero,
  but sometimes turned on (e.g., when $z_i$ is sufficiently positive to push the
  argument of the rectifier above 0). In that case gradients will not flow
  in most cases, but when they do, the signal will either push the weighted
  sum lower (if being active was not actually a good thing for that unit in
  that context) and reduce the chances of being active again, or it will
  push the weight sum higher (if being active was actually a good thing for
  that unit in that context) and increase the chances of being active
  again.
\end{enumerate}

So it appears that even though the gradient does not always flow
(as it would with sigmoid or tanh units), it might flow sufficiently
often to provide the appropriate training information. The important
thing to notice is that even when the basic state (second case, above)
is for the unit to be insensitive and zero, {\em there will be
an occasional gradient signal} that can draw it out of there.

One concern with this approach is that one can see an asymmetry between
the number of times that a unit with an active state can get a chance
to receive a signal telling it to become inactive, versus the number of
times that a unit with an inactive state can get a signal telling
it to become active. 

Another potential and related concern is that some of these units will
``die'' (become useless) if their basic state is inactive in the vast
majority of cases (for example, because their weights pushed them into that
regime due to random fluctations). Because of the above asymmetry, dead
units would stay dead for very long before getting a chance of being born
again, while live units would sometimes get into the death zone by chance
and then get stuck there. What we propose here is a simple mechanism to
{\em adjust the bias of each unit} so that in average its ``firing rate''
(fraction of the time spent in the active state) reaches some pre-defined
target. For example, if the moving average of being non-zero falls below a
threshold, the bias is pushed up until that average comes back above the
threshold.

\vs{3}
\subsection{STS Units: Stochastic Times Smooth}
\vs{2}

We propose here a novel form of stochastic unit that is related to
the noisy rectifier and to the stochastic binary unit, but that can
be trained by ordinary gradient descent with the gradient obtained
by back-propagation in a computational graph. We call it the
STS unit (Stochastic Times Smooth).
With $a_i$ the
activation before the stochastic non-linearity, the output of
the STS unit is
\begin{align}
\label{eq:sts-unit}
 p_i =& \; \sigm(a_i) \nonumber \\
 b_i \sim& \; {\rm Binomial}(\sqrt{p_i}) \nonumber \\
 h_i =& \; b_i \sqrt{p_i}
\end{align}

\begin{proposition}
\label{thm:sts-unit}
The STS unit benefits from the following properties: (a) $E[h_i]=p_i$,
(b) \mbox{$P(h_i>0)=\sqrt{\sigm(a_i)}$}, (c) for any differentiable function $f$
of $h_i$, we have
\[
   E[f(h_i)] = f(p_i) + o(\sqrt{p_i})
\]
as $p_i\rightarrow 0$, and 
where the expectations and probability are over the injected noise $b_i$, given $a_i$.
\end{proposition}
\begin{proof}
Statements (a) and (b) are immediately derived from the definitions
in Eq.~\ref{eq:sts-unit} by noting that $E[b_i]=P(b_i>0)=P(h_i>0)=\sqrt{p_i}$.
Statement (c) is obtained by first writing out the expectation,
\[
  E[f(h_i)] = \sqrt{p_i}f(\sqrt{p_i})+(1-\sqrt{p_i})f(0)
\]
and then performing a Taylor expansion of $f$ around $p_i$ as $p_i \rightarrow 0$):
\begin{eqnarray*}
  f(\sqrt{p_i}) &=& f(p_i) + f'(p_i)(\sqrt{p_i}-p_i) + o(\sqrt{p_i}-p_i) \\
  f(0) &=& f(p_i) + f'(p_i)(-p_i) + o(p_i) 
\end{eqnarray*}
where $\frac{o(x)}{x} \rightarrow 0$ as $x \rightarrow 0$, so we obtain
\begin{eqnarray*}
 E[f(h_i)] &=& \sqrt{p_i}(f(p_i)+f'(p_i)(\sqrt{p_i}-p_i))+(1-\sqrt{p_i})(f(p_i) + f'(p_i)(-p_i)) + o(\sqrt{p_i}) \\
   &=& \sqrt{p_i} f(p_i)+f'(p_i)(p_i-p_i\sqrt{p_i})
     +f(p_i)(1-\sqrt{p_i}) - f'(p_i)(p_i - p_i \sqrt{p_i}) + o(\sqrt{p_i}) \\
   &=& f(p_i) + o(\sqrt{p_i}) 
\end{eqnarray*}
using the fact that $\sqrt{p_i}>p_i$, $o(p_i)$ can be replaced by $o(\sqrt{p_i})$.
\end{proof}
Note that this derivation can be generalized to an expansion around $f(x)$ for
any $x\leq \sqrt{p_i}$, yielding
\[
   E[f(h_i)] = f(x) + (x-p_i)f'(x) + o(\sqrt{p_i}) 
\]
where the effect of the first derivative is canceled out when we choose $x=p_i$.
It is also a reasonable choice to expand around $p_i$ because $E[h_i]=p_i$.

\iffalse
It is interesting to compare the non-smoth stochastic neuron
described above with the stochastic max-pooling unit proposed
by~\citet{Zeiler+et+al-ICLR2013}, in which the pool output is
\[
 h_i = \sum_j z_{ij} x_{ij}
\]
with $z_{ij}$ sampled from the multinomial with
probabilities $P(z_{ij}=1|x_i) = \frac{x_{ij}}{\sum_j x_{ij}}$,
with $x_{ij}$ guaranteed to be non-negative.
\fi
\vs{3}
\section{Unbiased Estimator of Gradient for Stochastic Binary Neurons}
\label{sec:unbiased}
\vs{2}

The above proposals cannot deal with non-linearities like
the indicator function that have a derivative of 0 almost everywhere.
Let us consider this case now, where we want some component of our model
to take a hard binary decision but allow this decision to be stochastic,
with a probability that is a continuous function of some 
quantities, through parameters that we wish to learn. 
We will also assume that many such decisions can be taken
in parallel with independent noise sources $z_i$ driving the stochastic
samples. Without loss of generality, we consider here a
set of binary decisions, i.e., the setup corresponds to
having a set of stochastic binary neurons, whose output $h_i$
influences an observed future loss $L$. In the framework of
Eq.~\ref{eq:noisy-output}, we could have for example
\begin{equation}
\label{eq:stochastic-binary-neuron}
  h_i = f(a_i, z_i) = \one_{z_i > \sigm(a_i)}
\end{equation}
where $z_i \sim U[0,1]$ is uniform and $\sigm(u)=1/(1+\exp(-u))$
is the sigmoid function. 
We would ideally like to estimate
how a change in $a_i$ would impact $L$ in average over the
noise sources, so as to be able to propagate this estimated
gradients into parameters and inputs of the stochastic neuron.
\begin{theorem}
\label{thm:unbiased-estimator}
Let $h_i$ be defined as in Eq.~\ref{eq:stochastic-binary-neuron},
with $L=L(h_i,c_i,c_{-i})$ a loss that depends stochastically on $h_i$, $c_i$ the
noise sources that influence $a_i$, and $c_{-i}$ those that do not
influence $a_i$,
then \mbox{$\hat{g}_i=(h_i - \sigm(a_i)) \times L$}
is an unbiased estimator of $g_i=\frac{\partial
  E_{z_i,c_{-i}}[L|c_i]}{\partial a_i}$ where the expectation is over $z_i$
and $c_{-i}$, conditioned on the set of
noise sources $c_i$ that influence $a_i$.
\vs{3}
\end{theorem}

\begin{proof}
We will compute the expected value of the estimator and verify
that it equals the desired derivative. The set of all noise sources
in the system is $\{z_i\} \cup c_i \cup c_{-i}$. We can consider
$L$ to be an implicit deterministic function of all the noise sources
$(z_i,c_i,c_{-i})$, $E_{v_z}[\cdot]$ denotes the expectation over variable $v_z$,
while $E[\cdot|v_{z}]$ denotes
the expectation over all the other random variables besides $v_z$, i.e., conditioned on $v_Z$.
\begin{align}
 E[L|c_i] &= E_{c_{-i}}[E_{z_i}[L(h_i,c_i,c_{-i})]] \nonumber \\
                    &= E_{c_{-i}}[E_{z_i}[h_i L(1,c_i,c_{-i})+(1-h_i) L(0,c_i,c_{-i})]] \nonumber \\
                    &=E_{c_{-i}}[P(h_i=1|a_i) L(1,c_i,c_{-i})+P(h_i=0|a_i) L(0,c_i,c_{-i})] \nonumber \\
                    &=E_{c_{-i}}[\sigm(a_i) L(1,c_i,c_{-i})+(1-\sigm(a_i)) L(0,c_i,c_{-i})] 
\end{align}
Since $a_i$ does not influence $P(c_{-i})$, differentiating with respect to $a_i$ gives
\begin{align}
 g_i \defeq \frac{\partial E[L|c_i]}{\partial a_i} &= 
   E_{c_{-i}}[\frac{\partial \sigm(a_i)}{\partial a_i} L(1,c_i,c_{-i})-
            \frac{\partial \sigm(a_i)}{\partial a_i} L(0,c_i,c_{-i}) | c_i]  \nonumber \\
 &=  E_{c_{-i}}[\sigm(a_i)(1-\sigm(a_i))(L(1,c_i,c_{-i})-L(0,c_i,c_{-i}) | c_i] 
\label{eq:gradient}
\end{align}
First consider that since  $h_i \in \{0,1\}$,
\[
 L(h_i,c_i,c_{-i}) = h_i L(1,c_i,c_{-i}) + (1-h_i) L(0,c_i,c_{-i})
\]
$h_i^2=h_i$ and $h_i(1-h_i)=0$, so
\begin{align}
\hat{g}_i \defeq (h_i - \sigm(a_i)) L(h_i,c_i,c_{-i}) &= h_i(h_i -
\sigm(a_i)) L(1,c_i,c_{-i}) \\
& \hspace{1cm} + (h_i-\sigm(a_i))(1-h_i) L(0,c_i,c_{-i})) \nonumber \\
  &= h_i(1 - \sigm(a_i)) L(1,c_i,c_{-i}) \\
& \hspace{1cm} - (1-h_i) \sigm(a_i) L(0,c_i,c_{-i}).
\end{align}
Now let us consider the expected value of the estimator $\hat{g}_i=(h_i - \sigm(a_i)) L(h_i,c_i,c_{-i})$.
\begin{align}{ll}
 E[\hat{g}_i] &= 
   E[h_i(1 - \sigm(a_i)) L(1,c_i,c_{-i}) - (1-h_i) \sigm(a_i) L(0,c_i,c_{-i})] \nonumber \\
  &=    E_{c_i,c_{-i}}[\sigm(a_i)(1 - \sigm(a_i)) L(1,c_i,c_{-i}) - (1-\sigm(a_i)) \sigm(a_i) L(0,c_i,c_{-i})] \nonumber \\
  &=    E_{c_i,c_{-i}}[\sigm(a_i)(1 - \sigm(a_i)) (L(1,c_i,c_{-i}) -L(0,c_i,c_{-i}))] 
\end{align}
which is the same as Eq.~\ref{eq:gradient}, i.e., the expected value of the
estimator equals the gradient of the expected loss, $E[\hat{g}_i]=g_i$.
\end{proof}
This estimator is a special case of the REINFORCE
algorithm when the stochastic unit is a Bernoulli with probability
given by a sigmoid~\citep{Williams-1992}. In the REINFORCE paper, Williams
shows that if the stochastic action $h$ is sampled with probability
$p_\theta(h)$ and yields a reward $R$, then 
\[
  E_h[(R-b) \cdot  \frac{\partial \log p_\theta(h)}{\partial \theta}] = 
  \frac{\partial E_h[R]}{\partial \theta}
\]
where $b$ is an arbitrary constant,
i.e., the sampled value $h$ can be seen as a weighted maximum
likelihood target for the output distribution $p_\theta(\cdot)$,
with the weights $R-b$ proportional to the reward. The additive
normalization constant $b$ does not change the expected gradient,
but influences its variance, and an optimal choice can be computed,
as shown below.

\begin{corollary}
\label{eq:corollary}
Under the same conditions as Theorem~\ref{thm:unbiased-estimator},
and for any (possibly unit-specific) constant $\bar{L}_i$ 
the centered estimator $(h_i - \sigm(a_i))(L - \bar{L}_i)$,
is also an unbiased estimator of 
$g_i=\frac{\partial E_{z_i,c_{-i}}[L|c_i]}{\partial a_i}$. 
Furthermore, among all possible values of $\bar{L}_i$, the minimum 
variance choice is 
\begin{equation}
\label{eq:opt-L}
 \bar{L}_i = \frac{E[(h_i-\sigm(a_i))^2 L]}{E[(h_i-\sigm(a_i))^2]},
\end{equation}
which we note is a weighted average of the loss values $L$, whose
weights are specific to unit $i$.
\end{corollary}
\begin{proof}
The centered estimator $(h_i - \sigm(a_i))(L - \bar{L}_i)$ can be decomposed
into the sum of the uncentered estimator $\hat{g}_i$ and the term
$(h_i - \sigm(a_i))\bar{L}_i$.  Since $E_{z_i}[h_i|a_i]=\sigm(a_i)$, 
$E[\bar{L}_i(h_i - \sigm(a_i))|a_i]=0$, so that
the expected value of the centered estimator equals the
expected value of the uncentered estimator. By
Theorem~\ref{thm:unbiased-estimator} (the uncentered estimator is
unbiased), the centered estimator is therefore also unbiased,
which completes the proof of the first statement.

Regarding the optimal choice of $\bar{L}_i$,
first note that the variance of the uncentered estimator is
\[
 Var[(h_i - \sigm(a_i))L]=E[(h_i - \sigm(a_i))^2L^2] - E[\hat{g}_i]^2.
\]
Now let us compute the variance of the centered estimator:
\begin{align}
  Var[(h_i - \sigm(a_i))(L - \bar{L}_i)] &= 
  E[(h_i - \sigm(a_i))^2(L - \bar{L}_i)^2] - E[(h_i-\sigma(a_i))(L-\bar{L}_i)]^2 \nonumber \\
 &= E[(h_i - \sigm(a_i))^2L^2] + E[(h_i-\sigm(a_i))^2\bar{L}_i^2] \nonumber \\
   & - 2 E[(h_i-\sigm(a_i))^2L\bar{L}_i] -(E[\hat{g}_i]-0)^2 \nonumber \\
 &= Var[(h_i - \sigm(a_i))L] - \Delta 
\end{align}
where $\Delta=2 E[(h_i-\sigm(a_i))^2 L\bar{L}_i] - E[(h_i-\sigm(a_i))^2\bar{L}_i^2]$.
Let us rewrite $\Delta$:
\begin{align}
 \Delta &= 2 E[(h_i-\sigm(a_i))^2 L \bar{L}_i] - E[(h_i-\sigm(a_i))^2\bar{L}_i^2] \nonumber \\
      &= E[(h_i-\sigm(a_i))^2 \bar{L}_i(2L - \bar{L}_i)] \nonumber \\
   &= E[(h_i-\sigm(a_i))^2 (L^2 - (L-\bar{L}_i)^2)] 
\end{align}
$\Delta$ is maximized (to minimize variance of the estimator)
when $E[(h_i-\sigm(a_i))^2 (L-\bar{L}_i)^2]$ is minimized. Taking the
derivative of that expression with respect to $\bar{L}_i$, we obtain
\[
\vs{2}
  2 E[(h_i-\sigm(a_i))^2 (\bar{L}_i-L)] =0
%\vs{1}
\]
which, as claimed, is achieved for 
\[
\vs{3}
  \bar{L}_i = \frac{E[(h_i-\sigm(a_i))^2 L]}{E[(h_i-\sigm(a_i))^2]}.
\vs{2}
\]
\vspace*{-4mm}
\end{proof}
Note that a general formula for the lowest variance estimator
for REINFORCE~\citep{Williams-1992} had already been introduced in the reinforcement
learning context by \citet{Weaver+Tao-UAI2001},
which includes the above result as a special case. This followed from
previous work~\citep{Dayan-1990} for the case of binary immediate reward.

Practically, we could get the lowest variance estimator (among all choices of the $\bar{L}_i$)
by keeping track of two numbers (running or moving averages) 
for each stochastic neuron, one for the numerator
and one for the denominator of the unit-specific $\bar{L}_i$ in Eq.~\ref{eq:opt-L}.
This would lead the lowest-variance estimator $(h_i - \sigm(a_i))(L - \bar{L}_i)$.
%
Note how the unbiased estimator only requires broadcasting $L$ throughout the
network, no back-propagation and only local computation.
Note also how this could be applied even with an estimate
of future rewards or losses $L$, as would be useful in the context of
reinforcement learning (where the actual loss or reward will be measured farther
into the future, much after $h_i$ has been sampled).

\section{Straight-Through Estimator}

Another estimator of the expected gradient through stochastic neurons
was proposed by \citet{Hinton-Coursera2012}
in his lecture 15b. The idea is simply to back-propagate
through the hard threshold function (1 if the argument is positive, 0 otherwise)
as if it had been the identity function. It is clearly a biased estimator,
but when considering a single layer of neurons, it has the right sign
(this is not guaranteed anymore when back-propagating through more hidden layers).
We call it the {\bf straight-through} (ST) estimator. A possible variant
investigated here multiplies the gradient on $h_i$ by the derivative of the
sigmoid. Better results were actually obtained without multiplying by
the derivative of the sigmoid. With $h_i$ sampled as per Eq.~\ref{eq:stochastic-binary-neuron},
the straight-through estimator  of the gradient of the loss $L$ with respect to the
pre-sigmoid activation $a_i$ is thus 
\begin{equation}
  g_i = \frac{\partial L}{\partial h_i}.
\end{equation}
Like the other estimators, it is then back-propagated to obtain gradients on the parameters
that influence $a_i$.

\begin{figure}[ht]
\centering
%\hspace*{-2mm}
\vs{4}
\includegraphics[width=0.85\textwidth]{cond_comp_arch.pdf}
\vs{3}
\caption{Conditional computation architecture: gater path on the left
produces sparse outputs $h_i$ which are multiplied elementwise by
the expert outputs, hidden units $H_i$.}
\label{fig:arch}
\vs{2}
\end{figure}

\vs{3}
\section{Conditional Computation Experiments}
\vs{2}

We consider the potential application of stochastic neurons in the
context of conditional computation, where the stochastic neurons are
used to select which parts of some computational graph should be
actually computed, given the current input. The particular architecture we
experimented with is a neural network with a large hidden layer whose
units $H_i$ will be selectively turned off by gating units $h_i$,
i.e., the output of the i-th hidden unit is $H_i h_i$, as illustrated
in Figure~\ref{fig:arch}.
For this to make sense in the context of conditional computation, we
want $h_i$ to be non-zero only a small fraction $\alpha$ of the time (10\% in the
experiments) while the amount of computation required to compute $h_i$
should be much less than the amount of computation required to compute $H_i$.
In this way, we can first compute $h_i$ and only compute $H_i$ if $h_i \neq 0$,
thereby saving much computation. To achieve this, we connect the previous
layer to $h_i$ through a bottleneck layer. Hence if the previous layer
(or the input) has size $N$ and the main path layer also has size $N$
(i.e., $i \in \{1,\ldots N\}$), and the gater's bottleneck layer has size $M \ll N$,
then computing the gater output is $O(MN)$, which can reduce the main path
computations from $O(N^2)$ to $O(\alpha N^2)$.

\begin{figure}[ht]
\centering
%\hspace*{-2mm}
\vs{3}
\begin{minipage}{.4\textwidth}
\hspace*{-5mm}\includegraphics[width=1.3\textwidth]{comparison.png}
\end{minipage} \hspace*{-1mm} %
\begin{minipage}{.59\textwidth}
  \centering
{\small
\hspace*{4mm}  \begin{tabular}{|l|ccc|}
    \hline
    & train & valid & test \\
    \hline
    Noisy Rectifier & 6.7e-4 & 1.52 & 1.87 \\
    Straight-through & 3.3e-3  & 1.42 & 1.39 \\
    Smooth Times Stoch. & 4.4e-3 & 1.86 & 1.96 \\
    Stoch. Binary Neuron & 9.9e-3 & 1.78 & 1.89 \\
    \hline
    Baseline Rectifier & 6.3e-5 & 1.66 & 1.60 \\
    Baseline Sigmoid+Noise & 1.8e-3 & 1.88 & 1.87\\
    Baseline Sigmoid &3.2e-3 & 1.97 & 1.92\\
    \hline
  \end{tabular}
}
\end{minipage} 
\vs{5}
\caption{\small Left: Learning curves for the various stochastic approaches applied to the MNIST dataset. The y-axis indicates classification error on the validation set. Right: Comparison of training criteria  and valid/test classification error (\%) for stochastic (top) and baseline (bottom) gaters. }
\label{fig:curves+results}
\vs{1}
\end{figure}

Experiments are performed with a gater of 400 hidden units and 2000 output
units. The main path also has 2000 hidden units. A sparsity constraint is imposed
on the 2000 gater output units such that each is non-zero 10\% of the time,
on average. The validation and test errors of the stochastic models
are obtained after optimizing a threshold in their deterministic counterpart used during testing.
See the appendix in supplementary material for more details on
the experiments.

For this experiment, we compare with 3 baselines. The Baseline Rectifier is
just like the noisy rectifier, but with 0 noise, and is also
constrained to have a sparsity of 10\%. The Baseline Sigmoid is like the
STS and ST models in that the gater uses sigmoids for its output and tanh
for its hidden units, but has no sparsity constraint and only 200 output
units for the gater and main part. The baseline has the same hypothetical
resource constraint in terms of computational efficiency (run-time), but
has less total parameters (memory). The Baseline Sigmoid with Noise is the
same, but with Gaussian noise added during training.

%\begin{table}[!htb]
%  \caption{Comparison of training criteria  and valid/test classification error (\%) for stochastic (top) and baseline (%bottom) experiments.}
%  \label{results}
%\end{table}



\vs{3}
\section{Conclusion}
\vs{2}

In this paper, we have motivated estimators of the gradient through highly
non-linear non-differentiable functions (such as those corresponding to an
indicator function), especially in networks involving noise sources, such
as neural networks with stochastic neurons. They can be useful as
biologically motivated models and they might be useful for engineering
(computational efficiency) reasons when trying to reduce computation via
conditional computation or to reduce interactions between parameters via
sparse updates~\citep{Bengio-arxiv-2013}. We have proven interesting properties
of three classes of stochastic neurons, the noisy rectifier, the STS unit,
and the binary stochastic neuron, in particular showing the existence
of an unbiased estimator of the gradient for the latter. 
Unlike the SPSA~\citep{Spall-1992} estimator, our estimator is unbiased
even though the perturbations are not small (0 or 1), and it multiplies
by the perturbation rather than dividing by it.

Experiments show that all the tested methods actually allow training to
proceed.  It is interesting to note that the gater with noisy rectifiers
yielded better results than the one with the non-noisy baseline
rectifiers. Similarly, the sigmoid baseline with noise performed better
than without, {\em even on the training objective}: all these results
suggest that injecting noise can be useful not just as a regularizer but
also to help explore good parameters and fit the training objective.  These
results are particularly surprising for the stochastic binary neuron, which
does not use any backprop for getting a signal into the gater, and opens
the door to applications where no backprop signal is available. In terms of
conditional computation, we see that the expected saving is achieved,
without an important loss in performance. Another surprise is the good
performance of the straight-through units, which provided the best
validation and test error, and are very simple to implement.

\ifnipsfinal
\subsubsection*{Acknowledgments}

The authors would like to acknowledge the useful comments from Guillaume
Alain and funding from NSERC, Ubisoft, CIFAR (YB is a CIFAR Fellow),
and the Canada Research Chairs.
\fi

%References follow the acknowledgments. Use unnumbered third level heading for
%the references. Any choice of citation style is acceptable as long as you are
%consistent. It is permissible to reduce the font size to `small' (9-point) 
%when listing the references. {\bf Remember that this year you can use
%a ninth page as long as it contains \emph{only} cited references.}

\newpage

%\small{
\bibliography{strings,strings-shorter,ml,aigaion-shorter}
\bibliographystyle{natbib}
%}

\newpage

\ifnipsfinal
\appendix
\newif\ifsupplementary
\supplementaryfalse

\section{Details of the Experiments}

We frame our experiments using a conditional computation architecture. We
limit ourselves to a simple architecture with 4 affine transforms. The
output layer consists of an affine transform followed by softmax over the
10 MNIST classes. The input is sent to a gating subnetwork and to an experts
subnetwork, as in Figure
\ifsupplementary
1 of the main paper.
\else
\ref{fig:arch}.
\fi
There is one gating unit per expert unit, and the expert units are hidden
units on the main path. Each gating unit has a possibly stochastic
non-linearity (different under different algorithms evaluated here)
applied on top of an affine transformation of the gater path hidden layer
(400 tanh units that follow another affine transformation applied on the input).
The gating non-linearity are either Noisy Rectifiers, Smooth
Times Stochastic (STS), Stochastic Binary Neurons (SBN), Straight-through
(ST) or (non-noisy) Rectifiers over 2000 units.

The expert hidden units are obtained through an
affine transform without any non-linearity, which makes this part a simple linear
transform of the inputs into 2000 expert hidden units. Together, the gater and expert form a
conditional layer. These could be stacked to create deeper architectures,
but this was not attempted here. In our case, we use but one conditional
layer that takes its input from the vectorized 28x28 MNIST images. The
output of the conditional layer is the element-wise multiplication of the
(non-linear) output of the gater with the (linear) output of the
expert. The idea of using a linear transformation for the expert is derived
from an analogy over rectifiers which can be thought of as the product of a non-linear
gater ($\one_{h_i>0}$) and a linear expert ($h_i$).

\subsection{Sparsity Constraint}
Computational efficiency is gained by imposing a sparsity constraint on the
output of the gater. All experiments aim for an average sparsity of 10\%, such that
for 2000 expert hidden units we will only require computing approximately 200 of
them in average. Theoretically, efficiency can be
gained by only propagating the input activations to the selected expert
units, and only using these to compute the network output. 
For imposing the sparsity constraint we use a KL-divergence criterion for
sigmoids 
and an L1-norm criterion for rectifiers, where the amount of penalty
is adapted to achieve the target level of average sparsity.

A sparsity target of $s=0.1$ over units $i=1...2000$, each such unit having
a mean activation of $p_i$ within a mini-batch of 32 propagations, yields
the following KL-divergence training criterion:
\begin{equation}
{\rm KL}(s||p) = -\lambda \sum_i{\left(s\log{p_i} + (1-s)\log{1-p_i}\right)}
\end{equation}
where $\lambda$ is a hyper-parameter that can be optimized through
cross-validation. In the case of rectifiers, we use an L1-norm training
criteria:
\begin{equation}
{\rm L1}(p) = \lambda \sum{|p_i|}
\end{equation}
In order to keep the effective sparsity $s_e$ (the average proportion of
non-zero gater activations in a batch) of the rectifier near the target
sparsity of $s=0.1$, $\lambda$ is increased when $s_e > s+0.01$, and
reduced when $s_e < s-0.01$. This simple approach was found to be effective
at maintaining the desired sparsity.

\subsection{Beta Noise}
There is a tendency for the KL-divergence criterion to keep the sigmoids around the target
sparsity of $s=0.1$. This is not the kind of behavior desired from a gater, it indicates
indecisiveness on the part of the gating units. What we would hope to see is each unit maintain a 
mean activation of $0.1$ by producing sigmoidal values over 0.5 approximately 10\% of the time. 
This would allow us to round sigmoid values to zero or one at test time.

In order to encourage this behavior, we introduce noise at the input of the
sigmoid, as in the semantic hashing
algorithm~\citep{Salakhutdinov+Geoff-2009}. However, since we impose a
sparsity constraint of 0.1, we have found better results with noise sampled from a Beta
distribution (which is skewed) instead of a Gaussian distribution.  To do this we limit ourselves to
hyper-optimizing the $\beta$ parameter of the distribution, and make the $\alpha$
parameter a function of this $\beta$ such that the mode of the distribution is
equal to our sparsity target of 0.1. Finally, we scale the samples from the
distribution such that the sigmoid of the mode is equal to 0.1. We have
observed that in the case of the STS, introducing noise with a $\beta$ of
approximately 40.1 works bests. We considered values of 1.1, 2.6, 5.1,
10.1, 20.1, 40.1, 80.1 and 160.1. We also considered Gaussian noise and
combinations thereof. We did not try to introduce noise into the SBN or ST
sigmoids since they were observed to have good behavior in this regard.

\subsection{Test-time Thresholds} 

Although injecting noise is useful during training, we found that better
results could be obtained by using a deterministic computation at test time,
thus reducing variance, in a spirit of dropout~\citep{Hinton-et-al-arxiv2012}.
Because of the sparsity constraint with a target of 0.1, simply thresholding
sigmoids at 0.5 does not yield the right proportion of 0's. Instead, we
optimized a threshold to achieve the target proportion of 1's (10\%)
when running in deterministic mode.


\subsection{Hyperparameters}
The noisy rectifier was found to work best with a standard deviation of 1.0
for the gaussian noise. The stochastic half of the Smooth Times Stochastic
is helped by the addition of noise sampled from a beta distribution, as
long as the mean of this distribution replaces it at test time. The Stochastic 
Binary Neuron required a
learning rate 100 times smaller for the gater (0.001) than for the main
part (0.1). The Straight-Through approach worked best without multiplying
the estimated gradient by the derivative of the sigmoid,
i.e. estimating $\frac{\partial L}{\partial a_i}$ by $\frac{\partial L}{\partial  h_i}$ 
where $h_i=\one_{z_i>\sigm(a_i)}$ instead of 
$\frac{\partial L}{\partial h_i}(1-\sigm(a_i))\sigm(a_i) $. Unless indicated otherwise, we
used a learning rate of 0.1 throughout the architecture.

We use momentum for STS, have found that it has a negative effect for SBN, and has little to no effect on the ST and Noisy Rectifier units. In all cases we explored using hard constraints on the maximum norms of incoming weights to the neurons. We found that imposing a maximum norm of 2 works best in most cases.

\fi

\end{document}



\documentclass[times,art10,twocolumn,latex8]{article}

\usepackage[utf8]{inputenc}
\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.
\usepackage{wrapfig}
% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}
\usepackage{fancyvrb}


\newcommand{\RR}[0]{\mathbb{R}}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{22} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvprfinal\pagestyle{empty}\fi


\graphicspath{{./figures/}}

\begin{document}

%%%%%%%%% TITLE
\title{ReSeg: A Recurrent Neural Network-based Model \\for Semantic Segmentation}

%1
\author{Francesco Visin\thanks{%
Dipartimento di Elettronica
Informazione e Bioingegneria,
Politecnico di Milano,
Milan, 20133, Italy}
~\thanks{%
Montreal Institute for Learning Algorithms (MILA),
University of Montreal,
Montreal, QC, H3T 1J4, Canada
}\\
{\tt\small francesco.visin@polimi.it}
\\
%3
Adriana Romero\footnotemark[2]\\
%Montreal Institute for Learning Algorithms (MILA)\\
%University of Montreal\\
%Montreal, QC, H3T 1J4, Canada\\
{\tt\small adriana.romero.soriano@umontreal.ca}
\and
%2
Marco Ciccone\footnotemark[1]\\
%Dipartimento di Elettronica\\
%Informazione e Bioingegneria\\
%Politecnico di Milano\\
%Milan, 20133, Italy\\
{\tt\small marco.ciccone@mail.polimi.it}
\\
%4
Kyle Kastner\footnotemark[2]\\
%Montreal Institute for Learning Algorithms (MILA)\\
%University of Montreal\\
%Montreal, QC, H3T 1J4, Canada\\
{\tt\small kyle.kastner@umontreal.ca}
\and
%5
\hspace{1.2cm}
Kyunghyun Cho\thanks{%
Courant Institute and Center for Data Science,
New York University,
New York, NY 10012, United States}\\
\hspace{1.2cm}
{\tt\small kyunghyun.cho@nyu.edu}
\\
%7
\hspace{1.2cm}
Matteo Matteucci\footnotemark[1]\\
%Dipartimento di Elettronica\\
%Informazione e Bioingegneria\\
%Politecnico di Milano\\
%Milan, 20133, Italy\\
\hspace{1.2cm}
{\tt\small matteo.matteucci@polimi.it}
\and
\hspace{1cm}
%6
Yoshua Bengio\footnotemark[2]~~\thanks{%
CIFAR Senior Fellow}\\
%Montreal Institute for Learning Algorithms (MILA)\\
%University of Montreal\\
%Montreal, QC, H3T 1J4, Canada\\
\hspace{1cm}{\tt\small yoshua.bengio@umontreal.ca}
\\
%8
\hspace{1cm}
Aaron Courville\footnotemark[2]\\
%Montreal Institute for Learning Algorithms (MILA)\\
%University of Montreal\\
%Montreal, QC, H3T 1J4, Canada\\
\hspace{1cm}
{\tt\small aaron.courville@umontreal.ca}
}

%\let\thefootnote\relax\footnotetext{
%\relax\footnotetext{
%    \hspace{-2.25em}
%    $\star$
%	Dipartimento di Elettronica, Informazione e Bioingegneria,
%    Politecnico di Milano, Milan, 20133, Italy\\
%    \texttt{\{francesco.visin, matteo.matteucci\}@polimi.it} \\
%    $\circ$
%    Montreal Institute for Learning Algorithms (MILA),
%    University of Montreal,
%    Montreal, QC, H3T 1J4, Canada \\
%    \texttt{\{kyle.kastner, aaron.courville, yoshua.bengio\}@umontreal.ca} \\
%    $\diamond$
%    Courant Institute and
%    Center for Data Science,
%    New York University,
%    New York, NY 10012, United States \\
%    \texttt{kyunghyun.cho@nyu.edu}  \\
%    $\bullet$ CIFAR Senior Fellow
%}
\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
We propose a structured prediction architecture, which exploits the local
generic features extracted by Convolutional Neural Networks and the capacity of
Recurrent Neural Networks (RNN) to retrieve distant dependencies. The proposed
architecture, called ReSeg, is based on the recently introduced ReNet model for
image classification. We modify and extend it to perform the more challenging
task of semantic segmentation. Each ReNet layer is composed of four RNN that
sweep the image horizontally and vertically in both directions, encoding
patches or activations, and providing relevant global information. Moreover,
ReNet layers are stacked on top of pre-trained convolutional layers, benefiting
from generic local features. Upsampling layers follow ReNet layers to recover
the original image resolution in the final predictions. The proposed ReSeg
architecture is efficient, flexible and suitable for a variety of semantic
segmentation tasks. We evaluate ReSeg on several widely-used semantic
segmentation datasets: Weizmann~Horse, Oxford~Flower, and CamVid; achieving
state-of-the-art performance. Results show that ReSeg can act as a suitable
architecture for semantic segmentation tasks, and may have further applications
in other structured prediction problems. The source code and model
hyperparameters are available on
\href{https://github.com/fvisin/reseg}{https://github.com/fvisin/reseg}.
% to segment images for images centered around deep
%recurrent neural networks.
%, noting that the avoidance of pooling can greatly simplify pixel-wise tasks for images.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

In recent years, Convolutional Neural Networks (CNN) have become the {\em de
facto} standard in many computer vision tasks, such as image classification and
object detection \cite{Krizhevsky-2012,Erhan2014}. Top performing image
classification architectures usually involve {\em very} deep CNN trained in a
supervised fashion on a large datasets
\cite{Lin2014,Simonyan2015,szegedy2014going} and have been shown to
produce generic hierarchical visual representations that perform well on a
wide variety of vision tasks. However, these deep CNNs heavily reduce the
input resolution through successive applications of
pooling or subsampling layers. While these layers seem to contribute
significantly to the desirable invariance properties of deep CNNs, they
also make it challenging to use these pre-trained CNNs for tasks such as
semantic segmentation, where a per pixel prediction is required.

Recent advances in semantic segmentation tend to convert the standard deep
CNN classifier into Fully Convolutional Networks
(FCN)~\cite{long2014fully,noh2015learning,badrinarayanan2015segnet,
Ronneberger2015} to obtain coarse image representations, which are subsequently
upsampled to recover the lost resolution. However, these methods are not
designed to take into account and preserve both \emph{local} and \emph{global} contextual dependencies,
which has shown to be useful for semantic segmentation
tasks~\cite{Singh2013,Gatta14-deepvision}.
%Thus, they often require a Conditional Random Field (CRF) as
%post-processing step to exploit distant contextual dependencies.
These models often employ Conditional Random Fields (CRFs) as a
post-processing step to locally smooth the model predictions, however the
long-range contextual dependencies remain relatively unexploited.

Recurrent Neural Networks (RNN) have been introduced in the literature to
retrieve global spatial dependencies and further improve semantic
segmentation~\cite{Pinheiro:2014, Gatta14-deepvision, chen2015semantic,
byeon2015scene}. However, training spatially recurrent neural networks tends to
be computationally intensive.
%The literature on this subject shows that some effort has been devoted to
%mitigate this problem~\cite{visin2015renet,stollenga2015parallel}.

In this paper, we aim at the {\em efficient} application of Recurrent Neural
Networks RNN to retrieve contextual information from images. We propose to
extend the ReNet architecture~\cite{visin2015renet}, originally designed for
image classification, to deal with the more ambitious task of semantic
segmentation. ReNet layers can efficiently capture contextual dependencies from
images by first sweeping the image horizontally, and then sweeping the output
of hidden states vertically.  The output of a ReNet layer is therefore
implicitly encoding the local features at each pixel position with respect to
the whole input image, providing relevant global information. Moreover, in
order to {\em fully} exploit local and global pixel dependencies, we stack the
ReNet layers on top of the output of a FCN, i.e. the intermediate convolutional
output of VGG-16~\cite{Simonyan2015}, to benefit from generic local features.
We validate our method on Weizmann~Horse and Oxford~Flower
foreground/background segmentation datasets as a proof of concept for the
proposed architecture.  Then, we evaluate the performance in the standard
benchmark of urban scenes CamVid; achieving state-of-the-art in all three
datasets.~\footnote{Subsequent but independent
work~\cite{DBLP:journals/corr/YanZJBY16} investigated the combination of ReSeg
with Fully Convolutional Network (FCN) and CRFs, reporting state of the art
results on Pascal VOC.}

%-------------------------------------------------------------------------

\section{Related Work}

%% CNN for semantic segmentation
Methods based on FCN tackle the information recovery (upsampling) problem in a
large variety of ways. For instance, Eigen et al.~\cite{Eigen2015} introduce a
multi-scale architecture, which extracts coarse predictions, which are then
refined using finer scales. Farabet et al.~\cite{Farabet:2013} introduce a
multi-scale CNN architecture; Hariharan et al.~\cite{Hariharan2015} combine the
information distributed over all layers to make accurate predictions. Other
methods such as~\cite{long2014fully,badrinarayanan2015segnet} use simple
bilinear interpolation to upsample the feature maps of increasingly abstract
layers. More sophisticated upsampling methods, such as
unpooling~\cite{badrinarayanan2015segnet,noh2015learning} or
deconvolution~\cite{long2014fully}, are introduced in the literature. Finally,
\cite{Ronneberger2015} concatenate the feature maps of the downsampling layers
with the feature maps of the upsampling layers to help recover finer
information.

%% What we propose
RNN and RNN-like models have become increasingly popular in the semantic
segmentation literature to capture long distance pixel
dependencies~\cite{Pinheiro:2014, Gatta14-deepvision,
byeon2015scene,stollenga2015parallel}. For instance, in~\cite{Pinheiro:2014,
Gatta14-deepvision}, CNN are unrolled through different time steps to include
semantic feedback connections. In~\cite{byeon2015scene}, 2-dimensional Long
Short Term Memory (LSTM), which consist of 4 LSTM blocks
scanning all directions of an image (left-bottom, left-top, right-top, right-bottom),
are introduced to learn long range spatial dependencies. Following a similar direction,
in~\cite{stollenga2015parallel}, multi-dimensional LSTM are swept along
different image directions; however, in this case, computations are re-arranged
in a pyramidal fashion for efficiency reasons. Finally,
in~\cite{visin2015renet}, ReNet is proposed to model pixel dependencies in the
context of image classification. It is worth noting that one important
consequence of the adoption of the ReNet spatial sequences is that they are
even more easily parallelizable, as each RNN is dependent only along a horizontal or
vertical sequence of pixels; i.e., all rows/columns of pixels can be processed
at the same time.

%In this work we extend the preliminary results of~\cite{visin2015renet} modifying
%and extending the ReNet model to the more ambitious task of object segmentation.
%We test the performances of the model in the object segmentation domain on one
%of the historically most used datasets in this field, the
%Weizmann~Horse~dataset~\cite{Borenstein04combiningtop-down}, the
%Oxford~Flowers~17~dataset~\cite{Nilsback06}  and the more recent
%Fashionista~dataset~\cite{Fashionista}.
%
%Our experiments show that the proposed adaptation of the ReNet for pixel-level
%object segmentation can perform successfully on the object segmentation task. The proposed
%architecture can be easily merged into a joint network to perform both tasks at
%the same time, sharing most of the computation.  This could be interesting in
%application domains where object classification and segmentation have to be
%performed simultaneously, such as, autonomous driving and object retrieval.

%-------------------------------------------------------------------------
\section{Model Description}

The proposed ReSeg model builds on top of ReNet~\cite{visin2015renet} and
extends it to address the task of semantic segmentation. The model
pipeline involves multiple stages.

First, the input image is processed with the first layers of
VGG-16~\cite{Simonyan2015} network, pre-trained on
ImageNet~\cite{imagenet_cvpr09} and not fine-tuned, and is set such that the
image resolution does not become too small. The resulting feature maps are then
fed into one or more \emph{ReNet layers} that sweep over the image. Finally,
one or more \emph{upsampling layers} are employed to resize the last feature
maps to the same resolution as the input and a softmax non-linearity is applied
to predict the probability distribution over the classes for each pixel.

The recurrent layer is the core of our architecture and is composed by multiple
RNN that can be implemented as a vanilla $\tanh$ RNN layer, a Gated Recurrent
Unit~(GRU)~layer~\cite{Cho2014} or a
LSTM~layer~\cite{Hochreiter+Schmidhuber-1997}. Previous work has shown that
the ReNet model can perform well with little concern for the specific recurrent
unit used, therefore, we have chosen to use GRU units as they strike a good
balance between memory usage and computational power.

In the following section we will define the recurrent and the upsampling layers
in more detail.

\subsection{Recurrent layer}
As depicted in~\autoref{fig:first_layer}, each recurrent layer is composed
by 4 RNNs coupled together in such a way to capture the local and global
spatial structure of the input data.

% \subsubsection{Gated Recurrent Units}
% Recurrent units with added memory and gating, such as gated recurrent
% units~\cite[GRU,][]{Cho2014} and long short-term memory
% units~\cite[LSTM,][]{Hochreiter+Schmidhuber-1997}, have been key components to many successful
% applications using recurrent neural networks
% ~\cite[including][]{Cho2014,Sutskever-et-al-NIPS2014,Xu-et-al-arxiv2015}.
%
% The hidden state of the GRU at time $t$ is computed by
% \begin{align*}
%     h_t = (1-u_t) \odot h_{t-1} + u_t \odot \tilde{h}_t,
% \end{align*}
% where
% \begin{align*}
%     \tilde{h}_t = \tanh\left( W x_t + U (r_t \odot h_{t-1}) + b \right)
% \end{align*}
% and
% \begin{align*}
%     \left[ u_t; r_t\right] = \sigma\left( W_g x_t + U_g h_{t-1} + b_g\right).
% \end{align*}
% For an in-depth comparison of the similarities and trade-offs between GRU and
% LSTM, there are several resources including \cite{Chung2015}.

\begin{figure}[t]
    \begin{center}
        \includegraphics[width=0.3\columnwidth]{first_layer.pdf}
        \caption{A ReNet layer. The blue and green dots on the input
            image/feature map represent the steps of $f^{\downarrow}$ and
            $f^{\uparrow}$ respectively. On the concatenation of the resulting
            feature maps, $f^{\rightarrow}$ (yellow dots) and $f^{\leftarrow}$
            (red dots) are subsequently swept. Their feature maps are finally
            concatenated to form the output of the ReNet layer, depicted as a
            blue heatmap in the figure.}
        \label{fig:first_layer}
        \vspace{-5mm}
    \end{center}
\end{figure}


Specifically, we take as an input an image (or the feature map of the previous
layer) $\mathbf{X}$ of elements $x \in \RR^{H \times W \times C}$, where $H$,
$W$ and $C$ are respectively the height, width and number of channels (or
features) and we split it into $I \times J$ patches $p_{i,j} \in \RR^{H_p
\times W_p \times C}$. We then sweep %over each of its columns $X_{i,\Cdot}$ a
vertically a first time with two RNNs $f^{\downarrow}$ and $f^{\uparrow}$,
with $U$ recurrent units each, that move top-down and bottom-up respectively.
Note that the processing of each column is independent and can be done in
parallel.

\begin{figure*}[t]
    \advance\leftskip-0.2\textwidth
    \centering
    \makebox[\textwidth][c]{\includegraphics
    	[height=.135\textheight,width=\textwidth]{recseg_base.pdf}}
    \caption{The ReSeg network. For space reasons we do not represent the
        pretrained VGG-16 convolutional layers that we use to preprocess
        the input to ReSeg. The first 2 RNNs (blue and green) are applied on
        2x2x3 patches of the image, their 16x16x256 feature maps are
        concatenated and fed as input to the next two RNNs (red and yellow)
        which read 1x1x512 patches and emit the output of the first ReNet
        layer. Two similar ReNet layers are stacked, followed by an upsampling
        layer and a softmax nonlinearity.}
    \label{fig:ReSeg}
\end{figure*}

At every time step each RNN reads the next non-overlapping patch
$p_{i,j}$ and, based on its previous
state, emits a projection $o_{i,j}^{\star}$ and updates its state
$z_{i,j}^{\star}$:
\begin{align}
    o^{\downarrow}_{i,j} = f^{\downarrow}(z^{\downarrow}_{i-1,j},p_{i,j}),
        &\text{ for }i=1,\cdots, I\\
    o^{\uparrow}_{i,j} = f^{\uparrow}(z^{\uparrow}_{i+1,j},p_{i,j}),
        &\text{ for }i=I,\cdots,1
\end{align}
We stress that the decision to read non-overlapping patches is a modeling
choice to increase the image scan speed and lower the memory usage, but is not
a limitation of the architecture.

Once the first two vertical RNNs have processed the whole input $X$, we
concatenate their projections $o^{\downarrow}_{i,j}$ and $o^{\uparrow}_{i,j}$
to obtain a composite feature map $\mathbf{O^{\updownarrow}}$ whose elements
$o^{\updownarrow}_{i,j} \in \RR^{2U}$ can be seen as the activation of a
feature detector at the location $(i,j)$ with respect to all the patches in the
$j$-th column of the input. We denote what we described so far as the
\emph{vertical recurrent sublayer}.

After obtaining the concatenated feature map $\mathbf{O^{\updownarrow}}$, we
sweep over each of its rows with a pair of new RNNs, $f^{\rightarrow}$ and
$f^{\leftarrow}$. We chose not to split $\mathbf{O^{\updownarrow}}$ into
patches so that the second recurrent sublayer has the same granularity as the
first one, but this is not a constraint of the model and different
architectures can be explored.  With a similar but specular procedure as the
one described before, we proceed reading one element $o^\updownarrow_{i,j}$ at
each step, to obtain a concatenated feature map
$\mathbf{O^\leftrightarrow}~=~\left\{ h^\leftrightarrow_{i,j} \right\}
_{i=1\dots I}^{j=1\dots J}$, once again with $o^\leftrightarrow_{i,j} \in
\RR^{2U}$. Each element $o^\leftrightarrow_{i,j}$ of this \emph{horizontal
recurrent sublayer} represents the features of one of the input image patches
$p_{i,j}$ \emph{with contextual information from the whole image}.

It is trivial to note that it is possible to concatenate many recurrent layers
$\mathbf{O^{(1 \cdots L)}}$ one after the other and train them with any
optimization algorithm that performs gradient descent, as the composite model
is a smooth, continuous function.

\subsection{Upsampling layer}\label{sec:upsampling}
Since by design each recurrent layer processes non-overlapping patches, the
size of the last composite feature map will be smaller than the size of the
initial input $\mathbf{X}$, whenever the patch size is greater than one. To be
able to compute a segmentation mask at the same resolution as the ground truth,
the prediction should be expanded back before applying the softmax non-linearity.

Several different methods can be used to this end, e.g., fully connected
layers, full convolutions and transposed convolutions. The first is not a good
candidate in this domain as it does not take into account the topology of the
input, which is essential for this task; the second is not optimal either, as
it would require large kernels and stride sizes to upsample by the
required factor. Transposed convolutions are both memory and
computation efficient, and are the ideal method to tackle this problem.

Transposed convolutions -- also known as \emph{fractionally strided
convolutions} -- have been employed in many works in recent
literature~\cite{Zeiler-ICCV2011,ZeilerFergus14,long2015fully,
radford2015unsupervised,im2016generating}. This method is based on the observation
that direct convolutions can be expressed as a dot product between the
flattened input and a sparse matrix, whose non-zero elements are elements of the
convolutional kernel. The equivalence with the convolution is granted by the
connectivity pattern defined by the matrix.

Transposed convolutions apply the transpose of this transformation matrix to
the input, resulting in an operation whose input and output shapes are inverted
with respect to the original direct convolution. A very efficient implementation of
this operation can be obtained exploiting the gradient operation of the
convolution -- whose optimized implementation can be found in many of the most
popular libraries for neural networks. For an in-depth and comprehensive
analysis of each alternative, we refer the interested reader
to~\cite{dumoulin2016guide}.

\section{Experiments}\label{sec:Experiments}

\subsection{Datasets}
We evaluated the proposed ReSeg architecture on several benchmark datasets.
We proceeded by first assessing the performances of the model on the Weizmann
Horse and the Oxford Flowers datasets and then focused on the more challenging
Camvid dataset. We will describe each dataset in detail in this section.

\subsubsection{Weizmann Horse}
The Weizmann Horse dataset, introduced in~\cite{Borenstein04combiningtop-down},
is an image
segmentation dataset consisting of 329 variable size images in both RGB and
gray scale
format, matched with an equal number of groundtruth segmentation images, of
the same size as the corresponding image.
The groundtruth segmentations contain a foreground/background mask of the
focused horse, encoded as a real-value between 0 and 255. To convert this into
a boolean mask, we threshold in the center of the range setting all smaller
values to 0, and all greater values to 1.
%This dataset is one of the primary
%small-scale benchmarks found in existing image segmentation literature.

% \subsubsection{Fashionista}
% The Fashionista dataset from ~\cite{Fashionista} contains 685 RGB images of
% fashion models wearing a variety of different clothing.
% Each image and its corresponding mask are 400 pixels in width by 600 pixels
% in height, with encoded values for 53 clothing items. In this work we focus
% on foreground/background segmentation, and build the appropriate
% masks from the more complex maps provided by the dataset by creating a new map
% which preserves the background class as 0, and sets pixels which belong to all
% other classes to 1. This appears to be the same procedure undertaken in
% ~\cite{yang2015patchcut} to create a foreground/background task for this dataset.

\subsubsection{Oxford Flowers 17}
The Oxford Flowers 17 class dataset from~\cite{Nilsback06} contains 1363
variable size RGB images, with 848 image segmentations maps associated with
a subset of
the RGB images. There are 8 unique segmentation classes defined over all maps,
including flower, sky, and grass. To build a foreground/background mask,
we take the original segmentation maps, and set any pixel not belonging to
class 38 (flower class) to 0, and setting the flower class pixels to 1.
This binary segmentation task for Oxford Flowers 17 is further described
in~\cite{Xiaomeng14}.
%A larger 102 class Oxford Flowers dataset is available from the same authors.

\subsubsection{CamVid Dataset}
The Cambridge-driving Labeled Video Database
(CamVid)~\cite{Brostow2010semantic} is a real-world dataset which consists of
images recorded from a car with an internally mounted camera, capturing frames
of $960 \times 720$ RGB pixels per frame, with a recording frame rate of 30
frames per second. A total of ten minutes of video was recorded, and
approximately one frame per second has been manually annotated with per pixel
class labels, from one of 32 possible classes.  A small number of pixels were
labelled as void in the original dataset. These do not belong to any of the 32
classes prescribed in the original data, and are ignored during evaluation.  We
used the same subset of 11 class categories as~\cite{badrinarayanan2015segnet}
for experimental analysis.  The CamVid dataset itself is split into 367
training, 101 validation and 233 test images, and in order to make our
experimental setup fully comparable to~\cite{badrinarayanan2015segnet}, we
downsampled all the images by a factor of 2 resulting in a final $480 \times
360$ resolution.

% \subsection{Preprocessing}\label{sec:data_preprocessing}
% \subsubsection{Data Augmentation}
% Adding prior knowledge by augmenting the given data with transformed versions is
% well known to help generalization ~\cite[see, e.g.,][]{Krizhevsky-2012}. In light
% of this, we decided to employ several methods of data augmentation:
% {\it flipping}, {\it shifting}, {\it color flipping}, and {\it resizing}.
%
% For each sample there was a 50\% chance to flip the image horizontally.
% This mirrors the intuition that images which are inverted horizontally
% generally seem like the same scene visually.
%
% The shifting procedure was as follows: we either move 2 pixels to the left with
% 25\% chance, 2 pixels to the right with 25\% chance, or perform no shifting.
% After this step, we then shift up with 25\% chance, shift down with 25\% chance,
% or leave the image as is was before this step. This augmentation should make
% the model more robust to slight shifts of the object in the image.
%
% When working with gray-scale images, another augmentation which can be useful
% is to randomly invert the color of the image by changing darker colors into lighter
% colors, and vice-versa. This proved to be especially helpful
% when working with greyscale versions of the Weizmann Horse dataset to improve the segmentation
% performance of light horses that are less represented in the dataset.
%
% Resizing of images can also be of benefit, as eliminating unnecessary and
% easily explained variance can help the model focus on harder to model
% characteristics, which generally leads to better performance on the task at hand,
% especially in the case of segmentation where object scale between images has little
% impact on the class category. A common choice for resizing is to resize every image
% to the mean width and height, calculated over the entire dataset of variable size
% images.
%
% It should be noted that all transformations which involve changes in dimensionality
% or position must also be applied in some form to the segmentation mask, and great
% care must be taken (especially during resizing/shifting) not to introduce unexpected
% errors. It is also paramount to highlight that it is the prediction that
% should be resized to the ground truth size and not the opposite, not
% to misrepresent the segmentation accuracy.
%
% All of these possible augmentation procedures were treated as hyperparameters for
% training, and selected based on the best validation performance per dataset.

\begin{table*}[!ht]
    % HORSES
    \begin{minipage}{0.45\textwidth}
        \centering
        \small{%
            \begin{tabular}{c|c|c||c|c|c}
                \multicolumn{1}{c}{Method} & \multicolumn{1}{c}{Global acc} & \multicolumn{1}{c}{\textbf{Avg IoU}}\\ \hline \hline

                All foreground baseline & 25.4 & 79.9 \\ \hline
                All background baseline & 74.7 & 0.0 \\ \hline
                Kernelized structural SVM \cite{bertelli2011kernelized} & 94.6 & 80.1 \\ \hline
                ReSeg (no VGG) & 94.9 & 79.9 \\ \hline
                CRF learning \cite{liu2015crf} & 95.7 & 84.0 \\ \hline
                PatchCut \cite{yang2015patchcut} & 95.8 & 84.0 \\ \hline
                \textbf{ReSeg} & 96.8 & \textbf{91.6} \\ \hline

            \end{tabular}
            \vspace*{0.1cm}
        }
        \caption{Weizmann Horses. Per pixel accuracy and IoU are
            reported.}
        \label{tbl:WeizmannHorses_SOTA}
    \end{minipage}
    \quad
    % FLOWERS
    \begin{minipage}{0.45\textwidth}
        \centering
        \small{%
            \begin{tabular}{c|c|c||c|c|c}
                \multicolumn{1}{c}{Method} & \multicolumn{1}{c}{Global acc} & \multicolumn{1}{c}{\textbf{Avg IoU}}\\ \hline \hline

                All background baseline & 71.0 & 0.0 \\ \hline
                All foreground baseline & 29.0 & 29.2 \\ \hline
                % \textbf{ReSeg (no VGG)} & 86.9 & \textbf{65.8} \\ \hline
                GrabCut \cite{rother2004grabcut} & 95.9 & 89.3 \\ \hline
                Tri-map \cite{Xiaomeng14} & 96.7 & 91.7 \\ \hline
                \textbf{ReSeg} & 98 & \textbf{93.7} \\ \hline

            \end{tabular}
            \vspace*{0.1cm}
        }
        \caption{Oxford Flowers. Per pixel accuracy and IoU are
            reported.}
        \label{tbl:OxfordFlowers_SOTA}
    \end{minipage}
\end{table*}

% CAMVID RESULTS
\begin{table*}[!ht]
	\resizebox{\textwidth}{!}{%
		\small{%
            \begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c||c|c|c}

                \multicolumn{1}{c}{Method} & \multicolumn{1}{c}{\rotatebox{90}{Building}} & \multicolumn{1}{c}{\rotatebox{90}{Tree}} & \multicolumn{1}{c}{\rotatebox{90}{Sky}} & \multicolumn{1}{c}{\rotatebox{90}{Car}} & \multicolumn{1}{c}{\rotatebox{90}{Sign-Symbol}} & \multicolumn{1}{c}{\rotatebox{90}{Road}} & \multicolumn{1}{c}{\rotatebox{90}{Pedestrian}} & \multicolumn{1}{c}{\rotatebox{90}{Fence}} & \multicolumn{1}{c}{\rotatebox{90}{Column-Pole}} & \multicolumn{1}{c}{\rotatebox{90}{Side-walk}} & \multicolumn{1}{c}{\rotatebox{90}{Bicyclist}} & \multicolumn{1}{c}{\rotatebox{90}{Avg class acc}} & \multicolumn{1}{c}{\rotatebox{90}{Global acc}} & \multicolumn{1}{c}{\rotatebox{90}{\textbf{Avg IoU}}}\\ \hline \hline

				\multicolumn{15}{c}{\emph{Segmentation models}} \\ \hline
                Super Parsing   \cite{tighe2013superparsing} & \textbf{87.0} & 67.1 & \textbf{96.9} & 62.7 & 30.1 & 95.9 & 14.7 & 17.9 & 1.7 & 70.0 & 19.4 & 51.2 & 83.3 & n/a \\ \hline
				Boosting+Higher order \cite{sturgess2009combining} & 84.5 & 72.6 & \textbf{97.5} & 72.7 & 34.1 & 95.3 & 34.2 & 45.7 & 8.1 & 77.6 & 28.5 & 59.2 & 83.8 & n/a \\ \hline
				Boosting+Detectors+CRF \cite{ladicky2010and} & 81.5 & 76.6 & 96.2 & 78.7 & 40.2 & 93.9 & 43.0 & 47.6 & 14.3 & 81.5 & 33.9 & 62.5 & 83.8 & n/a \\ \hline

				\multicolumn{15}{c}{\emph{Neural Network based segmentation models}} \\ \hline
				SegNet-Basic (layer-wise training \cite{badrinarayanan2015segnetlayerwise}) & 75.0 & 84.6 & 91.2 & 82.7 & 36.9 & 93.3 & 55.0 & 37.5 & 44.8 & 74.1 & 16.0 & 62.9 & 84.3 & n/a \\ \hline
                SegNet-Basic \cite{badrinarayanan2015segnet} & 80.6 & 72.0 & 93.0 & 78.5 & 21.0 & 94.0 & 62.5 & 31.4 & 36.6 & 74.0 & 42.5 & 62.3 & 82.8 & 46.3 \\ \hline
                SegNet \cite{badrinarayanan2015segnet} & \textbf{88.0 } & \textbf{87.3} & 92.3 & 80.0 & 29.5 & \textbf{97.6} & 57.2 & \textbf{49.4} & 27.8 & 84.8 & 30.7 & 65.9 & 88.6 & 50.2 \\ \hline
                \emph{ReSeg + Class Balance} & 70.6 & 84.6 & 89.6 & 81.1 & \textbf{61.0} & 95.1 & \textbf{80.4} & 35.6 & \textbf{60.6} & \textbf{86.3} & \textbf{60.0} & 73.2 & 83.5 & 53.7 \\ \hline
                \textbf{ReSeg} & 86.8 & 84.7 & 93.0 & \textbf{87.3} & 48.6 & \textbf{98.0} & 63.3 & 20.9 & 35.6 & \textbf{87.3} & 43.5 & 68.1 & 88.7 & \textbf{58.8} \\ \hline

				\multicolumn{15}{c}{\emph{Sub-model averaging}} \\ \hline
                \emph{Bayesian SegNet-Basic} \cite{Kendall2015bayesiansegnet} & 75.1 & 68.8 & 91.4 & 77.7 & 52.0 & 92.5 & 71.5 & 44.9 & 52.9 & 79.1 & 69.6 & 70.5 & 81.6 & 55.8 \\ \hline
                \emph{Bayesian SegNet} \cite{Kendall2015bayesiansegnet} & 80.4 & 85.5 & 90.1 & 86.4 & 67.9 & 93.8 & 73.8 & 64.5 & 50.8 & 91.7 & 54.6 & 76.3 & 86.9 & 63.1 \\ \hline

			\end{tabular}
		}
    }
	\vspace*{0.1cm}
    \caption{%
        CamVid. The table reports the per-class accuracy, the average per-class
        accuracy, the global accuracy and the average intersection over union.
        The best values and the values within $1$ point from the best are
        highlighted in bold for each column. For completeness we report the
        Bayesian Segnet models even if they are not directly comparable to the
        others as they perform a form of model averaging.}
    \label{tbl:camvid_SOTA}
\end{table*}

% CAMVID PARAMS
\begin{table*}[!ht]
    \resizebox{\textwidth}{!}{%
        \small{%
            \begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c||c|c|c}
                \multicolumn{1}{c}{Model} & \multicolumn{1}{c}{${ps}_{\text{RE}}$}& \multicolumn{1}{c}{$d_{\text{RE}}$} & \multicolumn{1}{c}{${fs}_{\text{UP}}$}& \multicolumn{1}{c}{$d_{\text{UP}}$} & \multicolumn{1}{c}{\rotatebox{90}{Building}} & \multicolumn{1}{c}{\rotatebox{90}{Tree}} & \multicolumn{1}{c}{\rotatebox{90}{Sky}}  & \multicolumn{1}{c}{\rotatebox{90}{Car}}  & \multicolumn{1}{c}{\rotatebox{90}{Sign-Symbol}} & \multicolumn{1}{c}{\rotatebox{90}{Road}} & \multicolumn{1}{c}{\rotatebox{90}{Pedestrian}} & \multicolumn{1}{c}{\rotatebox{90}{Fence}} & \multicolumn{1}{c}{\rotatebox{90}{Column-Pole}} & \multicolumn{1}{c}{\rotatebox{90}{Side-walk}} & \multicolumn{1}{c}{\rotatebox{90}{Bicyclist}} & \multicolumn{1}{c}{\rotatebox{90}{Avg class acc}} & \multicolumn{1}{c}{\rotatebox{90}{Global acc}} & \multicolumn{1}{c}{\rotatebox{90}{\textbf{Avg IoU}}}\\ \hline \hline

                ReSeg + LCN & $(2 \times 2), (1 \times 1)$ & (100, 100) & $(2 \times 2)$ & (50, 50) & 81.5 & 80.3 & \textbf{94.7} & 78.1 & 42.8 & \textbf{97.4} & 53.5 & 34.3 & 36.8 & 68.9 & 47.9 & 65.1 & 84.8 & 52.6 \\ \hline
                ReSeg + Class Balance & $(2 \times 2), (1 \times 1)$ & (100, 100) & $(2 \times 2)$ & (50, 50) & 70.6 & \textbf{84.6} & 89.6 & 81.1 & \textbf{61.0} & 95.1 & \textbf{80.4} & \textbf{35.6} & \textbf{60.6} & \textbf{86.3} & \textbf{60.0} & 73.2 & 83.5 & 53.7 \\ \hline
                \textbf{ReSeg} & $(2 \times 2), (1 \times 1)$ & (100, 100) & $(2 \times 2)$ & (50, 50) & \textbf{86.8} & \textbf{84.7} & 93.0 & \textbf{87.3} & 48.6 & \textbf{98.0} & 63.3 & 20.9 & 35.6 & \textbf{87.3} & 43.5 & 68.1 & 88.7 & \textbf{58.8} \\ \hline
            \end{tabular}%
        }
    }
    \vspace*{0.1cm}
    \caption{%
        Comparison of the performance of different hyperparameter on CamVid.
    }
    \label{tbl:camvid_params}
\end{table*}
\subsection{Experimental settings}
To gain confidence with the sensitivity of the model to the different
hyperparameters, we decided to evaluate it first on the Weissman Horse and
Oxford Flowers datasets on a binary segmentation task; we then focused the most
of our efforts on the more challenging semantic segmentation task on the CamVid
dataset.

The number of hyperparameters of this model is potentially very high, as for
each ReNet layer different implementations are possible (namely vanilla RNN,
GRU or LSTM), each one with its specific parameters. Furthermore, the number of
features, the size of the patches and the initialization scheme have to be
defined for each ReNet layer as well as for each transposed convolutional
layer. To make it feasible to explore the hyperparameter space, some of the
hyperparameters have been fixed by design and the remaining have been finetuned.
In the rest of this section, the architectural choices for both sets of
parameters will be detailed.

All the transposed convolution upsampling layers were followed by a
ReLU~\cite{Krizhevsky2012-alexnet} non-linearity and initialized with the
fan-in plus fan-out initialization scheme described
in~\cite{glorot2010understanding}. The recurrent weight matrices were instead
initialized to be orthonormal, following the procedure defined
in~\cite{Saxe2014}. We also constrained the stride of the upsampling transposed
convolutional layers to be tied to their filter size.

In the segmentation task, each training image carries classification
information for all of its pixels. Differently from the image classification
task, small batch sizes provide the model with a good amount of information
with sufficient variance to learn and generalize well. We experimented with
various batch sizes going as low as processing a single image at the time,
obtaining comparable results in terms of performance. In our experiments we
kept a fixed batch size of $5$, as a compromise between train speed and memory
usage. In all our experiments, we used L2
regularization~\cite{Krogh92asimple}, also known as weight decay, set to
$0.001$ to avoid instability at the end of training. We trained all our models
with the Adadelta~\cite{Zeiler-2012} optimization algorithm, for its desired
property of not requiring a specific hyperparameter tuning. The effect of Batch
Normalization in RNNs has been a focus of attention~\cite{Laurent2015}, but it
does not seem to provide a reliable improvement in performance, so we decided
not to adopt it.
% we did not use any gradient clipping or weight noise.

In the experiments, we varied the number of ReNet layers and the number of
upsampling transposed convolutional layers, each of them defined respectively
by the number of features $d_{\text{RE}}(l)$ and $d_{\text{UP}}(l)$,
the size of the input patches (or equivalently of the filters)
${ps}_{\text{RE}}(l)$ and ${fs}_{\text{UP}}(l)$.

% \begin{table}[ht]
%
%     \centering
%     \caption{Hyperparameters} \label{tbl:hyperparams}
%     \begin{tabular}{l | l | l | l | l | l | l | l}
%         Dataset & $b$ & $d_{\text{RE}}$ & $w_p$ & $h_p$ & $a_c$ &
%         upscaling\\
%         \hline
% Weissman horses & 1 & [1100, 1300] & 2 & 2 & tanh & grad \\
% Fashonista & 1 & [400, 400] & 2 & 2 & tanh & linear \\
% Oxford Flowers & 5 & [150, 150] & 2 & 2 & tanh & linear
%     \end{tabular}
%     \vspace{2mm}
% \end{table}


% An adaptive learning rate algorithm known as Adam~\cite{Kingma2014} was a
% key ingredient to stable learning, though others such as ~\cite{Zeiler-2012}
% were useful during model development. In addition, we also utilized
% gradient norm rescaling to help with the problems described in
% ~\cite{bengio2013advances}.
%
% Regularization proved to be another important part of our process,
% especially on the smaller Weizmann Horse dataset. Weight noise, as described
% in ~\cite{Graves2011}, with a scale 0.075 was applied to all weight matrices
% before each forward pass during some experiments. Dropout ~\cite{Srivastava14} on
% each forward connection with drop probability of $0.2$ on the input, and/or with drop
% probability of $0.5$ on the hidden projections was applied during some experiments.
% Many experiments were performed using a minibatch size of 1. When computational
% constraints allowed, we also used larger batch sizes of 5, 10, 20, 35, and 50.
% Larger batch sizes greatly smoothed learning (as expected), and removed some
% issues related to spurious occurrences in the datasets such as misaligned or
% poorly segmented groundtruth masks.

\subsection{Results}

In~\autoref{tbl:WeizmannHorses_SOTA}, we report the results on the Weizmann
Horse dataset. On this dataset, we verified the assumption that processing
the input image with some pre-trained convolutional layers from VGG-16 could
ease the learning. Specifically, we restricted ourselves to only using the
first $7$ convolutional layers from VGG, as we only intended to extract some
low-level generic features and learn the task-specific high-level features with
the ReNet layers. The results indeed show an increase in terms of average Intersection
over Union (\emph{IoU}) when these layers are being used, confirming our
hypothesis.

\autoref{tbl:OxfordFlowers_SOTA} shows the results for Oxford Flowers
dataset, when using the full ReSeg architecture (i.e., including VGG convolutional layers).
As shown in the table, our method clearly outperforms the
state-of-the-art both in terms of global accuracy and average IoU.

\autoref{tbl:camvid_SOTA} presents the results on CamVid dataset using the full
ReSeg architecture. Our model exhibits state-of-the-art performance in terms of
IoU when compared to both standard segmentation methods and neural network
based methods, showing an increase of $17\%$ w.r.t.\ to the recent SegNet
model. It is worth highlighting that incorporating sub-model averaging to
SegNet model, as in \cite{Kendall2015bayesiansegnet}, boosts the original model
performance, as expected. Therefore, introducing sub-model averaging to ReSeg
would also presumably result in significant performance increase.  However,
this remains to be tested.

% There are number of difficulties in directly comparing our performance with the
% reported scores, some of which are intrinsic to using neural based models for
% segmentation, while others are specific to the evaluation datasets or competing
% methodologies.

%\begin{figure}[t]
%    \centering
%    \makebox[\columnwidth][c]{\includegraphics[width=1.1\columnwidth]
%    {Segm_horse258.pdf}}
%    \caption{Weizmann Horse example segmentation.
%        Left: Input image, Center: Ground truth segmentation,
%        Right: ReSeg predicted segmentation}
%    \label{fig:ReSeg_example}
%\end{figure}

%\begin{figure}[t]
%    \centering
%    \makebox[\columnwidth][c]{\includegraphics[width=1.1\columnwidth]
%    {Segm_horse283.pdf}}
%    \caption{Weizmann Horse failure segmentation.
%        Left: Input image, Center: Ground truth segmentation,
%        Right: ReSeg predicted segmentation}
%    \label{fig:ReSeg_failure}
%\end{figure}

\section{Discussion}

As reported in the previous section, our experiments on the Weizmann Horse
dataset show that processing the input images with some layers of VGG-16
pre-trained network improves the results. In this setting, pre-processing the input
with Local Contrast Normalization (LCN) does not seem to give any advantage
(see~\autoref{tbl:camvid_params}). We did not use any other kind of
pre-processing.

While on both the Weizmann Horse and the Oxford Flowers datasets we trained on
a binary background/foreground segmentation task, on CamVid we addressed the
full semantic segmentation task. In this setting, when the dataset is highly
imbalanced, the segmentation performance of some classes can drop
significantly as the network tries to maximize the score on the high-occurrence
classes, {\em de facto} ignoring the low-occurrence ones. To overcome this behaviour,
we added a term to the cross-entropy loss to bias the prediction towards the
low-occurrence classes. We use \emph{median frequency
balancing}~\cite{Eigen2015}, which re-weights the class predictions by the
ratio between the median of the frequencies of the classes (computed on the
training set) and the frequency of each class.
% \begin{equation}
% \alpha(c) = median\_frequency / f(c)
% \end{equation}
% where $f(c)$  is the number of pixels of the class $c$ divided by the total
% number of pixels in the images where $c$ is present, and $median\_frequency$ is
% the median of these frequencies.
This increases the score of the low frequency classes
(see~\autoref{tbl:camvid_params}) at the price of a more noisy segmentation
mask, as the probability of the underrepresented classes is overestimated and
can lead to an increase in misclassified pixels in the output segmentation
mask, as shown in~\autoref{fig:camvid_class_balance}.

On all datasets we report the per-pixel accuracy (\emph{Global acc}), computed
as the percentage of true positives w.r.t.\ the total number of pixels in the
image, and the average per-class Intersection over Union (\emph{Avg IoU}),
computed on each class as true positive divided by the sum of true positives,
false positives and false negatives and then averaged. In the full semantic
segmentation setting we also report the per-class accuracy and the average
per-class accuracy (\emph{Avg class acc}).

\begin{figure*}[!htb]
    \centering
    % \makebox[\textwidth][c]{
    \includegraphics[width=\textwidth]{camvid_classbal_diff.png}%}
    \caption{Camvid segmentation example with and without class balancing. From
        the left: input image, ground truth segmentation, ReSeg segmentation,
        ReSeg segmentation with class balancing. Class balancing improves the
        low frequency classes as e.g., the street lights, at the price of a
        worse overall segmentation.}
    \label{fig:camvid_class_balance}
\end{figure*}

\section{Conclusion}
We introduced the ReSeg model, an extension of the ReNet model for image
semantic segmentation. The proposed architecture shows state-of-the-art
performances on CamVid, a widely used dataset for urban scene semantic
segmentation, as well as on the much smaller Oxford Flowers dataset. We also
report state-of-the-art performances on the Weizmann Horses.

In our analysis, we discuss the effects of applying some layers of VGG-16 to
process the input data, as well as those of introducing a class balancing
term in the cross-entropy loss function to help the learning of
under-represented classes.
Notably, it is sufficient to process the input images with just a few layers of
VGG-16 for the ReSeg model to gracefully handle the semantic segmentation task, confirming
its ability to encode contextual information and long term dependencies.

\subsubsection*{Acknowledgments}
We would like to thank all the developers of
Theano~\cite{Bergstra2010,Bastien2012} and in particular Pascal Lamblin, Arnaud
Bergeron and Frédéric Bastien for their dedication. We are also thankful to
César Laurent for the moral support and to Vincent Dumoulin for the insightful
discussion on transposed convolutions. We are also very grateful to the
developers of Lasagne~\cite{Lasagne} for providing a light yet powerful
framework and to the reviewers for their valuable feedback. We finally
acknowledge the support of the following organizations for research funding and
computing support: NSERC, IBM Watson Group, IBM Research, NVIDIA, Samsung,
Calcul Qu\'{e}bec, Compute Canada, the Canada Research Chairs and CIFAR. F.V.
was funded by the AI*IA Young Researchers Mobility Grant and the Politecnico di
Milano PHD School International Mobility
Grant.

{\small
\bibliographystyle{ieee}
\bibliography{myrefs,ml,segm,camvid}
}

\end{document}




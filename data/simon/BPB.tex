%
\documentclass{article}

%
%
%
%
%
%

%

%
%
\usepackage[final,nonatbib]{nips_2016}
\usepackage[numbers]{natbib}

%
\usepackage[utf8]{inputenc} %
\usepackage[T1]{fontenc}    %
\usepackage{hyperref}       %
\usepackage{url}            %
\usepackage{booktabs}       %
\usepackage{amsfonts}       %
\usepackage{nicefrac}       %
\usepackage{microtype}      %

%
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{color}
\usepackage{graphicx}
\usepackage{subfig}

\usepackage{BPB-definitions}
\newtheorem{thm}{Theorem}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{conj}[thm]{Conjecture}
\theoremstyle{definition}
\newtheorem{definition}[thm]{Definition}
\newtheorem{remark}[thm]{Remark}
\theoremstyle{plain}

%
\definecolor{darkgreen}{rgb}{0,.4,.2}
\definecolor{darkblue}{rgb}{.1,.2,.6}
\definecolor{brightblue}{rgb}{0,0.6,0.8}
\hypersetup{
  colorlinks=true,
  linkcolor=darkblue,
  citecolor=darkgreen,
  filecolor=darkblue,
  urlcolor=darkblue
}
%

\def\LONGVERSION{}

\ifdefined\LONGVERSION
  \newcommand{\citeA}{\cite}
  \newcommand{\citepA}{\citep}
  \newcommand{\citetA}{\citet}
\else
  \usepackage{multibib}
  \newcites{A}{References}
\fi

%
%
%
  \title{PAC-Bayesian Theory Meets Bayesian Inference}
%

%
%
%
%
%
%
%
%
%

\author{
	Pascal Germain$^\dagger$ \quad Francis Bach$^\dagger$  \quad Alexandre Lacoste$^\ddagger$ \quad Simon Lacoste-Julien$^\dagger$\\
	$^\dagger$ INRIA Paris - \'Ecole Normale Sup\'erieure, \ \texttt{firstname.lastname@inria.fr}\\
		$^\ddagger$ Google, \ \texttt{allac@google.com}
}
%
%
%
%
%
%
%
%
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
%

\begin{document}
%

\maketitle

\begin{abstract}
We exhibit a strong link between frequentist PAC-Bayesian risk bounds and the Bayesian marginal likelihood.
That is, for the negative log-likelihood loss function, we show that the minimization of PAC-Bayesian generalization risk bounds maximizes the Bayesian marginal likelihood.
This provides an alternative explanation to the Bayesian Occam's razor criteria, under the assumption that the data is generated by an \iid~distribution. 
Moreover, as the negative log-likelihood is an unbounded loss function, we motivate and propose a PAC-Bayesian theorem tailored for the sub-gamma loss family, and we show that our approach is sound on classical Bayesian linear regression tasks. 
%
\end{abstract}

\section{Introduction}

Since its early beginning \citep{mcallester-99, shawetaylor-97}, the PAC-Bayesian theory claims to provide ``PAC guarantees to \emph{Bayesian} algorithms'' (\citet{mcallester-99}).
However, despite the amount of work dedicated to this statistical learning theory---many authors improved the initial results
\citep{catoni-07,lever-13,mcallester-03a,seeger-thesis,tolstikhin-13}
%
%
and/or generalized them for various machine learning setups
\citep{graal-aistats14,graal-icml16-dalc,grunwald&mehta-16,langford-02,pentina-14,seldin-10,seldin-11,seldin-12}%
%
---it is 
%
mostly used as a \emph{frequentist} method. That is, under the assumptions that the learning samples are \iid-generated by a data-distribution, this theory expresses \emph{probably approximately correct} (PAC) bounds on the generalization risk. In other words, with probability $1{-}\delta$, the generalization risk is at most $\varepsilon$ away from the training risk.  The \emph{Bayesian} side of PAC-Bayes comes mostly from the fact that these bounds are expressed on the averaging/aggregation/ensemble of multiple predictors (weighted by a \emph{posterior} distribution) and  incorporate prior knowledge.  Although it is still sometimes referred as a theory that bridges the Bayesian and frequentist approach 
\cite[\eg,][]{guyon-10},
%
 it has been merely used to  justify Bayesian methods until now.\footnote{Some existing connections \citep{banerjee-06,bissiri-16,grunwald-2012,lacoste-thesis,seeger-02,seeger-thesis,zhang-06} are discussed in Appendix~\ref{appendix:related}.}
%
%

In this work, we provide a direct connection between Bayesian inference techniques \citep[summarized by][]{bishop-2006,zoubin-15} and PAC-Bayesian risk bounds in a general setup.
Our study is based on a simple but insightful connection between the Bayesian marginal likelihood and PAC-Bayesian bounds (previously mentioned by \citet{grunwald-2012}) obtained by considering the negative log-likelihood loss function (Section~\ref{section:marginal_likelihood}). By doing so, we provide an alternative explanation for the Bayesian Occam's razor criteria  \citep{jeffreys-92,mackay-92} in the  context of model selection, expressed as the complexity-accuracy trade-off appearing in most PAC-Bayesian results. In Section~\ref{sec:more-bounds}, we extend PAC-Bayes theorems to regression problems with unbounded loss, adapted to the negative log-likelihood loss function. Finally, we study the Bayesian model selection from a PAC-Bayesian perspective (Section~\ref{sec:bayesian_model_comp}), and illustrate our finding on classical Bayesian regression tasks (Section~\ref{section:linreg}).
%

%
%
%
%
%
%


\section{PAC-Bayesian Theory}

\newcommand{\fcmt}[1]{\red{\footnote{\blue{#1}}}}
\newcommand{\cmt}[1]{\blue{\small{#1}}}

%
We denote the learning sample 
%
$(X,Y){=}\{(x_i,y_i)\}_{i=1}^n{\in}(\Xcal{\times}\Ycal)^n$, 
that contains $n$ input-output pairs. 
The main assumption of frequentist learning theories---including PAC-Bayes---is that $(\Dsample)$ is randomly sampled from a data generating distribution that we denote~$\Dgen$. 
%
Thus, we denote $(\Dsample){\sim}\Dgen^n$ the \iid observation of $n$ elements.
From a frequentist perspective, we consider in this work loss functions  $\loss:\Fcal{\times}\Xcal{\times}\Ycal\to\Rbb$,
%
where $\Fcal$ is a (discrete or continuous) set of predictors $f:\Xcal\to\Ycal$, and we write the empirical risk on the sample ($\Dsample$) and the generalization error on  distribution $\Dgen$ as
\begin{equation*}
\emploss(f)
\ = \ \frac1n \sum_{i=1}^n  \loss(f, x_i,y_i) \,;
\quad
%
\genloss(f)
\ = \, \Esp_{(x,y)\sim\Dgen} \loss(f, x,y) 
\,.
\end{equation*} 
The PAC-Bayesian theory \citep{mcallester-99,mcallester-03a} studies an averaging of the above losses according to a \emph{posterior} distribution $\post$ over  $\Fcal$. That is, it provides \emph{probably approximately correct} generalization bounds on the (unknown) quantity
%
$\Esp_{f\sim\post} \genloss(f)
\ = \, \Esp_{f\sim\post}  \Esp_{(x,y)\sim\Dgen} \loss(f, x,y) 
\,,$
%
given the empirical estimate  $\Esp_{f\sim\post} \emploss(f)$ and some other parameters. Among these, most PAC-Bayesian theorems rely on the \emph{Kullback-Leibler} divergence 
%
$\KL(\post\|\prior) \, = \, \Esp_{f\sim\post} \ln [\post(f) / \prior(f)]$ 
between a \emph{prior} distribution $\prior$ over~$\Fcal$---specified before seeing the learning sample $\Dsample$---and the posterior~$\post$---typically obtained by feeding a learning process with $(\Dsample)$. 

Two appealing aspects of PAC-Bayesian theorems are that they provide data-driven generalization bounds that are computed on the training sample  (\ie, they do not rely on a testing sample), and that they are uniformly valid for all $\post$ over $\Fcal$. This explains why  many works study them as model selection criteria or as an inspiration for learning algorithm conception. Theorem~\ref{thm:pacbayescatoni}, due to \citet{catoni-07}, has been used to derive or study learning algorithms \citep{graal-icml09,hazan-13,mcallester&keshet-11,noy-14}. 
\begin{thm}[\citet{catoni-07}] 
	\label{thm:pacbayescatoni}
	Given a distribution $\Dcal$ over  $\Xcal   \times   \Ycal$, a hypothesis set $\Fcal$, a loss function $\losszo:\Fcal\times\Xcal\times\Ycal\to[0,1]$,   a prior distribution $\prior$ over $\Fcal$, a real number $\delta \in (0,1]$, and a real number $\beta>0$,  with probability at least $1 - \delta$ over the choice of $(\Dsample) \sim  \Dgen^n $,  we have
	\begin{equation} \label{eq:catoni07}
	\forall \mbox{$\post$ on $\Fcal$}\,:\quad
	\Esp_{f\sim\post} \genlosszo(f) \ \leq\ 
	\frac{1}{1 - e^{-\beta}}    \left[1 - e^{ -\beta\,\Esp_{f\sim\post} \emplosszo(f){-}\tfrac1n\big(\KL(\post\|\prior) {+} \ln  \tfrac{1}{\delta}\big)}\right] .
	\end{equation}
\end{thm}
%
%
Theorem~\ref{thm:pacbayescatoni} is limited to loss functions mapping to the range $[0,1]$.  
Through a straightforward rescaling we can extend it to any bounded loss, \ie, $\loss:\Fcal\times\Xcal\times\Ycal\to[a,b]$, where $[a,b]\subset\Rbb$. This is done by 
%
using 
$\beta\eqdots b-a$ and with the \emph{rescaled}  loss function
$\losszo (f,x,y) \,\eqdots\, {(\loss(f,x,y){-}a)}/{(b{-}a)}\in[0,1]\,.$
After few arithmetic manipulations, we can rewrite Equation~\eqref{eq:catoni07} as
\begin{eqnarray}\label{eq:catoni_ab}
	\forall \mbox{$\post$ on $\Fcal$} :\ \ 
	\Esp_{f\sim\post}\! \genloss(f) \,\leq\, a+\tfrac{b-a}{1 - e^{a-b}}    \Big[1 {-} \exp\Big({ -\Esp_{\mathclap{f\sim\post}}\, \emploss(f)  {+}a{-}\tfrac1n\big(\KL(\post\|\prior) {+} \ln  \tfrac{1}{\delta}\big)}\Big)\Big]\,.
\end{eqnarray}
%
From an algorithm design perspective, Equation~\eqref{eq:catoni_ab} suggests optimizing a trade-off between the empirical expected loss and the Kullback-Leibler divergence. Indeed,
for fixed $\prior$, $X$, $Y$, $n$, and $\delta$, minimizing Equation~$\eqref{eq:catoni_ab}$
%
is equivalent to find the distribution $\post$ that minimizes
\begin{equation}\label{eq:a_minimser_beta}
n\,\Esp_{\mathclap{f\sim\post}}\, \emploss(f)  +  \KL(\post\|\prior)\,.
\end{equation}
%
It is well known \citep{alquier-15,catoni-07,graal-icml09,lever-13} that the \emph{optimal Gibbs posterior} $\post^*$ is given by
%
\begin{equation}\label{eq:post_catoni}
\post^*(f) \, = \, \tfrac1{\ZDs} \prior(f) \,e^{-n\,\emploss(f)}\,,
\end{equation}
where $\ZDs$ is a normalization term. Notice that the constant $\beta$ of Equation~\eqref{eq:catoni07} is now absorbed in the loss function as the rescaling factor 
setting the trade-off between the expected empirical loss and~$\KL(\post\|\prior)$. 
%


%


\section{Bridging Bayes and PAC-Bayes}
\label{section:marginal_likelihood}


In this section, we show that by choosing the negative log-likelihood loss function,
%
minimizing the PAC-Bayes bound is equivalent to maximizing the Bayesian marginal likelihood.
%
%
%
%
To obtain this result, we first consider the Bayesian approach that starts by defining a prior $p(\theta)$ over the set of possible model parameters $\Theta$. This induces a set of probabilistic estimators $f_\theta \in \Fcal$, mapping $x$ to a probability distribution over $\Ycal$. Then, we can estimate the likelihood of observing $y$ given $x$ and $\theta$, \ie, $p(y|x,\theta) \equiv f_\theta(y|x)$.\footnote{To stay aligned with the PAC-Bayesian setup, we only consider the discriminative case in this paper. One can extend to the generative setup by considering the likelihood of the form $p(y,x|\theta)$ instead.} Using Bayes' rule, we obtain the posterior $p(\theta|X,Y)$:
\begin{samepage}
\begin{equation} \label{eq:bayes_updaterule}
p(\theta|\Dsample) 
\,=\, \frac{p(\theta)\,p(Y|X,\theta)}{p(Y|X)}
\, \propto\, 
p(\theta)\,p(Y|X,\theta)\,,
\end{equation}
where $p(Y|X,\theta) \, = \, \prod_{i=1}^n p(y_i|x_i,\theta)$ and 
%
$p(Y|X) = \int_\Theta p(\theta) \,p(Y|X,\theta)\, d\theta$.
\end{samepage}

To bridge the Bayesian approach with the PAC-Bayesian framework, we consider the \emph{negative log-likelihood} loss function 
%
\citep{banerjee-06}, 
denoted $\lossnll$ and defined by 
\begin{equation} \label{eq:loss_vs_likelihood}
\lossnll(f_\theta,x,y) \ \equiv\ -\ln{p(y|x,\theta)}\,.
\end{equation}
Then, we can relate the \emph{empirical loss} $\emploss$ of a predictor to its likelihood:
\begin{equation*}
	\emplossnll(\theta) 
	\ = \ \frac1n \sum_{i=1}^n \lossnll(\theta, x_i ,y_i)
	\ = \  -\frac1n \sum_{i=1}^n \ln p( y_i |x_i, \theta)
   \ = \  -\frac1n \ln p(Y|X,\theta) \,,
\end{equation*}
or, the other way around,
\vspace{-2mm}
\begin{equation} \label{eq:dataset_likelihhod}
p(Y|X,\theta) \, = \, e^{-n\, \emplossnll(\theta)}\,. 
\end{equation}

Unfortunately, existing PAC-Bayesian theorems work with bounded loss functions or in very specific contexts \citep[\eg,][]{dalalyan-08,zhang-06},  and $\lossnll$ spans the whole real axis in its general form. In Section~\ref{sec:more-bounds}, we explore PAC-Bayes bounds for unbounded losses. Meanwhile, we consider priors with bounded likelihood. This can be done by assigning a prior of zero to any $\theta$ yielding $\ln \frac{1}{p(y|x,\theta)} \notin [a,b]$. 
%

Now, using Equation~\eqref{eq:dataset_likelihhod} in the optimal posterior (Equation~\ref{eq:post_catoni}) simplifies to
\begin{eqnarray} \label{eq:post_catoni_theta} 
\post^*(\theta)
=
\frac{\prior(\theta) \,e^{-n\,\emplossnll(\theta)}}{\ZDs}\,
\,=\,  
\frac{p(\theta)\,p(Y|X,\theta)}{p(Y|X)}
 = 
p(\theta|\Dsample)\,,
\end{eqnarray}
%
where the normalization constant $\ZDs$ corresponds to the Bayesian \emph{marginal likelihood}:
\begin{equation} \label{eq:post_catoni_Zb} 
\ZDs \ \equiv \ p(Y|X) \ = \ 
\int_{\Theta}\prior(\theta) \,e^{-n\,\emplossnll(\theta)} d \theta\,.
\end{equation}
This shows that the optimal PAC-Bayes posterior given by the generalization bound of Theorem~\ref{thm:pacbayescatoni} coincides with the Bayesian posterior, when one chooses $\lossnll$ as loss function and $\beta\eqdots b{-}a$ (as in Equation~\ref{eq:catoni_ab}).
%
Moreover, using the posterior of Equation~\eqref{eq:post_catoni_theta} inside Equation~\eqref{eq:a_minimser_beta}, we obtain
%
%
\begin{eqnarray} \label{eq:putback} 
& & \hspace{-.8cm} n \Esp_{\theta\sim\post^*} \emplossnll(\theta)  +  \KL(\post^*\|\prior)\\[-1mm]
&=& \nonumber
n  \int_\Theta \tfrac{\prior(\theta) \,e^{-n\,\emplossnll(\theta)}}{\ZDs} \emplossnll(\theta)\, d\theta
+  \int_\Theta \tfrac{\prior(\theta) \,e^{-n\,\emplossnll(\theta)}}{\ZDs} 
\ln \Big[ \tfrac{\prior(\theta) \,e^{-n\,\emplossnll(\theta)}}{\prior(\theta)\, \ZDs} %
\Big]
 d\theta\\[-1mm]
%
%
%
%
%
&=& \nonumber
\int_\Theta \tfrac{\prior(\theta) \,e^{-n\,\emplossnll(\theta)}}{\ZDs} 
\left[\ln\tfrac1{\ZDs}\right]
 d\theta
%
\ = \ \tfrac{\ZDs}{\ZDs}  \ln\tfrac1{\ZDs} \ = \ -\ln{\ZDs}\,.
\end{eqnarray}
In other words, minimizing the PAC-Bayes bound is equivalent to maximizing the marginal likelihood.
Thus, from the PAC-Bayesian standpoint, the latter encodes  a trade-off between the averaged negative log-likelihood loss function and the prior-posterior Kullback-Leibler divergence.
%
%
Note that Equation~\eqref{eq:putback} has been mentioned by \citet{grunwald-2012}, based on an earlier observation of \citet{zhang-06}. However, the PAC-Bayesian theorems proposed by the latter do not bound the generalization loss directly, as the ``classical'' PAC-Bayesian results \cite{catoni-07,mcallester-99,seeger-02} that we extend to regression in forthcoming Section~\ref{sec:more-bounds} (see the corresponding remarks in Appendix~\ref{appendix:related}).

We conclude this section by proposing a compact form of Theorem~\ref{thm:pacbayescatoni} by expressing it in terms of the  marginal likelihood, 
as a direct consequence  of Equation~\eqref{eq:putback}. 
\begin{cor}%
	\label{thm:pacbayescatoni_maglike}
	Given a data distribution $\Dcal$, a parameter set $\Theta$,  a prior distribution $\prior$ over $\Theta$, a $\delta \in (0,1]$, if $\lossnll$ lies in $[a,b]$, we have, with probability at least $1 - \delta$ over the choice of  $(\Dsample) \sim  \Dgen^n $,
	\begin{equation*} %
	\Esp_{\theta\sim\post^*} \genlossnll(\theta) 
			\ \leq \ 
			a + \tfrac{ b-a}{1 - e^{a-b}} \left[ 1-e^a \,\sqrt[n]{\ZDs\,\delta}\,\right] ,
	\end{equation*}		
	where $\post^*$ is the Gibbs optimal posterior (Eq.~\ref{eq:post_catoni_theta}) 
	and $\ZDs$ is the marginal likelihood (Eq.~\ref{eq:post_catoni_Zb}).
\end{cor}
%
%
%
%
%
%
%
%
%
 In Section~\ref{sec:bayesian_model_comp}, we exploit the link between PAC-Bayesian bounds and Bayesian marginal likelihood to expose similarities between both frameworks in the context of model selection.
 Beforehand, next Section~\ref{sec:more-bounds} extends the PAC-Bayesian generalization guarantees to unbounded loss functions. This is mandatory to make our study fully valid, as  the negative log-likelihood loss function is in general unbounded (as well as other common regression losses).
 \section{PAC-Bayesian Bounds for Regression}
\label{sec:more-bounds}

This section aims to extend the PAC-Bayesian results of Section~\ref{section:marginal_likelihood} to real valued unbounded loss.
%
These results are used in forthcoming sections to study $\lossnll$, but they are valid  for broader classes of loss functions. 
Importantly, our new results are focused on regression problems, as opposed to the usual PAC-Bayesian classification framework. 

%
The new bounds are obtained through a recent theorem of \citet{alquier-15}, stated below
(we provide a proof in Appendix~\ref{appendix:pacbayes_proofs} for completeness).
%
%
%
%
\begin{thm}[\citet{alquier-15}] \label{thm:general-alquier}
	Given a distribution $\Dcal$ over  $\Xcal   \times   \Ycal$, a hypothesis set $\Fcal$, a loss function $\loss:\Fcal\times\Xcal\times\Ycal\to\Rbb$,   a prior distribution $\prior$ over $\Fcal$, a $\delta \in (0,1]$, and a real number $\lambda>0$,  
%
%
%
%
%
%
%
%
%
%
%
	with probability at least $1{-}\delta$ over the choice of $(\Dsample)\sim \Dgen^n$,
	we have
	\begin{align} \label{eq:alquier}
	\forall \post \text{ on } \Fcal\colon \quad &
	\Esp_{f\sim\post} \genloss(f) 
	\ \le \  \Esp_{f\sim\post} \emploss(f) +
	\dfrac{1}{\lambda}\!\left[ \KL(\post\|\prior) +
	\ln\dfrac{1}{\delta}
	+ \Psi_{\ell,\prior,\Dgen}(\lambda,n)  \right],\\[2mm]
%
\mbox{where }\quad &
	\label{eq:alquier_assumption}
\Psi_{\ell,\prior,\Dgen}(\lambda,n) \ = \ 
\ln	\Esp_{f\sim\prior} \Esp_{{\Dsampleprim\sim \Dgen^n}} 
	\exp\left[{\lambda\left(\genloss(f) - \emplossprim(f)\right)}\right].
	\end{align}
\end{thm}
%
%
%
\citeauthor{alquier-15} used Theorem~\ref{thm:general-alquier} to design a learning algorithm for $\{0,1\}$-valued classification losses.  
Indeed, a bounded loss function $\loss:\Fcal\times\Xcal\times\Ycal\to[a,b]$ can be used along with Theorem~\ref{thm:general-alquier} by applying the
Hoeffding's lemma to Equation~\eqref{eq:alquier_assumption}, that gives
$
%
\Psi_{\ell,\prior,\Dgen}(\lambda,n) \leq
\lambda^2(b{-}a)^2/(2n).
$
More specifically, with $\lambda\eqdots n$, we obtain the following bound
\begin{align} \label{eq:alquier_hoeffding_n}
\forall \post \text{ on } \Fcal\colon \quad 
\Esp_{f\sim\post} \genloss(f) 
\ \le \  \Esp_{f\sim\post} \emploss(f) +
\tfrac{1}{n}\!\left[  \KL(\post\|\prior) +
\ln\tfrac{1}{\delta}\right]+\tfrac12(b-a)^2.
\end{align}
Note that the latter bound leads to the same trade-off as Theorem~\ref{thm:pacbayescatoni} (expressed by Equation~\ref{eq:a_minimser_beta}).
However, the choice $\lambda\eqdots n$ has the inconvenience that the bound value is at least $\frac12(b-a)^2$, even at the limit $n\to\infty$. With $\lambda\eqdots \sqrt n$ the bound converges (a result similar to Equation~\eqref{eq:alquier_hoeffding_sqrtn} is also formulated by \citet{pentina-14}):
\begin{align} \label{eq:alquier_hoeffding_sqrtn}
\forall \post \text{ on } \Fcal\colon \quad 
\Esp_{f\sim\post} \genloss(f) 
\ \le \  \Esp_{f\sim\post} \emploss(f) +
\tfrac{1}{\sqrt n}\!\left[  \KL(\post\|\prior) +
\ln\tfrac{1}{\delta} +\tfrac12(b-a)^2 \right].
\end{align}
%

%
\paragraph{Sub-Gaussian losses.}
%
In a regression context, it may be restrictive to consider strictly bounded loss functions. Therefore, we extend Theorem~\ref{thm:general-alquier} to \emph{sub-Gaussian} losses.
We say that a loss function~$\loss$ is sub-Gaussian with variance factor $s^2$ under a prior $\prior$ and a data-distribution $\Dcal$ if it can be described by a sub-Gaussian random variable $V{=}\genloss(f){-}\loss(f,x,y)$, \ie, its moment generating function is upper bounded by the one of a normal distribution of variance $s^2$ (see \citet[][Section~2.3]{boucheron-13}):
\begin{equation}\label{eq:subG_assumption}
\psi_{_V}(\lambda)
	\ = \ \ln \Esp e^{\lambda V}%
	\ =  \ \ln	\Esp_{f\sim\prior} \Esp_{{(x,y)\sim \Dgen}} 
	\exp\left[{\lambda\left(\genloss(f) - \loss(f, x,y)\right)}\right]
	\ \leq \ {\tfrac{\lambda^2 s^2}{2}}\,
, \quad \forall \lambda  \in \Rbb\,.
%
%
%
\end{equation}
The above sub-Gaussian assumption corresponds to the \emph{Hoeffding assumption} of \citet{alquier-15}, and allows to obtain the following result.
\begin{cor} \label{cor:alquier-subG}
Given $\Dcal$, $\Fcal$,  $\loss$,  $\prior$ and $\delta$ defined in the statement of  Theorem~\ref{thm:general-alquier}, if the loss is sub-Gaussian with variance factor $s^2$, we have,
with probability at least $1{-}\delta$ over the choice of $(\Dsample)\sim \Dgen^n$,
\begin{align*}%
\forall \post \text{ on } \Fcal\colon \quad 
\Esp_{f\sim\post} \genloss(f) 
\ \le \  \Esp_{f\sim\post} \emploss(f) +
\tfrac{1}{n}\!\left[  \KL(\post\|\prior) +
\ln\tfrac{1}{\delta}\right]+\tfrac12\, s^2\,.
\end{align*}
\end{cor}
\begin{proof}
	For  $i = 1\ldots n$, we denote  $\ell_i$ a \iid realization of the random variable 
	$\genloss(f) - \loss(f, x, y) $. 
%
		\begin{align*}\textstyle
	\Psi_{\ell,\prior,\Dgen}(\lambda,n)
		= \ln \Esp \exp \left[ \tfrac\lambda n \sum_{i=1}^n \ell_i\right]%
		= \ln \prod_{i=1}^n \Esp \exp \left[ \tfrac\lambda n\ell_i\right]
		= \sum_{i=1}^n \psi_{\loss_i}(\tfrac{\lambda}{n})
		\, \leq\, n \tfrac{\lambda^2s^2}{2n^2}
		%
		\,=\,   \tfrac{\lambda^2s^2}{2n},
		\end{align*}
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
	where the inequality comes from the sub-Gaussian loss assumption (Equation~\ref{eq:subG_assumption}).
	The result is then obtained from Theorem~\ref{thm:general-alquier}, with $\lambda\eqdots n$.
%
%
%
%
%
%
%
%
%
%
%
\end{proof}
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


\paragraph{Sub-gamma losses.}
%
%
We say that an unbounded loss function $\loss$ is sub-gamma with a variance factor~$s^2$ and scale parameter $c$, under a prior $\prior$ and a data-distribution $\Dgen$, if it can be described by a 
%
sub-gamma random variable $V$ %
(see \citet[][Section 2.4]{boucheron-13}), that is 
\begin{equation}\label{eq:subgamma_assumption}
\psi_{_V}(\lambda)
\ \leq \ \tfrac{s^2}{c^2}({-}\ln(1{-}\lambda c) - \lambda c)
\ \leq \ {\tfrac{\lambda^2 s^2}{2(1-c\lambda)}}\,,  \qquad \forall \lambda  \in (0, \tfrac1c)\,.
\end{equation}
Under this sub-gamma assumption, we obtain the following new result, which is necessary to study linear regression in the next sections. 
\begin{cor} \label{cor:subgamma}
	Given $\Dcal$, $\Fcal$,  $\loss$,  $\prior$ and $\delta$ defined in the statement of Theorem~\ref{thm:general-alquier}, if the loss is sub-gamma with variance factor $s^2$ and scale $c<1$, we have,
	with probability at least $1{-}\delta$ over 
%
$(\Dsample)\sim \Dgen^n$,
	\begin{align} \label{eq:subgamma}
	\forall \post \text{ on } \Fcal\colon \quad 
	\Esp_{f\sim\post} \genloss(f) 
	\ \le \  \Esp_{f\sim\post} \emploss(f) +
	\tfrac{1}{n}\!\left[  \KL(\post\|\prior) +
	\ln\tfrac{1}{\delta}\right]+\tfrac1{2(1-c)}\, s^2\,.
	\end{align}
As a special case, with $\loss\eqdots\lossnll$ and $\post \eqdots \post^*$ (Equation~\ref{eq:post_catoni_theta}), we have
\begin{equation} \label{eq:subgammaZ} 
		\displaystyle\Esp_{\theta\sim\post^*} \genlossnll(\theta) 
		\, \leq\,
		\tfrac{s^2 }{2(1-c)}- \tfrac{1}{n}\ln\left( \ZDs\, \delta \right).
\end{equation}
\end{cor}
\begin{proof}
Following the same path as in the proof of Corollary~\ref{cor:alquier-subG} (with $\lambda\eqdots n$), we have
		\begin{align*}\textstyle
		\Psi_{\ell,\prior,\Dgen}(n,n)
		= \ln \Esp \exp \left[ \sum_{i=1}^n \ell_i\right]%
		= \ln \prod_{i=1}^n \Esp \exp \left[ \ell_i\right]
		= \sum_{i=1}^n \psi_{\loss_i}(1)
		\, \leq\, n \tfrac{s^2}{2(1-c)}
		%
		\,=\,   \tfrac{n\,s^2}{2(1-c)}\,,
		\end{align*}
	where the inequality comes from the sub-gamma loss assumption, with $ 1\in(0,\frac1c)$.
\end{proof}

\paragraph{Squared loss.}

The parameters $s$ and $c$ of Corollary~\ref{cor:subgamma} rely on the chosen loss function and prior, and the assumptions concerning the data distribution.
As an example, consider a regression problem where $\Xcal{\times}\Ycal \subset \Rbb^d{\times}\Rbb$, a family of linear predictors $f_\wb(\xb) = \wb\cdot\xb$, with $\wb\in\Rbb^d$, and a Gaussian prior 
%
$\Ncal(\zerobf, \sigma_\prior^2\,\Ib)$.  
%
Let us assume that the input examples are generated by $\xb\!\sim\!\Ncal(\zerobf,\sigx^2\,\Ib)$ with label $y=\wb^*\!\cdot \xb+\epsilon$, where $\wb^*\! \in\! \Rbb^d$ and  $\epsilon\!\sim\!\Ncal(0,\sigma_\epsilon^2)$ is a Gaussian noise.
Under the squared loss function
\begin{equation}\label{eq:sqrloss}
\losssqr(\wb,\xb,y) \, =\, (\wb\cdot\xb-y)^2\,,
\end{equation}
we show in Appendix~\ref{appendix:s} that Corollary~\ref{cor:subgamma}  is valid with 
 $s^2 \geq 2\left[\sigx^2(\sigprior^2d+ \|\wb^*\|^2)+ \sigeps^2(1- c)\right]$ and $c \geq 2\sigx^2\sigprior^2$.
As expected, the bound degrades when the noise increases
%
%
%


\paragraph{Regression versus classification.}
\label{section:discussion}


The classical PAC-Bayesian theorems are stated in a classification context and bound the generalization error/loss of the stochastic \emph{Gibbs predictor} $G_\post$.
%
%
%
%
In order to predict the label of an example $x\in\Xcal$, the Gibbs predictor first draws a hypothesis $h\in\Fcal$ according to $\post$, and then returns $h(x)$.
\citet{maurer-04} shows that we can generalize PAC-Bayesian bounds on the generalization risk of the Gibbs classifier to any loss function with output between zero and one. 
%
Provided that $y\in\{-1,1\}$ and $h(x)\in[-1,1]$, a common choice is to use the linear loss function  $\losszozo(h,x,y) = \frac12 - \frac12 y\,h(x)$.
The Gibbs generalization loss is then given by
$R_\Dgen(G_\post) = \Esp_{(x,y)\sim\Dgen}  \Esp_{h\sim\post}  \losszozo(h,x,y) $\,.
%
%
%
%
%
%
Many PAC-Bayesian works use $R_\Dgen(G_\post)$ as a surrogate loss to study the zero-one classification loss of the majority vote classifier  $R_\Dgen(B_\post)$:
%
%
%
\begin{equation} \label{eq:bayes_risk} %
%
R_\Dgen(B_\post) 
\,=\, \Pr_{(x,y)\sim\Dgen} \Big( y\,\Esp_{h\sim\post}h(x) < 0 \Big)
\,=\, \Esp_{(x,y)\sim\Dgen} I\Big[ y\,\Esp_{h\sim\post}h(x) < 0 \Big]\,,
%
%
%
%
\end{equation} 
where $I[\cdot]$ being the indicator function. Given a distribution $\post$, %
an upper bound on the Gibbs risk is converted to an upper bound on the majority vote risk by $R_\Dgen(B_\post)\leq 2 R_\Dgen(G_\post)$ \citep{langford-02}.
In some situations, this \emph{factor of two} may be reached, \ie, $R_\Dgen(B_\post)\simeq 2 R_\Dgen(G_\post)$.
In other situations, we may have $R_\Dgen(B_\post)=0$ even if $R_\Dgen(G_\post) = \frac12{-}\epsilon$ (see \citet{graal-neverending} for an extensive study). 
Indeed, these bounds obtained via the Gibbs risk are exposed to be loose and/or unrepresentative of the majority vote generalization error.\footnote{It is noteworthy that the best PAC-Bayesian empirical bound values are so far obtained by considering a majority vote of linear classifiers, where the prior and posterior are Gaussian \citep{ambroladze-06,graal-icml09,langford-02}, similarly to  the Bayesian linear regression analyzed in Section~\ref{section:linreg}.} 

In the current work, we study regression losses instead of classification ones.
That is, the provided results express upper bounds on $\Esp_{f\sim\post} \genloss(f)$ for any (bounded, sub-Gaussian, or sub-gamma) losses. %
%
%
Of course, one may want to bound the regression loss of the averaged regressor $F_\post(x) = \Esp_{f\sim\post} f(x)$.  In this case, if the loss function $\loss$ is convex (as the squared loss), Jensen's inequality gives
%
$\genloss(F_\post)  \, \leq\, \Esp_{f\sim\post} \genloss(f)\,.$
%
Note that a strict inequality 
%
 replaces the factor two mentioned above for the classification case, due to the non-convex indicator function of Equation~\eqref{eq:bayes_risk}.

Now that we have generalization bounds for real-valued loss functions, we can continue our study linking PAC-Bayesian results to Bayesian inference. 
In the next section, we focus on model selection.


\section{Analysis of Model Selection}

\label{sec:bayesian_model_comp}

We consider $L$ distinct models $\{\model_i\}_{i=1}^L$, each one defined by a set of parameters $\Theta_i$.
The PAC-Bayesian theorems naturally suggest selecting the model that is best adapted for the given task  by evaluating the bound for each model $\{\model_i\}_{i=1}^L$ and selecting the one with the lowest bound \citep{ambroladze-06,mcallester-03a,zhang-06}. 
This is closely linked with the Bayesian model selection procedure, as we showed in Section~\ref{section:marginal_likelihood} that minimizing the PAC-Bayes bound amounts to maximizing the marginal likelihood. %
Indeed, given a collection of $L$ optimal Gibbs posteriors---one for each model---given by Equation~\eqref{eq:post_catoni_theta},
\begin{equation} \label{eq:optpost_modelselect}
p(\theta|X,Y, \model_i) \ \equiv\  \post_i^*(\theta) \, = \, \tfrac1{\Zi} \prior_i(\theta) \,e^{-n\,\emplossnll(\theta)}, \ \mbox{ for } \theta \in \Theta_i\,,
\end{equation}
%
%
%
the Bayesian Occam's razor criteria  \citep{jeffreys-92,mackay-92}  chooses the one with the higher \emph{model evidence}
\begin{equation} \label{eq:model_evidence}
p(Y|X,\model_i) \ \equiv \ \Zi \,=\, 
\int_{\Theta_i} \, \prior_i(\theta)\, e^{-n \,\emploss(\theta)} \, d \theta\,.
\end{equation}
Corollary~\ref{cor:pacbayescatoni_modelselect} below formally links the PAC-Bayesian and the Bayesian model selection.
To obtain this result, we simply use the bound of Corollary~\ref{cor:subgamma} $L$ times, together with $\lossnll$ and Equation~\eqref{eq:putback}.
From the union bound (\emph{a.k.a.} Bonferroni inequality), it is mandatory to compute each bound with a confidence parameter of
$\delta/L$,
to ensure that the final conclusion is valid with probability at least $1{-}\delta$.
%
%
%
\begin{cor}%
	\label{cor:pacbayescatoni_modelselect}
	Given a data distribution $\Dcal$, a family of model parameters $\{\Theta_i\}_{i=1}^L$  and associated priors $\{\prior_i\}_{i=1}^L$---where $\prior_i$ is defined over $\Theta_i$--- , a $\delta \in (0,1]$,  if the loss is sub-gamma with parameters $s^2$ and $c<1$, then, with probability at least $1 - \delta$ over $(\Dsample) \sim  \Dgen^n $, 
	\begin{equation*} %
	\forall i \in \{1,\ldots,L\}\,:\qquad
		\Esp_{\theta\sim\post_i^*} \genlossnll(\theta) 
		\ \leq \ 
%
\tfrac1{2(1-c)}\, s^2 - \tfrac{1}{n}\ln\left( \Zi \tfrac\delta L\right).
	\end{equation*}
	where $\post^*_i$ is the Gibbs optimal posterior (Eq.~\ref{eq:optpost_modelselect})
%
%
%
%
	and $\Zi$ is the marginal likelihood (Eq.~\ref{eq:model_evidence}).
\end{cor}

%
%
%
%
%
%
%
%
%
Hence, under the uniform prior over the $L$ models, choosing the one with the best model evidence is equivalent to choosing the one with the lowest PAC-Bayesian bound. 
%

\paragraph{Hierarchical Bayes.}
To perform proper inference on hyperparameters, we have to rely on the \emph{Hierarchical Bayes} approach. This is done by considering an \emph{hyperprior} $p(\eta)$ over the set of hyperparameters $\Hrm$. Then, the prior $p(\theta|\eta)$ can be conditioned on a choice of hyperparameter $\eta$. 
%
%
%
%
The Bayes rule of Equation~\eqref{eq:bayes_updaterule} becomes
%
%
$p(\theta,\eta|X,Y) 
=\frac{p(\eta)\, p(\theta | \eta) \,p(Y|X,\theta)}{p(Y|X)}\,.$
%
%
%

Under the negative log-likelihood loss function, we can rewrite the results of Corollary~\ref{cor:subgamma} as a generalization bound 
on $\Esp_{\eta\sim\post_0} \Esp_{\theta\sim\post_\eta^*} \genlossnll(\theta) $, where $\post_0(\eta) \propto {\prior_0(\eta) \,\ZDeta}$ is the hyperposterior on~$\Hrm$ and $\prior_0$ the hyperprior. Indeed, Equation~\eqref{eq:subgammaZ} becomes
		\begin{equation} \label{eq:h_bound}
		\Esp_{\theta\sim\post^*} \genlossnll(\theta) 
		=  \Esp_{\eta\sim\post_0^*} \Esp_{\theta\sim\post_\eta^*} \genlossnll(\theta) 
		\ \leq\ 
\tfrac1{2(1-c)}\, s^2 - \tfrac{1}{n}\ln\left( \Esp_{\eta\sim\prior_0}  \ZDeta\,\delta \right). 
%
		\end{equation}

		
%
To relate to the bound obtained in Corollary~\ref{cor:pacbayescatoni_modelselect}, we consider the case of a discrete hyperparameter set $\Hrm = \{\eta_i\}_{i=1}^L$, with a uniform prior $\prior_0(\eta_i)=\frac1L$ (from now on, we regard each hyperparameter $\eta_i$ as the specification of a model $\Theta_i$). 
Then, Equation~\eqref{eq:h_bound} becomes 
		\begin{equation*} %
		 \Esp_{\theta\sim\post^*} \genlossnll(\theta) 
		=  \Esp_{\eta\sim\post_0^*} \Esp_{\theta\sim\post_\eta^*} \genlossnll(\theta) 
		\ \leq\ 
%
\tfrac1{2(1-c)}\, s^2 - \tfrac{1}{n}\ln\left(  \textstyle \sum_{i=1}^L  Z_{X,Y,\eta_i}\,\tfrac \delta L \right). 
		\end{equation*}
%
This bound is now a function of $\sum_{i=1}^L  Z_{X,Y,\eta_i}$ instead of $\max_i Z_{X,Y,\eta_i}$ as in the bound given by the ``best'' model in Corollary~\ref{cor:pacbayescatoni_modelselect}.
This yields a tighter bound, corroborating the Bayesian wisdom that model averaging performs best. %
%
Conversely, when selecting a single hyperparameter $\eta^*\in\Hrm$, the hierarchical representation is equivalent to choosing a deterministic hyperposterior, satisfying $\post_0(\eta^*)=1$ and $0$ for every other values. We then have
\begin{align*}
 \KL(\post||\prior) \ =\ \KL(\post_0||\prior_0) + \Esp_{\eta \sim \post_0}  \KL(\post_\eta||\prior_\eta) 
 \ =\ \ln(L) + \KL(\post_{\eta^*}||\prior_{\eta^*})\,.
\end{align*}
With the optimal posterior for the selected $\eta^*$, we have
\begin{align*}
n \Esp_{\theta \sim \post} \emplossnll(\theta) +\KL(\post||\prior) & \ =\  n \Esp_{\theta \sim \post_\eta^*} \emplossnll(\theta) +\KL(\post_{\eta^*}^*||\prior_{\eta^*}) +\ln(L) \\[-2mm]
&\ =\ -\ln(Z_{X,Y,\eta^*}) + \ln(L) \, =\, -\ln \left( \tfrac{Z_{X,Y,\eta^*}}{L} \right).
\end{align*}
Inserting this result into Equation~\eqref{eq:subgamma}, we fall back on the bound obtained in Corollary~\ref{cor:pacbayescatoni_modelselect}.
Hence, by comparing the values of the bounds, one can get an estimate on the consequence of performing model selection instead of model averaging.

%

%
%

%
%

%
%


%
%
%


%
%
%



\section{Linear Regression}
\label{section:linreg}

In this section, we perform \emph{Bayesian linear regression} using the parameterization of \citet{bishop-2006}.  The output space is $\Ycal\eqdots\Rbb$ and, for an arbitrary input space $\Xcal$, we use a mapping function $\phib:\!\Xcal{\to}\Rbb^d$.  
%

\paragraph{The model.}
Given $(x,y)\in\Xcal\times\Ycal$ and model parameters $\theta\eqdots\tuple{\wb, \sigma}\in\Rbb^d\times\Rbb^+$, we consider the likelihood
%
$p(y|x,\tuple{\wb, \sigma}) = \Ncal(y|\wb\cdot\phib(\xb), \sigma^2)$.
%
%
Thus, the negative log-likelihood loss is
\begin{equation}\label{eq:lossnll_ws}
\lossnll(\dtuple{\wb, \sigma},x,y) \, =\, -\ln{p(y|x,\dtuple{\wb, \sigma})}\
= \tfrac12 \ln(2\pi\sigma^2) + \tfrac1{2\sigma^2} (y - \wb\cdot\phib(x))^2\,.
\end{equation}
For a fixed $\sigma^2$, minimizing Equation~\eqref{eq:lossnll_ws} is equivalent to minimizing the squared loss function of Equation~\eqref{eq:sqrloss}.
%
We also consider an isotropic Gaussian prior of mean $\mathbf0$ and variance~$\sigmaprior^2$:
$p(\wb|\sigmaprior) = \Ncal(\wb|\mathbf0, \sigmaprior^2\Ib)$.
%
%
For the sake of simplicity, we consider fixed parameters~$\sigma^2$ and $\sigmaprior^2$. The Gibbs optimal posterior (see Equation~\ref{eq:post_catoni_theta}) is then given by 
%
%
%
%
%
%
%
%
\begin{equation}\label{eq:post_reg} 
\post^*(\wb) \, \equiv\,  
p(\wb|X,Y,\sigma, \sigmaprior) 
\ = \ 
\tfrac{p(\wb| \sigmaprior)\,p(Y|X,\wb, \sigma)}{p(Y|X,\sigma, \sigmaprior)}%
\ =\ 
\Ncal(\wb\,|\,\wpost, A^{-1})\,,
\end{equation}
where 
%
$A\eqdots\frac1{\sigma^2}\Phib^T\Phib+\frac1{\sigmaprior^2}\Ib$
 ;  $\wpost\eqdots\frac1{\sigma^2}A^{-1}\Phib^T \yb$ 
 ;  $\Phib$ is a $n{\times} d$ matrix such that the $i^{th}$ line is~$\phib(x_i)$\,; 
  $\yb \eqdots [y_1, \ldots y_n]$ is the labels-vector
 ;  and the negative log marginal likelihood is
\begin{align*}
-\ln &\,p(Y|X,\sigma, \sigmaprior)
%
	=
	\tfrac1{2\sigma^2}\|\yb-\Phib\wpost\|^2
	+\tfrac n 2\ln(2\pi \sigma^2)
	+ 
	\tfrac1{2\sigmaprior^2}\|\wpost\|^2
	+ \tfrac12 \log|A|
	+ d\ln\sigmaprior \\
	&=
	\underbrace{
%
%
n\,\emplossnll(\wpost)
		+     \tfrac{1}{2 \sigma^2}  \tr ( \Phib^T \Phib A^{-1})
	}_{n\Esp_{\wb\sim\post^*}\emplossnll(\wb)} 
	{+} 
	\underbrace{
		\tfrac1{2\sigmaprior^2}\tr(A^{-1})
		- \tfrac d2
		+ \tfrac1{2\sigmaprior^2}\|\wpost\|^2
		+ \tfrac12 \log|A|
		+ d\ln\sigmaprior 
	}_{\KL\big(\Ncal(\wpost,A^{-1})\,\|\,\Ncal(\mathbf{0},\sigmaprior^2\Ib)\big)}.
\end{align*}
%
To obtain the second equality, we substitute
$\tfrac1{2\sigma^2}\|\yb{-}\Phib\wpost\|^2{+}\tfrac n 2\ln(2\pi \sigma^2) = n\,\emplossnll(\wpost) $ and insert\\
%
\phantom.\hfill
	$\tfrac{1}{2\sigma^2}  \tr ( \Phib^T \Phib A^{-1}) + \tfrac1{2\sigmaprior^2} \tr(A^{-1}) 
= \tfrac12 \tr ( \tfrac{1}{\sigma^2} \Phib^T \Phib A^{-1} + \tfrac1{\sigmaprior^2} A^{-1}) 
=  \tfrac12 \tr ( A^{-1} A )
= \tfrac d2\,.$\hfill\phantom.\\
%
%
This exhibits how the Bayesian regression optimization problem is related to the minimization of a PAC-Bayesian bound, expressed by a trade-off between
$\Esp_{\wb\sim\post^*}\emplossnll(\wb)$
and
 $\KL\big(\Ncal(\wpost,A^{-1})\,\|\,\Ncal(\mathbf{0},\sigmaprior^2\,\Ib)\big)$.
 See Appendix~\ref{appendix:linreg} for detailed calculations.

\paragraph{Model selection experiment.}
To produce Figures~\ref{fig:a} and ~\ref{fig:b}, we reimplemented the toy experiment of \citet[Section~3.5.1]{bishop-2006}. 
That is, we generated a learning sample of $15$ data points according to $y=\sin(x)+\epsilon$, where $x$ is uniformly sampled in  the interval $[0,2\pi]$ and $\epsilon\sim\Ncal(0,\frac14)$ is a Gaussian noise. We then learn seven different polynomial models applying Equation~\eqref{eq:post_reg}. More precisely, for a polynomial model of degree $d$, we map input $x\in\Rbb$ to a vector $\phib(x) = [1,x^1,x^2,\ldots,x^d]\in\Rbb^{d+1}$, and we fix parameters $\sigmaprior^2=\frac1{0.005}$ and $\sigma^2=\frac12$.
Figure~\ref{fig:a} illustrates the seven learned models.
Figure~\ref{fig:b} shows the negative log marginal likelihood  computed for each polynomial model, and is designed to reproduce \citet[Figure 3.14]{bishop-2006}, where it is explained that the marginal likelihood correctly indicates that the polynomial model of degree $d=3$ is ``the simplest model which gives a good explanation for the observed data''. We show that this claim is well quantified by the trade-off intrinsic to our PAC-Bayesian approach: the complexity $\KL$ term keeps increasing with the parameter $d\in\{1,2,\ldots,7\}$, while the empirical risk drastically decreases from $d=2$ to $d=3$, and only slightly afterward.  Moreover, we show that the generalization risk (computed on a test sample of size $1000$) tends to increase with complex models (for $d\geq4$). 

\begin{figure}\centering
	%
	\subfloat[Predicted models. Black dots are the $15$ training samples.]
	{\includegraphics[width=.48\textwidth]{figure1a-crop.pdf}
		\label{fig:a}}\ \ \ 
	\subfloat[Decomposition of the marginal likelihood into the empirical loss and $\KL$-divergence.]
	{\includegraphics[width=.47\textwidth]{figure1b-crop.pdf}
		\label{fig:b}}
	\vspace{-4mm}
	
	\subfloat[Bound values on a synthetic dataset according to the number of training samples.]
	{\hspace{2mm}\includegraphics[width=.97\textwidth]{figure2november-crop.pdf}	
		\label{fig:c}}
	
	\caption{Model selection experiment (a-b); and comparison of bounds values (c).}
	\label{fig:MS_experiment}
	\vspace{-3.5mm}
\end{figure}


\paragraph{Empirical comparison of bound values.} Figure~\ref{fig:c} compares the values of the PAC-Bayesian bounds presented in this paper on a synthetic dataset, where each input $\xb{\in}\Rbb^{20}$ is generated by a Gaussian $\xb{\sim} \Ncal(\mathbf0,\Ib)$.
%
The associated output $y{\in}\Rbb$ is given by $y{=}\wb^*\!\cdot\xb+\epsilon$, with $\|\wb^*\|{=}\frac1{2}$,  $\epsilon{\sim}\Ncal(0,\sigma_\epsilon^2)$, and $\sigma_\epsilon^2 {=} \frac19$. We perform Bayesian linear regression in the input space, \ie, $\phib(\xb){=}\xb$, fixing $\sigma_\prior^2{=}\frac{1}{100}$ and $\sigma^2{=}2$.
That is, we compute the posterior of Equation~\eqref{eq:post_reg} for training samples of sizes from $10$ to $10^6$.
 For each learned model, we compute the empirical negative log-likelihood loss of Equation~\eqref{eq:lossnll_ws}, and the three PAC-Bayes bounds, with confidence parameter of $\delta{=}\frac1{20}$.
Note that this loss function 
%
is an affine transformation of the squared loss studied in Section~\ref{sec:more-bounds} (Equation~\ref{eq:sqrloss}), \ie,
%
%
%
%
%
%
$\lossnll(\dtuple{\!\wb, \sigma\!},\xb,y) {=} \tfrac12 \ln(2\pi\sigma^2) {+} \tfrac1{2\sigma^2} \losssqr(\wb,\xb,y).$ It turns out that $\lossnll$ is sub-gamma with parameters
$s^2\geq  \frac1{\sigma^2} \big[\sigx^2(\sigprior^2d+ \|\wb^*\|^2)+ \sigeps^2(1- c) \big] $ and 
$c \geq  \tfrac{1}{\sigma^2} (\sigx^2\sigprior^2),$ as shown in Appendix~\ref{appendix:linregsubgamma}. The bounds of 
Corollary~\ref{cor:subgamma} are computed using the above mentioned values of 
$\|\wb^*\|, d,\sigma, \sigx,\sigma_\epsilon, \sigma_\prior$,
leading to $s^2\simeq 0.280$ and $c \simeq 0.005$. As the two other bounds of Figure~\ref{fig:c} are not suited for unbounded loss, we compute their value using a cropped loss $[a,b]=[1,4]$. 
Different parameter values could have been chosen, sometimes leading to another picture: a large value of~$s$ degrades our sub-gamma bound, as a larger $[a,b]$ interval does for the other bounds.\\[1mm]
%
%
In the studied setting, 
%
the bound of Corollary~\ref{cor:subgamma}---that we have developed for (unbounded) sub-gamma losses---gives  tighter guarantees than the two results for $[a,b]$-bounded losses (up to $n{=}10^6$). However, our new bound always maintains a gap of 
%
$\frac1{2(1{-}c)} s^2$ 
between its value and the generalization loss.
The result of Corollary~\ref{thm:pacbayescatoni_maglike} (adapted from \citet{catoni-07}) for bounded losses suffers from a similar gap, while having higher values than our sub-gamma result. Finally, the result of Theorem~\ref{thm:general-alquier} (\citet{alquier-15}), combined with $\lambda=1/\sqrt{n}$ (Eq.~\ref{eq:alquier_hoeffding_sqrtn}), converges to the expected loss, but it provides good guarantees only for large training sample ($n\gtrsim 10^5$). Note that the latter bound is not directly minimized by our ``optimal posterior'', as opposed to the one with $\lambda=1/{n}$ (Eq.~\ref{eq:alquier_hoeffding_n}), for which we observe values 
between $5.8$ (for $n{=}10^6$)  and $6.4$ (for $n{=}10$)---not displayed on Figure~\ref{fig:c}.

\vspace{-1mm}

\section{Conclusion}
 

The first contribution of this paper is to bridge the concepts underlying the Bayesian and the PAC-Bayesian approaches;
%
under proper parameterization, the minimization of the PAC-Bayesian bound maximizes the marginal likelihood. 
This study 
%
motivates the second contribution of this paper, which is to prove PAC-Bayesian generalization bounds for regression with unbounded sub-gamma loss functions, including the squared loss used in regression tasks.\\[1mm]
%
In this work, we studied model selection techniques.  On a broader perspective, we would like to suggest that both Bayesian and PAC-Bayesian frameworks may have more to learn from each other than what has been done lately (even if other works paved the way \cite[\eg,][]{bissiri-16,grunwald-2012,seeger-thesis}). 
%
%
%
%
Predictors learned from the Bayes rule can benefit from strong PAC-Bayesian frequentist guarantees (under the \iid assumption).
Also, the rich Bayesian toolbox may be incorporated in PAC-Bayesian driven algorithms and risk bounding techniques.

\subsubsection*{Acknowledgments}

We thank Gabriel DubÃ© and Maxime Tremblay for having proofread the paper and supplemental. 
%
%
%

%

%
%
%
%
%
%
%

\newpage

\ifdefined\LONGVERSION\else
\begin{small}
	\setlength{\bibsep}{1.5pt} %
		\bibliographystyle{plainnat}
 %
\bibliography{BPB-refs}
\end{small}

%


\newpage
\fi

\appendix
\section{Supplementary Material}
\label{appendix}

\subsection{Related Work}
\label{appendix:related}

In this section, we discuss briefly other works containing (more or less indirect) links between Bayesian inference and PAC-Bayesian theory, and explain how they relate to the current paper.

\newcommand{\mycite}[2][]{\paragraph{\if&#1&\citeauthor{#2}\else#1\fi\ (\citeyear{#2}) \citep{#2}\,.}}
%
	\mycite[\citeauthor{seeger-02}]{seeger-02,seeger-thesis}
	 Soon after the initial  work of \citet{mcallester-99,mcallester-03a}, Seeger shows how to apply the PAC-Bayesian theorems to bound the generalization error of Gaussian Processes in a classification context.
	%
   By building upon the PAC-Bayesian theorem initially appearing in \citetA{langford&seeger-01-techreport}---where the divergence between the training error and the generalization one is given by the Kullback-Leibler divergence between two Bernoulli distributions---it achieves very tight generalization bounds.\footnote{The PAC-Bayesian results for Gaussian processes are summarized in \citetA[Section~7.4]{rasmussen-06-book}}
   Also, the thesis of \citet[Section 3.2]{seeger-thesis} foresees this by noticing  that ``the log marginal likelihood incorporates a \emph{similar trade-off} as the PAC-Bayesian theorem'', but using another variant of the PAC-Bayes bound and in the context of classification.
%

\mycite{banerjee-06}
This paper shows similarities between the early PAC-Bayesian results (\citet{mcallester-03a}, \citetA{langford&seeger-01-techreport}), and the \emph{Bayesian log-loss bound}  (\citetA{freund-97,kakade-04}). This is done by highlighting that the proof of all these results are strongly relying on the same \emph{compression lemma} \citep[Lemma~1]{banerjee-06}, which is equivalent to our \emph{change of measure}  used in the proof of Theorem~\ref{thm:general-alquier} (see forthcoming Equation~\ref{eq:changeofmeasure}). 
%
Note that the loss studied in the Bayesian part of \citet{banerjee-06} is the negative log-likelihood of Equation~\eqref{eq:loss_vs_likelihood}. Also, as in Equation~\eqref{eq:putback}, the \emph{Bayesian log-loss bound} contains the Kullback-Leibler divergence between the prior and the posterior. However, the latter result is not a generalization bound, but a bound on the training loss that is obtained by computing a surrogate training loss in the specific context of online learning.
Moreover, the marginal likelihood and the model selection techniques are not addressed in \mbox{\citet{banerjee-06}.}

\mycite{zhang-06}
This paper presents a family of information theoretical bounds for \emph{randomized estimators} that have a lot in common with PAC-Bayesian results (although the bounded quantity is not directly the generalization error). Minimizing these bounds leads to the same optimal Gibbs posterior of Equation~\eqref{eq:post_catoni}. The author noted that using the negative log-likelihood (Equation~\ref{eq:loss_vs_likelihood}) leads to the Bayesian posterior, but made no connection with the marginal likelihood.

\mycite{grunwald-2012} This paper proposes the \emph{Safe Bayesian} algorithm, which selects a proper Bayesian \emph{learning rate} --- that is analogous to the parameter $\beta$ of our Equation~\eqref{eq:catoni07}, and the parameter $\lambda$ of our Equation~\eqref{eq:alquier} --- in the context of \emph{misspecified models}.\footnote{The empirical model selection capabilities of the \emph{Safe Bayesian} algorithm has been further studied in \citetA{grunwald-14}.} 
The standard Bayesian inference method is obtained with a fixed learning rate, corresponding to the case $\lambda\eqdots n$ (that is the case we focus on the current paper, see Corollaries~\ref{cor:alquier-subG} and \ref{cor:subgamma}). 
%
The analysis of \citet{grunwald-2012} relies both on the Minimum Description Length principle \citepA{grunwald-07-book} and PAC-Bayesian theory. Building upon the work of \citet{zhang-06} discussed above, they formulate the result that we presented as Equation~\eqref{eq:putback}, linking the marginal likelihood to the inherent PAC-Bayesian trade-off.
However, they do not compute explicit bounds on the generalization loss, which required us to take into account the complexity term of Equation~\eqref{eq:alquier_assumption}.


\mycite{lacoste-thesis} In a binary classification context, it is shown that the parameter $\beta$ of Theorem~\ref{thm:pacbayescatoni} can be interpreted as a Bernoulli label noise model from a Bayesian likelihood standpoint. For more details, we refer the reader to Section~2.2 of this thesis.

\mycite{bissiri-16}
This recent work studies Bayesian inference through the lens of loss functions. When the loss function is the negative log-likelihood (Equation~\ref{eq:loss_vs_likelihood}), the approach of \citet{bissiri-16} coincides with the Bayesian update rule. As mentioned by the authors, there is some connection between their framework and the PAC-Bayesian one, but ``the motivation and construction are very different.''

\paragraph{Other references.}
See also \citetA{grunwald-07,lacostejulien-11,meir-03,ng-01,rousseau-16} for other studies drawing links between frequentist statistics and Bayesian inference, but outside the PAC-Bayesian framework.

\subsection{Proof of Theorem~\ref{thm:general-alquier}}
\label{appendix:pacbayes_proofs}

%

Recall that Theorem~\ref{thm:general-alquier} originally comes from~\citet[Theorem 4.1]{alquier-15}. We present below a different proof that follows the key steps of the very general PAC-Bayesian theorem presented in~\citetA[Theorem 4]{graal-aistats16}.

\begin{proof}[Proof of Theorem~\ref{thm:general-alquier}]
	 The \emph{Donsker-Varadhan's change of measure} states that, for any measurable function $\phi:\Fcal\to\Rbb$, we have
	 \begin{equation} \label{eq:changeofmeasure}
	 \Esp_{f\sim\post} \phi(f) \ \leq\ \KL(\post\|\prior) + \ln\left(\Esp_{f\sim\prior}e^{\phi(f)}\right).
	 \end{equation}
	Thus, with
	$\phi(f){\eqdots} \lambda\big(\genloss(f){-}\emploss(f)\big)$, we obtain
	$\forall\, \post \mbox{ on }\Fcal :$
	\begin{eqnarray*}
		\lambda\, \big(	\Esp_{f\sim\post} \genloss(f)-	\Esp_{f\sim\post} \emploss(f)\big) %
		&=&
		\Esp_{f\sim \post} \lambda\, \big(\genloss(f)-\emploss(f)\big)
		\\
		&\leq&
		\KL(\post\|\prior) + \ln \bigg(
		%
		\Esp_{f\sim \prior} e^{\lambda\, \big(\genloss(f)-\emploss(f)\big)}
		%
		\bigg)\,.
	\end{eqnarray*}
	Now, we apply Markov's inequality on the random variable $\displaystyle\zeta_\prior(\Dsample) \eqdots \Esp_{\mathclap{f\sim \prior}} e^{\lambda \big(\genloss(f)-\emploss(f)\big)}$:
%
\begin{equation*}
		\Pr_{\Dsample\sim \Dgen^n} \left(
		\zeta_\prior(\Dsample)\,\le\,
		\frac{1}{\delta}\Esp_{{\Dsampleprim\sim \Dgen^n}}\zeta_\prior(\Dsampleprim)
		\right)\, \geq\, 1-\delta\,.
\end{equation*}

This  implies that with probability at least $1{-}\delta$ over the choice of $\Dsample\sim \Dgen^n$,
	we have
	$\forall\, \post \mbox{ on }\Fcal :$
	\begin{equation*}\label{eq:presque}
	\Esp_{f\sim\post} \genloss(f) \ \leq\ 	\Esp_{f\sim\post} \emploss(f) 
	+ \frac1\lambda\left[
	\KL(\post\|\prior) + \ln \frac{\displaystyle\Esp_{\Dsampleprim\sim \Dgen^n} \zeta_\prior(\Dsampleprim)}{\delta}
	\right].
	\end{equation*}
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
	%
	%
	%
\end{proof}

\subsection{Proof of Equations~\eqref{eq:alquier_hoeffding_n} and \eqref{eq:alquier_hoeffding_sqrtn}}
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\begin{proof}%
	Given a loss function $\loss:\Fcal\times \Xcal\times \Ycal$, and a fixed predictor $f\in\Fcal$, we consider the random experiment of sampling $(x,y)\in\Dgen$.  We denote  $\ell_i$ a realization of the random variable $\genloss(f) - \loss(f, x, y)$, for  $i = 1\ldots n$. Each $\ell_i$ is \iid,  zero mean, and bounded by $a-b$ and $b-a$, as $\ell(f, x, y) \in [a,b]$.  Thus,
	{\allowdisplaybreaks[4]
		\begin{eqnarray*}
			%
%
			\Esp_{{\Dsampleprim\sim \Dgen^n}} 
			\exp\left[{\lambda\left(\genloss(f) - \emplossprim(f)\right)}\right]%
			&=& \Esp \exp \left[ \frac\lambda n \sum_{i=1}^n \ell_i\right]\\
			&=& \prod_{i=1}^n \Esp \exp \left[ \frac\lambda n\ell_i\right]\\
			&\leq& \prod_{i=1}^n  \exp \left[ \frac{\lambda^2(a-b-(b-a))^2}{8n^2}\right]
			%
			\\
			&=& \prod_{i=1}^n  \exp \left[ \frac{\lambda^2(b-a)^2}{2n^2}\right]\\
			&=&  \exp \left[ \frac{\lambda^2(b-a)^2}{2n}\right],
		\end{eqnarray*}
	}%
	where the inequality comes from Hoeffding's lemma.
	
	With $\lambda\eqdots n$, Equation~\eqref{eq:alquier} becomes Equation~\eqref{eq:alquier_hoeffding_n} :
	\begin{align*} 
	%
	\Esp_{f\sim\post} \genloss(f) 
	&\ \le \  \Esp_{f\sim\post} \emploss(f) +
	\dfrac{1}{n}\!\left[ \KL(\post\|\prior) +
	\ln\dfrac{1}{\delta} + \frac{n^2(b-a)^2}{2n}\right]\\
	&\ = \  \Esp_{f\sim\post} \emploss(f) +
	\dfrac{1}{n}\!\left[ \KL(\post\|\prior) +
	\ln\dfrac{1}{\delta} \right] + \frac12 (b-a)^2\,.%
	\end{align*}
	Similarly, with $\lambda\eqdots \sqrt n$, Equation~\eqref{eq:alquier} becomes Equation~\eqref{eq:alquier_hoeffding_sqrtn} .
\end{proof}



%
%
%
%
%
%
%
%
%
%
%
%
%
%


\subsection{Study of the Squared Loss}
\label{appendix:s}

\newcommand{\mzx}{\mu_{z|\xb}}
\newcommand{\szx}{\sigma_{z|\xb}}
\newcommand{\mbar}{\bar\mu}
\newcommand{\sbar}{\bar\sigma}


We consider a regression problem where $\Xcal\times\Ycal \subset \Rbb^d\times\Rbb$, a family of linear predictors $f_\wb(\xb) = \wb\cdot\xb$, with $\wb\in\Rbb^d$, and a Gaussian prior 
$\Ncal(\zerobf, \sigma_\prior^2\,\Ib)$.  
Let us assume that the input examples 
are generated according to $\Ncal(\zerobf, \sigx^2\Ib)$
%
and  $\epsilon\sim\Ncal(0,\sigma_\epsilon^2)$ is a Gaussian noise.

%
We study the squared loss $\losssqr(\wb,\xb,y) \, =\, (\wb\cdot\xb-y)^2$ such that:
\begin{itemize}
	\item $\wb \sim \Ncal(\zerobf, \sigma_\prior^2\,\Ib)$ is given by the prior $\prior$, 
	%
%
	\item $\xb\sim  \Ncal(\zerobf, \sigx^2\Ib)$ (and $\xb\in\Rbb^d$),
	%
	\item $y = \wb^* \!\cdot \xb + \epsilon$, where $\epsilon\sim\Ncal(0,\sigma_\epsilon^2)$, corresponds to the labeling function.\\
	Thus $y|\xb\sim\Ncal (\xb\cdot \wb^*, \sigma_\epsilon^2)$.
\end{itemize}


Let us consider the random variable $v = \big[ \Esp_\xb \Esp_{y|\xb} \losssqr(\wb,\xb,y)  \big] - \losssqr(\wb,\xb,y)$.
To show that $v$ is a sub-gamma random variable, we will find  values of $c$ and $s$ such that the criterion of Equation~\eqref{eq:subgamma_assumption} is fulfilled, \ie,
\begin{equation*}
\psi_{v}(\lambda)
\ = \ \ln \Esp e^{\lambda v}
%
\ \leq \ {\tfrac{\lambda^2 s^2}{2(1-c\lambda)}}\,,  \qquad \forall \lambda  \in (0, \tfrac1c)\,.
\end{equation*}

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

We have,
{%
	\begin{align} 
	\psi_v(\lambda) 
	\ &=\ \nonumber
	\ln \Esp_\xb \Esp_{y|\xb} \Esp_\wb \exp\Big( \lambda \big[ \Esp_\xb \Esp_{y|\xb} (y-\wb\cdot \xb )^2  \big] - \lambda(y-\wb\cdot \xb )^2  \Big)\\
	\ &\leq\ \nonumber
	\ln \Esp_\wb \exp\Big( \lambda  \Esp_\xb \Esp_{y|\xb} (y-\wb\cdot \xb )^2\Big) \\
	\ &=\ \nonumber
	\ln \Esp_\wb \exp\Big( \lambda  \Esp_\xb [\xb\cdot(\wb^*-\wb)]^2 + \lambda\sigeps^2 \Big) \\
	&=\ \nonumber
	\ln  \Esp_\wb \exp\Big( \lambda  \sigx^2\|\wb^*-\wb\|^2 + \lambda\sigeps^2 \Big) \\
	%
	\ & = \ \nonumber
	\ln  \frac1{(1-2\lambda\sigx^2\sigprior^2)^{\frac d2}}  \exp\Big(\frac{ \lambda  \sigx^2 \|\wb^*\|^2}{1-2\lambda\sigx^2\sigprior^2}  + \lambda\sigeps^2 \Big) \\
	%
	&=\ \nonumber
	- \frac d2 \ln  (1-2\lambda\sigx^2\sigprior^2)
	+ \frac{ \lambda  \sigx^2 \|\wb^*\|^2}{1-2\lambda\sigx^2\sigprior^2} + \lambda\sigeps^2 \\
	&\leq\ \nonumber
	\frac{\lambda\sigx^2\sigprior^2d}{1-2\lambda\sigx^2\sigprior^2}
	+ \frac{ \lambda  \sigx^2 \|\wb^*\|^2}{1-2\lambda\sigx^2\sigprior^2} + \lambda\sigeps^2 \\
	&=\ \nonumber
	\frac{\lambda(\sigprior^2\sigx^2 d+\sigx^2 \|\wb^*\|^2+ (1-2\lambda\sigx^2\sigprior^2)\sigeps^2)}{1-2\lambda\sigx^2\sigprior^2}\\
	&=\nonumber
	\frac{\lambda^2 s^2}{2(1-\lambda c)}\,,
	\end{align}
}%
with $s^2 = \dfrac2\lambda\left[\sigx^2(\sigprior^2d+ \|\wb^*\|^2)+ \sigeps^2(1-\lambda c)\right]$ and $c = 2\sigx^2\sigprior^2$\,. 

Recall that Corollary~\ref{cor:subgamma} is obtained with $\lambda\eqdots 1$. 

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\subsection{Linear Regression : Detailed calculations}
\label{appendix:linreg}

%
Recall that, from Equation~\eqref{eq:post_reg}, the Gibbs optimal posterior of the described model is given by 
$$p(\wb|X,Y,\sigma, \sigmaprior)  =\Ncal(\wb\,|\,\wpost, A^{-1})\,,$$
with
$A\eqdots\frac1{\sigma^2}\Phib^T\,\Phib+\frac1{\sigmaprior^2}\Ib$
;  $\wpost\eqdots\frac1{\sigma^2}A^{-1}\Phib^T \yb$ 
;  $\Phib$ is a $n{\times} d$ matrix such that the $i^{th}$ line is~$\phib(x_i)$\,; 
$\yb \eqdots [y_1, \ldots y_n]$ is the labels-vector. For the complete derivation leading to this posterior distribution, see \citet[Section~3.3]{bishop-2006} or \citetA[Section~2.1.1]{rasmussen-06-book}. 
 
 \paragraph{Marginal likelihood.}
 We decompose of the marginal likelihood into the PAC-Bayesian trade-off:
\begin{align}
&\!\!\!\!-\ln \,p(Y|X,\sigma, \sigmaprior) \nonumber \\ 
%
&\ = \tag{$\dagger$}\label{eq:classic}
\tfrac1{2\sigma^2}\|\yb-\Phib\wpost\|^2
+\tfrac n 2\ln(2\pi \sigma^2)
+ 
\tfrac1{2\sigmaprior^2}\|\wpost\|^2
+ \tfrac12 \log|A|
+ d\ln\sigmaprior \\
&\ =
\underbrace{
	%
	%
	n\,\emplossnll(\wpost)
	+   \blue{  \tfrac{1}{2 \sigma^2}  \tr ( \Phib^T \Phib A^{-1}) }
}_{\green{n\Esp_{\wb\sim\post^*}\emplossnll(\wb)} }
\blue{+} 
\underbrace{
	\blue{\tfrac1{2\sigmaprior^2}\tr(A^{-1})
	- \tfrac d2}
	+ \tfrac1{2\sigmaprior^2}\|\wpost\|^2
	+ \tfrac12 \log|A|
	+ d\ln\sigmaprior 
}_{\red{\KL\big(\Ncal(\wpost,A^{-1})\,\|\,\Ncal(\mathbf{0},\sigmaprior^2\Ib)\big)}}\,.
\tag{$\star$} 
\label{eq:star}
\end{align}
Line~(\ref*{eq:classic}) corresponds to the classic form of the negative log marginal likelihood in a Bayesian linear regression context (see \citet[Equation 3.86]{bishop-2006}).

\mbox{Line~(\ref*{eq:star}) introduces three terms that cancel out\,: 
%
$\blue{\tfrac{1}{2 \sigma^2}  \tr \left( \Phib^T \Phib A^{-1}\right) + \tfrac1{2\sigmaprior^2} \tr\left(A^{-1}\right) - \tfrac 12 d} = 0\,.$}
%
The latter equality follows from the trace operator properties and the definition of matrix $A$:
\begin{align*}
\blue{\tfrac{1}{2 \sigma^2}  \tr \left( \Phib^T \Phib A^{-1}\right) + \tfrac1{2\sigmaprior^2} \tr\left(A^{-1}\right) }
&=  \tr \left( \tfrac{1}{2 \sigma^2} \Phib^T \Phib A^{-1} + \tfrac1{2\sigmaprior^2} A^{-1}\right) \\
& =  \tr \left( \tfrac12 A^{-1} ( \tfrac{1}{ \sigma^2} \Phib^T \Phib +\tfrac1{\sigmaprior^2}\Ib)\right) \\
& =  \tr \left( \tfrac12 A^{-1} A \right)\\
& = \blue{\tfrac 12\, d} \,.
\end{align*}		

We show below that the expected loss \green{$\Esp_{\wb\sim\post^*}\emplossnll(\wb)$} corresponds to the left part of Line~(\ref*{eq:star}). Note that a proof of equality 
$ \Esp_{\wb \sim  \post}   \wb^T \Phib^T  \Phib \wb = \tr \left( \Phib^T \Phib A^{-1} \right) + \wpost^T \Phib^T  \Phib \wpost $ (Line~\ref*{eq:quadform} below),
known as the ``expectation of the quadratic form'',
can be found in \citetA[Theorem 1.5]{seber-03}.

{\allowdisplaybreaks[4]
	\begin{align*}
%
\green{n \Esp_{\wb\sim\post} \emplossnll(\wb) }
&= \Esp_{\wb \sim \post} \sum_{i=1}^n -\ln p(y_i|x_i,\wb) \\
& = \Esp_{\wb \sim \post} \left( \frac{n}{2} \ln(2 \pi \sigma^2 ) +  \frac{1}{2 \sigma^2} \sum_{i=1}^n \left(y_i -  \wb \cdot \phib(\xb_i) \right)^2 \right) \\
& =   \frac{n}{2} \ln(2 \pi \sigma^2 ) +   \frac{1}{2 \sigma^2} \Esp_{\wb \sim \post}\left\Vert \yb -  \Phib \wb  \right\Vert ^2  \\
& = \frac{n}{2} \ln(2 \pi \sigma^2 ) +    \frac{1}{2 \sigma^2}\Esp_{\wb \sim \post}\left ( \Vert \yb \Vert^2  - 2 \yb  \Phib \wb +  \wb^T \Phib^T  \Phib \wb  \right) \\
& = \frac{n}{2} \ln(2 \pi \sigma^2 ) +   \frac{1}{2 \sigma^2}\left( \Vert \yb \Vert^2  - 2 \yb  \Phib \widehat{\wb} + \Esp_{\wb \sim  \post}   \wb^T \Phib^T  \Phib \wb  \right) \\
& = \frac{n}{2} \ln(2 \pi \sigma^2 ) +    \frac{1}{2 \sigma^2} \left( \Vert \yb \Vert^2  - 2 \yb  \Phib \widehat{\wb} + \tr \left( \Phib^T \Phib A^{-1} \right) + \wpost^T \Phib^T  \Phib \wpost \right) \tag{$\clubsuit$} \label{eq:quadform} \\
& = \frac{n}{2} \ln(2 \pi \sigma^2 ) +    \frac{1}{2 \sigma^2} \left\Vert \yb -  \Phib \wpost  \right\Vert ^2 +    \frac{1}{2 \sigma^2}  \tr \left( \Phib^T \Phib A^{-1} \right)\\
& = n\,\emplossnll(\wpost) +    \frac{1}{2 \sigma^2}  \tr \left( \Phib^T \Phib A^{-1}\right).
\end{align*}
}

Finally, the right part of Line~(\ref*{eq:star}) is equal to the Kullback-Leibler divergence between the two multivariate normal distributions
$\Ncal(\wpost,A^{-1})$ and $\Ncal(\mathbf{0},\sigmaprior^2\Ib)$\,:
\begin{align*}
\red{\KL\big(\Ncal(\wpost,A^{-1})\,\|\,\Ncal(\mathbf{0},\sigmaprior^2\Ib)\big)}
&= \frac12\left(
\tr\left((\sigmaprior^2\Ib)^{-1}A^{-1}\right)
+ 				\frac1{\sigmaprior^2}\|\wpost\|^2
- d
+  \log\frac{|\sigmaprior^2\Ib|}{|A|}
%
\right)\\
&= \frac12\left(
\frac1{\sigmaprior^2}\tr\left(A^{-1}\right)
+ 				\frac1{\sigmaprior^2}\|\wpost\|^2
- d
+  \log{|A|}
+d\ln\sigmaprior^2 
\right).
\end{align*}		

\subsection{Linear Regression: PAC-Bayesian sub-gamma bound coefficients}
\label{appendix:linregsubgamma}

We follow then exact same steps as in Section~\ref{appendix:s}, except that we replace the random variable $v$ (giving the squared loss value) by a random variable $v'$ giving the value of the loss
\begin{equation*}%
\lossnll(\dtuple{\wb, \sigma},\xb,y) 
%
= \tfrac12 \ln(2\pi\sigma^2) + \tfrac1{2\sigma^2} (y - \wb\cdot\xb)^2\,,
\end{equation*}
where $\wb$, $\xb$ and $y$ are generated as described in Section~\ref{appendix:s}. We aim to find the values of $c$ and $s$ such that the criterion of Equation~\eqref{eq:subgamma_assumption} is fulfilled, \ie,
\begin{equation*}
\psi_{v'}(\lambda)
\ = \ \ln \Esp e^{\lambda v'}
%
\ \leq \ {\tfrac{\lambda^2 s^2}{2(1-c\lambda)}}\,,  \qquad \forall \lambda  \in (0, \tfrac1c)\,.
\end{equation*}
We obtain
	\begin{align} 
\psi_{v'}(\lambda) 
\ &=\ \nonumber
\ln \Esp_\xb \Esp_{y|\xb} \Esp_\wb \exp\Big( \tfrac{\lambda}{2\sigma^2} \big[ \Esp_\xb \Esp_{y|\xb} (y-\wb\cdot \xb )^2  \big] - \lambda(y-\wb\cdot \xb )^2  \Big)\\
\ &\leq\ \nonumber
\ln \Esp_\wb \exp\Big( \tfrac{\lambda}{2\sigma^2}  \Esp_\xb \Esp_{y|\xb} (y-\wb\cdot \xb )^2\Big) \\
&\vdots\\
&=\ \nonumber
\frac{\tfrac{\lambda}{2\sigma^2}(\sigprior^2\sigx^2 d+\sigx^2 \|\wb^*\|^2+ (1-2\tfrac{\lambda}{2\sigma^2}\sigx^2\sigprior^2)\sigeps^2)}{1-2\tfrac{\lambda}{2\sigma^2}\sigx^2\sigprior^2}\\
&=\nonumber
\frac{\lambda^2 s^2}{2(1-\lambda c)}\,,
\end{align}
with 
\begin{align*}
c &\,=\, \tfrac{1}{2\sigma^2}\Big[  2\sigx^2\sigprior^2\Big]
\ =\  \tfrac{1}{\sigma^2} (\sigx^2\sigprior^2)\,,\\
s^2 &\,=\,  \tfrac{1}{2\sigma^2} \Big[\dfrac2\lambda\left[\sigx^2(\sigprior^2d+ \|\wb^*\|^2)+ \sigeps^2(1-\lambda c)\right]\Big] \ = \ 
 \dfrac1{\lambda\sigma^2} \Big[\sigx^2(\sigprior^2d+ \|\wb^*\|^2)+ \sigeps^2(1-\lambda c) \Big] 
\end{align*}
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%



\ifdefined\LONGVERSION
\begin{small}
	\setlength{\bibsep}{2pt} %
	\bibliographystyle{plainnat}
	%
	\bibliography{BPB-refs}
\end{small}
\else
\begin{small}
	\setlength{\bibsep}{2pt} %
	\renewcommand\refnameA{Supplementary Material References}
	\bibliographystyleA{plainnat}
	%
	\bibliographyA{BPB-refs}
\end{small}
\fi

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


\end{document}



%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\begin{figure*}[t!]
   \centering
     \includegraphics[width=\linewidth]{figs/overview_color.pdf}
     \caption{
     Given a set of clips that depict a manipulated object, we wish to automatically discover the main states that the object can take along with localizing the associated manipulation action.
     In this example, we show one video of someone filling a coffee cup.
     The video starts with an empty cup (\textit{state 1}), which is filled with coffee (\textbf{action}) to become full (\textit{state 2}).
     Given imperfect object detectors, we wish to assign to the valid object candidates either the initial state or the final state (encoded in~$Y$).
	 We also want to localize the manipulating action in time (encoded in~$Z$) while maintaining a joint action-state consistency.
     }
     \vspace{-2mm} %
     \label{fig:mainpaper}
 \end{figure*}	

\section{Modeling manipulated objects}
\label{sec:approach}

We are given a set of $N$ clips that contain a common \textbf{manipulation} of the same \textit{object} (such as ``\textbf{open} an \textit{oyster}").
%
We also assume that we are given an \emph{a priori} model of the corresponding object in the form of a pre-trained object detector~\cite{girsh15fastrcnn}.
Given these inputs, our goal is twofold: (i)~localize the temporal extent of the action and (ii)~spatially/temporally localize the manipulated object and identify its states over time.
%
This is achieved by jointly clustering the appearances of an object (such as an ``oyster") appearing in all clips into two classes, corresponding to the two different states (such as ``closed" and ``open"), while at the same time temporally localizing a consistent ``opening" action that separates the two states consistently in all clips.   
More formally, we formulate the problem as a minimization of a joint cost function that ties together the action prediction in time, encoded in the assignment variable~$Z$, with the object state discovery in space and time, defined by the assignment variable~$Y$:


\begin{align}
\label{eq:jointstateactionprob}
\underset{\substack{Y\in\{0,1\}^{M\times 2}\\Z\in\{0,1\}^T}}{\text{minimize}} \!\! & & & \phantom{aai} f(Z) \; +  \; g(Y) \; + \; d(Z, Y) \\[-5mm]
& & \text{ s.t. }   & \underbrace{Z \in \mathcal{Z}}_{\substack{\text{saliency of action} \\ \\ \text{\textbf{Action localization}}}} \ \text{ and }  \  \underbrace{Y \in \mathcal{Y}}_{\substack{\text{ordering + non overlap}\\ \\ \text{\textbf{Object state labeling}}}}   \nonumber
\end{align}
where $f(Z)$ is a discriminative clustering cost to temporally localize the action in each clip, $g(Y)$ is a discriminative clustering cost to identify and localize the different object states and $d(Z,Y)$ is a joint cost that relates object states and actions together. $T$ denotes the total length of all video clips and $M$ denotes the total number of tracked object candidate boxes (tracklets).
In addition, we impose constraints~$\mathcal{Y}$ and~$\mathcal{Z}$ that encode additional structure of the problem: we localize the action with its most salient time interval per clip (``saliency"); we assume that the ordering of object states is consistent in all clips (``ordering") and that only one object is manipulated at a time (``non overlap").

In the following, we proceed with describing different parts of the model~\eqref{eq:jointstateactionprob}.
In Sec.~\ref{sec:statemodel} we describe the cost function for the discovery of object states. In Sec.~\ref{sec:actionmodel} we detail our model for action localization. Finally,  in Sec.~\ref{sec:joint} we describe and motivate the joint cost $d$.
%
\subsection{Discovering object states}
\label{sec:statemodel}

The goal here is to both (i) spatially localize the manipulated object and (ii) temporally identify its individual states.
To address the first goal, we employ pre-trained object detectors. To address the second goal, we formulate the discovery of object states as a discriminative clustering task with constraints.
We obtain candidate object detections using standard object detectors pre-trained on large scale existing datasets such as ImageNet~\cite{imagenet09}. We assume that each clip $n$ is accompanied with a set of $M_n$ tracklets\footnote{In this work, we use short tracks of objects (less than one second) that we call tracklet. We want to avoid long tracks that continue across a state change of objects. By using the finer granularity of tracklets, our model has the ability to correct for detection mistakes within a track as well as identify more precisely the state change. 
%
} of the object of interest. %

We formalize the task of localizing the states of objects as a discriminative clustering problem where the goal is to find an assignment matrix~$Y_n\in\{0,1\}^{M_n\times 2}$, where
 $(Y_n)_{mk}=1$ indicates that the $m$-th tracklet represents the object in state $k$. 
 We also allow a complete row of~$Y_n$ to be zero to encode that no state was assigned to the corresponding tracklet. 
 This is to model the possibility of false positive detections of an object, or that another object of the same class appears in the video, but is not manipulated and thus is not undergoing any state change. 
 In detail, we minimize the following discriminative clustering cost~\cite{Bach07diffrac}:\footnote{We concatenate all the variables~$Y_n$ into one $M\!\times\!2$ matrix $Y\!$.}  
\begin{align}
    g(Y) &= \min_{W_{s} \in \mathbb{R}^{d_s \times 2}} \ \underbrace{\frac{1}{2M} \|Y - X_{s} W_{s}\|_F^2}_\text{Discriminative loss on data} + \underbrace{\frac{\mu}{2} \|W_{s}\|_F^2}_\text{Regularizer}
    \label{eq:discr_state} 
    %
\end{align}
where $W_s$ is the object state classifier that we seek to learn, $\mu$ is a regularization parameter and $X_s$ is a $M\!\times\!d_s$ matrix of features, where each row is a $d_s$-dimensional (state) feature vector storing features for one particular tracklet. 
The minimization in $W_s$ actually leads to a convex quadratic cost function in~$Y$ (see~\cite{Bach07diffrac}). 
The first term in~\eqref{eq:discr_state} is the discriminative loss on the data that measures how easily the input data~$X_{s}$ is classified by the linear classifier~$W_{s}$ when the object state assignment is given by matrix~$Y$. 
In other words, we wish to find a labeling~$Y$ for given object tracklets into two states (or no state) so that their appearance features~$X$ are easily classified by a linear classifier. To steer the cost towards the right solution, we employ the following constraints (encoded by $Y \in \mathcal{Y}$ in \eqref{eq:jointstateactionprob}). 

\noindent\textbf{Only one object is manipulated at a time : non overlap constraint.}
As it is common in instructional videos, we assume that only one object can be manipulated at a given time.
However, in practice, it is common to have multiple (spatially diverse) tracklets that occur at the same time, for example, due to a false positive detection in the same frame.
To overcome this issue, we impose that at most one tracklet can be labeled as belonging to \emph{state 1} or \emph{state 2} at any given time.
We refer to this constraint as ``\emph{non overlap}" in problem~\eqref{eq:jointstateactionprob}.

\noindent\textbf{\emph{state 1}  $\rightarrow$ \emph{Action} $\rightarrow$  \emph{state 2}: ordering constraints.}
%
We assume that the manipulating action transforms the object from an initial state to a final state and that both states are present in each video.
This naturally introduces two constraints.
The first one is the ordering constraints on the labeling~$Y_n$, \ie the \emph{state 1} should occur before \emph{state 2} in each video.
The second constraint imposes that we have \emph{at least one} tracklet labeled as \emph{state 1} and at least one tracklet labeled as \emph{state 2}.
We call this last constraint the ``\textbf{at least one}" constraint in contrast to forcing ``exactly one" ordered prediction as previously proposed in a discriminative clustering approach on video for action localization~\cite{Bojanowski15weakly}.
This new type of constraint brings additional optimization challenges that we address in Section~\ref{subsec:frank-wolfe}.


\subsection{Action localization}
\label{sec:actionmodel}

Our action model is equivalent to the one of~\cite{Bojanowski15weakly} applied to only \emph{one action}. 
More precisely, the goal is to find an assignment matrix~$Z_n\in\{0,1\}^{T_n}$ for each clip $n$, where
$Z_{nt}=1$  encodes that the $t$-th time interval of video is assigned to an action and $Z_{nt}=0$ encodes that no action is detected in interval $t$.
The cost that we minimize for this problem is similar to the object states cost:
\vspace{-0.6mm}
\begin{align}
	f(Z) = \min_{W_v \in \mathbb{R}^{d_v}} \ \underbrace{\frac{1}{2T} \|Z - X_{v} W_{v}\|_F^2}_\text{Discriminative loss on data} + \underbrace{\frac{\lambda}{2} \|W_{v}\|_F^2}_\text{Regularizer} ,  
	\label{eq:actioncost} 
\end{align}
where $W_v$ is the action classifier, $\lambda$ is a regularization parameter and $X_v$ is a matrix of visual features.
We constrain our model to predict \emph{exactly} one time interval for an action per clip, an approach for actions that was shown to be beneficial in a weakly supervised setting~\cite{Bojanowski15weakly} (referred to as ``\emph{action saliency}" constraint).
%
As will be shown in experiments, this model \emph{alone} is incomplete because the clips in our dataset can contain other actions that do not manipulate the object of interest. 
Our central contribution is to propose a \emph{joint formulation} that links this action model with the object state prediction model, thereby resolving the ambiguity of actions.
We detail the joint model next.

\subsection{Linking actions and object states}
\label{sec:joint}
Actions in our model are directly related to changes in object states.
We therefore want to enforce consistency between the two problems.
To do so, we design a novel joint cost function that operates on the action video labeling~$Z_n$ and the state tracklet assignment~$Y_n$ for each clip.
We want to impose a constraint that the action occurs in between the presence of the two different object states.
In other words, we want to penalize the fact that state~$1$ is detected $\emph{after}$ the action happens, or the fact that state~$2$ is triggered  $\emph{before}$ the action occurs.
%
%

\noindent\textbf{Joint cost definition.}
We propose the following joint symmetric cost function for each clip:
\vspace{-0.6mm}
\begin{align}
d(Z_n,Y_n) =   \hspace{-0.5em}\sum_{y\in\mathcal{S}_1(Y_n)} \!\!\! [t_y-t_{Z_n}]_{+}  + \hspace{-0.5em}\sum_{y\in\mathcal{S}_2(Y_n)}  \! [t_{Z_n} \!\!-t_y]_{+},
\label{eq:dist}
\end{align}
where $t_{Z_n}$ and $t_y$ are the times when the action~$Z_n$ and the tracklet $y$ occur in a clip~$n$, respectively.
$\mathcal{S}_1(Y_n)$ and $\mathcal{S}_2(Y_n)$ are the tracklets in the $n$-th clip that have been assigned to state $1$ and state $2$, respectively.
Finally $[x]_{+}$ is the positive part of $x$.
%
In other words, the function penalizes the inconsistent assignment of objects states~$Y_n$  by the
amount of time that separates the incorrectly assigned tracklet and the manipulation action in the clip.
%
The overall joint cost is the sum over all clips weighted by a scaling hyperparameter $\nu>0$:
\vspace{-0.7mm}
\begin{equation} 
d(Z,Y) = \nu \frac{1}{T} \sum_{n=1}^N d(Z_n,Y_n). \label{eq:distGlobal}
%
\end{equation}
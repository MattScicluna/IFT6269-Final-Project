\vspace{-2mm}
\section{Experimental Results\label{sec:expts}}
\vspace{-3mm}

\textbf{Setup: }
The L1 error in marginals is computed as:
$\zeta_{\mu} := \frac{1}{N} \sum_{i=1}^N |\mu_i(1)-\mu_i^*(1)|$. When using exact MAP inference, 
the error in $\log Z$ (denoted $\zeta_{\log Z}$) is computed
by adding the duality gap to the primal (since this guarantees us an upper bound). For approximate
MAP inference, we plot the primal objective. 
We use a non-uniform initialization of $\vrho$ computed with the Matrix Tree Theorem \citep{sontag2007new,koo2007structured}. 
We perform 10 updates to $\vrho$, optimize $\vmu$ to a duality gap of $0.5$ on $\MARG$, and always
perform correction steps. We use $\LOCALSEARCH$ only for the real-world instances.
%
%
%
We use the implementation of TRBP and the Junction Tree Algorithm (to compute exact marginals) in libDAI~\citep{Mooij_libDAI_10}.
%
Unless specified, we compute 
marginals by optimizing the TRW objective using the adaptive-$\shrinkAmount$ variant of the
algorithm (denoted in the figures as $M_{\shrinkAmount})$.
%
%
%

\textbf{MAP Solvers:} 
For approximate MAP, we run three solvers in parallel:
QPBO \citep{kolmogorov2007minimizing,boykov2004experimental},
TRW-S \citep{kolmogorov2006convergent} and ICM \citep{besag1986statistical} using
OpenGM \citep{andres2012opengm} and use the result that realizes the highest energy. 
For exact inference, we use \citet{gurobi} or toulbar2 \citep{allouche2010toulbar2}.

\textbf{Test Cases:} All of our test cases are on binary pairwise MRFs.
	(1) \textit{Synthetic 10 nodes cliques}: Same setup as
        \citet[Fig.~2]{sontag2007new}, with $9$ sets of $100$ instances each with coupling strength drawn from $\mathcal{U}[-\theta,\theta]$ for $\theta\in \{0.5,1,2,\ldots,8\}$.  
	(2) \textit{Synthetic Grids}: $15$ trials with $5\times5$ grids.
		We sample $\theta_i \sim\mathcal{U}[-1,1]$
          and $\theta_{ij}\in[-4,4]$ for nodes and edges. The
          potentials were $(-\theta_i,\theta_i)$ for nodes and
          $(\theta_{ij},-\theta_{ij}; -\theta_{ij},\theta_{ij})$ for edges.
	(3) \textit{Restricted Boltzmann Machines (RBMs)}: From the Probabilistic Inference Challenge 2011.\footnote{\url{http://www.cs.huji.ac.il/project/PASCAL/index.php}}
	(4) \textit{Horses}: Large ($N\approx 12000$) MRFs representing images from the Weizmann Horse Data \citep{classSpec} with potentials learned by
	\cite{domke2013learning}. 
	%\cite{pal2012learning}.
	(5) \textit{Chinese Characters}: An image completion task from
        the KAIST Hanja2 database, compiled in OpenGM
        by~\citet{andres2012opengm}. The potentials were learned using Decision Tree Fields~\citep{nowozin2011decision}.
		The MRF is not a grid due to skip edges that tie nodes at various offsets. The potentials are a combination of submodular and 
		supermodular and therefore a harder task for inference algorithms. 

%
\newpage
		\centerline{\textbf{On the Optimization of $\MARG$ versus $\Meps$\label{sec:M_M_eps}}}
We compare the performance of Alg.~\ref{alg:algInfadaptive} on optimizing over $\MARG$ (with and without correction), optimizing 
over $\Meps$ with fixed-$\shrinkAmount = 0.0001$ (denoted $M_{0.0001}$) and optimizing over $\Meps$ using the adaptive-$\shrinkAmount$ variant.
These plots are averaged across all the trials for the \emph{first} iteration of optimizing over $\TREEPOL$.
We show error as a function of the number of MAP calls since this is the bottleneck
for large MRFs. 
Fig.~\ref{fig:M_M_eps}, \ref{fig:M_M_eps2} depict the results of this optimization aggregated across trials. 
We find that all
variants settle on the same average error. The adaptive $\shrinkAmount$ variant converges faster on average
followed by the fixed $\shrinkAmount$ variant. Despite relatively quick convergence for
$\MARG$ with no correction on the grids, we found that correction was crucial
to reducing the
%
%
number of MAP calls in subsequent steps of inference after updates to $\vrho$. %
As highlighted earlier, correction steps on $\MARG$ (in blue) worsen 
convergence, an effect brought about by iterates wandering too close to the boundary of $\MARG$.

%
%

\vspace{2mm}
\centerline{\textbf{On the Applicability of Approximate MAP Solvers}}

\textbf{Synthetic Grids:}
Fig.~\ref{fig:approxVsExact_l1} depicts the accuracy of approximate MAP 
solvers versus exact MAP solvers aggregated across trials for $5\times5$ grids.
The results using approximate MAP inference are competitive with those of exact
inference, even as the optimization is tightened over $\TREEPOL$. 
This is an encouraging and non-intuitive result
since it indicates that one can achieve high quality marginals through
the use of relatively cheaper approximate MAP oracles.%

\begin{figure}[t]
\vspace{-1mm}
\centering
\subfigure[\small{$\zeta_{\log Z}$: $5\times5$ grids \qquad \newline $\MARG$ vs $\MARG_{\shrinkAmount}$}]{
	\label{fig:M_M_eps}
	\includegraphics[height=4cm,width=6cm,keepaspectratio]{./images/M_M_eps/gridWi5Tr_M_M_eps_logz_maxrho1.pdf}
}
\centering
\subfigure[\small{$\zeta_{\log Z}$: $10$ node cliques \qquad \newline $\MARG$ vs $\MARG_{\shrinkAmount}$}]{
	\label{fig:M_M_eps2}
\includegraphics[height=4cm,width=6cm,keepaspectratio]{./images/M_M_eps/synthetic_M_M_eps_logz_maxrho1.pdf}
}
\centering
\subfigure[\small{$\zeta_{\mu}$: $5\times5$ grids \newline Approx. vs. Exact MAP}]{
\label{fig:approxVsExact_l1}
	\includegraphics[height=4cm,width=6cm,keepaspectratio]{./images/approxVsExact/gridWi5Tr_approxVsExact_l1.pdf}
}
\centering
\subfigure[\small{$\zeta_{\log Z}$: 40 node RBM \newline Approx. vs. Exact MAP}]{
\label{fig:rbm20_logz}
\includegraphics[height=4cm,width=6cm,keepaspectratio]{./images/RBM/rbm_20-logz-unrolled.pdf}
}
\centering
\subfigure[\small{$\zeta_{\mu}$: 10 node cliques \newline Optimization over $\TREEPOL$}]{
\label{fig:syntheticComplete_tightening_l1}
\includegraphics[height=4cm,width=6cm,keepaspectratio]{./images/Synthetic/SyntheticL1VsTheta.pdf}
}
\centering
\subfigure[\small{$\zeta_{\log Z}$: 10 node cliques \newline Optimization over $\TREEPOL$}]{
\label{fig:syntheticComplete_tightening_logz}
\includegraphics[height=4cm,width=6cm,keepaspectratio]{./images/Synthetic/SyntheticlogzVsTheta.pdf}
}
\vspace{-3mm}
\caption{\small Synthetic Experiments: 
	In Fig. \ref{fig:approxVsExact_l1} \& \ref{fig:rbm20_logz}, we unravel MAP calls across updates to $\vrho$.
  Fig. \ref{fig:rbm20_logz} corresponds to a single RBM (not an aggregate over trials) where for
  ``Approx MAP'' we plot the absolute error between the primal
  objective and $\log Z$ (not guaranteed to be an upper bound).}
\vspace{-4mm}
\end{figure}

%

\textbf{RBMs:}
As in \cite{salakhutdinov2008learning}, we observe for RBMs that the bound provided by $\TRW$ over $\LOCAL_{\shrinkAmount}$ 
is loose and does not get better when optimizing over 
$\TREEPOL$.
%
%
As Fig.~\ref{fig:rbm20_logz} 
depicts for a single RBM,
optimizing over $\MARG_\shrinkAmount$
realizes significant gains in the upper bound on $\log Z$ which improves with updates to $\vrho$. 
The gains are preserved with the use of the approximate MAP solvers.
%
%
%
Note that there are also fast approximate MAP solvers specifically for RBMs \citep{sidaw2013rbm}.

\textbf{Horses:} See Fig.~\ref{fig:largeScale} (right). 
%
The models are close to submodular and the local relaxation is a
good approximation to the marginal polytope.
%
Our marginals are visually similar to those obtained by TRBP and our algorithm is able
to scale to large instances 
by using approximate MAP solvers. 
%
%
%
%
%
%


%
%
\vspace{1mm}
\centerline{\textbf{On the Importance of Optimizing over $\TREEPOL$}}

\textbf{Synthetic Cliques: }
In Fig.~\ref{fig:syntheticComplete_tightening_l1}, \ref{fig:syntheticComplete_tightening_logz},
%
%
we study the effect 
of tightening over $\TREEPOL$ against coupling strength $\theta$.
%
We consider the $\zeta_{\mu}$ and $\zeta_{\log Z}$ obtained for the final marginals before updating $\vrho$ (step~19)
and compare to the values obtained after optimizing over $\TREEPOL$ (marked with $\vrho_{opt}$). 
%
%
The optimization over $\TREEPOL$ has little effect on TRW optimized over $\LOCAL_\shrinkAmount$. 
For optimization over $\MARG_\shrinkAmount$,  
%
updating $\vrho$ realizes better marginals and bound on $\log Z$
%
(over and above 
those
%
obtained in 
\cite{sontag2007new}).

\textbf{Chinese Characters:} Fig.~\ref{fig:largeScale} (left) displays marginals across iterations of
optimizing over $\TREEPOL$.
The submodular and supermodular potentials lead to frustrated models for
which $\LOCAL_\shrinkAmount$ is very loose, which results in TRBP
obtaining poor results.\footnote{We run TRBP for 1000 iterations
  using damping = 0.9; the algorithm converges with a max norm difference between consecutive iterates of 0.002.
  Tightening over $\TREEPOL$ did not significantly change the results of TRBP.}
Our method produces reasonable marginals even before the first
update to $\vrho$, and these improve with tightening over $\TREEPOL$. 

\vspace{2mm}
\centerline{\textbf{Related Work for Marginal Inference with MAP Calls}}
%
%

\cite{hazan2012partition} estimate 
$\log Z$ by averaging MAP estimates obtained on randomly perturbed
inflated graphs. 
Our implementation of the method performed well 
in approximating $\log Z$ but the marginals (estimated by fixing the value of each
random variable and estimating $\log Z$ for the resulting graph) were
less accurate than our method (Fig.~\ref{fig:syntheticComplete_tightening_l1}, \ref{fig:syntheticComplete_tightening_logz}).

\begin{figure*}
\centering
\includegraphics[width=\textwidth]{./images/large/large_horizontal_annotated_final.pdf}
%
\caption{\small{Results on real world test cases. FW(i) corresponds to the final marginals at the $i$th iteration of optimizing $\vrho$. The area highlighted on the Chinese Characters depicts the region of uncertainty.}\label{fig:largeScale}}
\end{figure*}
%

\section{Conclusion}


We have generalized variance reduced SGD methods under the name of memorization algorithms and presented a corresponding analysis, which commonly applies to all such methods. We have investigated in detail the range of safe step sizes with their corresponding geometric rates as guaranteed by our theory. This has delivered a number of new insights, for instance about the trade-offs between small ($\sim \frac 1n$) and large ($\sim \frac 1{4L})$ step sizes in different regimes as well as about the role of the freshness of stochastic gradients evaluated at past  iterates. 


We have also investigated and quantified the effect of additional errors in the variance correction terms on the convergence behavior. Dependent on how $\mu$ scales with $n$, we have shown that such errors can be tolerated, yet, for small $\mu$, may have a negative effect on the convergence rate as much smaller step sizes are needed to still guarantee convergence to a small region. 
%
We believe this result to be relevant for a number of approximation techniques in the context of variance reduced SGD. 

Motivated by these insights and results of our analysis, we have proposed ${\epsilon \cal N}$-SAGA, a modification of SAGA that exploits similarities between training data points by defining a neighborhood system. Approximate versions of  per-data point gradients are then computed by sharing information among neighbors. This opens-up the possibility of variance-reduction in a streaming data setting, where each data point is only seen once. We believe this to be a promising direction for future work. 

Empirically, we have been able to achieve consistent speed-ups for the initial phase of regularized risk minimization. This shows that approximate computations of variance correction terms constitutes a promising approach of trading-off computation with solution accuracy. 
%

\paragraph{Acknowledgments} We would like to thank Yannic Kilcher, Martin Jaggi, R\'{e}mi Leblond and the anonymous reviewers for helpful suggestions and corrections. 

\section{Convergence with Adaptive-$\shrinkAmount$ \label{sec:theory_adaptive_eps}}

In this section, we show the convergence of the adaptive-$\shrinkAmount$ FW algorithm
to optimize a function $f$ satisfying the properties in Problem~\ref{prop:generic_fxn} and Property~\ref{prop:prop_bounded_grad} (Lipschitz gradient over $\DOMeps$ with bounded growth).

The adaptive update for $\shrinkAmount$ (given in Algorithm~\ref{alg:adaptive_update}) can
be used with the standard Frank-Wolfe optimization algorithm or also the fully corrective Frank-Wolfe (FCFW) variant. In FCFW, we ensure that every update makes more progress than a standard FW step with line-search, and thus we will show the convergence result in this section for standard FW (which also applies to FCFW). We describe the FCFW variant with approximate correction steps in Algorithm~\ref{alg:adaptive_eps}, as this is what we used in our experiments.

We first list a few definitions and lemmas that will be used for the main convergence convergence result given in Theorem~\ref{thm:convergence_adaptive_eps}.
We begin with the definitions of duality gaps that we use throughout this section. The Frank-Wolfe 
gap is our primary criterion for halting and measuring the progress of the optimization over $\DOM$.
The uniform gap is a measure of the decrease obtainable from moving towards the uniform distribution.

\begin{definition}
	\label{prop:g_k_properties_true}
	We define the following gaps:
	\begin{enumerate}
	\item The Frank-Wolfe (FW) gap is defined as:
	$\gk :=  \brangle{-\nabla f(\xk),\sk-\xk}$. 

	\item The uniform gap is defined as:
	$\gunif := \brangle{-\nabla f(\xk),\unif-\xk}$. 
	
	\item The FW gap over $\DOMeps$ is:
	$\gepsk{k} := \brangle{-\nabla f(\xk),\skeps-\xk}$. 
	\end{enumerate}
\end{definition}

The name for the uniform gap comes from the fact that the FW gap over $\DOMeps$ can be expressed as a convex combination of the FW gap over $\DOM$ and the uniform gap:
\begin{align}
\gepsk{k} &=  \brangle{-\nabla f(\xk), \,\, (1-\epsk{k}) \sk + \epsk{k} \unif-\xk} \nonumber \\
		  &=  (1-\shrinkAmount^{(k)})\gk + \shrinkAmount^{(k)} \gunif. \label{eq:gap_delta_decomposition}
\end{align}
The uniform gap represents the negative directional derivative of $f$ at $\xk$ in the direction $\unif - \xk$. When the uniform gap is negative (thus $f$ is increasing when moving towards $\unif$ from $\xk$), then the contraction is hurting progress, which explains the type of adaptive update for $\shrinkAmount$ given by Algorithm~\ref{alg:adaptive_update} where we consider shrinking $\shrinkAmount$ in this case. This enables us to crucially relate the FW gap over $\DOMeps$ with the one over $\DOM$, as given in the following lemma, using the assumption that $\shrinkAmount^{(\mathrm{init})} \leq \frac{1}{4}$.
\begin{lemma}[Gaps relationship]
	\label{prop:g_k_properties_adaptive}
	For iterates progressing as in Algorithm~\ref{alg:adaptive_eps} with adaptive update on $\shrinkAmount$ as given in Algorithm~\ref{alg:adaptive_update}, the gap over $\DOMeps$ and $\DOM$ are related as : $\gepsk{k}\geq\frac{\gk}{2}$.
\end{lemma}
\begin{proof}
	The duality gaps $\gk$ and $\gepsk{k}$ computed as defined in~\eqref{prop:g_k_properties_true} during Algorithm~\ref{alg:adaptive_eps} are related by equation~\eqref{eq:gap_delta_decomposition}.
	
We analyze two cases separately:

(1)	When $\gunif\geq 0$, for $\shrinkAmount^{(\mathrm{init})}\leq\frac{1}{4}$, we have $\gepsk{k} \geq \frac{3}{4}\gk$ as $\epsk{k} \leq \shrinkAmount^{(\mathrm{init})}$.

(2)	When $\gunif <0$, from the update rule in lines~5 to~7 in Algorithm~\ref{alg:adaptive_update}, we have
	$\epsk{k}\leq \frac{\gk}{-4\gunif}\implies \epsk{k}\gunif\geq-\frac{\gk}{4}$. Therefore, $\gepsk{k} \geq \frac{3}{4}\gk-\frac{\gk}{4}  = \frac{\gk}{2}$.

Therefore, the gap over $\DOMeps$ and $\DOM$ are related as : $\gepsk{k}\geq\frac{\gk}{2}$.
\end{proof}

Another property that we will use in the convergence proof is that $-\gunifNox$ is upper bounded for any convex function $f$:\footnote{Note that on the other hand, $\gunifNox(\x)$ might go to infinity as $\x$ gets close to the boundary of $\DOM$ as the gradient of $f$ is allowed to be unbounded. Fortunately, we only need an upper bound on $-\gunifNox$, not a lower bound.}
\begin{lemma}[Bounded negative uniform gap]
\label{prop:prop_bounded_gunif}
Let $f$ be a continuously differentiable convex function on the relative interior of $\DOM$. Then for any fixed $\unif$ in the relative interior of $\DOM$, $\exists \gunifBound$
s.t. 
\begin{equation} 
\forall \x\in\DOM, \;\; -\gunifNox(\x) = \innerProd{\nabla f(\x)}{\unif-\x} \leq \gunifBound.
\end{equation}
In particular, we can take the finite value:
\begin{equation} \label{eq:uniformBound}
	B := \| \nabla f(\unif) \|_* \diam_{\| \cdot \|} (\DOM)
\end{equation}
\end{lemma}
\begin{proof}
As $f$ is convex, its directional derivative is a monotone increasing function in any direction. Let $\unif$ and $\x$ be points in the relative interior of $\DOM$; then their gradient exists and we have by the monotonicity property:
\begin{align*}
\innerProd{\nabla f(\unif) - \nabla f(\x)}{\unif-\x} \geq 0 \\
\implies \innerProd{\nabla f(\unif)}{\unif-\x} \geq \innerProd{\nabla f(\x)}{\unif-\x} . 
\end{align*}
This inequality is valid for all $\x$ in the relative interior of $\DOM$, and can be extended to the boundary by taking limits (with potentially the RHS become minus infinity, but this is not a problem). Finally, by the definition of the dual norm (generalized Cauchy-Schwartz), we have $ \innerProd{\nabla f(\unif)}{\unif-\x} \leq \| \nabla f(\unif) \|_* \| \unif - \x \| \leq \| \nabla f(\unif) \|_*  \diam_{\| \cdot \|} (\DOM)$.
\end{proof}

Finally, we need a last property of Algorithm~\ref{alg:adaptive_eps} that allows us to bound the amount of perturbation $\epsk{k}$ of the polytope
at every iteration as a function of the sub-optimality over $\DOM$.  
\begin{lemma}[Lower bound on perturbation]
	\label{prop:eps_k_properties}
	Let $B$ be a bound such that $-8\gunifNox(\x)\leq \gunifBound$ for all $\x \in \DOM$ (given by Lemma~\ref{prop:prop_bounded_gunif}). Then at every stage of Algorithm~\ref{alg:adaptive_eps}, we have that:
	$$
		\epsk{\mathrm{init}} \geq \epsk{k}\geq\min\left\{\frac{\hk}{\gunifBound},\epsk{\mathrm{init}}\right\},
    $$
	where $\epsk{\mathrm{init}}$ is the initial value of $\shrinkAmount$ and $\hk := f(\xk)-f(\x^*)$ is the sub-optimality of the iterate.
\end{lemma}

\begin{proof}
	When defining $\epsk{k}$ in step 10 of Algorithm~\ref{alg:adaptive_eps}, we either preserve the value of $\epsk{k-1}$ or if we update it, then by the lines~6 and~7 of Algorithm~\ref{alg:adaptive_update}, we have $\epsk{k} \geq \frac{\tilde{\shrinkAmount}}{2} = \frac{1}{2} \frac{\gk}{-4\gunif} \geq  \frac{\gk}{\gunifBound}$ (by using $\gunif < 0$ in this case). Since $\gk\geq\hk$ (the FW gap always upper bounds the suboptimality), we conclude $\epsk{k} \geq \min\{\frac{h_k}{\gunifBound}, \epsk{k-1}\}$. 
	Unrolling this recurrence, we thus get:
	$$\epsk{k} \geq \min\left\{ \min_{0\leq l \leq k} \frac{\hkarg{l}}{\gunifBound}, \, \, \epsk{\mathrm{init}}\right\}
	=  \min\left\{ \frac{\hk}{\gunifBound}, \epsk{\mathrm{init}}\right\}. $$
	For the last equality, we used the fact that $h_k$ is non-increasing since Algorithm~\ref{alg:adaptive_eps} decreases the objective at every iteration (using the line-search in step~14).
\end{proof}

We now bound the generalization of a standard recurrence that will arise in the proof of convergence. This is a generalization of the technique used in~\citetsup{teo2007erm} (also used in the context of Frank-Wolfe in the proof of Theorem C.4 in \citet{lacoste2012block}). The basic idea is that one can bound a recurrence inequality by the solution to a differential equation. 
We provide a detailed proof of the 
bound for completeness here. 
\begin{lemma}[Recurrence inequality solution]
	\label{lem:differential_bound}
	Let $1< a \leq b$. Suppose that $\h_k$ is any non-negative sequence that satisfies the recurrence inequality: 
	$$\h_{k+1}\leq \h_k-\frac{1}{b C_0} (\h_k)^a \qquad \text{with initial condition} \quad\h_0^{a-1}\leq C_0.$$
	Then $\h_k$ is strictly decreasing (unless it equals zero) and can be bounded for $k \geq 0$ as: 
	$$\h_{k}\leq \left(\frac{C_0}{(\frac{a-1}{b})k+1}\right)^{\frac{1}{a-1}}$$
\end{lemma}

\begin{proof}
Taking the continuous time analog of the recurrence inequality, we consider
the differential equation:
$$\frac{\del h}{\del t} = \frac{-h^a}{bC_0} \qquad \text{with initial condition} \quad h(0)=C_0^{\frac{1}{a-1}}.$$
Solving it:
	\begin{equation*}
		\begin{split}
		&\frac{\del h}{\del t} = \frac{-h^a}{b C_0}\\
		\implies &\int \frac{\del h}{h^a} = \int \frac{-\del t}{b C_0}\\
		\implies &\left[\frac{-h^{1-a}}{a-1}\right]^{h(t)}_{h(0)} = -\frac{t-0}{b C_0}\\
		&\algComment{ Using the initial conditions:}\\
		\implies &\frac{-1}{h(t)^{a-1}} + \frac{1}{C_0} = \frac{-t(a-1)}{b C_0}\\
		\implies &\frac{1}{h(t)^{a-1}} = \left( (\frac{a-1}{b}) t + 1\right)\frac{1}{C_0}\\
		\implies &h(t) = \left(\frac{C_0}{ (\frac{a-1}{b}) t +1}\right)^{\frac{1}{a-1}}.
		\end{split}
	\end{equation*}
We now denote the solution to the differential equation as $\tilde{h}(t)$. Note that it is a strictly decreasing convex function 
(which could also be directly implied from the differential equation as: 
$\frac{\del^2 h}{\del t^2} = -a\underbrace{\frac{h^{a-1}}{b C_0}}_{>0} \underbrace{h'(t)}_{<0}>0$ ).
Our strategy will be to show by induction that if $\h_k\leq \tilde{h}(k)$, then $\h_{k+1}\leq \tilde{h}(k+1)$. This allows us to bound the recurrence by the solution to the differential equation. 

Assume that $\h_k\leq \tilde{h}(k)$. The base case is $\h_0\leq\tilde{h}(0)= C_0^{\frac{1}{a-1}}$, which is true by the initial condition on $\h_0$.

Consider the utility function $l(h) := h-\frac{h^a}{b C_0}$ which is maximized at $\bar{h} := \left(\frac{b C_0}{a}\right)^{\frac{1}{a-1}}$. This function can be verified to be strictly concave for $a > 1$ and therefore is increasing for $h\leq \bar{h}$. Note that the recurrence inequality can be written as $\h_{k+1} \leq l(\h_k)$.
Since $\tilde{h}$ is decreasing and that $\tilde{h}(0)) = C_0^{\frac{1}{a-1}} \leq \left(\frac{bC_0}{a}\right)^{\frac{1}{a-1}} = \bar{h}$ (the last inequality holds since $b\geq a$), we have $\tilde{h}(t) \leq \bar{h}$ for all $t \geq 0$, and so $\tilde{h}(t)$ is always in the monotone increasing region of $l$. 

From the induction hypothesis and the monotonicity of $l$, we thus get that $l(\h_k)\leq l(\tilde{h}(k))$. 

Now the convexity of $\tilde{h}(t)$ gives us $\tilde{h}(k+1)\geq \tilde{h}(k)+\tilde{h}'(k)=\tilde{h}(k)-\frac{\tilde{h}(k)^a}{b C_0} = l(\tilde{h}(k))$.
Combining these two facts with the recurrence inequality $\h_{k+1} \leq l(\h_k)$, we get:
$\h_{k+1}\leq l(\h_k) \leq l(\tilde{h}(k))\leq \tilde{h}(k+1)$, completing the induction step
and the main part of the proof.

Finally, whenever $\h_k > 0$, we have that $\h_{k+1} < \h_k$ from the recurrence inequality, and so 
$\h_k$ is strictly decreasing as claimed.
\end{proof}

Given these elements, we are now ready to state the main convergence result for Algorithm~\ref{alg:adaptive_eps}. The convergence rate goes through three stages with increasingly slower rate. The level of suboptimality $\hk$ determines the stage. We first give the high level intuition behind these stages. Recall that by Lemma~\ref{prop:eps_k_properties}, $\hk$ lower bounds the amount of perturbation $\epsk{k}$, and thus when $\hk$ is big, the function $f$ is well-behaved by Property~\ref{prop:prop_bounded_grad}. In the first stage, the suboptimality is bigger than some target constant (which implies that the FW gap is big), yielding a geometric rate of decrease of error (as is standard for FW with line-search in the first few steps). In the second stage, the suboptimality is in an intermediate regime: it is smaller than the target constant, but big enough compared to the initial $\shrinkAmount^{\mathrm{init}}$ so that $f$ is still well-behaved on $\DOM_{\epsk{k}}$. We get there the usual $O(1/k)$ rate as in standard FW. Finally, in the third stage, we get the slower $O(k^{-\frac{1}{p+1}})$ rate where the growth in $O(\shrinkAmount^{-p})$ of the Lipschitz constant of $f$ over $\DOMeps$ comes into play.
\begin{theorem}[Global convergence for adaptive-$\shrinkAmount$ variant over $\DOM$]
	\label{thm:convergence_adaptive_eps}
	Consider the optimization of $f$ satisfying the properties in Problem~\ref{prop:generic_fxn} and Property~\ref{prop:prop_bounded_grad}.  Let~$\Cconst := L \diam_{\|\cdot\|}(\DOM)^2$, where $L$ is from Property~\ref{prop:prop_bounded_grad}. Let~$B$ be the upper bound on the negative uniform gap:~$-8\gunifNox(\x)\leq \gunifBound$ for all $\x \in \DOM$, as used in Lemma~\ref{prop:eps_k_properties} (arising from Lemma~\ref{prop:prop_bounded_gunif}). Then the iterates~$\xk$ obtained by running the Frank-Wolfe updates over $\DOMeps$ with line-search with $\shrinkAmount$ updated according to Algorithm~\ref{alg:adaptive_update} (or as summarized in a FCFW variant in Algorithm~\ref{alg:adaptive_eps}), have suboptimality~$h_k$ upper bounded as:
	\begin{enumerate}
		\item $\hk\leq \left(\frac{1}{2}\right)^{k}\h_{0} +\frac{\Cconst}{\shrinkAmount_0^p}$ for $k$ such that $\hk \geq \max\{ B\shrinkAmount_0, \frac{2\Cconst}{\shrinkAmount_0^p}\}$, \\
		\item $\hk\leq\frac{2\Cconst}{\shrinkAmount_0^p}\left[\frac{1}{\frac{1}{4} (k-k_0)+1}\right]$ for $k$ such that $\gunifBound\shrinkAmount_0\leq\hk\leq\frac{2\Cconst}{\shrinkAmount_0^p}$,\\
		\item $\hk\leq \left[\frac{\max(\Cconst,\gunifBound\shrinkAmount_0^{p+1})\gunifBound^p}{\frac{p+1}{\max(8,p+2)}(k-k_1)+1}\right]^{\frac{1}{p+1}} = O(k^{-\frac{1}{p+1}})$ for $k$ such that $\hk\leq\gunifBound\shrinkAmount_0$,\\
	\end{enumerate}
	where $\shrinkAmount_0 = \epsk{\mathrm{init}}$, $h_0$ is the initial suboptimality, and $k_0$ and $k_1$ are the number of steps to reach stage~2 and~3 respectively which are bounded as: $k_0\leq \max(0,  \lceil\log_{\frac{1}{2}}\frac{\Cconst}{ h_0 \shrinkAmount_0^p}\rceil )$, 
	$k_0 \leq k_1\leq k_0 + \max\left(0,\lceil \frac{8\Cconst}{\gunifBound\shrinkAmount_0^{p+1}}\rceil -4\right)$.
	
\end{theorem}
\begin{proof}
	Let $\x_\stepsize := \xk + \stepsize \dd_k^\FW$ with $\dd_k^\FW$ defined in step~12 in Algorithm~\ref{alg:adaptive_eps}. Note that $\x_\stepsize \in \DOMeps$ with $\shrinkAmount = \epsk{k}$ for all $\stepsize \in [0,1]$. We apply the Descent Lemma~\ref{lem:descent_lemma} on this update to get:
	$$f(\x_\stepsize) \leq f(\xk) + \stepsize\brangle{\nabla f(\xk),\dd_k^\FW} + \stepsize^2\frac{L \|\dd_k^\FW\|^2}{2\left(\epsk{k}\right)^p} \qquad \forall \stepsize \in [0,1].$$
	We have $L \| \dd_k^\FW \|^2 \leq \Cconst$ by assumption and $\brangle{\nabla f(\xk),\dd_k^\FW} = -\geps$ by definition. Moreover, $\xnext$ is defined to make at least as much progress than the line-search result
	$\min_{\stepsize \in [0,1]} f(\x_\stepsize)$ (line~14 and~15), and so we have:
	%
	%
	\begin{align*}
		f(\xnext) &\leq f(\xk) - \stepsize\geps + \stepsize^2\frac{\Cconst}{2 \left(\epsk{k}\right)^p} \quad \forall \stepsize \hiderel{\in} [0,1] \\
		&\leq f(\xk) - \frac{\stepsize}{2}\gk + \stepsize^2\frac{\Cconst}{2 \left(\epsk{k}\right)^p} \quad \forall \stepsize \hiderel{\in} [0,1].
	\end{align*}
	For the final inequality, we used Lemma~\ref{prop:g_k_properties_adaptive} which relates the gap over $\DOMeps$ to the gap over $\DOM$.

	Subtracting $f(\xopt)$ from both sides and using $\gk\geq\hk$ by convexity, we get:
	$$\hnext\leq\hk-\frac{\stepsize\hk}{2} + \frac{\stepsize^2\Cconst}{2 \left(\epsk{k}\right)^p}.$$

	Now, using Lemma~\ref{prop:eps_k_properties}, we have that $\epsk{k}\geq\min(\frac{\hk}{\gunifBound},\epsk{\mathrm{init}})$:
	\begin{align}
		\label{eqn:master_eqn}
		\hnext &\leq\hk-\stepsize\frac{\hk}{2} + \frac{\stepsize^2}{2}\frac{\Cconst}{\left(\min(\frac{\hk}{\gunifBound},\epsk{\mathrm{init}})\right)^p} \quad \forall \stepsize \hiderel{\in} [0,1].
	\end{align}

	We refer to \eqref{eqn:master_eqn} as the master inequality. Since we no longer have a dependance on $\epsk{k}$, we refer to $\epsk{\mathrm{init}}$ as $\shrinkAmount_0$. 
	We now follow a similar form of analysis as in the proof of Theorem~C.4 in~\citet{lacoste2012block}.
	To solve this and bound the suboptimality, we consider three stages:
	\begin{enumerate}
		\item \textbf{Stage 1:} The $\min$ in the denominator is $\shrinkAmount_0$ and $h_k$ is big: $\hk\geq \max\{ B\shrinkAmount_0, \frac{2\Cconst}{\shrinkAmount_0^p}\}$.
		\item \textbf{Stage 2:} The $\min$ in the denominator is $\shrinkAmount_0$ and $h_k$ is small: $\gunifBound\shrinkAmount_0\leq\hk\leq\frac{2\Cconst}{\shrinkAmount_0^p}$.
		\item \textbf{Stage 3:} The $\min$ in the denominator is $\frac{\hk}{\gunifBound}$, i.e.: $\hk\leq\gunifBound\shrinkAmount_0$.
	\end{enumerate}

	Since $\hk$ is decreasing, once we leave a stage, we no longer re-enter it. 
	The overall strategy for each stage is as follows. For each recurrence that we get, we select
	a $\stepsize^*$ that realizes the tightest upper bound on it. 
%
%

	Since we are restricted that 
	$\stepsize^*\in[0,1]$, we have to consider when $\stepsize^*>1$ and $\stepsize^*\leq 1$. For the former,
	we bound the recurrence obtained by substituting $\stepsize=1$ into \eqref{eqn:master_eqn}. For the latter, 
	we substitute the form of $\stepsize^*$ into the recurrence and bound the result.

	\subsection*{Stage 1}
	We consider the case where $\hk\geq\gunifBound\shrinkAmount_0$. This yields: 
	\begin{align}
		\label{eqn:case_1}
		\hnext &\leq\hk - \frac{\stepsize\hk}{2} + \frac{\stepsize^2 \Cconst}{2 \left(\shrinkAmount_0\right)^p}	
	\end{align}
	The bound is minimized by setting $\stepsize^* = \frac{\hk\shrinkAmount_0^p}{2\Cconst}$. On the other hand, the bound is only valid for $\stepsize \in [0,1]$, and thus if $\stepsize^* > 1$, i.e. $\hk > \frac{2\Cconst}{\shrinkAmount_0^p}$ (stage 1), then $\stepsize = 1$ will yield the minimum feasible value for the bound. Unrolling the recursion~\eqref{eqn:case_1} for $\stepsize = 1$ during this stage (where $\h_l > \frac{2\Cconst}{\shrinkAmount_0^p}$ for $l < k$ as $\h_k$ is decreasing), we get:
	
	\begin{align}
		\hnext &\leq \frac{\hk}{2} + \frac{\Cconst}{2\shrinkAmount_0^p} \nonumber \\	
	&\leq \frac{1}{2}\left(\frac{\hprev}{2} + \frac{\Cconst}{2\shrinkAmount_0^p}\right) + \frac{\Cconst}{2\shrinkAmount_0^p} \nonumber \\
	&\leq \left(\frac{1}{2}\right)^{k+1} \h_0 + \frac{\Cconst}{2\shrinkAmount_0^p}\underbrace{\sum_{l=0}^{k}\left(\frac{1}{2}\right)^l}_{\leq\sum_{l=0}^{\infty}\left(\frac{1}{2}\right)^{l} = 2} \nonumber \\
	\text{thus} \quad \h_k &\leq \left(\frac{1}{2}\right)^{k} \h_0 + \frac{\Cconst}{\shrinkAmount_0^p} \label{eqn:hk_stage1},
	\end{align}
	giving the bound for the iterates in the first stage.

%
%
%
	
	We can compute an upper bound on the number of steps it takes to reach a suboptimality of $\frac{2\Cconst}{\shrinkAmount_0^p}$ by looking at the minimum $k$ which ensures that the bound in~\eqref{eqn:hk_stage1} becomes smaller than $\frac{2\Cconst}{\shrinkAmount_0^p}$, yielding
	$k_{\max} = \max(0,\lceil\log_{\frac{1}{2}}\frac{\Cconst}{\h_0\shrinkAmount_0^p}\rceil)$. Therefore, let $k_0\leq k_{\max}$ be the 
	first $k$ such that $\hk\leq\frac{2\Cconst}{\shrinkAmount_0^p}$.

	\subsection*{Stage 2}
	For this case analysis, we refer to $k$ as being the iterations \emph{after} $k_0$ steps have elapsed. I.e. if $k_{\mathrm{new}} := k-k_0$, then we refer to $k_{\mathrm{new}}$ as $k$ moving forward. 
	
	In stage~2, we suppose that $\gunifBound\shrinkAmount_0\leq\hk\leq\frac{2\Cconst}{\shrinkAmount_0^p}$. This means that $\stepsize^* = \frac{\hk\shrinkAmount_0^p}{2\Cconst}\leq 1$.
	
	Substituting $\stepsize=\stepsize^*$ into \eqref{eqn:case_1} yields: $\hnext\leq\hk -\hk^2\frac{\shrinkAmount_0^p}{8\Cconst}$.

	Using the result of Lemma~\ref{lem:differential_bound} with $a=2$, $b=4$ and $C_0 = \frac{2\Cconst}{\shrinkAmount_0^p}$, we get the bound:	
	$$\hk\leq\frac{\frac{2\Cconst}{\shrinkAmount_0^p}}{\frac{k-k_0}{4}+1}.$$
	
	It is worthwhile to point out at this juncture that the bound obtained for stage~$2$ is the same as the one for regular Frank-Wolfe, but with a factor of $4$ worse due to the factor of $\frac{1}{2}$ in front of the FW gap which appeared due to Lemma~\ref{prop:g_k_properties_adaptive}.

	\subsection*{Stage 3}
	Here, we suppose $\hk\leq\gunifBound\shrinkAmount_0$. We can compute a bound on the number of steps $k_1$ needed get to stage~3 by looking at the number of steps it takes for the bound in stage~2 to becomes less than $\gunifBound\shrinkAmount_0$:
	\begin{equation*}
		\begin{split}
		\frac{2\Cconst}{\shrinkAmount_0^p} \left[\frac{4}{k_1-k_0+4}\right] \leq \gunifBound\shrinkAmount_0\\
		\left[\frac{1}{k_1-k_0+4}\right] \leq \frac{\gunifBound\shrinkAmount_0^{p+1}}{8\Cconst}\\
		k_1 \geq k_0 + \lceil\frac{8\Cconst}{\gunifBound\shrinkAmount_0^{p+1}}\rceil - 4.
		\end{split}
	\end{equation*}

	As before, moving forward, our notation on $k$ represents the number of steps taken after $k_1$ steps. 
	
	Then, the master inequality~\eqref{eqn:master_eqn} becomes: 
		$$\hnext\leq \hk -\frac{\stepsize}{2}\hk + \frac{\stepsize^2\Cconst\gunifBound^p}{2\hk^p}.$$

	To simplify the rest of the analysis, we replace $\Cconst\gunifBound^p$ with $F := \max(\gunifBound\shrinkAmount_0^{p+1},\Cconst)\gunifBound^p$. We then get the bound:
	\begin{equation}
		\label{eqn:case_2_mod}
		\hnext\leq \hk-\frac{\stepsize}{2}\hk + \frac{\stepsize^2F}{2\hk},
	\end{equation}
	which is minimized by setting $\stepsize^*:=\frac{\hk^{p+1}}{2F}$.  
	Since $F\geq \gunifBound^{p+1}\shrinkAmount_0^{p+1}$ (by construction) and $\hk^{p+1}\leq (\gunifBound\shrinkAmount_0)^{p+1}$ (by the condition to be in stage~3),
	we necessarily have that $\stepsize^*\leq 1$. We chose the value of $F$ to avoid
	having to consider the possibility $\stepsize^* > 1$ as we did in the distinction between stage~1 and stage~2.
	
	Hence, substituting $\stepsize = \stepsize^*$ in~\eqref{eqn:case_2_mod}, we get:
	$$\hnext\leq\hk-\frac{\hk^{p+2}}{8F}.$$

	Using the result of Lemma~\ref{lem:differential_bound} with $a=p+2$, $b=\max(8,p+2)$ and $C_0 = F$, we get the bound:	
	$$\hk\leq\left[\frac{\max(\Cconst,\gunifBound \shrinkAmount_0^{p+1})\gunifBound^p}{\frac{p+1}{\max(8,p+2)}(k-k_1)+1}\right]^{\frac{1}{p+1}} = O(k^{-\frac{1}{p+1}}),$$
	concluding the proof.
\end{proof}

Interestingly, the obtained rate of $O(1/\sqrt{k})$ for $p=1$ (for the TRW objective e.g.) is the standard rate that one would get for the optimization of a general non-smooth convex function with the projected subgradient method (and it is even a lower bound for some class of first-order methods; see e.g. Section~3.2 in~\citetsup{nesterov2004lectures}). The fact that our function $f$ does not have Lipschitz continuous gradient on the whole domain brings us back to the realm of non-smooth optimization. It is an open question whether Algorithm~\ref{alg:adaptive_eps} has an optimal rate for the class of functions defined in the assumptions of Theorem~\ref{thm:convergence_adaptive_eps}.


\documentclass[twoside]{article} 
\usepackage[accepted]{aistats2017}
\pdfoutput=1 %


\usepackage[round]{natbib}
\usepackage{multibib} %
\newcites{sup}{Supplementary References}


\usepackage{amsthm}
\usepackage[leqno]{amsmath}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{bm,color}
\usepackage{mathdots}
\usepackage{booktabs}       %
\usepackage{amsfonts}       %
\usepackage{nicefrac}       %
\usepackage{microtype}      %
\usepackage{graphicx}
\usepackage{hyperref}       %
\usepackage{algcompatible} %
\usepackage{algorithm}
\usepackage{pict2e}
\usepackage{blindtext}


\hypersetup{ %
    pdftitle={Frank-Wolfe Algorithms for Saddle Point Problems},
    pdfkeywords={},
    pdfborder=0 0 0,
	pdfpagemode=UseNone,
    colorlinks=true,
    linkcolor=blue, %
    citecolor=blue, %
    filecolor=blue, %
    urlcolor=blue, %
    pdfview=FitH,
    pdfauthor={Gauthier Gidel, Tony Jebara, Simon Lacoste-Julien},
} 

%
%
%

\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\renewcommand{\L}{\mathcal{L}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\Z}{\mathcal{Z}}
\newcommand{\M}{\X \times \Y}
\newcommand{\prodscal}[2]{\left\langle#1,#2\right\rangle}
\newcommand{\x}{\bm{x}}
\newcommand{\y}{\bm{y}}
\newcommand{\z}{\bm{z}}
\renewcommand{\u}{\bm{u}}
\newcommand{\s}{\bm{s}}
\newcommand{\xt}{\bm{x}^{(t)}}
\newcommand{\xtt}{\bm {x}^{(t+1)}}
\newcommand{\zt}{\bm{z}^{(t)}}
\newcommand{\ztt}{\bm {z}^{(t+1)}}
\newcommand{\st}{\bm{s}^{(t)}}
\newcommand{\stt}{\bm{s}^{(t+1)}}
\newcommand{\vt}{\bm{v}^{(t)}}
\newcommand{\dt}{\bm{d}^{(t)}}
\newcommand{\rt}{\r^{(t)}}
\newcommand{\yt}{\bm{y}^{(t)}}
\newcommand{\ytt}{\bm{y}^{(t+1)}}
\newcommand{\xtm}{\widehat{\bm{x}}^{(t)}}
\newcommand{\ytm}{\widehat{\bm{y}}^{(t)}}
\newcommand{\wt}{w_t}
\newcommand{\wtt}{w_{t+1}}
\newcommand{\gn}{\nabla \L(\xt,\yt)}
\newcommand{\gnx}{\nabla_x\L(\xt,\yt)}
\newcommand{\gny}{\nabla_y\L(\xt,\yt)}
\newcommand{\minimax}{\underset{ \x  \in \X}{ \min \;} \underset{ \y \in \Y}{ \max \;}    \L(\x,\y) }
\newcommand{\maximin}{  \underset{ \y \in \Y}{\max \;}\underset{\x \in \X}{ \min} \; \L(\x,\y) }
\renewcommand{\SS}{ \bm{S}}
\newcommand{\St}{ \bm{S}^{(t)}}
\newcommand{\Xt}{\bm{X}^{(t)}}
\newcommand{\Xtt}{\bm{X}^{(t+1)}}
\newcommand{\gap}{g_{t}}
\newcommand{\et}{\qquad \text{and} \qquad}
\newcommand{\CondNumb}{\nu}

\newcommand{\FF}{\r} %


%
%
%


%
\newtheorem*{rep@theorem}{\rep@title}
\newcommand{\newreptheorem}[2]{%
\newenvironment{rep#1}[1]{%
 \def\rep@title{#2 \ref{##1}}%
 \begin{rep@theorem}}%
 {\end{rep@theorem}}}

\newreptheorem{lemma}{Lemma'}
\newreptheorem{proposition}{Proposition'}
\newreptheorem{theorem}{Theorem'}
\newreptheorem{remark}{Remark'}

%
%
\makeatletter
\newcommand{\leqnomode}{\tagsleft@true\let\veqno\@@leqno}
\newcommand{\reqnomode}{\tagsleft@false\let\veqno\@@eqno}
\makeatother

%
%
%

\newtheorem{definition}{Definition}
\newtheorem{theorem}[definition]{Theorem}
\newtheorem{proposition}[definition]{Proposition}
\newtheorem{lemma}[definition]{Lemma}
\newtheorem{corollary}[definition]{Corollary}
\newtheorem{observation}[definition]{Observation}
\newtheorem{conjecture}[definition]{Conjecture}
\newtheorem{remark}[definition]{Remark}
\newtheorem{problem}{Problem}
\newtheorem{claim}{Claim}
\newtheorem{open}{Open Problem}
\newtheorem*{example}{Example}

%
\DeclareMathOperator*{\conv}{conv}
\DeclareMathOperator{\diam}{diam}
\DeclareMathOperator*{\E}{\rm E}
\DeclareMathOperator*{\argmin}{\arg\min}
\DeclareMathOperator*{\argmax}{\arg\max}
\DeclareMathOperator{\lmax}{\lambda_{\max}}
\DeclareMathOperator{\lmin}{\lambda_{\min}}
\DeclareMathOperator*{\supp}{supp}
\DeclareMathOperator*{\diag}{diag}
\DeclareMathOperator*{\sign}{sign}
\DeclareMathOperator*{\tr}{Tr}
\DeclareMathOperator*{\rk}{Rk}
\DeclareMathOperator*{\card}{card}
\DeclareMathOperator*{\relint}{\mathop{relint}}

%
\providecommand{\abs}[1]{\left\lvert#1\right\rvert}
\providecommand{\norm}[1]{\left\lVert#1\right\rVert}
\providecommand{\normEucl}[1]{\left\lVert#1\right\rVert_2}
\providecommand{\dualnorm}[1]{\norm{#1}_*}
\providecommand{\frobnorm}[1]{\norm{#1}_{Fro}}
\providecommand{\tracenorm}[1]{\norm{#1}_{tr}}
\providecommand{\opnorm}[1]{\norm{#1}_{op}}
\providecommand{\maxnorm}[1]{\norm{#1}_{\max}}
\providecommand{\latGrNorm}[1]{\norm{#1}_{\groups}}

\newcommand{\ball}{\mathcal{B}}
%

\newcommand{\bigO}{O}
\newcommand{\Sym}{\mathbb{S}}

%
\newcommand{\domain}{\mathcal{\X}} %
\newcommand{\stepsize}{\gamma}
\newcommand{\stepmax}{\stepsize_{\textnormal{\scriptsize max}}} %
\newcommand{\stepbound}{\stepsize^\textnormal{\scriptsize B}} %
\newcommand{\FW}{{\hspace{0.05em}\textnormal{FW}}}
\newcommand{\PW}{{\hspace{0.05em}\textnormal{PFW}}}
\newcommand{\away}{{\hspace{0.05em}\textnormal{A}}}
\newcommand{\Cf}{C_{\hspace{-0.08em}f}}
\newcommand{\CfA}{C_{\hspace{-0.08em}f}^-}
\newcommand{\CfAFW}{C_{\hspace{-0.08em}f}^\away}
\newcommand{\strongConvFW}{\mu_{\hspace{-0.08em}f}^\FW}
\newcommand{\strongConvAFW}{\mu_{\hspace{-0.08em}f}^\away}
\newcommand{\distToBoundary}{{\delta_{\x^*\!,\domain}}}
\newcommand{\dirW}{\mathop{dirW}}
\newcommand{\dd}{\bm{d}}
\newcommand{\uu}{\bm{u}}
\newcommand{\vv}{\bm{v}} %
\newcommand{\atoms}{\mathcal A}
\newcommand{\groups}{\mathcal G}
\newcommand{\row}{\text{row}}
\newcommand{\col}{\text{col}}
\newcommand{\lft}{\text{left}}
\newcommand{\rgt}{\text{right}}
\newcommand{\cst}{\mathrm{cst}}
%
\newcommand{\mapprox}{\nu} %
\newcommand{\CfTotal}{C_f^{\text{\tiny{prod}}}}
\DeclareMathOperator*{\lmo}{LMO_{\!\Vertices}}
%
\newcommand{\A}{\mathcal{A}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\bv}{\bm{b}}
\newcommand{\err}{\bm{e}} %

%
\newcommand{\VVertices}{\mathcal{V_\A}\times \mathcal{V_\B}}
\newcommand{\Vertices}{\mathcal{A}\times \mathcal{B}} %
\newcommand{\Coreset}{\mathcal{S}}
\renewcommand{\SS}{\mathcal{S}}
\renewcommand{\aa}{\bm{\alpha}}
\renewcommand{\r}{\bm{r}}
\newcommand{\PdirW}{\mathop{PdirW}}
\newcommand{\PWidth}{\mathop{PW\!idth}}
\newcommand{\innerProd}[2]{\left\langle #1 , #2 \right\rangle}
\newcommand{\innerProdCompressed}[2]{\langle #1 , #2 \rangle}
\newcommand{\C}{\mathcal{C}}
\newcommand{\proj}{\bm{P}}
\newcommand{\K}{\bm{K}}
\newcommand{\Kface}{\mathcal{K}}

\newcommand{\xc}{\x_c} %
\newcommand{\yc}{\y_c} %
\newcommand{\muIntL}{\mu^{\text{int}}_\L} %


\newcommand{\xdim}{d}  %
\newcommand{\ydim}{p}  %

\newcommand{\strongConvGeneralized}{\tilde{\mu}_{\hspace{-0.08em}f}}

\DeclareMathOperator{\ddim}{dim}

%
\newcommand{\Spectahedron}{\mathcal{S}}
\newcommand{\SpectahedronLeOne}{\mathcal{S}_{\le 1}} %
\newcommand{\SpectahedronLeT}{\mathcal{S}_{t}} %
\newcommand{\MaxCutPolytope}{\boxplus}
\newcommand{\FourPointPSD}{\Spectahedron^4_{\text{sparse}}}
\newcommand{\FourPointPSDplus}{\Spectahedron^{4+}_{\text{sparse}}}
\newcommand{\FourPointPSDminus}{\Spectahedron^{4-}_{\text{sparse}}}
\newcommand{\LOneBall}{\diamondsuit}
\newcommand{\signVec}{\mathbf{s}}
\newcommand{\id}{\mathbf{I}} %
\newcommand{\ind}{\mathbf{1}} %
\newcommand{\0}{\mathbf{0}} %
\newcommand{\unit}{\mathbf{e}} %
\newcommand{\one}{\mathbf{1}} %
\newcommand{\zero}{\mathbf{0}}
\newcommand\SetOf[2]{\left\{#1\vphantom{#2}\right.\left|\vphantom{#1}\,#2\right\}}
\newcommand{\ignore}[1]{}%


\newcommand{\todo}[1]{\marginpar[\hspace*{4.5em}\textbf{TODO}\hspace*{-4.5em}]{\textbf{TODO}}\textbf{TODO:} #1}
\newcommand{\note}[1]{{\textbf{\color{red}#1}}}
\newcommand{\remove}[1]{} %




%


%
%
%
%
%
%
%
%
%





\begin{document}
\reqnomode %

\twocolumn[

\aistatstitle{Frank-Wolfe Algorithms for Saddle Point Problems}

\aistatsauthor{Gauthier Gidel
\And
Tony Jebara
\And
Simon Lacoste-Julien}

\aistatsaddress{ INRIA - Sierra Project-team\\
\'Ecole normale sup\'erieure, Paris 
\And Department of Computer Science\\
Columbia U. \& Netflix Inc., NYC
\And
Department of CS \& OR (DIRO) \\
Universit\'e de Montr\'eal, Montr\'eal} ]



\begin{abstract}
We extend the Frank-Wolfe (FW) optimization algorithm to solve constrained smooth convex-concave saddle point (SP) problems.
Remarkably, the method only requires access to linear minimization oracles. 
Leveraging recent advances in FW optimization, we provide the first proof of convergence of a FW-type saddle point solver over polytopes, thereby partially answering a 30 year-old conjecture.
We also survey other convergence results and highlight gaps in the theoretical underpinnings of FW-style algorithms. 
Motivating applications without known efficient alternatives are explored through structured prediction with combinatorial penalties as well as games over matching polytopes involving an exponential number of constraints. 
%
\end{abstract}

%
%
%

\section{Introduction}
The Frank-Wolfe (FW) optimization algorithm~\citep{Frank:1956vp}, also known as the conditional gradient method~\citep{demyanov1970approximate}, is a first-order method for smooth constrained optimization over a compact set. 
It has recently enjoyed a surge in popularity thanks to its ability to cheaply exploit the structured constraint sets appearing in machine learning applications~\citep{jaggi2013revisiting,lacoste2015global}. 
A known forte of FW is that it only requires access to a \emph{linear minimization oracle} (LMO) over the constraint set, i.e., the ability to minimize linear functions over the set, in contrast to projected gradient methods which require the minimization of \emph{quadratic} functions or other nonlinear functions.
In this paper, we extend the applicability of the FW algorithm to solve the following convex-concave saddle point problems:
%
%
\vspace{-2mm}  
\begin{equation} 
\label{eq:convex_concave}
   \min_{x \in \X} \max_{y \in \Y} \L(\x,\y), \vspace{-1mm}  
\end{equation}
\begin{equation*} 
  \text{with only access to }\text{LMO}(\mathbf{r}) \in \argmin_{\s \in \X \times \Y} \innerProd{\s}{\mathbf{r}}, \
\end{equation*}
%
where $\L$ is a smooth (with $L$-Lipschitz continuous gradient) \emph{convex-concave function}, i.e., $\L(\cdot,\y)$ is convex for all $\y \in \Y$ and $\L(\x,\cdot)$ is concave for all $\x \in \X$.
We also assume that $\X\times\Y$ is a convex compact set such that its LMO is cheap to compute. 
A \emph{saddle point solution} to~\eqref{eq:convex_concave} is a pair $(\x^*, \y^*) \in \X \times \Y$~\citep[VII.4]{hiriart2013convexI} such that: $\forall \x \in \X,\, \forall \y \in \Y$,
\begin{equation}\label{eq:saddle_point}
   \L(\x^*,\y) \leq \L(\x^*, \y^*) \leq \L(\x, \y^*).  
\vspace{-1mm}
\end{equation}
\paragraph{Examples of saddle point problems.} 
\citet{taskar2006dualExtrag} cast the maximum-margin estimation of structured output models as a bilinear saddle point problem $\L(\x,\y) = \x^\top M\y$, where $\X$ is the regularized set of parameters and $\Y$ is an encoding of the set of possible structured outputs. 
They considered settings where the projection on $\X$ and $\Y$ was efficient, but one can imagine many situations where only LMO's are efficient. 
For example, we could use a structured sparsity inducing norm~\citep{martins2011structuredSparsity} for the parameter~$\x$, such as the overlapping group lasso for which the projection is expensive~\citep{bach2012structuredSparsity}, while $\Y$ could be a combinatorial object such as a the ground state of a planar Ising model (without external field) which admits an efficient oracle~\citep{barahona1982} but has potentially intractable projection.

Similarly, two-player games~\citep{von2007theory} can often be solved as bilinear minimax problems. 
When a strategy space involves a polynomial number of constraints, the equilibria of such games can be solved efficiently~\citep{koller1994}. 
However, in situations such as the Colonel Blotto game or the Matching Duel~\citep{ahmadinejad2016}, the strategy space is intractably large and defined by an exponential number of linear constraints. 
Fortunately, despite this apparent prohibitive structure, some linear minimization oracles such as the blossom algorithm~\citep{edmonds1965} can efficiently optimize over the matching polytopes.
%

Robust learning is also often cast as a saddle point minimax problem~\citep{KimMagBoy05}. 
Once again, a FW implementation could leverage fast linear oracles while projection methods would be plagued by slower or intractable sub-problems. 
For instance, if the LMO is max-flow, it could have almost linear runtime while the corresponding projection would require cubic runtime quadratic programming~\citep{Kelner2014}.
Finally, note that the popular generative adversarial networks~\citep{goodfellow14GAN} are formulated as a (non-convex) saddle point optimization problem.

\vspace{-2mm}
\paragraph{Related work.}
The standard approaches to solve smooth constrained saddle point problems are projection-type methods (surveyed in~\citet{xiu2003surveyVIP}), with in particular variations of Korpelevich's extragradient method~\citep{korpelevich1976extragradient}, such as~\citep{nesterov2007dualExtrag} which was used to solve the structured prediction problem~\citep{taskar2006dualExtrag} mentioned above.
There is surprisingly little work on FW-type methods for saddle point problems, although they were briefly considered for the more general \emph{variational inequality} problem (VIP):
\vspace{-0.5mm}
\begin{equation} \label{eq:VIP}
\text{find } \,\, \z^* \in \Z \,\, \text{ s.t.} \,\, \innerProd{\FF(\z^*)}{\z - \z^*} \geq 0, \;\; \forall \z \in \Z,
\end{equation}
where $\FF$ is a Lipschitz mapping from $\R^p$ to itself and $\Z \subseteq \R^p$. By using $\Z = \X \times \Y$ and $\FF(\z) = (\nabla_x \L(\z),-\nabla_y \L(\z))$, the VIP~\eqref{eq:VIP} reduces to the equivalent optimality conditions for the saddle point problem~\eqref{eq:convex_concave}. 
\citet{hammond1984solving} showed that a FW algorithm with a step size of $O(1/t)$ converges for the VIP~\eqref{eq:VIP} when the set $\Z$ is strongly convex,
while FW with a generalized line-search on a saddle point problem is sometimes non-convergent when $\Z$ is a polytope (see also~\citep[\S~3.1.1]{patriksson1999nonlinear}). 
She conjectured though that using a step size of $O(1/t)$ was also convergent when $\Z$ is a polytope -- a problem left open up to this point. 
More recently, \citet{juditsky2016VIP} (see also~\citet{cox2015decompositionLMO}) proposed a method to transform a VIP on $\Z$ where one has only access to a LMO, to a ``dual'' VIP on which they can use a projection-type method. 
\citet{lan2013FW} proposes to solve the saddle point problem~\eqref{eq:convex_concave} by running FW on $\X$ on the \emph{smoothed} version of the problem $\max_{\y \in \Y} \L(\x,\y)$, thus requiring a projection oracle on $\Y$. 
In contrast, in this paper we study simple approaches that do not require any transformations of the problem~\eqref{eq:convex_concave} nor any projection oracle on $\X$ or $\Y$. Finally, \citet{he2015semi} introduced an interesting extragradient-type method to solve~\eqref{eq:VIP} by approximating the projections using linear oracles. In contrast to our proposal, their work does not cover the geometric convergence for the strongly convex case.

\vspace{-1.5mm}
\paragraph{Contributions.}
In \S~\ref{sec:algorithms}, we extend several variants of the FW algorithm to solve the saddle point problem~\eqref{eq:convex_concave} that we think could be of interest to the machine learning community. 
In \S~\ref{sec:SP-FW_strong_convexity}, we give a first proof of (geometric) convergence for these methods over polytope domains under the assumptions of sufficient strong convex-concavity of~$\L$, giving a partial answer to the conjecture from~\citet{hammond1984solving}. 
In \S~\ref{sec:strongly_set}, we extend and refine the previous convergence results when $\X$ and $\Y$ are strongly convex sets and the gradient of $\L$ is non-zero over $\X \times \Y$, while we survey the pure bilinear case in \S~\ref{sec:bilinear}.
We finally present illustrative experiments for our theory in \S~\ref{sec:experiments}, noticing that the convergence theory is still incomplete for these methods.




%
%
%


\begingroup
\setlength{\intextsep}{-1.5mm}
\setlength{\floatsep}{-1.5mm}
\setlength{\textfloatsep}{-1.5mm}
\setlength{\intextsep}{-1.5mm}
\begin{figure*}[t]
\begin{minipage}[t]{.45\textwidth}
  \begin{algorithm}[H]
    \caption{Frank-Wolfe algorithm}\label{FW}
    \begin{algorithmic}[1]
      \STATE Let $\x^{(0)} \in \X$
      \FOR{$t=0 \ldots T$} 
      \vspace*{.4mm}
      \STATE Compute $\rt = \nabla f(\x^{(t)})$
      \vspace*{.85mm}
      \STATE Compute $\st := \underset{\s \in \X}{\text{ argmin }} \innerProd{\s}{\rt}$
      \vspace*{.45mm}
      \STATE Compute $g_t := \left\langle \xt-\st, \rt\right\rangle$
      \vspace*{.85mm}
      \STATE \textbf{if} $ \gap \leq \epsilon$ \textbf{then} \textbf{return} $\x^{(t)}$ 
      \vspace*{.85mm}
      \STATE Let $\gamma = \frac{2}{2+t}$ (or do line-search)
      \vspace*{.85mm}
      \STATE Update $\xtt := (1- \gamma) \xt + \gamma \st$
      \ENDFOR
    \end{algorithmic}
  \end{algorithm}
\end{minipage}
\hspace{-2mm}
\begin{minipage}[t]{.55\textwidth}
\begin{algorithm}[H]
  \caption{Saddle point Frank-Wolfe algorithm: \textbf{SP-FW}}\label{alg:SP-FW}
  \begin{algorithmic}[1]
      \STATE Let $\z^{(0)} =(\x^{(0)},\y^{(0)}) \in \M$
      \FOR{$t=0 \ldots T$}
      \STATE Compute $ \rt := \begin{pmatrix} 
                  \nabla_x \L(\xt,\yt) \\
                  -\nabla_y \L(\xt,\yt)\end{pmatrix}$ \label{algLine:rk}
      \STATE Compute $\st := \underset{\z \in \X\times \Y}{\text{ argmin }} \innerProd{\z}{\rt}$
      \STATE Compute $g_t := \left\langle \zt-\st, \rt\right\rangle $
      \STATE \textbf{if} $ g_t\leq \epsilon$ \textbf{then} \textbf{return} $\z^{(t)}$
      \STATE Let $\gamma =  \min\left(1, \frac\CondNumb{2C}\gap\right) $ or $\gamma = \frac{2}{2+t}$ 
      %
      \hfill \raisebox{0pt}[0pt][0pt]{
      		\begin{minipage}{2.5cm}
      			\emph{\small($\CondNumb$ and $C$ set as \\ case~(I)~in~Thm.~\ref{thm:conv})}
      		\end{minipage}
      		}
      \STATE Update $\ztt := (1- \gamma) \zt + \gamma \st$
      \ENDFOR
\end{algorithmic}
\end{algorithm}
\end{minipage}
%
%
\vspace{-3mm}
\end{figure*}
\begin{figure*}[ttt]
\begin{minipage}[t]{\textwidth}
\begin{algorithm}[H]
    \caption{Saddle point away-step Frank-Wolfe algorithm: \textbf{SP-AFW}$(\z^{(0)}, \Vertices, \epsilon)$}\label{alg:AFW}
  \begin{algorithmic}[1]
%
  \STATE Let $\z^{(0)}=(\x^{(0)},\y^{(0)}) \in \Vertices$, $\Coreset_x^{(0)} := \{\x^{(0)}\}$ and $\Coreset_y^{(0)} := \{\y^{(0)}\}$
  \FOR{$t=0\dots T$}
    \STATE Let $\st := \lmo \!\left(\rt\right)$ %
       and $\dt_\FW := \st - \z^{(t)}$ \hspace{27mm}~~ \emph{\small($\rt$ as defined in L\ref{algLine:rk} in Algorithm~\ref{alg:SP-FW})}
    \STATE Let $\vt \in \displaystyle\argmax_{\vv \in \Coreset_x^{(t)} \times \Coreset_y^{(t)} } \textstyle\left\langle \rt, \vv \right\rangle$ and $\dt_\away := \z^{(t)} - \vt$ \hspace{51mm} \emph{\small(the away direction)}  \label{algLine:awayCorner}
    \STATE \textbf{if} $g_t^\FW  := \left\langle -\rt, \dt_\FW\right\rangle  \leq \epsilon$ \textbf{then} \textbf{return} $\z^{(t)}$ \hspace{38mm}\emph{\small(FW gap is small enough, so return)} \label{algLine:gapFW}
      %
      \IF{$\left\langle -\rt, \dt_\FW\right\rangle  \geq \left\langle -\rt, \dt_\away\right\rangle$ } \label{algLine:begAFW}
      \STATE $\dt :=  \dt_\FW$, and $\stepmax := 1$  
           \hspace{77mm}\emph{\small(choose the FW direction)}
      \ELSE
      \STATE $\dt :=  \dt_\away$, and $\stepmax := \min\left\{\tfrac{\alpha_{\vv_x^{(t)}}}{1- \alpha_{\vv_x^{(t)}}},\tfrac{\alpha_{\vv_y^{(t)}}}{1- \alpha_{\vv_y^{(t)}}}\right\}$
        \emph{\small(maximum feasible step size; a \emph{drop step} is when $\stepsize_t = \stepmax$)} \label{algLine:drop_step}
       \vspace{-2mm}	
      \ENDIF \label{algLine:endAFW}
      \STATE Let $\gap^{\PW} = \innerProd{-\rt}{\dt_{\FW} + \dt_{\away}}$ \textbf{and} $\gamma_t = \min \left\{\gamma_{\max} , \frac{\CondNumb^{\PW}}{2 C}\gap^{\PW} \right\}$ \hfill \emph{\small($\CondNumb$ and $C$ set as case (P) in Thm.~\ref{thm:conv})} \label{algLine:gapPFW}
      \STATE Update $\z^{(t+1)} := \z^{(t)} + \stepsize_t \dt$  
        \hspace{4mm}\emph{\small(and accordingly for the weights $\aa^{(t+1)}$, see~\citet{lacoste2015global})} 
      \STATE Update $\Coreset_x^{(t+1)} := \{\vv_x \in \A \,\: \mathrm{ s.t. } \,\: \alpha^{(t+1)}_{\vv_x} > 0\}\;$ and $\;\Coreset_y^{(t+1)}:=\{\vv_y \in \B \,\: \mathrm{ s.t. } \,\: \alpha^{(t+1)}_{\vv_y} >0\}$ \label{algLine:activeSet}
  \ENDFOR
\end{algorithmic}
\end{algorithm}
~\vspace{-1.5mm}
\begin{algorithm}[H]
  \caption{Saddle point pairwise Frank-Wolfe algorithm: \textbf{SP-PFW}$(\z^{(0)}, \Vertices, \epsilon)$}\label{alg:PFW}
  \begin{algorithmic}[1]
  \STATE In Alg.~\ref{alg:AFW}, replace L\ref{algLine:begAFW} to~\ref{algLine:endAFW} by:  $\,\, \dd^{(t)} := \dd_{\PW}^{(t)} \!:= \s^{(t)} - \vv^{(t)}$, and $\stepmax := \min\left\{ \alpha_{\vv_x^{(t)}},\alpha_{\vv_x^{(t)}}\right\}$.
  \end{algorithmic}
\end{algorithm}
\end{minipage}
\vspace{-4mm}
\end{figure*}
\endgroup


\vspace{-0.3mm}
\section{Saddle point Frank-Wolfe (SP-FW)} \label{sec:algorithms}
\vspace{-2.4mm}
\paragraph{The algorithms.} %
\label{par:the_algorithms}
This article will explore three SP extensions of the classical {\em Frank-Wolfe} (FW) algorithm (Alg.~\ref{FW}) which are summarized in Alg.~\ref{alg:SP-FW}, \ref{alg:AFW} and~\ref{alg:PFW}.\footnote{Alg.~\ref{alg:SP-FW} was already proposed by~\citet{hammond1984solving} for VIPs, while our step sizes and Alg.~\ref{alg:AFW} \&~\ref{alg:PFW} are novel.}
We denote by $\zt := (\xt,\yt)$ the iterate computed after $t$ steps.
We first obtain the {\em saddle point FW} (SP-FW) algorithm (Alg.~\ref{alg:SP-FW}) by simultaneously doing a FW update on both convex functions $\L(\cdot,\yt)$ and  $-\L(\xt,\cdot)$ with a properly chosen step size. As in standard FW, the point $\zt$ has a sparse representation as a convex combination of the points previously given by the FW oracle, that is,
\begin{equation}  
\x^{(t)} = \sum_{\vv_x \in \Coreset_x^{(t)}} \alpha_{\vv_x} \vv_x \;\; \text{and}\;\; \y^{(t)} = \sum_{\vv_y \in \Coreset_y^{(t)}} \alpha_{\vv_y} \vv_y.
\end{equation}
These two sets $\Coreset_x^{(t)}$, $\Coreset_y^{(t)}$ of points are called the \emph{active sets}, and we can maintain them separately (thanks to the product structure of $\X \times \Y$) to run the other two FW variants that we describe below (see L\ref{algLine:activeSet} of Alg.~\ref{alg:AFW}).

If we assume that $\X$ and $\Y$ are the convex hulls of two finite sets of points $\A$ and $\B$, we can also extend the {\em away-step Frank-Wolfe} (AFW) algorithm~\citep{Guelat1986AFW,lacoste2015global} to saddle point problems.
As for AFW, this new algorithm can choose an \emph{away} direction~$\dd_\A$ to remove mass from ``bad" atoms in the active set, i.e. to reduce~$\alpha_{\vv}$ for some~$\vv$ (see L\ref{algLine:drop_step} of Alg.~\ref{alg:AFW}),
%
thereby avoiding the zig-zagging problem that slows down standard FW~\citep{lacoste2015global}. Note that because of the special product structure of the domain, we consider more away directions than proposed in~\citep{lacoste2015global} for AFW (see Appendix~\ref{par:active_set} for more details). Finally, a straightforward saddle point generalization for the {\em pairwise Frank-Wolfe} (PFW) algorithm~\citep{lacoste2015global} is given in Alg.~\ref{alg:PFW}.
%
%
%
The proposed algorithms all preserve several nice properties of previous FW methods (in addition to only requiring LMO's): simplicity of implementation, affine invariance~\citep{jaggi2013revisiting}, gap certificates computed for free, sparse representation of the iterates {and the possibility to have adaptive step sizes using the gap computation.
We next analyze the convergence of these algorithms.}



%
%
%

%
%
\paragraph{The suboptimality  error and the gap.} %
\label{par:the_subopti}
To establish convergence, we first define several quantities of interest. 
In classical convex optimization, the suboptimality error $h_t$ is well defined as $h_t := f(\xt)- \min_{\x \in \X} f(\x)$. 
This quantity is clearly non-negative and proving that $h_t$ goes to 0 is enough to establish convergence. 
Unfortunately, in the saddle point setting the quantity $\L(\xt,\yt) - \L^*$ is no longer non-negative and can be equal to zero for an infinite number of points $(\x,\y)$ while $(\x,\y) \notin (\X^*,\Y^*)$. 
For instance, if $\L(\x,\y)=\x \cdot \y$ with $\X = \Y = [-1,1]$, then $\L^* = 0$ and $(\X^*,\Y^*)=\{(0,0)\}$. 
But for all $\x \in \X$ and $\y \in \Y$, $\; \x \cdot 0 = 0 \cdot \y = \L^*$.
The saddle point literature thus considers a non-negative gap function (also known as a merit function \citep{larsson1994class,zhu1998convergence} and \cite[Sec 4.4.1]{patriksson1999nonlinear}) which is zero only for optimal points, in order to quantify progress towards the saddle point. 
We can define the following \emph{suboptimality error} $h_t$ for our saddle point problem:
\vspace{-2mm}
\begin{equation}
\begin{aligned}\label{def:subopt} 
   h_t := \L(\xt,\ytm) - \L(\xtm,\yt), \\
   \text{where} \quad \xtm := \argmin_{\x \in \X} \L(\x,\yt), \\[-1mm]
   \text{and }  \quad \ytm := \argmax_{\y \in \Y} \L(\xt,\y).
\end{aligned}
\vspace{-1mm}
\end{equation}
This is an example of \emph{primal-dual} gap function by noticing that 
\begin{align}
h_t &= \L(\xt,\ytm) -\L^* +\L^* - \L(\xtm,\yt) \notag\\ 
    &= p(\xt) - p(\x^*) + g(\y^*) - g(\yt),  
\end{align}
where $p(\x) := \max_{\y \in \Y} \L(\x,\y)$ is the convex primal function and $g(\y) := \min_{\x \in \X} \L(\x,\y)$ is the concave dual function.
By convex-concavity, $h_t$ can be upper-bounded by the following FW linearization gap 
\citep{jaggi2011sparse,jaggi2013revisiting,larsson1994class,zhu1998convergence}:
\begin{equation}
\begin{aligned}\label{eq:gap}
  \hspace{-2mm} g_t^{\FW}
  & \!:= \left. \max_{\s_x \in \X} \innerProd{\xt- \s_x}{\nabla_x \L(\xt,\yt)} \right\} {\small \!:= g^{(x)}_t}  \\
  & \!+ \left. \max_{\s_y \in \Y} \innerProd{\yt - \s_y}{-\nabla_y \L(\xt,\yt)} \right\} {\small \!:=g^{(y)}_t} \!.
\end{aligned} \hspace{-4mm}
\end{equation}
This gap is easy to compute and gives a stopping criterion since $g_t^{\FW} \geq h_t$.

%


%
%
%




\paragraph{Compensation phenomenon and difficulty for SP.} %
\label{par:the_compensation_phenomenon}
Even when equipped with a suboptimality error and a gap function (as in the convex case), we still cannot apply the standard FW convergence analysis. 
The usual FW proof sketch uses the fact that the gradient of $f$ is Lipschitz continuous to get
\begin{equation} \label{eq:FWstandard}
  h_{t+1} \leq h_t - \gamma_t g_t^{\FW} + \gamma_t^2 \frac{L\|\dt\|^2}{2}
\end{equation}
which then provides a rate of convergence. 
Roughly, since $g_t \geq h_t$ by convexity, if $\gamma_t$ is small enough then $(h_t)$ will decrease and converge. 
For simplicity, in the main paper, $\|\cdot\|$ will refer to the $\ell_2$ norm of $\R^d$. 
The partial Lipschitz constants and the diameters of the sets are defined with respect to this norm (see~\eqref{def:lipL} in Appendix~\ref{sub:the_lipschitz_constants} for more general norms).

Using the $L$-Lipschitz continuity of $\L$ and letting $\L_t := \L(\xt, \yt)$ as a shorthand, we get
\begin{equation}
\begin{aligned}
  \L_{t+1}
  & \leq \L_t + \gamma_t \innerProd{\dt_x}{\nabla_x \L_t}  + \gamma_t \innerProd{\dt_y}{\nabla_y \L_t}   \\
  & \quad+ \gamma_t^2\frac{L\|\dt\|^2}{2}
\end{aligned}
\end{equation}
%
%
%
%
%
%
where $\dt_x = \st_x - \xt$ and $\dt_y=\st_y-\yt$. 
Then 
\begingroup
\setlength{\thinmuskip}{0mu}
\setlength{\medmuskip}{0mu}
\setlength{\thickmuskip}{0mu}
\begin{equation}\label{eq:oscilation}
  \hspace{-1mm} \L_{t+1} - \L^*
   \hspace{1mm} \leq \hspace{1mm} \L_t - \L^*
    - \gamma_t \left( g^{(x)}_t -  g_t^{(y)}\right) 
    + \gamma_t^2 \frac{L \|\dt\|^2}{2}. \hspace{-1mm}
\end{equation}
\endgroup
Unfortunately, the quantity $\gap^{\FW}$ does \emph{not} appear above and we therefore cannot control the oscillation of the sequence (the quantity $g^{(x)}_t -  g_t^{(y)}$ can make the sequence increase or decrease). 
Instead, we must focus on more specific SP optimization settings and introduce other quantities of interest in order to establish convergence.
%
 %



%


%
%
%


\paragraph{The asymmetry of the SP.} %
\label{par:the_asymmetry_of_the_sp}
\citet[p.~165]{hammond1984solving} showed the divergence of the SP-FW algorithm with an extended line-search step-size on some bilinear objectives. She mentioned that the difficulty for SP optimization is contained in this bilinear coupling between $\x$ and $\y$.  
More generally, most of the examples of SP functions cited in the introduction can be written in the form:
\begin{equation}
\L(\x,\y) = f(\x) + \x^\top \!M \y - g(\y), \; \text{$f$ and $g$ convex.}
\end{equation}
In this setting, the bilinear part $M$ is the only term preventing us to apply theorems on standard FW.  
\citet[p.~175]{hammond1984solving} also conjectured that the SP-FW algorithm with $\stepsize_t=\nicefrac{1}{(t+1)}$ performed on a uniformly strongly convex-concave objective function (see~\eqref{def:uniform}) over a polytope should converge. We give a partial answer to this conjecture in the following section.
%


%
%
%


\section{SP-FW for strongly convex functions}
\label{sec:SP-FW_strong_convexity}
\paragraph{Uniform strong convex-concavity.} %
\label{par:strong_convex_concavity}
%
In this section, we will assume that $\L$ is uniformly $(\mu_\X,\mu_\Y)$-strongly convex-concave, which means that the following function is convex-concave:
\begin{equation}\label{def:uniform}
 (\x,\y) \mapsto \L(\x,\y) - \frac{\mu_\X}{2} \|\x\|^2 + \frac{\mu_\Y}{2}\|\y\|^2.
\end{equation}
\paragraph{A new merit function.} %
\label{par:the_new_quantity}
%
To prove our theorem, we use a different quantity $w_t$ which is smaller than $h_t$ but still a valid merit function in the case of \emph{strongly convex-concave} SPs (where $(\x^*,\y^*)$ is thus unique); see~\eqref{eq:relate_h_w} below. 
For $(\x^*,\y^*)$ a solution of~\eqref{eq:convex_concave}, we define the non-negative quantity $w_t$:
\begin{equation}
 w_t
   := \underbrace{\L(\xt,\y^*) -\L^*}_{:=  w_t^{(x)}} + \underbrace{\L^* - \L(\x^*,\yt)}_{:=w_t^{(y)}}.
\end{equation}
%
%
%
%
%
%
%
%
Notice that $ w_t^{(x)}$ and $w_t^{(y)}$ are non-negative, and that $w_t \leq h_t$ since:
\[
  \L(\xt,\ytm) - \L(\xtm,\yt) \geq \L(\xt,\y^*) - \L(\x^*,\yt).
\]
In general, $w_t$ can be zero even if we have not reached a solution.
For example, with $\L(\x,\y) = \x \cdot \y$ and $\X = \Y = [-1,1]$, then $\x^* = \y^* = \0$, implying $w_t = 0$ for any $(\x^{(t)},\y^{(t)})$. 
But for a uniformly strongly convex-concave~$\L$, this cannot happen and we can prove that~$w_t$ has the following nice property (akin to $\|\x - \x^*\| \leq \sqrt{\nicefrac{2}{\mu} (f(\x) - f(\x^*))}$ for a $\mu$-strongly convex function~$f$; see Proposition~\ref{prop:relation_primal} in Appendix~\ref{sub:relation_between_the_primal_errors}):
\begin{equation} \label{eq:relate_h_w}
  h_t \leq \sqrt{2} P_\L \sqrt{w_t} \, , \vspace{-1mm} 
\end{equation}
where 
%
%
%
%
%
\begin{equation}
  \!\!\! P_\L \leq \sqrt{2} \underset{\z \in \M}{\sup} \left\{\frac{\|\nabla_x \L(\z)\|_{\X^*}}{\sqrt{\mu_\X}} ,\frac{\|\nabla_y \L(\z)\|_{\Y^*}}{\sqrt{\mu_\Y}}   \right\} .
\end{equation}
%
\paragraph{Pyramidal width and distance to the border.} %
\label{par:pyramidal_width}
We now provide a theorem that establishes convergence in two situations: \eqref{enu:situation1} when the SP belongs to the interior of $\M$; \eqref{enu:situation2} when the set is a polytope, i.e. when there exist two finite sets such that $\X = \conv(\mathcal A)$ and $\Y = \conv(\mathcal B)$).
Our convergence result holds when (roughly) the strong convex-concavity of~$\L$ is big enough in comparison to the cross Lipschitz constants $L_{XY}$, $L_{YX}$ of $\nabla \L$ (defined in~\eqref{eq:cross-lip} below) multiplied by geometric ``condition numbers'' of each set. The condition number of $\X$ (and similarly for $\Y$) is defined as the ratio of its \emph{diameter} $D_\X : = \sup_{\x, \x' \in \X} \|\x -\x'\|$ over the following appropriate notions of ``width'':
\begin{align} 
\hspace{-7mm}\text{\small border distance: }\, \delta_\X &:= \min_{\s \in \partial  \X} \|\x^* -\s\|  \!\!\! &&\text{for \eqref{enu:situation1},} \hspace{-2mm} \\
\hspace{-7mm}\text{\small pyramidal width: }\, \delta_{\mathcal A} &:=\PWidth(\mathcal A)  &&\text{for \eqref{enu:situation2}.} & \label{eq:PwidthL} \hspace{-2mm}
\end{align}
The pyramidal width~\eqref{eq:PwidthL} is formally defined in~Eq.~9 of~\citet{lacoste2015global} and in  Appendix~\ref{sub:affine_invariant_measures_of_strong_convexity}.
Given the above constants, we can state below a non-affine invariant version of our convergence theorem (for simplicity). The affine invariant versions of this theorem are given in Thm.~\ref{thm:conv_affine_invariant} and~\ref{thm:conv_sublin} in Appendix~\ref{subsec:proof_main_thm} (with proofs).

\begin{theorem}\label{thm:conv}
  Let $\L$ be a convex-concave function and $\M$ a convex and compact set. 
  Assume that the gradient of  $\L$ is $L$-Lipschitz continuous, that $\L$ is $(\mu_\X,\mu_\Y)$-strongly convex-concave, and that we are in one of the two following situations:
  \leqnomode %
  \begin{align*}
  &\parbox{7cm}{The SP belongs to the interior of $\M$. In this case, set $\gap = \gap^{\FW}$ (as in L\ref{algLine:gapFW} of Alg.~\ref{alg:AFW}), $\delta_\mu:= \sqrt{\min(\mu_\X\delta_\X^2,\mu_\Y\delta_\Y^2)}$ and $a :=  1 $. ``Algorithm'' then refers to SP-FW.
    }
    \label{enu:situation1} 
    \tag{I}
      \\[2mm]
    &\parbox{7cm}{The sets $\X$ and $\Y$ are polytopes. In this case, set $\gap = \gap^{\PW}$ (as in L\ref{algLine:gapPFW} of Alg.~\ref{alg:AFW}), $\delta_\mu := \sqrt{\min(\mu_\X \delta_{\mathcal A}^2, \mu_\Y \delta_{\mathcal B}^2)}$ and $a:=  \frac{1}{2}$. ``Algorithm'' then refers to SP-AFW. Here $\delta_\mu$ needs to use the Euclidean norm for its defining constants.
    }
    \label{enu:situation2} 
    \tag{P}
  \end{align*}
  \reqnomode
  In both cases, if $\CondNumb:= a -  \tfrac{\sqrt{2}}{\delta_\mu}\max\left\{ \tfrac{D_\X L_{XY}}{\sqrt{\mu_\Y}}, \tfrac{D_\Y L_{YX}}{\sqrt{\mu_\X}}\right\}$ is positive, then the errors $h_t$~\eqref{def:subopt} of the iterates of the algorithm with step size
  $\gamma_t = \min\{\stepmax, \frac\CondNumb{2C}\gap\}$ decrease
  geometrically as 
  \begin{equation*} 
    h_t = O \left( (1- \rho)^{\frac{k(t)}{2}} \right) \: \text{ and }
 \: \min_{s\leq t} g_s^\FW = O \left( (1- \rho)^{\frac{k(t)}{2}} \right)
  \end{equation*} 
  where
  $\rho := \CondNumb^2\frac{\delta_\mu^2}{2C}$, $C := \tfrac{L D_\X^2+ L D_\Y^2}{2}$ and $k(t)$ is the number of non-drop step after $t$ steps (see L\ref{algLine:drop_step} in Alg.~\ref{alg:AFW}). In case~\eqref{enu:situation1} we have $k(t)=t$ and in case~\eqref{enu:situation2} we have $k(t)\geq t/3$.
  For both algorithms, if $\delta_\mu >2\max\left\{ \tfrac{D_\X L_{XY}}{\mu_\X}, \tfrac{D_\Y L_{YX}}{\mu_\Y}\right\}$, we also obtain a sublinear rate with the universal choice $\gamma_t =\min\{\stepmax, \frac{2}{2+k(t)} \}$. This yields the rates:
  \vspace{-3mm}
  \begin{equation}
         \min_{s\leq t} h_s \leq  \min_{s\leq t} g_s^\FW = O\left( \frac{1}{t} \right).
    \end{equation}
\end{theorem}

Clearly, the sublinear rate seems less interesting than the linear one but has the added convenience that the step size can be set without knowledge of various constants that characterize $\L$.  Moreover, it provides a partial answer to the conjecture from~\citet{hammond1984solving}.

%
%
%



\paragraph{Proof sketch.} %
\label{par:proof_sketch}

%

%
Strong convexity is an essential assumption in our proof; it allows us to relate~$w_t$ to how close we are to the optimum. 
Actually, by $\mu_\Y$-strong concavity of $\L(\x^*,\cdot)$, we have
\begingroup
\setlength{\thinmuskip}{0.5mu}
\setlength{\medmuskip}{0mu}
\setlength{\thickmuskip}{0.5mu}
\begin{equation}\label{eq:close_y_w}
   \| \yt - \y^*\| \leq \sqrt{\frac{2}{\mu_\Y} \left( \L^*  - \L(\x^*,\yt)\right)} = \sqrt{\frac{2}{\mu_\Y} w_t^{(y)}}.
 \end{equation}
\endgroup
Now, recall that we assumed that $\nabla \L$ is Lipschitz continuous. In the following, we will call $L$ the {\em Lipschitz continuity constant} of $\nabla \L$ and $L_{XY}$ and $L_{YX}$ its (cross) {\em  partial Lipschitz constants}. For all $\x, \, \x' \in \X, \;\y, \, \y'  \in \Y$, these constants satisfy
\begin{equation}
\begin{aligned}\label{eq:cross-lip}
   \!\! \|\nabla_x\L(\x,\y) - \nabla_x\L(\x,\y')\|_{\X^*} \leq L_{XY} \|\y- \y'\|_\Y,\\
   \!\! \|\nabla_y\L(\x,\y) - \nabla_y\L(\x',\y)\|_{\Y^*} \leq L_{YX} \|\x- \x'\|_\X.
\end{aligned}
\end{equation}
Note that $L_{XY},L_{YX}\leq L$ if $\|(\x,\y)\| := \|\x\|_\X + \|\y\|_\Y$.
Then, using Lipschitz continuity of the gradient,
%
%
%
%
%
\begin{align}
 \L(\xtt,\y^*)  & \leq  \L(\xt,\y^*)  +  \stepsize \innerProdCompressed{\dt_x}{\nabla_x \L(\xt,\y^*)} \notag \\
                  & \quad+ \stepsize^2 \frac{L \|\dt_x\|^2}{2}.
\end{align}
%
Furthermore, setting $(\x,\y) = (\xt, \y^*)$ and $\y'= \yt$ in Equation~\eqref{eq:cross-lip}, we have
\begin{equation}
\begin{aligned}\label{eq:intermediary_scheme}
 w_{t+1}^{(x)} & \leq w_t^{(x)} 
                  - \stepsize g_t^{(x)} 
                  + \stepsize D_\X L_{XY} \|\yt - \y^*\| \\
                 & \quad + \stepsize^2 \frac{L D_\X^2}{2} \, .
\end{aligned}
\end{equation}
Finally, combining \eqref{eq:intermediary_scheme} and \eqref{eq:close_y_w}, we get
\begin{equation}
 \begin{aligned}\label{eq:intermediary_scheme_2}
   w_{t+1}^{(x)} & \leq w_t^{(x)} 
                  - \stepsize g_t^{(x)} 
                  + \stepsize D_\X L_{XY}\sqrt{\frac{2}{\mu_\Y}} \sqrt{w_t^{(y)}} \\
                  & \quad + \stepsize^2 \frac{L D_\X^2}{2}.
 \end{aligned}
\end{equation}
A similar argument on $-\L(\x^*, \ytt)$ gives a bound on $w_t^{(y)}$ much like~\eqref{eq:intermediary_scheme_2}. Summing both yields:
\begin{align}
    w_{t+1} &\leq w_t - \stepsize g_t + 2\stepsize \max\left\{\tfrac{D_\X L_{XY}}{\sqrt{\mu_\Y}},\tfrac{D_\Y L_{YX}}{\sqrt{\mu_\X}}\right\} \sqrt{w_t}  \notag \\
            & \quad + \stepsize^2 \frac{L D_\X^2+ L D_\Y^2}{2}.
\end{align}
We now apply recent developments in the convergence theory of FW methods for strongly convex objectives. \citet{lacoste2015global} crucially upper bound the square root of the suboptimality error on a convex function with the FW gap if the optimum is in the interior, or with the PFW gap if the set is a polytope (Lemma~\ref{lemme:lingap} in Appendix~\ref{app:gapInequalities}). We continue our proof sketch for case~\eqref{enu:situation1} only:\footnote{The idea is similar for case~\eqref{enu:situation2}, but with the additional complication of possible drop steps.}
\begin{equation}
\begin{aligned}
 \label{eq:def_delta_x}
  & {2 \mu_\X \delta_\X^2}\left(\L(\xt,\yt)-\L(\x^*,\yt)\right) \leq \left(\gap^{(x)}\right)^2  \\
  &\qquad \text{where} \quad \delta_\X :=  \min_{\s \in \partial  \X} \|\x^* -\s\|.
 \end{aligned}
\end{equation}
  We can also get the respective equation on $\y$ with $\delta_\Y :=  \min_{\y \in \partial  \Y} \|\y^* -\y\|$ and sum it with the previous one \eqref{eq:def_delta_x} to get:
  \vspace{-1mm}
\begin{equation}\label{eq:def_delta}
     \delta_\mu \sqrt{2w_t} \leq \gap
     \;\, \text{where} \;\,
      \delta_\mu := \sqrt{\min(\mu_\X\delta_\X^2,\mu_\Y\delta_\Y^2)}.
\end{equation}
%
%
%
%
%
%
%
%
%
%
%
%
Plugging this last equation into \eqref{eq:intermediary_scheme_2} gives us 
\begingroup
\setlength{\thinmuskip}{2mu}
\setlength{\medmuskip}{2mu}
\setlength{\thickmuskip}{2mu}
\begin{equation}
\begin{aligned}\label{eq:cst}
    w_{t+1} & \leq w_t 
                  - \CondNumb\stepsize g_t 
                  + \stepsize^2 {C}
                  \;\;\;\; \text{where} \;\;\;\;  C := \tfrac{ L D_\X^2+ L D_\Y^2}{2}
                  \\
    &\text{and}  \;\;\;\; \CondNumb  := 1-  \tfrac{\sqrt{2}}{\delta_\mu}\max\left\{ \tfrac{D_\X L_{XY}}{\sqrt{\mu_\Y}}, \tfrac{D_\Y L_{YX}}{\sqrt{\mu_\X}}\right\}.
\end{aligned} 
\end{equation}
 \endgroup
%
 %
 %
 %
 %
 %
 %
 %
 %
The recurrence~\eqref{eq:cst} is typical in the FW literature. We can re-apply standard techniques on the sequence~$w_t$ to get a sublinear rate with $\stepsize_t = \frac{2}{2+t}$, or a linear rate with $\stepsize_t= \min\left\{ \stepmax, \frac{\CondNumb g_t}{2C} \right\}$ (which minimizes the RHS of~\eqref{eq:cst} and actually guarantees that $w_t$ will be \emph{decreasing}). Finally, thanks to strong convexity, a rate on $w_t$ gives us a rate on $h_t$ (by~\eqref{eq:relate_h_w}). \qed
  %
%
%
%






%






%
%
%


%
%

\section{SP-FW with strongly convex sets}
\label{sec:strongly_set}
  \paragraph{Strongly convex set.} %
  \label{par:strongly_convex_set}
  One can (roughly) define strongly convex sets as sublevel sets of strongly convex functions~\citep[Prop.~4.14]{vial1983strong}. In this section, we replace the strong convex-concavity assumption on~$\L$ with the assumption that $\X$ and $\Y$ are $\beta$-strongly convex \emph{sets}.
  \begin{definition}[\citet{vial1983strong,polyak1966existence}]\label{def:strong_set}
    A convex set $\X$ is said to be $\beta$-strongly convex with respect to~$\|.\|$ 
    if for any $\x,\y \in \X$ and any $\gamma \in [0,1]$, $B_\beta(\gamma,\x,\y) \subset \X$ where $B_\beta(\gamma,\x,\y)$ is the $\|.\|$-ball of radius ${\gamma(1 - \gamma) \frac{\beta}{2}\|\x-\y\|^2}$ centered at $\gamma \x + (1- \gamma)\y$.
    %
    %
  \end{definition}

  Frank-Wolfe for convex optimization over strongly convex sets has been studied by
  \citet{levitin1966constrained,demyanov1970approximate} and \citet{dunn1979rates}, amongst others. 
  They all obtained a linear rate for the FW algorithm if the norm of the gradient is lower bounded by a constant. 
  More recently, \citet{garber2014faster} proved a sublinear rate $O(1/t^2)$ by replacing the lower bound on the gradient by a strong convexity assumption on the function.
  In the VIP setting \eqref{eq:VIP}, the linear convergence has been proved if the optimization is done under a strongly convex set but this assumption does \emph{not} extend to $\X \times \Y$ which \emph{cannot} be strongly convex if $\X$ or $\Y$ is not reduced to a single element. 
  In order to prove the convergence, we first prove the Lipschitz continuity of the \emph{FW-corner} function $\s(\cdot)$ defined below. 
  A proof of this theorem is given in Appendix \ref{sec:strong_conv_proof}.
  \begin{theorem}\label{thm:s_lip}
  %
   Let $\X$ and $\Y$ be $\beta$-strongly convex sets. 
   If $\min(\|\nabla_{\!x} L(\z)\|_{\X^*}, \|\nabla_{\!y} L(\z)\|_{\Y^*}) \geq\delta>0$ for all $\z \in \M$, then the oracle function $\z \mapsto \s(\z) := \arg\min_{\s \in \M}\innerProd{\s}{\FF(\z)}$ is well defined and is $\frac{4L}{\delta \beta }$-Lipschitz continuous (using the norm $\|(\x,\y)\|_{\X \times \Y} := \|\x\|_\X + \|\y\|_\Y$), where $\FF(\z) := \left(
     \nabla_x \L(\z),
     -\nabla_y \L(\z)
     \right)$.
  \end{theorem}

  \paragraph{Convergence rate.} %
  \label{par:convergence_rate}
  When the FW-corner function~$\s(\cdot)$ is Lipschitz continuous (by Theorem~\ref{thm:s_lip}), we can actually show that the FW gap is decreasing in the FW direction and get a similar inequality as the standard FW one~\eqref{eq:FWstandard}, but, in this case, on the \emph{gaps}: $g_{t+1} \leq \gap (1- \gamma_t) + \gamma_t^2 \|\st - \zt \|^2 C_\delta$. 
  Moreover, one can show that the FW gap on a strongly convex set~$\X$ can be lower-bounded by $\|\s_x^{(t)} - \x^{(t)} \|^2$ (Lemma~\ref{lemma:lower_gap} in Appendix~\ref{sec:strong_conv_proof}), by using the fact that~$\X$ contains a ball of sufficient radius around the midpoint between~$\s_x^{(t)}$ and~$ \x^{(t)}$. 
  From these two facts, we can prove the following linear rate of convergence (\emph{not} requiring any \emph{strong} convex-concavity of~$\L$). 
  %
  %
  %
  %
  %
    \begin{theorem}\label{thm:conv_strong}
     Let $\L$ be a convex-concave function and $\X$ and $\Y$ two compact $\beta$-strongly convex sets. 
     Assume that the gradient of  $\L$ is $L$-Lipschitz continuous and that there exists $\delta>0$ such that $\min(\|\nabla_{\!x} L(\z)\|_*, \|\nabla_{\!y} L(\z)\|_*) \geq \delta \;\, \forall \z \in \M$. Set $C_\delta := 2L + \frac{8L^2}{\beta \delta}$. 
     Then the gap $\gap^{\FW}$~\eqref{eq:gap} of the SP-FW algorithm with step size $\gamma_t = \tfrac{\gap^{\FW}}{\|\st-\zt\|^2 C_\delta}$
     converges linearly as $\gap^{\FW} \leq g_0 \left( 1- \rho \right)^{t}$,
    where $\rho :=  \tfrac{\beta \delta}{16 C_\delta}$.
    \end{theorem}
  %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %


    %

%
%
%







\section{SP-FW in the bilinear setting}\label{sec:bilinear}

\paragraph{Fictitious play.} %
\label{par:the_fictitious_play_}
  In her thesis, \citet[\S~4.3.1]{hammond1984solving} pointed out that for the bilinear setting:
\begin{equation}\label{eq:pb_bilin}
  \min_{\x \in \Delta_p} \max_{\y \in \Delta_q} \x^\top M \y 
\end{equation}
where $\Delta_p$ is the probability simplex on $p$ elements,
the SP-FW algorithm with step size $\gamma_t = 1/ \left( 1+t \right)$ is equivalent to the fictitious play (FP) algorithm introduced by \citet{brown1951iterative}. The FP algorithm has been widely studied in the game literature. Its convergence has been proved by \citet{robinson1951iterative}, while \citet{shapiro1958note} showed that one can deduce from Robinson's proof a $O(t^{-1/(p+q-2)})$ rate.
Around the same time, \citet{karlin1960mathematical} conjectured that the FP algorithm converged at the better rate of $O(t^{-1/2})$, though this conjecture is still open and Shapiro's rate is the only one we are aware of.
Interestingly, \citet{daskalakis2014counter} recently showed that Shapiro's rate is also a lower bound if the tie breaking rule gets the worst pick an infinite number of times.
Nevertheless, this kind of adversarial tie breaking rule does not seems realistic since this rule is a priori defined by the programmer.
In practical cases (by setting a fixed prior order for ties or picking randomly for example), Karlin's Conjecture~\citep{karlin1960mathematical} is still open. Moreover, we always observed an empirical rate of at least $O(t^{-1/2})$ during our experiments, we thus believe the conjecture to be true for realistic tie breaking rules.

%
 \paragraph{Rate for SP-FW.} %
 \label{par:Rate for SP-FW.}
 Via the affine invariance of the FW algorithm and the fact that every polytope with $p$ vertices is the affine transformation of a probability simplex of dimension $p$, any rate for the fictitious play algorithm implies a rate for SP-FW.
\begin{corollary}\label{cor:Bilin}
For polytopes $\X$ and $\Y$ with $p$ and $q$ vertices respectively and $\L(\x,\y) = \x^\top M \y$, the SP-FW algorithm with step size $\gamma_t = \frac{1}{t+1}$ converges at the rate $ h_t = O \left(t^{-\frac{1}{p+q-2}} \right).$
\end{corollary}
 %
 This (very slow) convergence rate is mainly of theoretical interest, providing a safety check that the algorithm actually converges.
 Moreover, if Karlin's strong conjecture is true, we can get a $O (1 /\sqrt{t})$ worst case rate which is confirmed by our experiments. 
  

%
%
%
%
%
%
%
%
%

%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%


    %
    %
    %

\section{Experiments}\label{sec:experiments}

\begin{figure*}
    \centering
    \begin{subfigure}[t]{0.32\linewidth}
        \centering
        \includegraphics[height = .8 \linewidth, width = 1 \linewidth]{figures/SP-FW.pdf}
        \caption{SP in the interior, $d=30$}
        \label{fig:conv_int}
    \end{subfigure}
    \;
    \begin{subfigure}[t]{0.32\linewidth}
        \centering
        \includegraphics[width = 1 \linewidth]{figures/SP-AFW2.pdf}
        \caption{$\M$ is a polytope, $d=30$}
        \label{fig:conv_geom}
    \end{subfigure}
    \;
    \begin{subfigure}[t]{0.32\linewidth}
        \centering
        \includegraphics[height = .8 \linewidth,width = 1 \linewidth]{figures/SP-AFW-heuristic.pdf}
        \caption{$\M$ polytope, $d=30$, $\CondNumb<0$}
        \label{fig:better_step_size}
    \end{subfigure}
    \\
    \begin{subfigure}[t]{0.32\linewidth}
        \centering
        \includegraphics[width = 1 \linewidth]{figures/BIG5.pdf}
        \caption{Graphical games}
        \label{fig:graphical}
    \end{subfigure} %
    \;
    \begin{subfigure}[t]{0.32\linewidth}
        \centering
        \includegraphics[height = .75 \linewidth,width = 1 \linewidth]{figures/ocr_small_beta.pdf}
        \caption{OCR dataset, $R=0.01$.}
        \label{fig:beta_small}
    \end{subfigure}
    \;
    \begin{subfigure}[t]{0.32\linewidth}
        \centering
        \includegraphics[height = .75 \linewidth, width = 1 \linewidth]{figures/ocr_large_beta.pdf}
        \caption{OCR dataset, $R=5$.}
        \label{fig:beta_big}
    \end{subfigure}
    \caption{\small On Figures~\ref{fig:conv_int}, \ref{fig:conv_geom} and~\ref{fig:better_step_size}, we plot on a semilog scale the best gap observed $\min_{s\leq t} g^\FW_s$ as a function of $t$. For experiments~\ref{fig:graphical}, \ref{fig:beta_small} and~\ref{fig:beta_big}, the objective function is bilinear and the convergence is sublinear. An effective pass is one iteration for SP-FW or the subgradient method and $n$ iterations for SP-BCFW or SSG.
    We give more details about these experiments in Appendix~\ref{sec:details_on_the_experiments}.
    }
    \vspace{-2mm}
\end{figure*}
  \paragraph{Toy experiments.} %
  \label{par:experiments_on_toy_examples}
First, we test the empirical convergence of our algorithms on a simple saddle point problem over the unit cube in dimension~$d$ (whose pyramidal width has the explicit value $1/\sqrt{d}$ by Lemma~4 from \citet{lacoste2015global}). 
Thus $\X = \Y := [0,1]^d$ and the linear minimization oracle is simply $\text{LMO}(\cdot)\! = \!-0.5 \cdot(\text{sign}(\cdot)-\bm{1})$. 
We consider the following objective function:
  \begin{equation}
    \frac{\mu}{2}\|\x- \x^*\|_2^2 + (\x-\x^*)^\top M(\y-\y^*) - \frac{\mu}{2} \|\y-\y^*\|_2^2
  \end{equation}
for which we can control the location of the saddle point $(\x^*,\y^*) \in \M$. We generate a matrix $M$ randomly as $M \sim \mathcal U([-0.1,0.1]^{d\times d})$ and keep it fixed for all experiments. For the interior point setup~\eqref{enu:situation1}, we set~$(\x^*,\y^*) \sim \mathcal U ([0.25,0.75]^{2d})$, while we set~$\x^*$ and $\y^*$ to some fixed random vertex of the unit cube for the setup~\eqref{enu:situation2}. With all these parameters fixed, the constant $\CondNumb$ is a function of $\mu$ only. We thus vary the strong convexity parameter $\mu$ to test various $\CondNumb$'s.

We verify the linear convergence expected for the SP-FW algorithm for case~\eqref{enu:situation1} in Figure~\ref{fig:conv_int}, and for the 
SP-AFW algorithm for case~\eqref{enu:situation2} in Figure~\ref{fig:conv_geom}. 
As the adaptive step size (and rate) depends linearly on~$\CondNumb$, the linear rate becomes quite slow for small $\CondNumb$. In this regime (in red), the step size $2/(2+k(t))$ (in orange) can actually perform better, despite its theoretical sublinear rate.
%

Finally, figure \ref{fig:better_step_size} shows that we can observe a linear convergence of SP-AFW even if $\CondNumb$ is negative by using a different step size. In this case, we use the heuristic adaptive step size $\gamma_t := \gap / \tilde{C}$ where 
$\tilde{C} := L D_\X^2+LD_\Y^2+ L_{XY} L_{YX}\left( D_\X^2/\mu_\X + D_\Y^2/\mu_\Y \right) $. Here $\tilde{C}$ takes into account the coupling between the concave and the convex variable and is motivated from a different proof of convergence that we were not able to complete. The empirical linear convergence in this case is not yet supported by a complete analysis, highlighting the need for more sophisticated arguments.


\paragraph{Graphical games.} We now consider a bilinear objective $\L(\x,\y) = \x^\top M\y$ where exact projections on the sets is intractable, but we have a tractable LMO. The problem is motivated from the following setup. We consider a game between 
two universities ($A$ and $B$) that are admitting $s$ students and have to assign pairs of students into dorms.
  %
If students are unhappy with their dorm assignments, they will go to the other university. The game has a payoff matrix $M$ belonging to $\R^{(s(s-1)/2)^2}$ where $M_{ij,kl}$ is the expected tuition that $B$ gets (or $A$ gives up) if $A$ pairs student $i$ with $j$ and $B$ pairs student $k$ with $l$.
  %
Here the actions $\x$ and $\y$ are both in the marginal polytope of all perfect unipartite matchings. Assume that we are given a graph $G=(V,E)$ with vertices $V$ and edges $E$. For a subset of nodes $S \subseteq V$, let the induced subgraph $G(S)=(S,E(S))$. \citet{edmonds1965} showed that any subgraph forming a triangle can contain at most one edge of any perfect matching. This forms an exponential set of linear equalities which define the matching polytope ${\cal P}(G) \subset \mathbb{R}^E $ as
\begingroup
\setlength{\thinmuskip}{1.3mu}
\setlength{\medmuskip}{1.3mu}
\setlength{\thickmuskip}{1.3mu}
\begin{equation} 
\{ \x \,| \, \x_e \geq 0, \hspace{-2.1mm} %
\sum_{e \in E(S)}\hspace{-1.7mm} \x_e \leq k, \, \forall S \subseteq V,\, { |S|=2 k+1 },\forall e \in E \}.
\end{equation}
\endgroup
While this strategy space seems daunting, the LMO can be solved in ${\cal O}(s^3)$ time using the blossom algorithm~\citep{edmonds1965}. We run the SP-FW algorithm with $\stepsize_t = \nicefrac{2}{(t+2)}$ on this problem with $s = 2^j$ students for $j=3, \ldots, 8$ with results given in Figure~\ref{fig:graphical} ($d=s(s-1)/2$ in the legend represents the dimensionality of the $\x$ and $\y$ variables). The order of the complexity of the LMO is then $O(d^{3/2})$.
In Figure~\ref{fig:graphical}, the observed empirical rate of the SP-FW algorithm (using $\stepsize_t = \nicefrac{2}{(t+2)}$) is $O(1/t^2)$. Empirically, faster rates seem to arise if the solution is at a corner (a pure equilibrium, to be expected for random payoff matrices in light of~\citep{barany2007nash}).
 %

 \paragraph{Sparse structured SVM.} %
  \label{par:structured_svm}
  We finally consider a challenging optimization problem arising from structured prediction. We consider the saddle point formulation~\citep{taskar2006dualExtrag} for a $\ell_1$-regularized structured SVM objective that minimizes the primal cost function
%
%
$  p(\bm{w}) :=
  %
    \frac{1}{n} \sum_{i=1}^n \tilde H_i(\bm{w}) 
%
$,
where $\tilde H_i(\bm{w})= \max_{\y \in \mathcal Y_i} L_i(\y) - \innerProd{\bm{w}}{\bm{\psi}_i(\y)}$ is the structured hinge loss (using the notation from~\citet{lacoste2013block}). We only assume access to the linear oracle computing $\tilde H_i(\bm{w})$. Let $M_i$ have $\big(\bm{\psi}_i(\y)\big)_{\y \in \mathcal Y_i}$ as columns.
  We can rewrite the minimization problem as a bilinear saddle point problem:
  \vspace{-2mm}
  \begin{equation}
  \begin{aligned}
   & \qquad \min_{\|\bm{w}\|_1  \leq R} \frac{1}{n} \sum_i \Big( \max_{\y_i \in \mathcal Y_i} \; \bm{L}_i^\top \y_i - \bm{w}^\top M_i \y_i \Big)  \\
   & = \min_{\|\bm{w}\|_1 \leq R} \frac{1}{n} \sum_i \Big( \max_{\bm{\alpha}_i \in \Delta(|\mathcal Y_i|)} \!\!\bm{L}_i^\top \bm{\alpha}_i - \bm{w}^\top M_i \bm{\alpha}_i \Big). \label{eq:structuredSVM}
  \end{aligned}
  \end{equation}
  Projecting onto $\Delta(|\Y_i|)$ is normally intractable as the size of $|\Y_i|$ is exponential, but the linear oracle is tractable by assumption.
  We performed experiments with 100 examples from the OCR
  dataset ($d_\omega \!= \!4028$)~\citep{taskarmax}. We encoded the structure $\Y_i$ of the $i^{th}$ word with a Markov model: its $k^{th}$ character $\Y_i^{(k)}$ only depends on $\Y_i^{k-1}$ and $\Y_i^{k+1}$. In this case, the oracle function is simply the Viterbi algorithm~\citet{viterbi1967error}. 
 The average length of a word is approximately 8, hence the dimension of $\Y_i$ is $d_{\Y_i} \approx 26^2 \cdot 8 = 5408$ leading to a large dimension for $\Y$, $d_\Y := \sum_{i=1}^n d_{\Y_i} \approx  5 \cdot 10^5$.  
 We run the SP-FW algorithm with step size $\gamma_t = 1/(1+t)$ for which we have a convergence proof (Corollary~\ref{cor:Bilin}), and with $\gamma_t = 2/(2+t)$, which normally gives better results for FW optimization. We compare with the projected subgradient method (projecting on the $\ell_1$-ball is tractable here) with step size $O(1/\sqrt{t})$ (the subgradient of $\tilde H_i(\bm{w})$ is $-\bm{\psi}_i(\y_i^*)$). Following~\citet{lacoste2013block}, we also implement a block-coordinate (SP-BCFW) version of SP-FW and compare it with the stochastic projected subgradient method (SSG). As some of the algorithms only work on the primal and to make our result comparable to~\citet{lacoste2013block}, we choose to plot the primal suboptimality error $p(\bm{w}_t)- p^*$ for the different algorithms in Figure~\ref{fig:beta_small} and~\ref{fig:beta_big} (the $\bm{\alpha}_t$ iterates for the SP approaches are thus ignored in this error).
The performance of SP-BCFW is similar to SSG when we regularize the learning problem heavily (Figure~\ref{fig:beta_small}). 
However, under lower regularization (Figure~\ref{fig:beta_big}), SSG (with the correct step size scaling) is faster. This is consistent with the fact that $\bm{\alpha_t} \neq \bm{\alpha}^*$ implies larger errors on the primal suboptimality for the SP methods, but we note that an advantage of the SP-FW approach is that the scale of the step size is automatically chosen.
  %

%
%
%

\paragraph{Conclusion.}\label{sec:conclusion} 
We proposed FW-style algorithms for saddle-point optimization with the same attractive properties as FW, in particular only requiring access to a LMO. We gave the first convergence result for a FW-style algorithm towards a saddle point over polytopes by building on the recent developments on the linear convergence analysis of AFW.
However, our experiments let us believe that the condition $\CondNumb > 0$ is not required for the convergence of FW-style algorithms.
We thus conjecture that a refined analysis could yield a linear rate for the general uniformly strongly convex-concave functions in both cases~\eqref{enu:situation1} and~\eqref{enu:situation2}, paving the way for further theoretical work.

\clearpage
%
\subsubsection*{Acknowledgments} %
\label{par:acknowledgments} 
Thanks to N. Ruozzi and
A. Benchaouine for helpful discussions. Work supported in
part by DARPA N66001-15-2-4026, N66001-15-C-4032 and NSF
III-1526914, IIS-1451500, CCF-1302269.
%
%
%
  %
%
  \bibliographystyle{abbrvnat} 
  {
  \bibliography{bib}
  }

  \clearpage


  %
  %
  %

  \appendix
  \onecolumn
  \fontsize{11}{13}
  \selectfont

%
  {\huge  Appendix}
  \paragraph{Outline.} %
  \label{par:outline}
  Appendix~\ref{sec:away_step_frank_wolfe} provides more details about the saddle point away-step Frank-Wolfe (SP-AFW) algorithm. Appendix~\ref{sec:affine_invariant} is about the affine invariant formulation of our algorithms, therein, we introduce some affine invariant constants and prove relevant bounds. 
  Appendix~\ref{sec:relations_gaps} presents some relationships between the primal suboptimalities and dual gaps useful for the convergence proof.
  Appendix~\ref{appendix:analysis} gives the affine invariant convergence proofs of SP-FW and SP-AFW in the strongly convex function setting introduced in Section~\ref{sec:SP-FW_strong_convexity}. 
  Appendix~\ref{sec:strong_conv_proof} gives the proof of linear convergence of SP-FW in the strongly convex set setting as defined in Section~\ref{sec:strongly_set}.
  Finally, Appendix~\ref{sec:details_on_the_experiments} provides details on the experiments.
  %
  
  \section{Saddle point away-step Frank-Wolfe (SP-AFW)} %
  \label{sec:away_step_frank_wolfe}
  In this section, we describe our algorithms SP-AFW and SP-PFW with a main focus on how the away direction is chosen. We also rigorously define a \emph{drop step} and prove an upper bound on their number. In this section, we will assume that there exist two finites sets $\A$ and $\B$ such that 
  $\X = \conv(\A)$ and $\Y = \conv(\B)$.
  \paragraph{Active sets and away directions.} %
  \label{par:active_set}
   Our definition of \emph{active set} is an extension of the one provided in \citet{lacoste2015global}, we follow closely their notation and their results. Assume that we have the current expansion,
   %
   %
   %
  \begin{equation}
    \xt = \sum_{\vv_x \in \mathcal S_x^{(t)}} \alpha_{\vv_x}^{(t)} \vv_x
    \quad \text{where} \quad
     \mathcal S_x^{(t)} := \left\{ \vv_x \in \A\;;\; \alpha_{\vv_x}^{(t)} >0 \right\},
  \end{equation}
  and a similar one for $\yt$.
   Then, the current iterate has a sparse representation as a convex combination of all possible pairs of atoms belonging to $S_x^{(t)}$ and $S_y^{(t)}$, i.e. 
   \begin{equation} \label{eq:ActiveSetImplicit}
     \zt = \sum_{\vv \in \mathcal S^{(t)}} \alpha_{\vv}^{(t)} \vv
     \quad \text{where} \quad
     \mathcal S^{(t)} := \left\{ \vv \in \Vertices\;;\; \alpha_{\vv}^{(t)}:= \alpha_{\vv_x}^{(t)}\alpha_{\vv_y}^{(t)} > 0\right\}.
   \end{equation}
  The set $\mathcal S^{(t)}$ is the current (implicit) \emph{active set} arising from the special product structure of the domain and it defines potentially more away directions than proposed in~\citep{lacoste2015global} for AFW, and these are the directions that we use in SP-AFW and SP-PFW.
  Namely, for every corner $\vv=(\vv_x,\vv_y)$ and $\vv' = (\vv'_x,\vv'_y)$ already picked, $\x-\vv_x$ is a feasible directions in $\X$ and $\y-\vv_y'$ is a feasible direction in $\Y$.
  Thus the combination $(\x-\vv_x,\y-\vv'_y)$ is a feasible direction even if the particular corners $\vv_x$ and $\vv_y'$ have never been picked together. We thus maintain the iterates on $\X$ and $\Y$ as independent convex combination of their respective active sets of corners (Line~\ref{algLine:activeSet} of Algorithm~\ref{alg:AFW}).
  
  Note that after $t$ iteration, the current iterate $\zt$ is $t$-sparse whereas the size of the active set $\mathcal S^{(t)}$ defined in~\eqref{eq:ActiveSetImplicit} can be of size~$t^2$. Nevertheless, because of the block formulation of the away oracle (Line~\ref{algLine:awayCorner} in Algorithm~\ref{alg:AFW}), the away direction can be found in $O(t)$ since we only need to use $S_x^{(t)}$ and $S_y^{(t)}$ separately to compute the away direction in $\mathcal S^{(t)}$.  %
  Moreover, we only need to track at most $t$ corners in $\A$ and $t$ ones in $\B$ to get this bigger active set.
  %
  We can now define the maximal step size for an away direction.
   %
  %
  %
  %
  \paragraph{Maximal step size.} %
  \label{par:maximal_step size}
 For the standard AFW algorithm,  
 \citet{lacoste2015global} suggest to use the maximum step size $\stepmax = \alpha_{\vv^{(t)}}/(1- \alpha_{\vv^{(t)}})$ when using the away direction $\z^{(t)}-\vv^{(t)}$, to guarantee that the next iterate stays feasible. Because we have a product structure of two blocks, we actually consider more possible away directions by maintaining a separate convex combination on each block in our Algorithm~\ref{alg:AFW} (SP-AFW) and~\ref{alg:PFW} (SP-PFW). More precisely, suppose that we have $\xt = \sum_{\vv_x \in S^{(t)}_x} \alpha^{(t)}_{\vv_x} \vv_x$ and $\yt = \sum_{\vv_y \in S^{(t)}_y} \alpha^{(t)}_{\vv_y} \vv_y$, then the following maximum step size $\gamma_{\max}$ (for AFW) ensures that the iterate $\ztt$ stays feasible:
   \begin{equation} \label{eq:update}
    \ztt := \zt + \gamma_t  \dt_A 
    \quad \text{with} \quad \gamma_t \in [0,\gamma_{\max}]
    \quad \text{and} \quad
     \gamma_{\max} := \min\left\{\frac{\alpha^{(t)}_{\vt_x}}{1 -\alpha^{(t)}_{\vt_x}},\frac{\alpha^{(t)}_{\vt_y}}{1 -\alpha^{(t)}_{\vt_y}} \right\}.
    \end{equation}
    A larger $\stepsize_t$ makes one of the coefficients in the convex combination for the iterate negative, thus no more guaranteeing that the iterate stays feasible. A similar argument can be used to derive the maximal step size for the PFW direction in Algorithm~\ref{alg:PFW}.
  %
  \paragraph{Drop steps.} %
  \label{par:drop_steps}
     A \emph{drop step} is when $\gamma_t=\stepmax$ for the away-step update~\eqref{eq:update} \citep{lacoste2015global}. In this case, at least one corner is removed from the active set. We show later in Lemma~\ref{lemme:Lin} that we can still guarantee progress for this step, i.e. $w_{t+1} < w_t$, but this progress be arbitrarily small since $\stepmax$ can be arbitrarily small. \citet{lacoste2015global} shows that the number of drop steps for AFW is at most half of the number of iterations. Because we are maintaining two independent active sets in our formulation, we can obtain more drop steps, but we can still adapt their argument to obtain that the number of drop steps for SP-AFW is at most two thirds the number of iterations (assuming that the algorithm is initialized with only one atom per active set). In the SP-AFW algorithm, either a FW step is jointly made on both blocks, or an away-step is done on both blocks. Let us call~$A_t$ the number of FW steps (which potentially adds an atom in $S^{(t)}_x$ and $S^{(t)}_y$) and $D_t^{(x)}$ (resp $D_t^{(y)}$) the number of steps that removed at least one atom from $S^{(t)}_x$  ($S^{(t)}_y$). Finally, we call $D_t$ the number of \emph{drop steps}, i.e., the number of \emph{away steps} where at least one atom from $S^{(t)}_x$ or $S^{(t)}_y$ have been removed (and thus $\stepsize_t = \stepmax$ for these). Because a step is either a FW step or an away step, we have:
     \begin{equation} \label{eq:NumberSteps}
     A_t + D_t \leq t \, .
     \end{equation}
     We also have that $D_t^{(x)} + D_t^{(y)} \geq D_t$ by definition of $D_t$. Because a FW step adds at most one atom in an active set while a drop step removes one, we have (supposing that $|S^{(0)}_x|=| S^{(0)}_y|=1$):
    \begin{equation}
      1 + A_t -  D_t^{(x)} \geq |S^{(t)}_x| \quad \text{and} \quad 1 + A_t -  D_t^{(y)} \geq |S^{(t)}_y| .
    \end{equation}
    Adding these two relations, we get:
    \begin{equation}
    2 + 2 A_t \geq |S^{(t)}_x| + |S^{(t)}_y| + D_t^{(x)} + D_t^{(y)} \geq 2 + D_t \, ,
    \end{equation}
    using the fact that each active set as at least one element. We thus obtain
    $D_t \leq 2 A_t$. Combining with~\eqref{eq:NumberSteps}, we get:
    \begin{equation} \label{eq:upper_bound_drop_steps}
    D_t \leq \frac{2}{3} t \, ,
    \end{equation}
  as claimed.
  %

%
%
  
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%

  %
\section{Affine invariant formulation of SP-FW}
  \label{sec:affine_invariant}
  In this section, we define the affine invariant constants of a convex function $f$ and their extension to a convex-concave function $\L$. 
  These constants are important as the FW-type algorithms are affine invariant if their step size are defined using affine invariant quantities. 
  We can upper bound these constants using the non affine invariant constants defined in the main paper. 
  Hence a convergence rate with affine invariant constants will immediately imply a rate with the constant introduced in the main paper.
  \subsection{The Lipschitz constants} %
    \label{sub:the_lipschitz_constants}
    We define the Lipschitz constant~$L$ of the gradient of the function $f$ with respect to the norm~$\| \cdot\|$ by using a dual pairing of norms, i.e. $L$ is a constant such that 
    \begin{equation}
       \label{def:lip}
        \forall \x, \x' \in \X, \qquad \|\nabla f(\x) - \nabla f (\x') \|_* \leq L \| \x - \x'\|,
     \end{equation} 
     where $\|\y\|_* := \sup_{\x \in \R^d, \|\x\| \leq 1} \y^{T} \x$ is the dual norm of $\|\cdot \|$. For a convex-concave function, we also consider the partial Lipschitz constants with respect to different blocks as follows.

     For more generality, we consider the dual pairing of norms $({\|\cdot\|_\X}, {\|\cdot\|_{\X^*}})$ on $\X$, and similarly $({\|\cdot\|_\Y}, {\|\cdot\|_{\Y^*}})$ on $\Y$. We also define the norm on the product space $\X \times \Y$ as the $\ell_1$-norm on the components: $\|(\x,\y)\|_{\X \times \Y} := \|\x\|_\X + \|\y\|_\Y$. We thus have that the dual norm of $\X \times \Y$ is the $\ell_\infty$-norm of the dual norms: $\|(\x,\y)\|_{(\X \times \Y)^*} = \max ( \|\x\|_{\X^*}, \|\y\|_{\Y^*})$.
     The \emph{partial} Lipschitz constants $L_{XX},L_{YY},L_{XY} \text{ and } L_{YX}$ of the gradient of the function $\L$ with respect to these norms are the constants such that for all $\x, \,\x' \in \X$ and $\y, \,\y' \in \Y$,
     \begin{equation}
     \label{def:lipL}
     \begin{aligned}
        \hspace{-3mm} \|\nabla_x \L (\x,\y) - \nabla_x \L(\x',\y) \|_{\X^*} &\leq L_{XX} \| \x - \x'\|_\X, &\! 
        \|\nabla_y \L (\x,\y) - \nabla_y \L(\x,\y') \|_{\Y^*} &\leq L_{YY}\,\| \y - \y'\|_\Y, \\
        \hspace{-3mm} \|\nabla_x \L (\x,\y) - \nabla_x \L(\x,\y') \|_{\X^*} &\leq L_{XY} \| \y - \y'\|_\Y,  
        &\!
        \|\nabla_y \L (\x,\y) - \nabla_y \L(\x',\y) \|_{\Y^*} &\leq L_{YX} \,\| \x - \x'\|_\X.
     \end{aligned}
     \end{equation}
  Note that the cross partial Lipschitz constants $L_{XY}$ and $L_{YX}$ do not necessarily use a dual pairing as $\X$ and $\Y$ could be very different spaces. 
  On the other hand, as the possibilities in~\eqref{def:lipL} are special cases of~\eqref{def:lip} when considering the $\ell_1$-norm of this product domain, one can easily deduce that the partial Lipschitz constants can always be taken to be smaller than the full Lipschitz constant for the gradient of $\L$, i.e., we have that $L \geq \max(L_{XX}, L_{XY}, L_{YX}, L_{YY})$.
  %

    
    %
    %
    \subsection{The curvature: an affine invariant measure of smoothness} \label{sub:curv}
    
    To prove the convergence of the Frank-Wolfe algorithm, the typical affine invariant analysis proof in the FW literature assumes that the curvature of the objective function is bounded, where the curvature is defined by~\citet{jaggi2013revisiting} for example. 
    We give below a slight generalization of this curvature notion in order to handle the convergence analysis of FW with away-steps.\footnote{\label{foot:Cfcomment}The change is to consider the more general directions $\s - \vv$ instead of just $\s - \x$, and also any feasible positive step size. 
    See also Footnote~8 in~\citesup{lacoste2013affine} for a related discussion. A different (bigger) constant was required in~\citep{lacoste2015global} for the analysis of AFW because they used a line-search.} 
    It has the same upper bound as the traditional curvature constant (see Proposition~\ref{prop:C_fbounded}).

    \paragraph{Curvature.}[Slight generalization of~\citet{jaggi2013revisiting}]  
    \label{par:curvature}
    Let $f : \X \to \R$ be a convex function, we define the curvature $C_f$ of $f$ as 
      \begin{equation} 
    \label{CurvB}
      \Cf := 
        \sup_{\substack{\scalebox{0.65}{
        $\begin{matrix}
                    \x, \s, \vv \in \X , \\ \stepsize > 0 \text{ s.t.} \\
                    \x_{\stepsize} := \x + \stepsize \dd \in \X \\ 
                    \text{with } \dd := \s - \vv 
                  \end{matrix}$
          }}}
          \frac{2}{\stepsize^2}( f(\x_{\stepsize})-f(\x)- \stepsize \prodscal{\dd}{ \nabla f(\x)}). 
          \end{equation}
    Note that only the \emph{feasible} step sizes~$\stepsize$ are considered in the definition of~$\Cf$, i.e., $\stepsize$ such that $\x_{\stepsize} \in \X$. 
    If the gradient of the objective function is Lipschitz continuous, the curvature is upper bounded. 
    \begin{proposition}[Simple generalization of Lemma~7 in~\citet{jaggi2013revisiting}]
        \label{prop:C_fbounded}
    Let $f$ be a convex and continuously differentiable function on~$\X$ with its gradient $\nabla f$ L-Lipschitz continuous w.r.t. some norm $\|.\|$ in dual pairing over the domain $\X$. Then
      \begin{equation} \label{eq:CfUpperBound}
      C_f \leq D_\X^2 L \, ,
      \end{equation}
      where $D_\X : = \sup_{\x,\x' \in \X} \|\x - \x'\|$ is the diameter of $\X$.
  \end{proposition}  
  \proof[Lemma 1.2.3 in~\citetsup{nesterov2004introductory}, \citet{jaggi2013revisiting}] 
    Let $\x, \s, \vv \in \X$, set $\dd := \s - \vv$ and $\x_{\stepsize} = \x + \gamma \dd $ for some $\stepsize > 0$ such that $\x_{\stepsize} \in \X$. Then by the fundamental theorem of calculus, 
    \begin{equation}
      f(\x_{\stepsize}) = f(\x) + \int_0^\gamma \prodscal{ \dd }{\nabla f(\x + t \dd )} dt.
    \end{equation}
    Hence, we can write 
    \begin{align}
      f(\x_{\stepsize})-f(\x)- \stepsize \prodscal{\dd}{ \nabla f(\x)}
      & = \int_0^\gamma \prodscal{ \dd }{\nabla f(\x + t \dd) -  \nabla f(\x)} dt  \notag\\
      & \leq \|\dd\| \int_0^\gamma  \| \nabla f(\x + t \dd) - \nabla f(\x)\|_*dt \notag \\
      & \leq D_\X^2 L\int_0^\gamma t dt \notag \\ 
      & \leq \frac{\gamma^2}{2} D_\X^2 L.
    \end{align}
  Thus for all $\x,\s,\vv \in \X$ and $\x_{\stepsize} = \x + \stepsize (\s - \vv)$ for $\stepsize > 0$ such that $\x_{\stepsize} \in \X$, we have
  \begin{equation}
    \frac{2}{\gamma^2}( f(\x_{\stepsize})-f(\x)- \stepsize \prodscal{\s - \vv}{ \nabla f(\x)}) \leq L D_\X^2 .
  \end{equation}
  The supremum is then upper bounded by the claimed quantity. 
  \endproof
  
  \citetsup[Appendix C.1]{osokin2016gapBCFW} illustrate well the importance of the affine invariant curvature constant for Frank-Wolfe algorithms in their paragraph titled ``Lipschitz and curvature constants". They provide a concrete example where the wrong choice of norm for a specific domain~$\X$ can make the upper bound of Proposition~\ref{prop:C_fbounded} extremely loose, and thus practically useless for an analysis.
  
  We will therefore extend the curvature constant to the convex-concave
    function $\L$ by simply defining it as the maximum of the curvatures of the functions
    belonging to the family $\left(\x' \mapsto \L(\x',\y),\y' \mapsto
      -\L(\x,\y')\right)_{\x \in \X, \y \in \Y}$ (see Section~\ref{sub:curvature_and_interior_strong_convexity_constant_for_a_convex_concave_function}). But before that, we review affine invariant analogues of the strong convexity constants that will be useful for the analysis.
    %



  %



  \subsection{Affine invariant measures of strong convexity} %
  \label{sub:affine_invariant_measures_of_strong_convexity}

  In this section, we review two affine invariant measures of strong convexity that were proposed by~\citetsup{lacoste2013affine}\citep{lacoste2015global} for the affine invariant linear convergence analysis of the standard Frank-Wolfe algorithm (using the ``interior strong convexity constant'') or the away-step Frank-Wolfe algorithm (using the ``geometric strong convexity constant''). We will re-use them for the affine invariant analysis of the convergence of SP-FW or SP-AFW algorithms. In a similar way as the curvature constant~$\Cf$ includes information about the constraint set~$\X$ and the Lipschitz continuity of the gradient of~$f$ together, these constants both include the information about the constraint set~$\X$ and the strong convexity of a function~$f$ together.

  %
  \paragraph{Interior strong convexity constant.} %
  \label{par:interior_strong_convexity_constant_}
  [based on \citetsup{lacoste2013affine}]
  Let $\xc$ be a point in the relative interior of $\X$.
   The \emph{interior strong convexity constant} for~$f$ with respect to the reference point~$\xc$ is defined as
    \begin{equation}
  \label{def:int_strong}
    \mu^{\xc}_f :=  \inf_{\substack{\scalebox{0.65}{
        $\begin{matrix}
                  \x \in \X \setminus \{\xc\} \\
                  \s = \bar{\s}(\x,\xc,\X) \\
                  \gamma \in (0,1], \\
                  \z = \x + \gamma(\s-\x)
        \end{matrix}$
        }}}
       \frac{2}{\gamma^2} \left(f(\z)-f(\x)- \prodscal{\z-\x}{ \nabla f(\x)}\right).
       \end{equation}
  Here, we follow the notation of \citetsup{lacoste2013affine} and take the point $\s$ to be the point where the ray from $\x$ to the reference point $\xc$ pinches the boundary of the set $\X$,
    i.e. $\bar{\s}(\x,\xc,\X) := {\rm ray}(\x,\xc) \cap \partial \X$, where $ \partial \X$ is the boundary of the convex set~$\X$.
    
  We note that in the original definition~\citepsup{lacoste2013affine}, $\xc$ was the (unique) optimum point for a strongly convex function~$f$ over~$\X$. The optimality of~$\xc$ is actually not needed in the definition and so we generalize it here to any point~$\xc$ in the relative interior of~$\X$, as this will be useful in our convergence proof for SP-FW.

  For completeness, we include here the important lower bound from~\citepsup{lacoste2013affine} on the interior strong convexity constant in terms of the strong convexity of the function~$f$.

  \begin{proposition}[Lower bound on~$\mu^{\xc}$ from {\citetsup[Lemma~2]{lacoste2013affine}}] \label{prop:delta}
  Let $f$ be a convex differentiable function and suppose that $f$ is strongly convex w.r.t. to some arbitrary norm $\norm{\cdot}$ over the domain~$\X$ with strong-convexity constant $\mu_f > 0$. Furthermore, suppose that the reference point $\xc$ lies in the relative interior of~$\X$, i.e.,
  $\delta_c := \min_{\s \in \partial \X}
    \; ||\s-\xc|| >0$. Then the interior strong convexity constant~$\mu^{\xc}_f$~\eqref{def:int_strong} is lower bounded as follows:
    \begin{equation} \mu^{\xc}_f \geq \mu_f \delta_c^2. \end{equation}
  \end{proposition}



  \proof 
  Let $\x$ and $\z$ be defined as in~\eqref{def:int_strong}, i.e., $\z = \x + \stepsize (\s - \x)$ for some $\stepsize > 0$ and where $\s$ intersects the boundary of $\X$ with the ray going from $\x$ to $\xc$.
  By the strong convexity of~$f$, we have
    \begin{equation} \label{eq:prop7strongConvex}
    f(\z)-f(\x) - \prodscal{\z-\x}{\nabla f(\x)} \geq ||\z-\x||^2 \frac{\mu_f}{2}
    = \stepsize^2 ||\s-\x||^2 \frac{\mu_f}{2}.
    \end{equation}
  From the definition of $\s$, we have that $\xc$ lies between $\x$ and $\s$ and thus:
  $||\s-\x||
  \geq ||\s - \xc|| \geq \delta_c$. Combining with~\eqref{eq:prop7strongConvex}, we conclude
    \begin{equation} f(\z)-f(\x) - \prodscal{\z-\x}{\nabla f(\x)} \geq
   \stepsize^2 \delta_c^2 \frac{\mu_f}{2}, \end{equation}
  and therefore 
    \begin{equation} \mu^{\xc}_f \geq \delta_c^2 \mu_f. \end{equation}
  \endproof
  %
  
  We now present the affine invariant constant used in the global linear convergence analysis of Frank-Wolfe variants when the convex set~$\X$ is a polytope. The \emph{geometric strong convexity constant} was originally introduced by~\citetsup{lacoste2013affine} and~\citep{lacoste2015global}. To avoid any ambiguity, we will re-use their definitions verbatim in the rest of this section, starting first with a few geometrical definitions and then presenting the affine invariant constant. In these definitions, they assume that a \emph{finite} set~$\A$ of vectors (that they call \emph{atoms}) is given such that~$\X = \conv(\A)$ (which always exists when~$\X$ is a polytope).  

  \paragraph{Directional Width.}[\citet{lacoste2015global}]
  The \emph{directional width} of a set $\A$ with respect to a direction $\r$
  is defined as $\dirW(\A,\r) := \max_{\s, \vv \in\A}
  \big\langle \frac{\r}{\normEucl{\r}}, \s - \vv \big\rangle$. The \emph{width} of~$\A$
  is the minimum directional width over all possible directions in its affine
  hull.

  \paragraph{Pyramidal Directional Width.}[\citet{lacoste2015global}] We define the \emph{pyramidal directional
  width} of a set $\A$ with respect to a direction $\r$ and a base point
  $\x \in \domain$ to be
  \begin{equation} \label{eq:TruePdirW}
  \PdirW(\A,\r, \x) := \min_{\SS \in \SS_{\x}} \dirW( \SS \cup
  \{\s(\A,\r) \} , \; \r) = \min_{\SS \in \SS_{\x}} \max_{\s \in
  \A, \vv \in \SS} \textstyle \big\langle \frac{\r}{\normEucl{\r}}, \s - \vv \big\rangle
  ,
  \end{equation}
  where $\SS_{\x} := \{ \SS \, | \, \SS \subseteq \A$ such that $\x$ is a
  proper\footnote{By \emph{proper} convex combination, we mean that all
  coefficients are non-zero in the convex combination.} convex combination of
  all the elements in $\SS\}$, and $\s(\A,\r) := \argmax_{\vv \in
  \A} \innerProdCompressed{\r}{\vv}$ is the FW atom used as a summit, when using the convention in this section that $\r := -\nabla f(\x)$. 


  \paragraph{Pyramidal Width.}[\citet{lacoste2015global}]
  To define the pyramidal width of a set, we take the minimum over the cone of
  possible \emph{feasible} directions~$\r$ (in order to avoid the problem of zero width).\\
  %
  A direction~$\r$ is \emph{feasible} for $\A$ from $\x$ if it points inwards $\conv(\A)$, 
  %
  (i.e. $\r \in \text{cone}(\A-\x)$).\\
  We define the \emph{pyramidal width} of a set $\A$ to be the smallest pyramidal width of all its faces, i.e.
  \begin{equation} \label{eq:Pwidth}
  \PWidth(\A) := \displaystyle \min_{\substack{\Kface \in \textrm{faces}(\conv(\A)) \\
                            \x \in \Kface \\
                            \r \in \text{cone}(\Kface-\x) \setminus \{\0\}} 
                                     } \PdirW(\Kface \cap \A,\r, \x)                   .                               
  \end{equation}
  %



\paragraph{Geometric strong convexity constant.}[\citet{lacoste2015global}] %
\label{par:strong_}
%
   The \emph{geometric strong convexity constant} of~$f$ (over the set of atoms $\A$ which is left implicit) is: 
    \begin{equation} \label{def:geom_strong}
    \mu^{\away}_f := 
            \underset{ \x \in \X}{\inf}
            \inf_{\substack{\scalebox{0.6}{
            $\begin{matrix}
                          \x^* \in \X  \\
                          s.t \;   \prodscal{\nabla f(\x)}{\x^* - \x} <0
             \end{matrix}$
              }}}
          \frac{2}{\gamma^\away(\x,\x^*)^2}( f(\x^*)-f(\x)- \prodscal{\x^*-\x}{ \nabla f(\x)}) 
      \end{equation}
  %
  where $\gamma^\away(\x,\x^*) := \frac{\prodscal{-\nabla f(\x)}{\x^*-\x}}{\prodscal{-\nabla f(\x)}{\s_f(\x)-\vv_f(\x)}}$ and $\X = \conv(\A)$. The quantity $\s_f(\x)$ represents the FW corner picked when running the FW algorithm on~$f$ when at~$\x$; while $\vv_f(\x)$ represents the worst-case possible away atom that AFW could pick (and this is where the dependence on $\A$ appears). We now define these quantities more precisely. Recall that the set of possible active sets is $\SS_{\x} := \{ \SS \, | \, \SS \subseteq \A$ such that $\x$ is a
  proper convex combination of
  all the elements in $\SS\}$.
  For a given set~$\SS$, we write $\vv_{\SS}(\x) := \argmax_{\vv \in \SS }
  \left\langle \nabla f(\x), \vv \right\rangle$ for the away atom in the
  algorithm supposing that the current set of active atoms is $\SS$.
  Finally, we define $\vv_f(\x) := \hspace{-3mm}\displaystyle\argmin_{\{\vv = \vv_{\SS}(\x)
  \,|\, \SS \in \SS_{\x} \}} \textstyle \hspace{-3mm}\left\langle \nabla f(\x), \vv
  \right\rangle$ to be the worst-case away atom (that is, the atom which would
  yield the smallest away descent). An important property coming from this definition that we will use later is that for $\st$ and $\vt$ being possible FW and away atoms (respectively) appearing during the AFW algorithm (consider Algorithm~\ref{alg:AFW} ran only on~$\X$), then we have:
  \begin{equation}
    \label{eq:gammaA}
    \gap^{\PW} := \prodscal{\st - \vt}{- \nabla f (\xt)} \geq \prodscal{\s_f(\xt) - \vv_f(\xt)}{- \nabla f (\xt)}.
  \end{equation}

  The following important theorem from~\citep{lacoste2015global} lower bounds the geometric strong convexity constant of~$f$ in terms of both the strong convexity constant of $f$, as well as the pyramidal width of~$\X = \conv{(\A)}$ defined as $\PWidth(\A)$~\eqref{eq:Pwidth}.

  \begin{proposition}[Lower bound for~$\strongConvAFW$ from {\citet[Theorem 6]{lacoste2015global}}]
  \label{thm:muFdirWinterpretation2}
  Let $f$ be a convex differentiable function and suppose that $f$ is
  $\mu$-\emph{strongly convex} w.r.t. to the Euclidean norm
  $\normEucl{\cdot}$ over the domain $\domain=\conv(\A)$ with strong-convexity constant $\mu
  \geq 0$. Then
  \begin{equation} \label{eq:muDirWidthBound}
  \strongConvAFW \geq \mu \cdot \left( \PWidth(\A) \right)^2.
  \end{equation}
  \end{proposition}
  
  The pyramidal width~\eqref{eq:Pwidth} is a geometric quantity with a somewhat intricate definition. Its value is still unknown for many sets (though always strictly positive for finite sets), but \citet[Lemma~4]{lacoste2015global} give its value for the unit cube in $\R^d$ as $1/{\sqrt{d}}$. 
  


\subsection{Curvature and interior strong convexity constant for a convex-concave function} %
\label{sub:curvature_and_interior_strong_convexity_constant_for_a_convex_concave_function}

  In this subsection, we propose simple convex-concave extensions of the definitions of the affine invariant constants defined introduced in the two previous sections.
    
  To define the convex-concave curvature, we introduce the sets $\mathcal{F}$ and $\mathcal G$ of the marginal convex functions. 
    \begin{equation}\label{eq:F_x}
       \mathcal{F} := \{\x' \mapsto \L(\x',\y)  \}_{\y \in \Y} 
       \quad \text{and} \quad
        \mathcal{G} := \{\y' \mapsto -\L(\x, \y') \}_{\x \in \X} .
    \end{equation}
  Let $\L : \M \to \R$ a convex-concave function, we define the curvature pair $(C_{\L_x},C_{\L_y})$ of $\L$ as 
    \begin{equation} \label{eq:CLx}
    (C_{\L_x},C_{\L_y}) := \left(\underset{f \in \mathcal{F}}
        {\sup}\; C_{f},\underset{g \in \mathcal{G} }
        {\sup}\; C_{g} \right).
      \end{equation}
 and the curvature of $\L$ as
  \begin{equation}\label{eq:CL}
  C_\L := \frac{C_{\L_x}+C_{\L_y}}{2}. 
  \end{equation}
 An upper bound on this quantity follows directly from the upper bound on the convex case (Lemma~7 of \citet{jaggi2013revisiting}, repeated in our Proposition~\ref{prop:C_fbounded}) :


  \begin{proposition}\label{prop:Cbounded}
    Let $\L: \M \to \R$ be a differentiable convex-concave function. If $\X$ and $\Y$ are compact and $\nabla\L$ is Lipschitz continuous, then the curvature of $\L$ is bounded by $\frac{1}{2}(L_{XX} D_\X^2+L_{YY}D_\Y^2)$, where $L_{XX}$ (resp $L_{YY}$) is the largest Lipschitz constant respect to $\x$ ($\y$) of $\x \mapsto \nabla_x \L(\x,\y)$ ($\y \mapsto \nabla_y \L(\x,\y)$).
  \end{proposition}
  \proof
    Let $f$ in $\mathcal{F}$,
    \begin{equation}
     C_f \leq Lip(\nabla f) D_\X^2 \leq L_{XX} D_\X^2.
     \end{equation}
     Similarly, let $g$ in $\mathcal{G}$,
     \begin{equation}
     C_g \leq Lip(\nabla g) D_\Y^2 \leq L_{YY} D_\Y^2.
     \end{equation}
     Consequently,
     \begin{equation}
     C_\L = \frac{1}{2}(\underset{f \in \mathcal{F}}{\sup} C_f + \underset{g \in \mathcal{G}}{\sup} C_g) \leq \frac{1}{2} (L_{XX}D_\X^2 + L_{YY} D_\Y^2).
     \end{equation} 
     Where $D_\X$ and $D_\Y$ are the respective diameter of $\X$ and $\Y$.
  \endproof
  Note that $L_{XX}$ and $L_{YY}$ are upper bounded by the global Lipschitz constant of $\nabla \L$.
  Similarly, we define various notions of strong convex-concavity in the following.
\paragraph{Uniform strong convex-concavity constant.} %
\label{par:uniform_strong_convex_concavity_constant}
%
    The uniform strong convex-concavity constants is defined as 
    \begin{equation}
  \label{def:strongL}
      (\mu_\X,\mu_\Y) := \left(\underset{f \in \mathcal{G}}
        {\inf} \; \mu_f,\underset{g \in \mathcal{G}}
        {\inf} \; \mu_g\right)
    \end{equation}
    where $\mu_f$ is the strong convexity constant of $f$ and $\mu_g$ the strong convexity of $g$.


  Under some assumptions this quantity is positive.

      
  \begin{proposition} \label{prop:strongpos}
    If the second derivative of $\L$ is continuous, $\X$ and $\Y$ are compact and if for all $f \in \mathcal{F}\cup \mathcal G, \,\mu_f >0$, then
     $\mu_\X$ and $\mu_\Y$ are positive.  
  \end{proposition}

  \proof
    Let us introduce $H_x(\x,\y):= \nabla_x^2 \L(\x,\y)$ the Hessian of the function $\x \mapsto \L(\x,\y)$. We want to show that the smallest eigenvalue is uniformly bounded on $\M$. We know that the smallest eigenvalue lower bounds $\mu_\X$,
      \begin{equation} 
        \mu_{\X} \geq \inf_{\substack{\scalebox{0.65}{
                $\begin{array}{ll}
                    \; (\x,\y) \in \M \\
                    \|\u\|_2=1
                \end{array}$
                }}} \prodscal{\u}{H_x(\x,\y) \cdot\u}.
      \end{equation}
    But $H_x(\cdot)$ is continuous (because $\nabla_x^2 \L(\cdot)$ is continuous by assumption) and then the function $(\u,\x,\y) \mapsto \prodscal{\u}{H_x(\x,\y) \cdot\u}$ is continuous. 
    Hence since $\M$ and the unit ball are compact, the infimum is a minimum which can't be 0 by assumption. Hence $\mu_\X$ is positive. 
    Doing the same thing with the smallest eigenvalue of $-\nabla_y^2 \L(\x,\y)$, we get that 
    $\mu_\Y>0$.
  \endproof
  A common  family of saddle point objectives is of the form $f(x) +x^TMy - g(y)$. In this case, we get simply that $\left( \mu_\X, \mu_\Y \right) = (\mu_f,\mu_g).$
  An equivalent definition for the uniform strong convex-concavity constant is: $\L$ is $(\mu_\X,\mu_\Y)$-uniform strongly convex-concave function if
  \begin{equation}
    \left( \x,\y \right) \mapsto\L(\x,\y) - \frac{\mu_\X}{2} \|\x\|^2 + \frac{\mu_\Y}{2} \|\y\|^2
  \end{equation}
  is convex-concave.

  The following proposition relates the distance between the saddle point and the values of the function. It is a direct consequence from the uniform strong convex-concavity definition~\eqref{def:strongL}.
  \begin{proposition}\label{prop:strongL}
    Let $\L$ be a uniformly strongly convex-concave function and $(\x^*,\y^*)$ the saddle point of $\L$. 
    %
    Then we have for all $\x$ in $\X$ and $\y \in \Y$,
    \begin{equation} \sqrt{\L(\x,\y^*) - \L^*} \geq \|\x^*-\x\|\sqrt{\frac{\mu_\X}{2}} \quad  
     \text{and} \quad
     \sqrt{\L^* - \L(\x^*,\y)} \geq \|\y^*-\y\|\sqrt{\frac{\mu_\Y}{2}}.
    \end{equation} 
  \end{proposition}


  \proof 
    The saddle point $(\x^*,\y^*)$ is the optimal point of the two strongly convex functions $\x \mapsto \L(\x,\y^*)$ and the function $\y \mapsto -\L(\x^*,\y)$, so we can use the property of strong convexity on each function and the fact that $\mu_\X$ lower bounds the strong convexity constant of $\L(\cdot,\y^*)$ (and similarly for $\mu_\Y$ with $-\L(\x^*,\y)$) as per the definition~\eqref{def:strongL}, to get the required conclusion.
  \endproof
  Now we will introduce the uniform strong convex-concavity constants relatively to our saddle point.
\paragraph{Interior strong convex-concavity.} %
\label{par:interior_strong_convex_concavity}
%
   The SP-FW interior strong convex-concavity constants (with respect to the reference point $(\xc, \yc)$) are defined as: 
    \begin{equation} 
  \label{def:interior_strongL}
    \left(\mu^{\xc}_\L,\mu^{\yc}_\L \right) := \left(\inf_{f \in \mathcal F} \mu^{\xc}_f, \inf_{g\in \mathcal G} \mu^{\yc}_g \right)
    \end{equation}
    where $\mu^{\xc}_f$ is the  interior strong convexity constant of $f$ w.r.t to the point $\xc$ and $ \mu^{\yc}_g$ is the interior strong convexity constant w.r.t to the point $\yc$. The sets $\mathcal F$ and $\mathcal G$ are defined in~\eqref{eq:F_x}. We also define the smallest quantity of both (with the reference point $(\xc, \yc)$ implicit):
    \begin{equation}
    \muIntL = \min\{\mu^{\xc}_\L, \mu^{\yc}_\L\}.
    \end{equation}
    We can lower bound this constant by a quantity depending on the uniform strong convexity constant and the distance of the saddle point to the boundary.
   The propositions on the strong convex-concavity directly follow from the previous definitions and the analogous proposition on the convex case (Proposition~\ref{prop:delta})

  \begin{proposition}\label{prop:deltaL}
    Let $\L$ be a convex-concave function. If the reference point
    $(\xc,\yc)$ belongs to the relative interior of $\X \times \Y$ and if the function
    $\L$ is strongly convex-concave with a strong convex-concavity constant $\mu > 0$,
    then $\muIntL$ is lower bounded away from zero. More precisely, define $\delta_x :=\min_{\s_x\in \partial\X } \; \|\s_x-\xc\|>0$ and $\delta_y := \min_{\s_y \in \partial\Y}\|\s_y-\yc\|$. Then we have,
    \begin{equation} \mu^{\xc}_\L \geq  \mu_\X \delta_x^2 
    \qquad \text{and}\qquad
    \mu^{\yc}_\L \geq \mu_\Y \delta^2_y. \end{equation}
  \end{proposition}



  \proof 
     Using the Proposition~\ref{prop:delta} we have, 
      \begin{equation}
        \mu^{\xc}_f \geq \mu_f \cdot \delta_x^2 \geq \mu_\X\cdot \delta_x^2,
      \end{equation}
     and,
        \begin{equation}
        \mu^{\yc}_g \geq \mu_g \cdot \delta_y^2 \geq \mu_\Y\cdot \delta_y^2.
      \end{equation}
  \endproof
  When the saddle point is not in the interior of the domain, we define next a constant that takes in consideration the geometry of the sets. If the sets are polytopes, then this constant is positive.
\paragraph{Geometric strong convex-concavity.} %
\label{par:geometric_strong_convex_concavity}
%
   The SP-FW \emph{geometric} strong convex-concavity constants are defined analogously as the interior strong convex-concavity constants,
    \begin{equation} 
  \label{def:geom_strongL}
      \left(\mu^{\away}_{\L_x}, \mu^{\away}_{\L_y}\right) := \left( \min_{f \in \mathcal F} \mu^{\away}_f,  \min_{g \in \mathcal G} \mu^{\away}_g \right) \, ; \quad \mu^{\away}_\L:=\min \left(\mu^{\away}_{\L_x}, \mu^{\away}_{\L_y}\right)  ,
    \end{equation}
    where $\mu^{\away}_f$ is the  geometric strong convexity constant of $f \in \mathcal F$ (over $\A$) as defined in~\eqref{def:geom_strong} (and similarly $\mu^{\away}_g$ is the geometric strong convexity constant of $g \in \mathcal G$ over $\B$).

  It is straightforward to notice that the lower bound on the geometric strong convexity constant (Proposition~\ref{thm:muFdirWinterpretation2}) can be extended to the geometric strong convex-concavity constants (where $\mu_\X$ and $\mu_\Y$ are now assumed to be defined with respect to the Euclidean norm): 
  \begin{equation} \label{eq:muAwayLInequality}
  \mu^{\away}_{\L_x} \geq \mu_\X \PWidth(\A)^2 \quad \text{ and } \quad \mu^{\away}_{\L_y} \geq \mu_\Y \PWidth(\B)^2 .
  \end{equation} 
  %


  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %

  %




%

\subsection{The bilinearity coefficient}

In our proof, we need to relate 
  the gradient at the point $(\xt,\yt)$ with the one at
  the point $(\xt, \y^*)$. We can use the Lipschitz continuity of the gradient for this. We define below affine invariant quantities that can upper bound this difference.
  %
\paragraph{Bilinearity coefficients.} %
\label{par:bilinearity_coefficients}
%
  let $\L$ be a strongly convex-concave function, and let $(\x^*,\y^*)$ be its unique saddle point. We define the bilinearity coefficients $(M_{XY},M_{YX})$ as, 
    \begin{equation}   \label{def:M}
    M_{XY} = \hspace{-2mm} \sup_{\substack{\scalebox{0.65}{
       $ \begin{matrix}
                 \y \in \Y \\
                 \x, \s,\vv \in \X \\
                 \dd = \s -\vv   %
               \end{matrix}$
        }}} \hspace{-3mm} \prodscal{\dd}{\frac{\nabla_x\L(\x,\y^*)-\nabla_x\L(\x,\y)}{\sqrt{\L^*-\L(\x^*,\y)}}} 
    \end{equation}
  and, 
  \begin{equation}
      M_{YX}:=  \hspace{-2mm} \sup_{\substack{\scalebox{0.65}{
       $ \begin{matrix}
                 \x \in \X\\
                 \y, \s,\vv \in \Y \\
               \dd = \s-\vv
          \end{matrix}$
        }}} \hspace{-3mm} \prodscal{\dd}{\frac{\nabla_y\L(\x,\y)-\nabla_y\L(\x^*,\y)}{\sqrt{\L(\x,\y^*)-\L^*}}}.
  \end{equation}
  We also define the global bilinearity coefficient as
  \begin{equation}\label{eq:def_bilin_coef}
    M_\L := \max\{M_{XY},M_{YX}\}.
  \end{equation}
  We can upper bound these affine invariant constants with the Lipschitz constant of the gradient, the uniform strong convex-concavity constants and the diameters of the sets.
  \begin{proposition}\label{Mbounded}
  If  $\X$ and $\Y$ are compact, $\nabla \L$ is Lipschitz continuous and $\L$ is uniformly strongly convex-concave with constants $(\mu_\X, \mu_\Y)$, then
    \begin{equation}
      M_{XY} \leq \sqrt{\frac{2}{\mu_\Y}}L_{XY} \cdot D_\X 
      \quad \text{and} \quad
      M_{YX} \leq \sqrt{\frac{2}{\mu_\X}}L_{YX} \cdot D_\Y
    \end{equation}
  where $L_{XY}$ and $L_{YX}$ are the \emph{partial} Lipschitz constants defined in Equation~\eqref{def:lipL}. The quantity $D_\X$ is the diameter of the compact set $\X$
  and $D_\Y$ is the diameter of $\Y$.
  \end{proposition}
  \proof
   \begin{align*}
   M_{XY}
   &= \sup_{\substack{\scalebox{0.65}{
          $ \begin{matrix}
                    \y \in \Y \\
                    \x, \s,\vv \in \X \\
                    \dd = \s -\vv 
                  \end{matrix}$
     }}} \;\prodscal{\dd}{\frac{\nabla_x\L(\x,\y^*)-\nabla_x\L(\x,\y)}{\sqrt{\L^*-\L(\x^*,\y)}}} \\        
    &\leq \sup_{\substack{\scalebox{0.65}{
              $ \begin{matrix}
                        \y \in \Y \\
                        \x, \s,\vv \in \X \\
                        \dd = \s -\vv 
                      \end{matrix}$
         }}} \; {\frac{ \|\dd\|_\X \, \|\nabla_x\L(\x,\y^*)-\nabla_x\L(\x,\y) \|_{\X^*}}{\sqrt{\L^*-\L(\x^*,\y)}}} \\
    & \leq \sup_{\substack{\scalebox{0.65}{
              $ \begin{matrix}
                        \y \in \Y \\
                        \s,\vv \in \X \\
                        \dd = \s -\vv 
                      \end{matrix}$
         }}} \; {\frac{ \|\dd\|_\X \,  L_{XY} \|\y^*-\y \|_\Y}{\sqrt{\L^*-\L(\x^*,\y)}}} \\
     & \leq  \sup_{\y \in \Y} D_\X L_{XY} \frac{\|\y^*-\y\|_\Y}{\sqrt{\L^*-\L(\x^*,\y)}}.
  \end{align*}
  Then using the relation between $\|\y^*-\y\|_\Y$ and $\sqrt{\L^*-\L(\x^*,\y)}$ due to strong convexity (Proposition~\ref{prop:strongL})
  \begin{equation}
  M_{XY} \leq \sqrt{\frac{2}{\mu_\Y}}L_{XY}\cdot D_\X.
  \end{equation}
  We use a similar argument for $M_{YX}$ which allows us to conclude.
  \endproof


  %


%
%
%


\subsection{Relation between the primal suboptimalities} %
\label{sub:relation_between_the_primal_errors}
In this section, we are going to show that if the objective function $\L$ is uniformly strongly convex-concave, then we have a relation between $h_t$ and $w_t$. First let us introduce affine invariant constants to relate these quantities (in the context of a given saddle point $(\x^*, \y^*)$):
\begin{equation}
 \label{def:constant_relation_primal}
  P_\X := \sup_{\x \in \X} \frac{\prodscal{\nabla_x \L (\x,\hat \y(\x))}{\x-\x^*}}{\sqrt{\L(\x,\y^*)-\L(\x^*,\y^*)}}
\quad
\text{and}
\quad
  P_\Y := \sup_{\y \in \Y} \frac{\prodscal{\nabla_y \L (\hat \x (\y),\y)}{\y-\y^*}}{\sqrt{\L(\x^*,\y^*)-\L(\x^*,\y)}},
\end{equation}
where $\hat \y (\x) : = \argmax_{\y \in \Y} \L(\x,\y)$ and $\hat \x (\y) : = \argmin_{\x \in \X} \L(\x,\y)$. We also define: 
\begin{equation} 
  P_\L := \max \{P_\X, P_\Y\}.
\end{equation}
These constants can be upper bounded by easily computable constants.
\begin{proposition} For any $(\mu_\X,\mu_\Y)$-uniformly convex-concave function $\L$,
\label{prop:upper_bound_P_L}
\begin{equation}
  P_\X \leq \sqrt{\frac{2}{\mu_\X}}\sup_{\z \in \M}\| \nabla_x \L(\z)\|_{\X^*}
\quad
\text{and}
\quad
   P_\Y \leq \sqrt{\frac{2}{\mu_\Y}} \sup_{\z\in \M}\|\nabla_y \L (\z)\|_{\Y^*}.
\end{equation}
\end{proposition}
\proof 
Let us start from the definition of $P_\X$, let $\x \in \X$,
\begin{align*}
  \frac{\prodscal{\nabla_x \L (\x,\hat \y(\x))}{\x-\x^*}}{\sqrt{\L(\x,\y^*)-\L(\x^*,\y^*)}} 
  & \leq \frac{\|\x-\x^*\|_\X \cdot \sup \left(\| \nabla_x \L (\z)\|_{\X^*}\right) }{\sqrt{\L(\x,\y^*)-\L(\x^*,\y^*)}} \\
  & \leq \sqrt{\frac{ 2}{\mu_\X}} \sup_{\z \in \M}\| \nabla_x \L(\z) \|_{\X^*} \qquad \qquad\text{(by strong convexity.)} 
\end{align*}
The same way we can get 
\begin{equation}
  P_\Y \leq \sqrt{\frac{2}{\mu_\Y}} \sup_{\z\in \M}\|\nabla_y \L(\z) \|_{\Y^*}.
\end{equation}
It concludes our proof.
\endproof 
One way to compute an upper bound on the supremum of the gradient is to use any reference point $\bar{\z}$ of the set:
\begin{equation}
  \forall \bar{z} \in \M, \quad \sup_{\z \in \M}\| \nabla_x \L(\z) \|_{\X^*} 
  \leq \nabla_x \L(\bar{\z}) + L_{XX} D_\X + L_{XY} D_\Y.
\end{equation}
We recall that $L_{XX}$ is the largest (with respect to $\y$) Lipschitz constant of $\x \mapsto \nabla_x \L(\x,\y)$. Note that $L_{XX}$ is upper bounded by the global Lipschitz constant of $\nabla \L$. We can compute an upper bound on the supremum of the norm of $\nabla_y \L$ the same way.

With these above defined affine invariant constants,  we can finally relate the two primal suboptimalities as $h_t \leq \mathcal{O}(\sqrt{w_t})$.
\begin{proposition}
\label{prop:relation_primal}
For a $(\mu_\X,\mu_\Y)$-uniformly strongly convex-concave function $\L$,
\begin{equation}
  h_t \leq P_\L \sqrt{2w_t} 
  \quad 
  \text{and} \quad
  P_\L \leq \sqrt{2} \underset{\z \in \M}{\sup} \left\{\frac{\|\nabla_x \L(\z)\|_{\X^*}}{\sqrt{\mu_\X}} ,\frac{\|\nabla_y \L(\z)\|_{\Y^*}}{\sqrt{\mu_\Y}}   \right\}.
\end{equation}
\end{proposition}

\proof We will first work on $h_t^{(x)}$:
  \begin{align*}
   h_t^{(x)} &=\L(\xt,\ytm) -\L^* \\
    &\leq \L(\xt,\ytm) -\L(\x^*,\ytm)\\
    &\leq \prodscal{\xt - \x^*}{\nabla_x \L(\xt,\ytm} 
      \qquad\qquad \text{(by convexity)}\\
    & \leq P_\X \sqrt{\wt^{(x)}} \qquad (\text{def of } P_\X \: \eqref{def:constant_relation_primal}).
  \end{align*}
We can do the same thing for $h_t^{(y)}$ and $w_t^{(y)}$, thus 
\begin{equation}
h_t \leq P_\L \left(\sqrt{\wt^{(x)}} + \sqrt{\wt^{(y)}} \right) \leq P_\L \sqrt{2 w_t} ,    
\end{equation}
where the last inequality uses $\sqrt{a} + \sqrt{b} \leq \sqrt{2 (a+b)}$.
Finally, the inequality on $P_\L$ is from Proposition~\ref{prop:upper_bound_P_L}.
\endproof
%

\section{Relations between primal suboptimalities and dual gaps}\label{sec:relations_gaps}
\subsection{Primal suboptimalities}
  Recall that we introduced $\xtm := \argmin_{\x \in \X} \L(\x, \yt)$ 
  and similarly $\ytm := \argmax_{\y \in \Y} \L(\xt, \y).$
  Then the primal suboptimality is the positive quantity 
    \begin{equation}\label{def:primal}
    h_t := \L(\xt,\ytm)- \L(\xtm,\yt). 
    \end{equation}
  To get a convergence rate, one has to upper bound the primal suboptimality defined in~\eqref{def:primal}, but it is hard to work with the moving quantities $\xtm$ and $\ytm$ in the analysis. This is why we use in our analysis a different merit function that uses the (fixed) saddle point $(\x^*,\y^*)$ of $\L$ in its definition. We recall its definition below.

   \paragraph{Second primal suboptimality.} %
   \label{par:second_primal_gap}
  We define the second primal suboptimality for $\L$ of the iterate $(\xt, \yt)$ with respect to the saddle point $(\x^*,\y^*)$ as the positive quantity: 
    \begin{equation} 
  \label{def:primal2}
    \wt := \L(\xt,\y^*)- \L(\x^*,\yt). 
    \end{equation}
  It follows from $\L(\xt,\ytm) \geq \L(\xt,\y^*)$ and $\L(\x^*,\yt) \geq \L(\xtm,\yt)$ that $w_t \leq h_t$. 
  Furthermore, under the assumption of uniform strong convex-concavity, we proved in Proposition~\ref{prop:relation_primal} that the square root of $w_t$ upper bounds $h_t$ up to a constant.
   %

  %
  %
  %
  %
  %

  %

  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
    
  %


\subsection{Gap inequalities} \label{app:gapInequalities}


  In this section, we will prove the crucial inequalities relating suboptimalities and the gap function.
  %
  %
  Let's recall the definition of $\st$ and $\vt$:
    \begin{equation}\st:=  \argmin_{\s \in \M}
         \prodscal{\s}{
       \rt}
    \quad \text{  and } \quad 
    \vt:=  \argmax_{\vv \in \SS^{t}_x\times \SS^{t}_y}
         \prodscal{\vv}{
        \rt}
         \end{equation}
  where $(\rt)^\top := ( (\rt_x)^\top, (\rt_y)^\top ) :=  \left(
        \nabla_x \L(\xt,\yt),
         -\nabla_y \L(\xt,\yt)
         \right)$. 
  Also, the following various gaps are defined as 
    \begin{equation}\label{eq:gaps} 
    \gap^{\FW} :=\prodscal{\dt_{\FW}}{-\rt} 
    , \qquad
    \gap^{\PW} :=\prodscal{\dt_{\PW}}{-\rt}
    \quad \text{and} \quad
    \gap := \prodscal{\dt}{-\rt}
    \end{equation}
  where $\dt_{\FW} = \st - \zt$ and $\dt_{\PW} = \st - \vt$. The direction $\dt$ is the direction chosen by the algorithm at step $t$: it is always $\dt_{FW}$ for SP-FW, and can be either $\dt_{FW}$ or $\dt_{\away} := \zt - \vt$ for SP-AFW. Even if the definitions of these gaps are different, the formalism for the analysis of the convergence of both algorithms is going to be fairly similar.
  It is straightforward to notice that $\gap^{\PW} \geq \gap$ and one can show that the current gap $\gap$ is lower bounded by half of $\gap^{\PW}$:
  \begin{lemma}\label{lemma:gap_FW_PW}
     For the SP-AFW algorithm, the current gap $\gap$ can be bounded as follows:
     \begin{equation}
      \frac{1}{2} \gap^{\PW} \leq \gap \leq \gap^{\PW}
     \end{equation}
   \end{lemma} 
   \proof
    First let's show the RHS of the inequality, 
    \begin{equation}
      \gap^{\PW} := \prodscal{\dt_{\PW}}{-\rt} = \prodscal{\dt_{\away}}{-\rt} + \prodscal{\dt_{\FW}}{-\rt} \geq \prodscal{\dt}{-\rt}
    \end{equation}
    because both $\prodscal{\dt_{\away}}{-\rt}\geq 0$ and $ \prodscal{\dt_{\FW}}{-\rt} \geq 0$ from their definition.
    For the LHS inequality, we use the fact that $\gap = \max \left\{ \prodscal{\dt_{\away}}{-\rt} , \prodscal{\dt_{\FW}}{-\rt} \right\}$ for SP-AFW and thus:
    \begin{equation}
       \gap^{\PW} = \prodscal{\dt_{\away}}{-\rt} + \prodscal{\dt_{\FW}}{-\rt} \leq 2 \gap .
    \end{equation}
   \endproof
   In the following, we will assume that we are in one of the two following cases: 
   \leqnomode
  \begin{equation}
    \label{case:1}
    \text{The saddle point of } \L \text{ belongs to the relative interior of } \M.
    \tag{I}
  \end{equation}
  \begin{equation}
    \label{case:2}
    \X \text{ and } \Y \text{ are polytopes}, \quad \text{i.e.} \; \exists \A, \B \text{ finite s.t}\;\; \X = \conv(\A), \; \Y = \conv(\B).
    \tag{P}
  \end{equation}
  \reqnomode

  Then either $\muIntL>0$ (case~\ref{enu:situation1}) or $\mu^{\away}_\L>0$ (case~\ref{enu:situation2}).
  Let's write the gap function as the sum of two smaller gap functions:
    \begin{equation} \label{eq:gtGeneralized}
      \gap 
      = \quad \underbrace{\prodscal{ \dt_{(x)}}{-\rt_x}}_{=:\gap^{(x)}} \quad
      + \quad
    \underbrace{\prodscal{ \dt_{(y)}}{ -\rt_y}}_{=:\gap^{(y)}} 
    \end{equation}
  %
  %
  %
  Because of the convex-concavity of $\L$, this scalar product bounds
  the differences between the value of $\L$ at the point
  $(\xt,\yt)$ and the value of $\L$ at another point. Hence this
  gap function upper-bounds $h_t$ and $w_t$ defined in~\eqref{def:primal} and~\eqref{def:primal2}. More concretely, we
  have the following lemma.



  \begin{lemma} \label{lemme:g}
    For all $t$ in $\N$, $\x\in \X$ and $ \y \in \Y$
      \begin{equation} \label{eq:encadre}
      \gap^{\PW} \geq \gap^{\FW} \geq  \L(\xt,\y)-\L(\x, \yt),
      \end{equation}
    and, furthermore,
      \begin{equation} \label{eq:gtInequality_with_ht}
      \gap \geq  h_t  \geq \wt.
      \end{equation}
  \end{lemma}
  \proof 
    First let's show the LHS of \eqref{eq:encadre}, 
    \begin{equation}
      \gap^{\PW} = \prodscal{\dt_{\PW}}{-\rt} = \prodscal{\dt_{\away}}{-\rt} + \prodscal{\dt_{\FW}}{-\rt} \geq \prodscal{\dt_{\FW}}{-\rt} = \gap^{\FW}
    \end{equation}
    because one can easily derive that $\prodscal{\dt_{\away}}{-\rt}\geq 0$ from the definition of the away direction $\dt_{\away}$.
    It follows from convexity of $\x \mapsto \L(\x, \yt)$ that for all $\x$ in $\X$, 
        \begin{align} \label{eq:gapx}
           (\gap^{\FW})_{x} :=\prodscal{ (\dt_{\FW})_x}{ -\gnx}
              & \geq \prodscal{ \x - \xt}{-\gnx}\\
              &\geq \L(\xt,\yt) - \L(\x, \yt). 
        \end{align}
    A similar inequality emerges through the convexity of $\y \mapsto -\L(\xt,\y)$,
      \begin{equation}
        (\gap^{\FW})_{y} := \prodscal{ (\dt_{\FW})_y}{\gny} \geq \L(\xt, \y) - \L(\xt,\yt),
      \end{equation}
    which gives us 
      \begin{equation} 
      \gap^{\FW} \geq \L(\xt, \yt) - \L(\x, \yt) + \L(\xt, \y) - \L(\xt , \yt) ,
      \end{equation}
     which shows~\eqref{eq:encadre}.
     By using $\x = \xtm$ and $\y = \ytm$ in~\eqref{eq:encadre}, we get $\gap^{\FW} \geq h_t$. We also know that $g_t = \max(\gap^\away, \gap^\FW) \geq \gap^{\FW}$ for SP-AFW. So combining with $h_t \geq w_t$ that we already knew, we get~\eqref{eq:gtInequality_with_ht}.
  \endproof
  Next, we recall two lemmas, one from~\citesup{lacoste2013affine} and the other one from \citep{lacoste2015global}. These lemmas upper bound the primal suboptimality with the square of the gap times a constant depending on the geometric (or the interior) strong convexity constant.

  %

  \begin{lemma}[\citet{lacoste2015global}, \citetsup{lacoste2013affine}] \label{lemme:lingap}
   If $f$ is strongly convex, then for any $\xt \in \X$, 
    \begin{equation}
     f(\xt)-f(\xc) \leq \frac{\left( \gap^{\FW} \right)^2}{2 \mu^{\xc}_f}
     \quad
     \text{if } \xc \in \text{ interior of } \X
     \quad \text{\citepsup{lacoste2013affine}} 
     \end{equation}
  and 
     \begin{equation}
     f(\xt)-f^* \leq \frac{\left( \gap^{\PW} \right)^2}{2 \mu^{\away}_f}
     \quad
     \text{if } \X = \conv(\A) 
     \quad
     \text{\citep{lacoste2015global}}
    \end{equation}
  where $\gap^{\FW}= \prodscal{\xt-\st}{\nabla f(\xt)}$, $\gap^{\PW}= \prodscal{\vt-\st}{\nabla f(\xt)}$ and $f^* = \min_{x \in \X} f$.
  \end{lemma}
  Notice once again that in this lemma~\emph{we do not need $\xc$ to be optimal}.
  \proof 
  Let $\xt \neq \xc$. Using the definition of interior strong convexity~\eqref{def:int_strong} and choosing $\gamma$ such that $\xc = \xt + \gamma \left( \bar \s\left(\xt,\xc\right) - \xt \right)$, we get
    \begin{align*}
    f(\xc)-f(\xt) &\geq \gamma\prodscal{\bar\s(\xc,\xt)-\xt}{\nabla f(\xt)} + \gamma^2 \frac{ \mu^{\xc}_f}{2}\\
    & \geq - \gamma\gap^{\FW}+ \gamma^2 \frac{ \mu^{\xc}_f }{2}  \\
    & \geq  \frac{ - \left( \gap^{\FW} \right)^2 }{2\mu^{\xc}_f}.
    \end{align*}
  The last line of this derivation is obtained through the inequality: $-a^2 + 2ab - b^2 \leq 0$. 
  If $\xt = \xc$ the inequality is just the positivity of the gap.

  For the second statement, we will use the definition of the geometric strong convexity constant (Equation~\eqref{def:geom_strong}) at the point $\x = \xt$ and $\x^* \in \argmin_{\x \in \X} f(x)$. 
  Recall that $\gamma^\away(\x,\x^*) = \frac{\prodscal{-\nabla f(\x)}{\x^*-\x}}{\prodscal{-\nabla f(\x)}{\s_f(\x)-\vv_f(\x)}}$.
    \begin{align*}
    f(\x^*)-f(\xt) &\geq \prodscal{\x^*-\xt}{\nabla f(\xt)} + \frac{ \mu^{\away}_f}{2}\gamma^\away(\xt, \x^*)^2 \\
    & = -\gamma^\away(\xt, \x^*) \prodscal{\s_f(\xt) - \vv_f(\xt)}{-\nabla f(\xt)}  + \frac{ \mu^{\away}_f}{2} \gamma^\away(\xt, \x^*)^2  \\
    & \geq - \gamma^\away(\xt, \x^*)\gap^{\PW}+  \frac{ \mu^{\away}_f }{2}\gamma^\away(\xt, \x^*)^2 \qquad \text{(Equation~\eqref{eq:gammaA})}  \\
    & \geq  \frac{ - \left( \gap^{\PW} \right)^2 }{2\mu^{\away}_f}.
    \end{align*}
  \endproof
  Lemma~\ref{lemme:lingap} is useful to understand the following lemma and its proof which is just an extension to
  the convex-concave case.


  \begin{lemma}[Quadratic gap upper bound on second suboptimality for~\eqref{case:1} or~\eqref{case:2}]\label{lemme:gap}
   If $\L$ is a strongly convex-concave function, then for any $(\xt,\yt) \in \M$,
    \begin{equation}
     \wt  \leq \frac{(\gap^{\FW})^2}{2 \muIntL } 
     \quad 
      \text{for \eqref{case:1}}
     \quad \text{and} \quad
     \wt \leq h_t \leq \frac{(\gap^{\PW})^2}{2 \mu^{\away}_\L}
     \quad
     \text{for \eqref{case:2}} %
    \end{equation}
  where the gaps are defined in \eqref{eq:gaps}, $\muIntL := \min \{ \mu^{\x^*}_\L, \mu^{\y^*}_\L\}$ (i.e. using the reference points $(\xc,\yc) := (\x^*,\y^*)$ in the definition~\eqref{def:interior_strongL})
  and $\mu^{\away}_\L$ is the geometric strong convex-concavity of $\L$ over $\Vertices$, as defined in~\eqref{def:geom_strongL}.
  \end{lemma}

  \proof For \eqref{case:1}:

   Let the function $f$ on $\X$ be defined by $f(\x) = \L(\x',\yt)$, and the function $g$ on $\Y$ be $g(\y') = -\L(\xt, \y')$. Then using the Lemma~\ref{lemme:lingap} on the function $f$ with the reference point $\x^*$, and on $g$ with reference point $\y^*$, we get
    \begin{align*}
      \L(\xt, \yt) - \L(\x^*,\yt)  & \leq  \frac{ \prodscal{\st_x - \xt }{-\nabla_x \L(\xt,\yt)}^2 }{2 \mu_f^{\x^*}}  \\
      \L(\xt, \y^*) - \L(\xt,\yt)  & \leq   \frac{ \prodscal{\st_y - \yt }{\nabla_y \L(\xt,\yt)}^2 }{2 \mu^{\y^*}_g} .
    \end{align*}
    As $\muIntL$ is smaller than both $\mu_f^{\x^*}$ and $\mu_g^{\y^*}$ by the definition~\eqref{def:interior_strongL}, we can use it in the denominator of the above two inequalities.
    As we saw from Section~\ref{app:gapInequalities} in~\eqref{eq:gtGeneralized}, the gap can be split as sum of the gap of the block $\X$ and the gap of the block $\Y$, i.e. $\gap^{\FW} =  \prodscal{\st_x- \xt}{-\nabla_x \L(\xt,\yt)} +  \prodscal{\st_y -\yt}{\nabla_y \L(\xt,\yt)} $.
    Then, using the inequality: $a^2+b^2 \leq (a+b)^2$ for $(a  ,b \geq
    0)$, we obtain
      \begin{equation} \label{eq:proof_w_FWgap}
      w_t \leq \frac{ (\gap^{\FW})^2}{2 \muIntL}.
      \end{equation}

      For \eqref{case:2}:

    Using the Lemma~\ref{lemme:lingap} for case~\eqref{enu:situation2} on the same functions $f$ and $g$ defined above, we get
      \begin{align*}
        \L(\xt, \yt) - \L(\xtm,\yt)  & \leq  \frac{  \prodscal{\st_x - \vt_x }{\nabla_x \L(\xt,\yt)}^2 }{2 \mu^{\away}_{f}}  \\
        \L(\xt, \ytm) - \L(\xt,\yt)  & \leq   \frac{  \prodscal{\st_y -\vt_y }{-\nabla_y \L(\xt,\yt)}^2 }{2 \mu^{\away}_{g}}.
      \end{align*}
    Using a similar argument as the one to get~\eqref{eq:proof_w_FWgap}, using that $\mu^{\away}_\L$ is smaller than both $\mu^{\away}_{f}$ and $\mu^{\away}_{g}$, and referring to the separation of the gap~\eqref{eq:gtGeneralized}, we get
      \begin{equation} 
       h_t \leq \frac{ (\gap^{\PW})^2}{2 \mu^{\away}_\L}.
      \end{equation}

  \endproof



%

\section{Convergence analysis}
\label{appendix:analysis}
In this section, we are going to show two important lemmas. The first one shows that under some assumptions we can get a Frank-Wolfe-style induction scheme relating the second suboptimality of the potential update $w_{\gamma}$, the current value of the second suboptimality $w_t$, the gap~$\gap$ and any step size $\gamma \in [0,\stepmax]$. The second lemma will relate the gap and the square root of $w_t$; this relation enables us to get a rate on the gap after getting a rate on $w_t$.

\subsection{First lemmas}
  %
  %
  The first lemma in this section is inspired from the standard FW progress lemma, such as Lemma~C.2 in~\citep{lacoste2013block}, though it requires a non-trivial change due to the compensation phenomenon for~$\L$ mentioned in the main text in~\eqref{eq:oscilation}.
  In the following, we define the possible updated iterate $\z_\stepsize$ for $\stepsize \in [0, \stepmax]$:
  \begin{equation}
  \z_\gamma := (\x_\gamma, \y_\gamma):= \zt + \gamma \dt, \quad \text{ where } \quad \dt \text{ is the direction of the step.}
  \end{equation}
  For a FW step $\dt = \dt_{\FW}:= \st - \zt$ and for an away step $\dt = \dt_{\away} := \zt - \vt$. We also define the corresponding new suboptimality for $\z_\gamma$:
  \begin{equation}
    %
    w_\gamma := \L(\x_\gamma, \y^*) - \L(\x^*,\y_\gamma).
  \end{equation}
  %
  %
  %
  %
  %
  %
  \begin{lemma}[Suboptimality progress for SP-FW and SP-AFW] \label{lemme:ineg}
  Let $\L$ be strongly convex-concave, 
  %

    If we are in case~\eqref{case:1} and $\dt = \dt_{\FW}$ is a FW direction, we have for any $\stepsize \in [0,1]$:
    %
      \begin{equation}  \label{eq:wInequalityCaseI}
      w_\gamma 
       \leq \wt  
      - \CondNumb^{\FW}\gamma \gap^{\FW}
      + \gamma^2 {C_\L},
      \quad \text{where} \quad 
      \CondNumb^{\FW} :=1- \frac{M_\L }{\sqrt{\muIntL}}.
      \end{equation}
    If we are in case~\eqref{case:2} and $\dt$ is defined from a step of SP-AFW (Algorithm~\ref{alg:AFW}), we have for any $\stepsize \in [0, \stepmax]$:
      \begin{equation} \label{eq:wInequalityCaseP} 
      w_\gamma 
       \leq \wt  
      - \CondNumb^{\PW} \gamma \gap^{\PW}
      + \gamma^2 {C_\L},
      \quad \text{where} \quad
      \CondNumb^{\PW} := \frac{1}{2}- \frac{M_\L }{\sqrt{\mu^{\away}_\L}}.
      \end{equation}
  %
 %
  \end{lemma}

  \proof 
    The beginning of the argument works with any direction $\dt$. 
    Recall that $\wt^{(x)} := \L(\xt, \y^*)- \L^*$ and $\wt^{(y)} := -\L(\x^*,\yt) + \L^*$.
    Now writing $\x_\gamma = \xt + \gamma \dt_x$ and using the definition of the curvature $C_f$~\eqref{CurvB} for the function $\x \mapsto f(\x) := \L(\x,\y^*)$, we get
      \begin{align}
      w_\gamma^{(x)} &:= \L(\x_\gamma,\y^*) - \L^* \notag\\
        &\leq \L(\xt, \y^*) - \L^* + \gamma \prodscal{\dt_x}{\nabla_x\L(\xt, \y^*)} + \gamma^2 \frac{C_\L}{2},
      \end{align} 
    since $C_\L \leq C_f$ by definition~\eqref{eq:CL}. 
    Recall that the gap function $\gap$ can be decomposed by~\eqref{eq:gtGeneralized} into two smaller gap functions $\gap^{(x)} := \prodscal{\dt_x}{-\rt_x}$ and $\gap^{(y)} := \prodscal{\dt_y}{-\rt_y}$. We define $\epsilon_t := \prodscal{\dt_x}{\nabla_x\L(\xt, \y^*)- \gnx}$ to be the sequence representing the error between the gradient used for the minimization and the gradient at the point $(\xt, \y^*)$. Then,
       \begin{equation}\label{eq:un}
          w_\gamma^{(x)}   \leq   \wt^{(x)} - \gamma \gap^{(x)} +\gamma \epsilon_t + \gamma^2 \frac{C_\L}{2}.
      \end{equation}
    Now, as $M_{XY}$ is finite (under Lipschitz gradient assumption), we can use the definition of the bilinearity constant~\eqref{def:M} to get
      \begin{equation}\label{eq:grad}
      |\epsilon_t| = \left|\prodscal{\dt_x}{\nabla_x\L(\xt, \y^*)- \gnx}\right| \leq \sqrt{\wt^{(y)}} M_{XY}.
       \end{equation}
    Combining equations \eqref{eq:un} and \eqref{eq:grad} we finally obtain
      \begin{equation}
        w_\gamma^{(x)} \leq \wt^{(x)} - \gamma \gap^{(x)} + \gamma M_{XY} \sqrt{\wt^{(y)}}
          + \gamma^2\frac{ C_\L}{2}.
      \end{equation}
     We can get an analogous inequality for $w_\gamma^{(y)}$,
       \begin{equation}
        w_\gamma^{(y)} \leq \wt^{(y)} - \gamma \gap^{(y)} + \gamma M_{YX} \sqrt{ \wt^{(x)}}
          + \gamma^2 \frac{C_\L}{2}.
      \end{equation}
    Then adding $w_\gamma^{(x)}$ and $w_\gamma^{(y)}$ and using $\sqrt{a} + \sqrt{b} \leq \sqrt{2(a+b)}$ (coming from the concavity of $\sqrt{\cdot}$), we get
      \begin{equation} \label{eq:wgammaInt}
      w_\gamma\leq  \wt - \gamma \gap +\gamma M_{\L} \sqrt{2\wt}+ 2\gamma^2 \frac{C_\L}{2}.
      \end{equation}
    We stress that the above inequality~\eqref{eq:wgammaInt} is valid for any direction $\dt$, using $g_t := \prodscal{\dt}{-\rt}$, and for any feasible step size $\stepsize$ such that $\z_\stepsize \in \X \times \Y$ (the last condition was used in the definition of $C_f$; see also footnote~\ref{foot:Cfcomment} for more information).
      
  To finish the argument, we now use the specific property of the direction $\dt$ and use the crucial Lemma~\ref{lemme:gap} that relates~$w_t$ with the square of the appropriate gap.
  
  For the case~\eqref{case:1} of interior saddle point, we consider $\dt = \dt_{\FW}$ and thus $g_t = \gap^{\FW}$. Then combining Lemma~\ref{lemme:gap} (using the interior strong convexity constant) with~\eqref{eq:wgammaInt}, we get
      \begin{equation}\label{eq:int_strong_usual_gap}w_\gamma\leq  \wt - \gamma \left( 1 - \frac{M_\L}{\sqrt{\muIntL}}\right)\gap^{\FW}+ \gamma^2 C_\L.
      \end{equation}
      
    For the case~\eqref{case:2} of polytope domains, we consider $\dt$ as defined by the SP-AFW algorithm. We thus have $\gap \geq \frac{1}{2} \gap^{\PW}$ by Lemma~\ref{lemma:gap_FW_PW}. Then combining Lemma~\ref{lemme:gap} (using the geometric strong convexity constant) with~\eqref{eq:wgammaInt}, we get
      \begin{equation}\label{eq:geom_strong_pfw_gap}
      w_\gamma\leq  \wt - \gamma \left( \frac{1}{2} - \frac{M_\L}{\sqrt{\mu^{\away}_\L}}\right)\gap^{\PW}+ \gamma^2 C_\L ,
      \end{equation}
      which finishes the proof. Still in the case~\eqref{case:2}, we also present an inequality in terms of the direction gap $\gap$ (which yields a better constant that will be important for the sublinear convergence proof in Theorem~\ref{thm:conv_sublin}) by using instead the inequality $\gap^{\PW} \geq \gap$ (Lemma~\ref{lemma:gap_FW_PW}) with Lemma~\ref{lemme:gap} and~\eqref{eq:wgammaInt}:
      \begin{equation}\label{eq:geom_strong_gt_gap}
      w_\gamma\leq  \wt - \gamma \left( 1 - \frac{M_\L}{\sqrt{\mu^{\away}_\L}}\right)\gap + \gamma^2 C_\L.
      \end{equation}
  \endproof

The above lemma uses a specific update direction~$\dt$ to get a potential new suboptimality~$w_\stepsize$. By using the property that $w_\stepsize \geq 0$ always, we can actually derive an upper bound on the gap in terms of $w_t$ \emph{irrespective of any algorithm} (i.e. this relationship holds for any possible feasible point $(\xt,\yt)$). More precisely, for SP-FW algorithm (case~\eqref{enu:situation1}) the only thing we need to set is a feasible point $\zt$ but for the SP-AFW algorithm (case~\eqref{enu:situation2}) we also need an active set expansion for $\zt$ for which the maximum away step size is larger than $\frac{\CondNumb\gap}{2C_\L}$ (which can potentially not be the active set calculated by an algorithm). 
%
This is stated in the following theorem, which is a saddle point generalization of the gap upper bound given in Theorem~2 of~\citep{lacoste2015global}. 

\begin{theorem}[Bounding the gap with the second suboptimality] \label{thm:subopt_gap}
  If $\L$ is strongly convex-concave and has a finite curvature constant then
  \begin{itemize}
    \item case~\eqref{enu:situation1}: For any $\zt \in \M$,
      \begin{equation}    
        \gap^{\FW} \leq \frac{2}{\CondNumb^{\FW}} \max \left\{\sqrt{C_\L w_t},w_t \right\}.
      \end{equation}
      Since $\zt$ is fixed, this statement is algorithm free.
    \item case~\eqref{enu:situation2}:  For any $\zt \in \M$, if there exists  an active set expansion for $\zt$ for which $\stepmax = 1$ or $\stepmax \geq \frac{\CondNumb^{\PW}\gap^{\PW}}{2C_\L}$ (see~\eqref{eq:maxstep} for the definition of $\stepmax$) then,
      \begin{equation}    
        \gap^{\PW} \leq \frac{2}{\CondNumb^{\PW}} \max \left\{\sqrt{C_\L w_t},w_t \right\}.
     \end{equation}
  \end{itemize}
  Both statement are algorithm free but $\gap^{\PW}$ depends on a chosen expansion of $\zt$:
   \begin{equation}\label{eq:expansion}
    \zt = \sum_{(\vv_x,\vv_y) \in \mathcal S^{(t)}} \alpha_{\vv_x}^{(t)}\alpha_{\vv_y}^{(t)} (\vv_x,\vv_y)
    \quad \text{where} \quad
     \mathcal S^{(t)} := \left\{ (\vv_x,\vv_y) \in \A\times\B\;;\; \alpha_{\vv_x}^{(t)}\alpha_{\vv_y}^{(t)} >0 \right\},
  \end{equation}
    because,
   \begin{equation}
   \begin{aligned}
   &\gap^{\PW} := \innerProdCompressed{-\rt}{\dt_\FW + \dt_\away} \\[1mm] 
    &\text{where} \quad \dt_\away := \zt - \argmax_{\vv \in \Coreset_x \times \Coreset_y} \innerProdCompressed{\rt}{\vv}, \\ 
    &\text{and} \quad \dt_\FW := \argmin_{\vv \in \A \times \B} \innerProdCompressed{\rt}{\vv} - \zt.
    \end{aligned} 
  \end{equation}
  The maximum step size associated with the active set expansion described in Equation~\eqref{eq:expansion} is 
  \begin{equation} \label{eq:maxstep}
     \gamma_{\max} := \left\{ \begin{array}
     {ll}
     1 \quad \text{if} \quad \innerProd{-\rt}{\dt_\away} \leq \innerProd{-\rt}{\dt_\FW},&\\
     \min\left\{\frac{\alpha^{(t)}_{\vt_x}}{1 -\alpha^{(t)}_{\vt_x}},\frac{\alpha^{(t)}_{\vt_y}}{1 -\alpha^{(t)}_{\vt_y}} \right\} \quad\text{otherwise}.&
     \end{array}\right.
  \end{equation}
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  \end{theorem}
  \proof In this proof, we let $(\gap, \CondNumb)$ to stand respectively for $(\gap^{\FW},\CondNumb^{\FW})$ for case~\eqref{case:1} or $(\gap^{\PW},\CondNumb^{\PW})$ for case~\eqref{case:2}.
  %
  %
    We will start from the inequalities~\eqref{eq:wInequalityCaseI} and~\eqref{eq:wInequalityCaseP} in Lemma~\ref{lemme:ineg}. Equation~\eqref{eq:wInequalityCaseI} is valid considering a FW direction $\dt_\FW$, Equation~\eqref{eq:wInequalityCaseP} is valid if we consider the direction that would have be set by the SP-AFW algorithm if it was run at point $\zt$ with the active set expansion described in the theorem statement.
    Since $w_\gamma \geq 0$,  for both cases become :
    \begin{equation}
     0 \leq \wt - \gamma \CondNumb\gap+ \gamma^2 C_\L,
    \end{equation}
    then we can put the gap on the LHS,
    \begin{equation} \label{eq:int_subopt_gap}
    \stepsize\CondNumb\gap - \stepsize^2 C_\L \leq \wt.
    \end{equation}
    This inequality is valid for any $\stepsize \in [0,\stepmax]$.
    In order to get the tightest bound between the gap and the suboptimality, we will maximize the LHS. It can be maximized with $\bar{\gamma} := \frac{\CondNumb\gap}{2C_\L}= \stepsize_t$.
    Now we have two cases: 

    If $\bar{\gamma} \in [0,\stepmax]$, then we get: $\: \CondNumb\gap \leq 2\sqrt{C_\L \, w_t}.$

    And if $\stepmax = 1$ and $\bar{\gamma} = \frac{\CondNumb\gap}{2C_\L} > 1$, then setting $\stepsize=1$ we get: $\: \CondNumb\gap \leq 2 w_t$. By taking the maximum between the two options, we get the theorem statement.
    %
  \endproof
  %
  The previous theorem guarantees that the gap gets small when $w_t$ gets small (only for the non-drop steps if in situation~\eqref{case:2}).
  As we use the gap as a stopping criterion in the algorithm, this is a useful theorem to provide an upper bound on the number of iterations needed to get a certificate of suboptimality.
  
  The following corollary provides a better bound on $h_t$ than the inequality $h_t \leq \textrm{cst} \sqrt{w_t}$ previously shown \eqref{eq:relate_h_w} when the situation is \eqref{enu:situation2}. It will be useful later to get a better rate of convergence for $h_t$ under hypothesis~\eqref{case:2}. 
  \begin{corollary}[Tighter bound on $h_t$ for non-drop steps in situation~\eqref{case:2}]
  \label{cor:h_w}
  Suppose that $\L$ is strongly convex-concave and has a finite curvature constant, and that the domain is a product of polytopes (i.e. we are in situation~\eqref{case:2}). Let $\zt \in \M$  be given. If there exists an active set expansion for $\zt$ for which the maximum step size is larger than $\frac{\CondNumb\gap}{2C_\L}$ (see Theorem~\eqref{thm:subopt_gap} for more details) ,
    \begin{equation}
            h_t \leq \frac{2 \max \{C_\L,w_t\}}{\mu^{\away}_\L (\CondNumb^\PW)^2} w_t ,
    \end{equation}
    where $\CondNumb^\PW$ is defined in~\eqref{eq:wInequalityCaseP}.
  \end{corollary}
  \proof
  By Lemma~\ref{lemme:gap} in situation~\eqref{case:2}, we have:
  \begin{equation*}
  h_t \leq \frac{(\gap^{\PW})^2}{2 \mu^{\away}_\L} \leq \frac{2\max \{C_\L,w_t\}}{\mu^{\away}_\L (\CondNumb^\PW)^2} w_t.
  \end{equation*}
  The last inequality is obtained by applying the upper bound on the gap given in the previous Theorem~\ref{thm:subopt_gap}.
  \endproof

%

\subsection{Proof of Theorem~\ref{thm:conv}}
  \label{subsec:proof_main_thm}
  In this section, we will prove that under some conditions on the constant defined in subsections~\ref{sub:curv} and~\ref{sub:affine_invariant_measures_of_strong_convexity}, the suboptimalities $\wt$
  vanish linearly with the adaptive step size $\gamma_t = \min \left\{\stepmax,\frac{\CondNumb}{2C_\L}\gap\right\}$ or sublinearly with the universal step size $\gamma_t = \min \left\{\stepmax,\frac{2}{2+k(t)}\right\}$.
  \begin{lemma}[Geometric decrease of second suboptimality] 
    \label{lemme:Lin} Let $\L$ be a 
    strongly convex-concave function with a smoothness constant $C_\L$, a positive interior strong convex-concavity constant $\muIntL$~\eqref{def:interior_strongL} or a positive geometric strong convex-concavity $\mu^{\away}_\L$~\eqref{def:geom_strongL}. Let us also define the rate multipliers~$\CondNumb$ as
  \begin{equation}
    \CondNumb^{\FW} :=1- \frac{M_\L }{\sqrt{\muIntL}} \quad\text{and}\quad \CondNumb^{\PW} := \frac{1}{2}- \frac{M_\L }{\sqrt{\mu^{\away}_\L}} \quad(\text{see Equation~\eqref{eq:def_bilin_coef} for the definition of } M_\L).
  \end{equation}
    Let the tuple $(\gap,\CondNumb,\mu_\L)$ refers to
  either $(\gap^{\FW},\CondNumb^{\FW},\muIntL)$ for case~\eqref{case:1} where the algorithm is SP-FW , or $(\gap^{\PW},\CondNumb^{\PW},\mu^{\away}_\L)$ for case~\eqref{case:2} %
  %
  where the algorithm is SP-AFW,

        If $\CondNumb > 0$, then  at each non-drop step (when $\gamma_t < \gamma_{\max}$ or $ \gamma_{\max} \geq 1$ ),
        the suboptimality $w_t$ of the algorithm with
        step size $\stepsize_t = \min(\stepmax, \frac{\CondNumb}{2C_\L}\gap)$ decreases
        geometrically as
          \begin{equation} 
             \wtt \leq (1- \rho_\L) \wt
          \end{equation}
        where $\rho_\L := \frac{\CondNumb^2}{2} \frac{\mu_\L}{C_\L}$.
        Moreover, for case \eqref{enu:situation1} there is no drop step and for case~\eqref{enu:situation2} the number of drop step (when $\gamma_t = \gamma_{\max}$) is upper bounded by two third of the number of iteration (see Section~\ref{par:drop_steps}, Equation~\eqref{eq:upper_bound_drop_steps}), while when we have a drop step, we still have:
        \begin{equation}
         \wtt \leq \wt.
        \end{equation}
  \end{lemma}

  \proof 
  The bulk of the proof is of a similar form for both SP-FW and SP-AFW, and so in the following, we let $(\gap, \mu_\L, \CondNumb)$ to stand respectively for $(\gap^{\FW},\muIntL,\CondNumb^{\FW})$ for SP-FW (case~\eqref{case:1}) or $(\gap^{\PW},\mu^{\away}_\L,\CondNumb^{\PW})$ for SP-AFW (case~\eqref{case:2}). As $\stepsize_t \leq \stepmax$, we can apply the important Lemma~\ref{lemme:ineg} with $\stepsize = \stepsize_t$ (the actual step size that was taken in the algorithm) to get:
  \begin{equation} \label{eq:wtIneq}
  w_{t+1} = w_{\stepsize_t}  
         \leq \wt  
        - \CondNumb \stepsize_t \gap
        + \gamma_t^2 {C_\L} . 
  \end{equation}
  We note in passing that the adaptive step size rule $\stepsize_t = \min(\stepmax, \frac{\CondNumb}{2 C_\L}\gap)$ was specifically chosen to minimize the RHS of~\eqref{eq:wtIneq} among the feasible step sizes. 
  
  If $\frac\CondNumb{2C_\L}\gap\leq \stepmax$, then we have $\gamma_t = \frac\CondNumb{2C_\L}\gap$ and so~\eqref{eq:wtIneq} becomes:
      \begin{equation}
        \wtt \leq \wt - \frac{\CondNumb^2}{ 2C_\L}  (\gap)^2+ \frac{\CondNumb^2}{ 4C_\L}(\gap)^2 = \wt - \frac{\CondNumb^2}{4C_\L} (\gap)^2.
      \end{equation}
    Applying the fact that the square of the appropriate gap upper bounds $w_t$ (Lemma~\ref{lemme:gap} with a similar form for both cases~\eqref{case:1} and~\eqref{case:2}), we directly obtain the claimed geometric decrease
      \begin{equation}
        \wtt \leq \wt \left(1- \frac{\CondNumb^2}{2} \frac{\mu_\L}{C_\L} \right).
      \end{equation}
    If $ \frac\CondNumb{2 C_\L}\gap> \stepmax$, then we have $\gamma_t = \stepmax$ and so~\eqref{eq:wtIneq} becomes:
    \begin{align}
      \wtt 
      &\leq  \wt - \CondNumb \stepmax \gap + \stepmax^2 C_\L & \notag\\
      & \leq \wt - \CondNumb \stepmax \gap + \frac\CondNumb{2} \stepmax \gap &( \text{using}\quad  C_\L < \frac{\CondNumb}{2 \stepmax}\gap) \\
      & \leq \wt - \frac\CondNumb{2} \stepmax \gap &\notag\\
       & \leq \wt \left(1-\frac{\CondNumb}{2}\stepmax\right).& (w_t \leq g_t \text{ by Lemma \ref{lemme:g}})
%
%
    \end{align}
    If $\stepmax \geq 1$ (either we are taking a FW step or an away step with a big step size), then the geometric rate is at least $(1-\frac{\CondNumb}{2})$, which is a better rate than $\rho_\L$ since $\CondNumb^2 \leq \CondNumb$ as $\CondNumb \leq 1$, and one can show that $\frac{\mu_\L}{C_\L} \leq 1$ always (see Remark~7 in Appendix D of~\citep{lacoste2015global} for case~\eqref{enu:situation2} and use a similar argument for case~\eqref{enu:situation1}). Thus $\rho_\L$ is valid both when $\stepsize_t < \stepmax$ or $\stepmax \geq 1$, as claimed in the theorem.
    
    When $\stepsize_t = \stepmax < 1$, we cannot guarantee sufficient progress as $\stepmax$ could be arbitrarily small (this can only happen for an away step as $\stepmax = 1$ for a FW step). These are the problematic \emph{drop steps}, but as explained in Appendix~\ref{par:drop_steps} with Equation~\eqref{eq:upper_bound_drop_steps}, they cannot happen too often for SP-AFW.
    
    Finally, to show that the suboptimality cannot increase during a drop step ($\gamma_t= \gamma_{\max}$), we point out that 
    the function $\gamma \mapsto \wt - \gamma \CondNumb^{\PW}\gap^{\PW}+ \gamma^2 C_\L$ is a convex function that is minimized by $\bar{\gamma} = \frac{\CondNumb^{\PW}}{2C_\L} \gap^{\PW}$ and so is decreasing on $[0, \bar{\stepsize}]$. When $\stepsize_t = \stepmax$, we have that $\stepmax \leq \bar{\gamma}$, and thus the value for $\gamma = \gamma_{\max}$ is lower than the value for $\gamma = 0$, i.e.
      \begin{equation}
            \wtt \leq \wt - \stepmax \CondNumb^{\PW}\gap^{\PW}+ \gamma_{\max}^2 C_\L \leq \wt.
      \end{equation}
  \endproof
  %
    The previous lemma (Lemma~\ref{lemme:Lin}), the fact that the gap upper bounds the suboptimality (Lemma~\ref{lemme:g}) and the primal suboptimalities analysis lead us directly
    to the following theorem. This theorem is the affine invariant formulation with adaptive step size of Theorem~\ref{thm:conv}.

  \begin{theorem}\label{thm:conv_affine_invariant}
  Let $\L$ be a 
    strongly convex-concave function with a finite smoothness constant $C_\L$, a positive interior strong convex-concavity constant $\muIntL$~\eqref{def:interior_strongL} or a positive geometric strong convex-concavity $\mu^{\away}_\L$~\eqref{def:geom_strongL}. Let us also define the rate multipliers~$\CondNumb$ as
  \begin{equation}\label{eq:thm24_state_multiplier}
    \CondNumb^{\FW} :=1- \frac{M_\L }{\sqrt{\muIntL}} \quad\text{and}\quad \CondNumb^{\PW} := \frac{1}{2}- \frac{M_\L }{\sqrt{\mu^{\away}_\L}} \quad(\text{see Equation~\eqref{eq:def_bilin_coef} for the definition of } M_\L).
  \end{equation}
    Let the tuple $(\gap,\CondNumb,\mu_\L)$ refers to
  either $(\gap^{\FW},\CondNumb^{\FW},\muIntL)$ for case~\eqref{case:1} where the algorithm is SP-FW , or $(\gap^{\PW},\CondNumb^{\PW},\mu^{\away}_\L)$ for case~\eqref{case:2} where the algorithm is SP-AFW,

  If $\CondNumb>0$, then the suboptimality $h_t$ of the iterates of the algorithm with
  step size $\stepsize_t = \min(\stepmax, \frac{\CondNumb}{2C_\L}\gap)$ decreases
  geometrically\footnote{For a non-drop step one can use Corollary~\ref{cor:h_w} to get the better rate on $h_t$ losing the square root but with a potentially worse constant $ h_t \leq \frac{2\max \{C_\L,w_0(1-\rho_\L)^{k(t)}\}}{\mu^{\away}_\L (\CondNumb^\PW)^2} w_0(1-\rho_\L)^{k(t)}$.} as,
        \begin{equation} 
          h_t \leq P_\L \sqrt{2w_0}(1- \rho_\L)^{k(t)/2}
        \end{equation}
  where $\rho_\L := \frac{\CondNumb^2\mu_\L }{2C_\L}$ and $k(t)$ is the number of non-drop step after $t$ steps. For SP-FW, $k(t)=t$ and for SP-AFW, $k(t)\geq t/3$. Moreover we can also upper bound the minimum gap observed, for all $T \in \N$
  \begin{equation}
    \min_{t \leq T} \gap \leq \frac{2\max\{\sqrt{C_\L},\sqrt{w_0} (1-\rho_\L)^{k(T)/2}\}}{\CondNumb{}} \sqrt{w_0} \left( 1- \rho_\L \right)^{k(T)/2}.
  \end{equation}
      %
      %
      %
      %
      %
  The Theorem~\ref{thm:conv} statement can be deduced from this theorem using the lower and upper bounds on the affine invariant constant of this statement. More precisely, one can upper bound $C_\L$, $M_\L$, $P_\L$ respectively with Propositions~\ref{prop:Cbounded}, \ref{Mbounded} and \ref{prop:upper_bound_P_L} and lower bound $\muIntL$ and $\mu^{\away}_\L$ respectively with Proposition~\ref{prop:deltaL} and Equation~\eqref{eq:muAwayLInequality}.\footnote{Note that only the definition of $\delta_\mu$ in~Theorem~\ref{thm:conv} for case~\eqref{case:2} requires to use the Euclidean norm (because inequality~\eqref{eq:muAwayLInequality} with the pyramidal width only holds for the Euclidean norm). On the other hand, any norm could be used for (separately) bounding $C_\L$, $M_\L$ and $P_\L$.} 
  If we apply these bounds to the rate multipliers in~\eqref{eq:thm24_state_multiplier}, it gives the smaller rate multipliers $\CondNumb$ stated in Theorem~\ref{thm:conv}. 
  \end{theorem}
  \proof
  We uses the Lemma~\ref{lemme:Lin} giving a geometric scheme, with a straightforward recurrence we prove that,
    \begin{equation}
      w_t \leq w_0 (1-\rho_\L)^{k(t)},
    \end{equation}
    where $k(t)$ is the number of non-drop step steps. This number is equal to $t$ for the SP-FW algorithm and it is lower bounded by $t/3$ for the SP-AFW algorithm (see Section~\ref{par:drop_steps} Equation~\eqref{eq:upper_bound_drop_steps}).
    Then by using Proposition \eqref{prop:relation_primal}  relating $h_t$ and the square root of $w_t$ we get the first statement of the theorem,
      \begin{equation}
       h_t \leq   P_\L \sqrt{2w_0}(1- \rho_\L)^{k(t)/2}.
      \end{equation}
    To prove the second statement of the theorem we just use Theorem~\ref{thm:subopt_gap} for the last \emph{non-drop step} after $T$ iterations (let us assume it was at step $t_0$),
    \begin{align}
     g_{t_0} &\leq   \frac{2 \max\{\sqrt{C_\L},\sqrt{w_0} (1-\rho_\L)^{k(t_0)/2}\}}{\CondNumb{}} \sqrt{w_0}(1- \rho_\L)^{k(t_0)/2} &\\
          &=\frac{2\max\{\sqrt{C_\L},\sqrt{w_0} (1-\rho_\L)^{k(T)/2}\}}{\CondNumb{}} \sqrt{w_0}(1- \rho_\L)^{k(T)/2}. & (\text{because } k(t_0)= k(T))
    \end{align}
    The minimum of the gaps observed is smaller than the gap at time $t_0$ then,
    \begin{equation}
      \min_{t\leq T}\gap \leq g_{t_0} \leq \frac{2\max\{\sqrt{C_\L},\sqrt{w_0} (1-\rho_\L)^{k(T)/2}\}}{\CondNumb{}} \sqrt{w_0}(1- \rho_\L)^{k(T)/2}.
    \end{equation}
  \endproof
  The affine invariant formulation with the universal step size $\stepsize_t = \min \left\{\stepmax,\frac{2}{2+k(t)}\right\}$ of Theorem~\ref{thm:conv} also follows from Lemma~\ref{lemme:Lin} by re-using standard FW proof patterns.
   \begin{theorem}\label{thm:conv_sublin}
   Let $\L$ be a 
    strongly convex-concave function with a finite smoothness constant $C_\L$, a positive interior strong convex-concavity constant $\muIntL$~\eqref{def:interior_strongL} or a positive geometric strong convex-concavity $\mu^{\away}_\L$~\eqref{def:geom_strongL}. Let us also define the rate multipliers~$\CondNumb$ as
  \begin{equation}
    \CondNumb^{\FW} :=1- \frac{M_\L }{\sqrt{\muIntL}} \quad\text{and}\quad \tilde \CondNumb^{\PW} := 1- \frac{
        M_\L}{\sqrt{\mu^{\away}_\L}} \quad(\text{see Equation~\eqref{eq:def_bilin_coef} for the definition of } M_\L).
  \end{equation} 
    Let $\CondNumb$ refers to
  either $\CondNumb^{\FW}$ for case~\eqref{case:1} where the algorithm is SP-FW, or $\tilde\CondNumb^{\PW}$ for case~\eqref{case:2} where the algorithm is SP-AFW,

  If $\CondNumb>\frac{1}{2}$, then the suboptimality $w_t$ of the iterates of the algorithm with universal step size $\stepsize_t = \min\left\{\stepmax,\frac{2}{2+k(t)}\right\}$ (see Equation~\eqref{eq:update} for more details about $\stepmax$) has the following decreasing upper bound:
        \begin{equation}\label{eq:sub_rate_w}
          w_t \leq \frac{C}{{2+k(t)}}
        \end{equation}
      where $C = 2 \max\left(w_0,\frac{2 C_\L}{2 \CondNumb-1}\right)$ and $k(t)$ is the number of non-drop step after $t$ steps. For SP-FW, $k(t)=t$ and for SP-AFW, $k(t)\geq t/3$. 
      Moreover we can also upper bound the minimum FW gap observed for $T\geq 1$,
  \begin{equation}
    \min_{t \leq T} \gap^\FW \leq \frac{5C}{\CondNumb{}(k(T)+1)}.
  \end{equation}
    Note that in this theorem the constant $\tilde \CondNumb^{\PW}$ is slightly different from the constant $\CondNumb^{\PW}$ in Theorem~\ref{thm:conv_affine_invariant}.
  \end{theorem}
  \proof
    We can put both the recurrence~\eqref{eq:int_strong_usual_gap} for the SP-FW algorithm and the recurrence~\eqref{eq:geom_strong_gt_gap} for the SP-AFW algorithm (from the proof of Lemma~\ref{lemme:ineg}) in the following form by using our unified notation introduced in the theorem statement:
          \begin{equation}\label{eq:proof_sublinear2}
         \wtt 
       \leq \wt  
      - \stepsize_t \CondNumb \gap
      + \stepsize_t^2 {C_\L}. 
      \end{equation}
    Note that the gap $\gap$ is the one defined in Equation~\eqref{eq:gaps} and depends on the algorithm.
    Let $(\CondNumb)$ to stand respectively for $(\CondNumb^{\FW})$ for SP-FW (case~\eqref{case:1}) or $(\tilde \CondNumb^{\PW})$ for SP-AFW (case~\eqref{case:2}). With this notation, the inequality $\gap \geq w_t$ leads to,
      \begin{equation}\label{eq:def_f}
          \wtt 
       \leq \wt \left( 1- \CondNumb\gamma_t \right)   
      + \gamma_t^2 {C_\L}. 
      \end{equation}
      Our goal is to show by induction that 
      \begin{equation}\label{eq:induction_sublin}\tag{$\star$}
      w_t \leq \frac{C}{2+k(t)} \quad \text{ where } C := 2\max \left( w_0, \frac{2C_\L}{2 \CondNumb -1} \right).
      \end{equation}
      Let us first define the convex function $f_t :\gamma \mapsto  \wt \left( 1- \CondNumb\gamma \right)
      + \gamma^2 {C_\L}$. 
      We will show that under~\eqref{eq:induction_sublin}, the function $f_t$ has the following property:
      \begin{equation}
        f_t\left(\frac{2}{2+k(t)}\right) \leq \frac{C}{3+k(t)} .
      \end{equation}
      This property is due to a simple inequality on integers; let $k = k(t)$, from the crucial induction assumption, we get:
      \begin{equation}
        f_t\left(\frac{2}{2+k}\right) 
       = 
          w_{t} \frac{2+k - 2 \CondNumb}{2+k}+ \frac{4}{(2+k)^2} {C_\L} 
       \leq 
          \frac{C}{3+k} \left[ \frac{(3+k)(k+1 - (2 \CondNumb - 1) +\frac{4C_\L}{C})}{(2+k)^2} \right], 
      \end{equation}
      but $(2 \CondNumb - 1) \geq \frac{4C_\L}{C}$ and $(3+k)(1+k)<(2+k)(2+k)$ for any $k$, thus 
      \begin{equation}\label{eq:sublin_f}
       f_t\left(\frac{2}{2+k}\right)  \leq \frac{C}{3+k}.
      \end{equation}
      Equation~\eqref{eq:sublin_f} is crucial for the inductive step of our recurrence.
      \begin{itemize}
        \item 
      Hypothesis~\eqref{eq:induction_sublin} is true for $t=0$ because $k(0)=0$. 
        \item 
      Now let us assume that \eqref{eq:induction_sublin} is true for a $t \in \N$.
      We set the stepsize $\stepsize_t := \min \left\{\stepmax,\frac{2}{2+k(t)}\right\}$.

      If $k(t+1)=k(t)+1$, it means that $\stepsize_t = \frac{2}{2+k(t)}$ and then by~\eqref{eq:def_f} and~\eqref{eq:sublin_f},
      \begin{equation} \label{eq:proof_w_sublin}
         w_{t+1} 
         \leq 
          f_t\left(\frac{2}{2+k(t)}\right) 
         \leq \frac{C}{3+k(t)}
         =  \frac{C}{2+k(t+1)}.
      \end{equation}

      If $k(t+1)= k(t)$, then it means that $0 \leq \stepsize_t < \frac{2}{2+k(t)}$. Hence, the convexity of the function $f_t$ leads us to the inequality
      \begin{align} \notag 
       w_{t+1} \leq f_t(\stepsize_t) 
       &\leq \max \left\{ f_t(0),f_t\left(\frac{2}{2+k(t)}\right) \right\} \\
       & = \max \left\{ w_t,f_t\left(\frac{2}{2+k(t)}\right) \right\}\label{eq:ineg_wt_wtt} \\
       &\leq \max \left\{ \frac{C}{2+k(t)},\frac{C}{3+k(t)} \right\} \\
       &\leq \frac{C}{2+k(t)}.
       \end{align} 
      where we used~\eqref{eq:sublin_f} and the induction hypothesis~\eqref{eq:induction_sublin} to get the penultimate inequality~\eqref{eq:ineg_wt_wtt}. Since we assumed that $k(t+1)= k(t)$, we get
      \begin{equation}
        w_{t+1} \leq  \frac{C}{2+k(t+1)}, 
      \end{equation}
      completing the induction proof for~\eqref{eq:sub_rate_w}.
      \end{itemize}
      



      In case \eqref{case:1}, $k(t) = t$ and in case \eqref{case:2}, $k(t) \geq t/3$ (see Equation~\eqref{eq:upper_bound_drop_steps}), leading us to the first statement of our theorem.

The proof of the second statement is inspired by the proof of Theorem C.3 from~\citep{lacoste2013block}.

With the same notation as the proof of Lemma~\ref{lemme:ineg}, we start from Equation~\eqref{eq:proof_sublinear2} where we isolated the gap $\gap$ to get the crucial inequality
  \begin{equation}
   \gap
         \leq \frac{\wt  -w_{t+1}}{\CondNumb \stepsize_t }
        + \gamma_t \frac{C_\L}{\CondNumb}. 
  \end{equation}
  Since the gap $\gap$ is the one depending on the algorithm  defined by $\gap := \innerProd{-\rt}{\dt}$, we have $\gap = \gap^\FW$ for SP-FW and $\gap = \max \left( \gap^\FW, \gap^\away \right)  \geq \gap^\FW$ for SP-AFW. Thus,
     \begin{equation} \label{eq:Ineg_g_wt_wtt}
    \gap^\FW \leq \gap
         \leq \frac{\wt  -w_{t+1}}{\CondNumb \stepsize_t }
        + \gamma_t \frac{C_\L}{\CondNumb}. 
  \end{equation}
  In the following in order not to be too heavy with notation we will work with de FW gap and note $\gap$ for $\gap^\FW$. 

  The proof idea is to take a convex combination of the inequality~\eqref{eq:Ineg_g_wt_wtt} to obtain a new upper-bound on a convex combination of the gaps computed from step $0$ to step $T$. Let us introduce the convex combination weight $\rho_t := \frac{\stepsize_t\cdot  k(t)( k(t) +2) }{S_T}$ where $k(t)$ is the number of non-drop steps after $t$ steps and $S_T$ is the normalization factor. Let us  also call $N_T := \{ t \leq T \, | \, t \text{ is a non-drop step}\}$.  Taking the convex combination of~\eqref{eq:Ineg_g_wt_wtt}, we get
  \begin{equation} \label{eq:Ineg_mean_g_wt_wtt}
        \sum_{t=0}^T \rho_t \gap \leq 
        \sum_{t=0}^T \rho_t \frac{\wt  -w_{t+1}}{\CondNumb \stepsize_t }+ 
        \sum_{t=0}^T \rho_t \gamma_t \frac{C_\L}{\CondNumb}. 
  \end{equation}
  By regrouping the terms and ignoring the negative term, we get
  \begin{equation} \label{eq:sum_gt_wt_rhot}
        \sum_{t=0}^T \rho_t \gap \leq 
        \frac{w_0 \rho_0}{\CondNumb \stepsize_0} +
        \frac{1}{\CondNumb}\sum_{t=0}^{T-1} w_{t+1}\left( \frac{\rho_{t+1}}{ \stepsize_{t+1} }   -\frac{\rho_t}{ \stepsize_t}\right)+ 
        \sum_{t=0}^{T} \rho_t \gamma_t \frac{C_\L}{\CondNumb}. 
  \end{equation}
 %
 %
 %
 %
 %
 %
 %
 %
 %
 %
 %
 %
  By definition $\frac{\rho_t}{\stepsize_t} := \frac{k(t)(k(t)+2)}{S_T}$ and notice that $\rho_0 = 0$. We now consider two possibilities: if $\gamma_t$ is a drop step, then $k(t+1) = k(t)$ and so
  \begin{equation}
   \frac{\rho_{t+1}}{ \stepsize_{t+1}} - \frac{\rho_{t}}{ \stepsize_{t}} = 0.
  \end{equation}
  If $\stepsize_t$ is a non-drop step, then $k(t+1) = k(t) + 1$ and thus we have
%
  \begin{align}
     \frac{\rho_{t+1}}{ \stepsize_{t+1}} - \frac{\rho_{t}}{ \stepsize_{t}} 
     &=   \frac{(k(t)+1)(k(t)+3)}{S_T} -  \frac{k(t)(k(t)+2)}{S_T} \\
     &= \frac{2k(t)+3}{S_T}. \label{eq:rho/gamma} 
     %
     %
  \end{align}
  As $\stepsize_t \leq \frac{2}{k(t)+2}$, we also have $\rho_t \stepsize_t \leq \frac{4 k(t)}{S_T (k(t)+2))} $.
  The normalization factor $S_T$ to define a convex combination is equal to 
  \begin{equation}
    S_T := \sum_{u=0}^T \stepsize_u \cdot k(u)( k(u) +2)  
    \geq  \sum_{\small\substack{u=0 \\ u \in N_{T} }}^T  \frac{2}{2 +k(u)} \cdot k(u)( k(u) +2) 
    = \sum_{k = 0}^{k(T)} 2 k
    = k(T)(K(T)+1).  
  \end{equation}
  Plugging this and~\eqref{eq:rho/gamma} in the inequality~\eqref{eq:sum_gt_wt_rhot} with the rate~\eqref{eq:induction_sublin} shown by induction gives us,
  \begin{align}
       \sum_{t=0}^T \rho_t \gap 
        &\leq 
        0 +
        \frac{1}{\CondNumb} \sum_{\small\substack{t=0 \\ t \in N_{T} }}^{T-1} \frac{C}{2+k(t+1)}\frac{2k(t)+3}{k(T)(k(T)+1)} + 
         \sum_{t=0}^T \frac{4k(t)}{k(T)(k(T)+1)(k(t)+2)}\frac{C_\L}{\CondNumb} \\
        & \leq \frac{2C}{\CondNumb{}} \frac{1}{k(T)(k(T)+1)} \Big( \sum_{\small\substack{t=0 \\ t \in N_{T} }}^{T-1}  1 + \frac{2C_\L}{C} \sum_{t=1 }^T  1 \Big) \\
        & \leq \frac{2C}{\CondNumb (k(T)+1)} (1 + \frac{\CondNumb}{2}\frac{T}{k(T)}) \leq \frac{5C}{\CondNumb (k(T)+1)},
  \end{align}
  by using $k(T) \geq T/3$.
  Finally, the minimum of the gaps is always smaller than any convex combination, so we can conclude that (for $T\geq1$):
  \begin{equation}
    \min_{0 \leq t \leq T} \gap \leq \frac{5C}{\CondNumb{}(k(T)+1)}.
  \end{equation}
  \endproof
  
  %
  %
  %

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
  %


\section{Strongly convex sets}
  \label{sec:strong_conv_proof}
  In this section, we are going to prove that the function $\s(\cdot)$ is Lipschitz continuous when the sets $\X$ and $\Y$ are strongly convex and when the norm of the two gradient components are uniformly lower bounded. We will also give the details of the convergence rate proof for the strongly convex sets situation.
  Our proof uses similar arguments as~\citet[Theorem~3.4 and~3.6]{dunn1979rates}.
  \begin{reptheorem}{thm:s_lip}  
   Let $\X$ and $\Y$ be $\beta$-strongly convex sets. 
   If $\min(\|\nabla_{\!x} L(\z)\|_{\X^*}, \|\nabla_{\!y} L(\z)\|_{\Y^*}) \geq\delta>0$ for all $\z \in \M$, then the oracle function $\z \mapsto \s(\z) := \arg\min_{\s \in \M}\innerProd{\s}{F(\z)}$ is well defined and is $\frac{4L}{\delta \beta }$-Lipschitz continuous (using the norm $\|(\x,\y)\|_{\X \times \Y} := \|\x\|_\X + \|\y\|_\Y$), where $F(\z) := \left(
     \nabla_x \L(\z),
     -\nabla_y \L(\z)
     \right)$.
  \end{reptheorem}
 \proof First note that since the sets are strongly convex, the minimum is reached at a unique point. Then, we introduce the following lemma which can be used to show that each component of the gradient is Lipschitz continuous irrespective of the other set.
    \begin{lemma} \label{lemma:FxLip}
    Let $F_x:\X\times \A \to \R^d$ be a $L$-Lipschitz continuous function (i.e. $\|F_x(\z)-F_x(\z')\|_{\X^*} \leq L \|\z - \z'\|_{\X \times \A}$) and $\X$ a $\beta$-strongly convex set. If $\forall \z \in \X\times \A, \; \|F_x(\z)\|_{\X^*}\geq \delta > 0$, then $\s_x:\z \mapsto \argmin_{\s \in \X} \prodscal{\s}{F_x(\z)}$ is $ \frac{2L}{\delta \beta }$-Lipschitz continuous.
    \end{lemma}
    \proof
    Let $\z, \, \z' \in \X\times \A$ and let $\bar \x = \frac{\s_x(\z)+ \s_x(\z')}{2}$, then
    \begin{align} 
      \prodscal{\s_x(\z)-\s_x(\z')}{-F_x(\z)} 
        & = 2\prodscal{\s_x(\z)-\bar \x}{-F_x(\z)} \notag \\
        & \geq  2\prodscal{\x-\bar \x}{-F_x(\z)}. \quad \forall \x \in \X \quad \text{(by definition of $s_x$)} \label{eq:Sinequality}
    \end{align}
    Now~\eqref{eq:Sinequality} holds for any $\x \in B_\beta\left(\tfrac{1}{2},\s_x(\z),\s_x(\z') \right)$ as this set is included in $\X$ by $\beta$-strong convexity of~$\X$.
    Then since $\bar \x$ is the center of $B_\beta\left(\tfrac{1}{2},\s_x(\z),\s_x(\z') \right)$, we can choose a $\x$ in this ball such that $\x - \bar \x$ is in the direction which achieves the dual norm of $-F_x(\z)$.\footnote{For the Euclidean norm, we choose $\x - \bar \x$ proportional to $-F_x$; but for general norms, it could be a different direction.} More specifically, we have that:
    $$
    \|-F_x(\z)\|_{\X^*} = \sup_{\| \vv \|_{\X} \leq 1} \prodscal{-F_x(\z)}{\vv}.
    $$
    As we are in finite dimensions, this supremum is achieved by some vector $\vv$. So choose $\x := \bar{\x} + \frac{\vv}{\|\vv\|_\X} \frac{\beta}{8} \|\s_x(\z)-\s_x(\z')\|_\X^2 \in B_\beta\left(\tfrac{1}{2},\s_x(\z),\s_x(\z') \right)$ and plug it in~\eqref{eq:Sinequality}:
    
    \begin{align}
      \prodscal{\s_x(\z)-\s_x(\z')}{-F_x(\z)} 
       & \geq \frac{\beta\|\s_x(\z)-\s_x(\z')\|_\X^2}{4 \|\vv\|_\X} \prodscal{\vv}{-F_x(\z)} \\
        & =  \frac{\beta\|\s_x(\z)-\s_x(\z')\|_\X^2}{4 \|\vv\|_\X}  \|F_x(\z)\|_{\X^*} \\
        & \geq \frac{\beta}{4} \|\s_x(\z)-\s_x(\z')\|_\X^2 \|F_x(\z)\|_{\X^*} . \label{eq:s_lip1}
    \end{align}
    Switching $\z$ and $\z'$ and using a similar argument, we get,
    \begin{equation} \label{eq:s_lip2}
         \prodscal{\s_x(\z')-\s_x(\z)}{-F_x(\z')} \geq \frac{\beta}{4} \|\s_x(\z)-\s_x(\z')\|_\X^2 \|F_x(\z')\|_{\X^*}  .
    \end{equation}
    Hence summing \eqref{eq:s_lip1} and \eqref{eq:s_lip2},
    \begin{align}\label{eq:s_lip_final}
       \frac{\beta}{4} (\|F_x(\z)\|_{X^*} +\|F_x(\z')\|_{\X^*}) \, \|\s_x(\z)-\s_x(\z')\|_\X^2 
       &\leq 
       \prodscal{\s_x(\z)-\s_x(\z')}{F_x(\z')-F_x(\z)} \notag\\
       &\leq
       \|\s_x(\z)-\s_x(\z')\|_\X \|F_x(\z')-F_x(\z)\|_{\X^*} \\
       & \leq L \|\s_x(\z)-\s_x(\z')\|_\X \|\z'-\z\|_{\X \times \Y} \;\;\;\text{(Lip. cty. of  $F_x$)}\notag
    \end{align}
    and finally
    \begin{equation}
      \|\s_x(\z)-\s_x(\z')\|_\X \leq \frac{4L}{\beta \left( \|F_x(\z)\|_{\X^*} +\|F_x(\z')\|_{\X^*} \right) } \|\z-\z'\|_{\X \times \Y} \leq \frac{2L}{\delta \beta } \|\z-\z'\|_{\X \times \Y}.
    \end{equation}
    \endproof
    To prove our theorem, we will notice that for the saddle point setup,
    the oracle function $\s(\cdot) := \argmin_{\s \in \M} \prodscal{\s}{F(\cdot)}$ can be decomposed as $\s(\cdot)=(\s_x(\cdot),\s_y(\cdot))$
    where $\s_x(\cdot) := \argmin_{\s \in \X} \prodscal{\s}{F_x(\cdot)}$ 
    and   $\s_y(\cdot) := \argmin_{\s \in \Y} \prodscal{\s}{F_y(\cdot)}$. 
    Then applying our lemma, the function $\s_x(\cdot)$ is Lipschitz continuous. 
    The same way $\s_y(\cdot)$ is Lipschitz continuous. Then, for all $\z,\z'$ in $\M$
    \begin{equation}
      \|\s(\z)-\s(\z')\|_{\X \times \Y} = \|\s_x(\z)-\s_x(\z')\|_\X + \|\s_y(\z)-\s_y(\z')\|_\Y \leq \frac{4L}{\delta\beta} \|\z-\z'\|_{\X \times \Y},
    \end{equation}
    which gives the definition of the Lipschitz continuity of our function and proves the theorem.
    \endproof
    In this theorem,  we introduced the function $F$. This function is monotone in the following sense:
  \begin{equation}\label{eq:nondecreasing}
        \forall \z, \z' \quad \prodscal{\z - \z'}{F(\z)-F(\z')} \geq 0. 
  \end{equation}
  Actually this property follows directly from the convexity of $\L(\cdot,\y)$ and the concavity of $\L(\x,\cdot)$.
  We can also prove that when the sets $\X$ and $\Y$ are strongly convex and when the gradient is uniformly lower bounded, we can relate the gap and the distance between $\zt$ and $\st$. 
   \begin{lemma}\label{lemma:lower_gap}
   If $\X$ is a $\beta$-strongly convex set and if $\|\nabla f\|_{\X^*}$ is uniformly lower bounded by $\delta$ on $\X$, then
  \begin{equation}
    \max_{\s \in \X} \prodscal{\s - \x}{- \nabla f(\x)}\geq \frac{\beta}{4}  \delta \|\s(\x) - \x\|^2,
  \end{equation}
  where $\s(\x) = \argmax_{\s \in \X}\prodscal{\s-\x}{-\nabla f (\x)}$.
  \end{lemma}
  \proof Let $\x$ and $\s(\x)$ be in $\X$. We have $B_\beta \left( \frac{1}{2}, \s(\x), \x \right) \subset \X$ by $\beta$-strong convexity. So as in the proof of Lemma~\ref{lemma:FxLip}, let $\vv$ be the vector such that $\|\vv\|_\X \leq 1$ and $\prodscal{-\nabla f(\x)}{\vv} = \|\nabla f(\x)\|_{\X^*}$. Let
  \begin{equation}
    \bar \s :=\frac{\s(\x)+ \x}{2} + \frac{\beta}{8} \| \s(\x) - \x\|^2 \frac{\vv}{\|\vv\|_\X} \in \X.
  \end{equation}
  Then 
  \begin{align}
    \prodscal{\s(\x) - \x}{- \nabla f(\x)} 
    & \geq \prodscal{ \bar \s - \x}{- \nabla f(\x)} \notag \\
    &= \frac{1}{2}\prodscal{\s(\x) - \x}{- \nabla f(\x)} +  \frac{\beta}{8}  \|\s(\x) - \x\|^2 \frac{\|\nabla f (\x)\|_{\X^*}}{\|\vv\|_\X} \notag \\
    &\geq \frac{1}{2}\prodscal{\s(\x) - \x}{- \nabla f(\x)} +  \frac{\beta}{8} \delta \|\s(\x) - \x\|^2 
  \end{align}
  which leads us to the desired result.
  \endproof
  From this lemma, under the assumption that $\min(\|\nabla_{\!x} L(\z)\|_{\X^*}, \|\nabla_{\!y} L(\z)\|_{\Y^*}) \geq \delta$ $\forall \z \in \X \times \Y$, it directly follows that 
  \begin{align}
    \gap^\FW = \gap^{(x)} + \gap^{(y)} &\geq \frac{\beta}{4} \delta \left( \|\st_x - \xt\|_\X^2 + \| \st_y - \yt\|_\Y^2\right) \notag \\ 
    &\geq \frac{\beta}{8} \delta \left( \|\st_x - \xt\|_\X + \| \st_y - \yt\|_\Y\right)^2 = \frac{\beta}{8} \delta  \|\st - \zt\|_{\X \times \Y}^2. \label{eq:gap_lower_dist}
  \end{align}
  Now we recall the convergence theorem for strongly convex sets from the main text, Theorem~\ref{thm:conv_strong}:
  \begin{reptheorem}{thm:conv_strong}
       Let $\L$ be a convex-concave function and $\X$ and $\Y$ two compact $\beta$-strongly convex sets. 
       Assume that the gradient of  $\L$ is $L$-Lipschitz continuous and that there exists $\delta>0$ such that $\min(\|\nabla_{\!x} L(\z)\|_{\X^*}, \|\nabla_{\!y} L(\z)\|_{\Y^*}) \geq \delta \;\, \forall \z \in \M$. Set $C_\delta := 2L + \frac{8L^2}{\beta \delta}$. 
       Then the gap $\gap^{\FW}$~\eqref{eq:gap} of the SP-FW algorithm with step size $\gamma_t = \tfrac{\gap^{\FW}}{\|\st-\zt\|^2 C_\delta}$
       converges linearly as 
     \begin{equation}
       \gap^{\FW} \leq g_0 \left( 1- \rho \right)^{t}    
     \end{equation}
    where $\rho :=  \frac{\beta \delta}{16 C_\delta}$. The initial gap $g_0$ is cheaply computed during the first step of the SP-FW algorithm. Alternatively, one can use the following upper bound to get uniform guarantees:
    \begin{equation}  
     g_0 \leq \sup_{\z\in \M} \|\nabla_x \L(\z)\|_{\X^*} D_\X + \sup_{\z\in \M} \|\nabla_y \L(\z)\|_{\Y^*} D_\Y.
     \end{equation} 
    \end{reptheorem}

     \proof
      We compute the following relation on the gap:
      \begin{align}
      g_{t+1} 
      & = \prodscal{\ztt - \stt}{F(\ztt)} \notag\\
      & = \prodscal{\zt - \stt}{F(\ztt)} + \stepsize_t\prodscal{\st - \zt}{F(\ztt)}\notag\\
      & = \prodscal{\zt - \stt}{F(\zt)} +  \prodscal{\zt - \stt}{F(\ztt)-F(\zt)} \notag\\
      &\quad + \stepsize_t\prodscal{\st - \zt}{F(\zt)} + \stepsize_t \prodscal{\st-\zt}{F(\ztt)-F(\zt)} \notag\\
      & \leq \prodscal{\zt - \stt}{F(\zt)} +  \prodscal{\zt - \stt}{F(\ztt)-F(\zt)} \notag\\
      &\quad + \stepsize_t\prodscal{\st - \zt}{F(\zt)} + \stepsize_t^2 \|\st-\zt\|^2 L \label{al:lip gradient}
      \end{align}
      where in the last line we used the fact that the function $F(\cdot)$ is Lipschitz continuous. Then using that $\prodscal{\zt - \stt}{F(\zt)} \leq \prodscal{\zt - \st}{F(\zt)}$ (by definition of $\st$), we get
      \begin{align}
      g_{t+1} 
      & \leq \gap(1- \stepsize_t) + \prodscal{\zt - \stt}{F(\ztt)-F(\zt)} +\stepsize_t^2 \|\st-\zt\|^2 L  \notag\\
      & \leq \gap(1- \stepsize_t) + \prodscal{\st - \stt}{F(\ztt)-F(\zt)} +\stepsize_t^2 \|\st-\zt\|^2 L. \label{eq:183}
      \end{align}
      The last line uses the fact that $F$ is monotone by convexity (Equation~\eqref{eq:nondecreasing}). Finally, using once again the Lipschitz continuity of $F$ and the one of $\s(\cdot)$ (by Theorem~\ref{thm:s_lip}), we get
      \begin{align}
      \prodscal{\st- \stt}{F(\ztt) - F(\zt)} 
      & \leq \| \st - \stt\| L \| \ztt - \zt\| \notag\\
      & \leq \frac{4L^2}{\beta \delta} \|\ztt - \zt \|^2 \qquad\quad (\text{Lipschitz continuity of } \s)\notag\\
      & = \frac{4L^2}{\beta \delta} \stepsize_t^2 \|\st - \zt \|^2. \label{eq:184} 
      \end{align}
      Combining~\eqref{eq:184} with~\eqref{eq:183}, we get
      \begin{equation}
        \label{eq:ineg_strongly convex_sets}
      g_{t+1} \leq \gap (1- \stepsize_t) + \stepsize_t^2 \|\st - \zt \|^2 \frac{C_\delta}{2} \qquad \text{where} \qquad C_\delta := 2L  + \frac{8L^2}{\beta \delta}.
      \end{equation}
      Thus by setting the step size $\stepsize_t = \frac{\gap}{\|\st - \zt\|^2 C_\delta}$,
       we get 
     \begin{equation}
        g_{t+1} \leq \gap - \frac{\gap}{2 C_\delta} \left( \frac{\gap}{\|\st-\zt\|^2} \right) \leq \gap \left( 1- \frac{\beta \delta}{16 C_\delta} \right), 
     \end{equation}
     using the fact that the gap is lower bounded by a constant times the square of the distance between $\st$ and $\zt$ (Equation~\eqref{eq:gap_lower_dist}).
    \endproof
    Note that the bound in this theorem is not affine invariant because of the presence of Lipschitz constants and strong convexity constants of the sets. The algorithm is not affine invariant either because the step size rule depends on these constants as well as on $\|\st-\zt\|$. Deriving an affine invariant step size choice and convergence analysis is still an interesting open problem in this setting.

\section{Details on the experiments} %
\label{sec:details_on_the_experiments}

  \paragraph{Graphical Games.} %
  \label{par:graphical_games}
  The payoff matrix $M$ that we use encodes the following simple
model of competition between universities with their respective benefits: 
\begin{enumerate}
  \item University~1 (respectively University~2) has benefit $b_i^{(1)}$ ($b_i^{(2)}$) to get student $i$. 
  \item Student $i$ ranks the possible
roommates with a permutation $\sigma_i \in \mathcal S_p$. Let $\sigma_i(j)$ represents the rank of $j$ for $i$ (first in the list is the preferred one). 
  \item They go to the university that matched them with their preferred roommate, in case of equality the student chooses randomly.
  \item Supposing that $\x$ encodes the roommate assignment proposed by University~1 (and $\y$ for University~2), then the expectation of the benefit of University~1 is $\x^\top M \y$,
with the following definition for the payoff matrix $M$ indexed by pairs
of matched students. For the pairs $(i,j)$ with $i < j$ and $(k,l)$ with $k < l$ with elements in $1, \ldots, s$, we have:
\begin{enumerate}
  \item $M_{ij,il} = \left\{ 
            \begin{array}{lll}
            b_i^{(1)} & \text{if} \; \sigma_i(j) < \sigma_i(l) \quad \text{\emph{i.e. student $i$ preferred $j$ over $l$}}\\
            -b_i^{(2)} & \text{if} \; \sigma_i(j) > \sigma_i(l) \\
            \frac{b_i^{(1)} - b_i^{(2)}}{2} & \text{otherwise}  \quad (\text{in that case }j = l).
            \end{array} \right.$
  \item $M_{ij,kj} = M_{ji,jk}$ %
  \item $M_{ij,ki} = M_{ij,ik}$
  \item $M_{ij,jl} = M_{ij,lj}$
  \item $M_{ij,kl} = 0 \quad \text{otherwise}$
\end{enumerate}
\end{enumerate}
Note that we need to do unipartite matching here (and not bipartite
matching) since we have to match students together and not students with dorms. 

For our experiments, in order to get a realistic payoff matrix, we set $\mu_i \sim \mathcal U[0,1]$ the \emph{true} value of student $i$. Then we set $b_i^{(U)}\sim \mathcal N(\mu_i, 0.1)$ the value of the student $i$ \emph{observed} by University~$U$. To solve the perfect matching problem, we used Blossom V by \citetsup{kolmogorov2009blossom}. 
  \paragraph{Sparse structured SVM.} %
  \label{par:Structured SVM}
  We give here more details on the derivations of the objective function for the structured SVM problem.
  We first recall the structured prediction setup with the same notation from~\citep{lacoste2013block}. In structured prediction, the goal is to predict a structured object $\y \in \Y(\x)$ (such as a sequence of tags) for a given input $\x \in \X$. For the structured SVM approach, a structured feature map $\bm{\phi} : \X \times \Y \rightarrow \R^d$ encodes the relevant information for input / output pairs, and a linear classifier with parameter $\bm{w}$ is defined by $h_{\bm{w}} = \argmax_{\y \in \Y(\x)} \prodscal{\bm{w}}{\bm{\phi}(\x, \y)}$. We are also given a task-dependent structured error $L(\y', \y)$ that gives the loss of predicting $\y$ when the ground truth is $\y'$. Given a labeled training set $\{(\x^{(i)}, \y^{(i)})\}_{i=1}^n$, the standard $\ell_2$-regularized structured SVM objective in its non-smooth formulation for learning as given for example in Equation~(3) from \citep{lacoste2013block} is:
  \begin{equation}
    \min_{ \bm{w} \in \R^d} \frac{\lambda}{2}\|\bm{w}\|_2^2 + \frac{1}{n} \sum_i \tilde H_i(\bm{w})
  \end{equation}
  where $\tilde H_i(\bm{w}) := \max_{\y \in \mathcal Y_i} \; L_i(\y) - \prodscal{\bm{w}}{\bm{\psi}_i(\y)}$ is the structured hinge loss, and the following notational shorthands were defined: $\Y_i := \Y(\x^{(i)})$, $L_i(\y) := L(\y^{(i)}, \y)$ and $\bm{\psi}_i(\y) := \bm{\phi}(\x^{(i)},\y^{(i)}) - \bm{\phi}(\x^{(i)},\y)$.
  
  In our setting, we consider a sparsity inducing $\ell_1$-regularization instead. Moreover, we use the (equivalent) constrained formulation instead of the penalized one, in order to get a problem over a polytope. We thus get the following challenging problem:
  \begin{equation}
    \min_{\|\bm{w}\|_1  \leq R} \frac{1}{n} \sum_i \tilde H_i(\bm{w}).
  \end{equation}
  To handle any type of structured output space $\Y$, we use the following generic encoding.
  Enumerating the elements of $\Y_i$, we can represent the $j^{th}$ element of $\Y_i$ as $(\overbrace{0,\ldots,0}^{j-1},1,0,\ldots,0 ) \in \R^{|\Y_i|}$. Let $M_i$ have $\big(\bm{\psi}_i(\y)\big)_{\y \in \mathcal Y_i}$ as columns and let $\bm{L}_i$ be a vector of length $|\Y_i|$ with $L_i(\y)$ as its entries. 
  The functions $\tilde H_i(\bm{w})$ can then be rewritten as the maximization of linear functions in $\y$: $\tilde H_i(\bm{w}) = \max_{\y \in \mathcal Y_i} \; \bm{L}_i^\top \y - \bm{w}^\top M_i \y$.
  As the maximization of linear functions over a polytope is always obtained at one of its vertex, we can equivalently define the maximization over the convex hull of $\Y_i$, which is the probability simplex in $\R^{|\Y_i|}$ that we denote $\Delta(|\Y_i|)$: 
  \begin{equation}
    \max_{\y_i \in \mathcal Y_i} \; \bm{L}_i^\top \y_i - \bm{w}^\top M_i \y_i =  \max_{\bm{\alpha}_i \in \Delta(|\mathcal Y_i|)} \!\!\bm{L}_i^\top \bm{\alpha}_i - \bm{w}^\top M_i \bm{\alpha}_i
  \end{equation}
  Thus our equivalent objective is 
  \begin{equation}
   \min_{\|\bm{w}\|_1  \leq R} \frac{1}{n} \sum_i \Big( \max_{\y_i \in \mathcal Y_i} \; \bm{L}_i^\top \y_i - \bm{w}^\top M_i \y_i \Big) 
    = 
    \min_{\|\bm{w}\|_1 \leq R} \frac{1}{n} \sum_i \Big( \max_{\bm{\alpha}_i \in \Delta(|\mathcal Y_i|)} \!\!\bm{L}_i^\top \bm{\alpha}_i - \bm{w}^\top M_i \bm{\alpha}_i \Big) ,
  \end{equation}
  which is the bilinear saddle point formulation given in the main text in~\eqref{eq:structuredSVM}.
  %
%
%
%
%

  

\bigskip
\bigskip
%
\bibliographystylesup{abbrvnat}
\bibliographysup{bib}

\end{document}\vspace{-2mm}
\section{Algorithm}
\vspace{-3mm}

Alg.~\ref{alg:algInfadaptive}
describes the pseudocode for our proposed algorithm to do marginal inference with $\TRW$. 
$\textrm{\textbf{minSpanTree}}$ finds the minimum spanning tree of a
weighted graph, and $\MIfunction(\vmu)$ computes the mutual information of edges of $G$ 
from the pseudomarginals in $\vmu$\footnote{The component $ij$ has value $H(\bmu_i)+H(\bmu_j)-H(\bmu_{ij}).$}
(to perform FW updates over $\vrho$ as in Alg. 2 in~\citet{wainwright2005new}).
It is worthwhile to note that our approach uses three levels of Frank-Wolfe: (1) for the (tightening) optimization
of~$\vrho$ over~$\TREEPOL$, (2) to perform approximate marginal inference, i.e for the optimization of $\vmu$ over $\MARG$, and (3) to perform the correction steps (lines~16 and~23).
%
%
%
\begin{algorithm}[t]
	\caption{Approximate marginal inference over $\MARG$ (solving~\eqref{eqn:upperBoundPartition}). Here $f$ is the negative TRW objective.}
	\label{alg:algInfadaptive}
	\begin{algorithmic}[1]
		\STATE Function \textbf{TRW-Barrier-FW}$(\vrho^{(0)}, \stopCrit, \epsk{\mathrm{init}},\unif)$:
		\STATE \textbf{Inputs:} Edge-appearance probabilities $\vrho^{(0)}$, $\epsk{\mathrm{init}}\leq \frac{1}{4}$ initial contraction of polytope, inner loop stopping criterion $\stopCrit$, fixed reference point $\unif$ in the interior of $\mathcal{M}$. Let $\epsk{-1} = \epsk{\mathrm{init}}$.
		\STATE Let $V := \{\unif\}$ (visited vertices),  $\x^{(0)} = \unif$ \quad (Initialize the algorithm at the uniform distribution) 
	\FOR[\emph{FW outer loop to optimize $\vrho$ over $\TREEPOL$}]{$i=0\dots\MAXRHOITS$}
		\FOR[\emph{FCFW inner loop to optimize $\x$ over $\MARG$}]{$k=0\dots \MAXITS$}
			\STATE Let $\tilde{\theta} = \nabla f(\x^{(k)};\vtheta,\vrho^{(i)})$ \algComment{Compute gradient}
			\STATE Let $\sk \in \displaystyle\argmin_{\vv \in \MARG} \,\, \brangle{\tilde{\theta},\vv}$ \algComment{Run MAP solver to compute FW vertex} 
			\STATE Compute $\gk=\brangle{-\tilde{\theta},\sk-\xk}$ \algComment{Inner loop FW duality gap}
			\IF{$\gk \leq \stopCrit$}
				\STATE \textbf{break} FCFW inner loop  \algComment{$\x^{(k)}$ is $\stopCrit$-optimal}
			\ENDIF
			\STATE $\epsk{k} = \epsk{k-1}$ \algComment{For Adaptive-$\shrinkAmount$: Run Alg. \ref{alg:adaptive_update} to modify $\shrinkAmount$}
			%
			%
			%
			%
			%
			%
			%
			%
			\STATE Let $\skeps = (1-\epsk{k})\sk + \epsk{k} \unif$ and $\dkeps = \skeps - \x^{(k)}$
				\algComment{$\shrinkAmount$-contracted quantities}
			\STATE $\x^{(k+1)} = \arg\min \{ f(\x^{(k)}+\gamma \, \dkeps) : \stepsize \in [0,1] \}$ \algComment{FW step with line search}
			\STATE Update correction polytope: $V := V \cup \{ \sk \}$
			\STATE $\x^{(k+1)} := \CORRECTION(\x^{(k+1)}, V, \epsk{k}, \vrho^{(i)})$ 
			\algComment{optional: correction step}
			\STATE $\x^{(k+1)},V_{\mathrm{search}} := \LOCALSEARCH(\x^{(k+1)}, \sk,\epsk{k}, \vrho^{(i)})$ \algComment{optional: fast MAP solver}
			\STATE Update correction polytope (with vertices from $\LOCALSEARCH$): $V := V \cup \{ V_{\mathrm{search}}\}$
		\ENDFOR
		\STATE $\vrho^{v} \leftarrow \textrm{\textbf{minSpanTree}}(\MIfunction(\x^{(k)}))$ \algComment {FW vertex of the spanning tree polytope}
		\STATE $\vrho^{(i+1)} \leftarrow \vrho^{(i)}+(\frac{i}{i+2})(\vrho^{v}-\vrho^{(i)})$ \algComment{Fixed step-size schedule FW update for $\vrho$ kept in $\mathrm{relint}(\TREEPOL$)}
		\STATE $\x^{(0)}\leftarrow\x^{(k)}$, $\quad \epsk{-1} \leftarrow \epsk{k-1}$ \algComment{Re-initialize for FCFW inner loop}
		\STATE If $i<\MAXRHOITS$ then $\x^{(0)} = \CORRECTION(\x^{(0)},V,\epsk{-1},\vrho^{(i+1)})$ %
	\ENDFOR
	\RETURN $\x^{(0)}$ and $\vrho^{(i)}$
	\end{algorithmic}
\end{algorithm}
%
%
%
%
%
%
We detail a few heuristics that aid practicality. 

%
%

\textbf{Fast Local Search: }
Fast methods for MAP inference such as Iterated Conditional Modes \citep{besag1986statistical} offer a cheap,
low cost alternative to a more expensive combinatorial MAP solver. We warm start the ICM solver
with the last found vertex $\sk$ of the marginal polytope.
The subroutine $\LOCALSEARCH$ (Alg.~\ref{alg:algLocalSearch} in Appendix) performs
a fixed number of FW updates to the pseudomarginals using ICM as the (approximate) MAP solver.  

\textbf{Re-optimizing over the Vertices of~$\mathcal{M}$ (FCFW algorithm): }
As the iterations of FW progress, we keep track of the vertices of the marginal polytope 
found by Alg.~\ref{alg:algInfadaptive} in the set~$V$.
We make use of these vertices in the $\CORRECTION$ subroutine
(Alg.~\ref{alg:algReopt} in Appendix)
which re-optimizes the objective function over (a contraction of) the convex hull of the elements of $V$ (called the correction polytope).
%
%
$\x^{(0)}$ in Alg.~\ref{alg:algInfadaptive} is initialized to the uniform distribution
which is guaranteed to be in~$\MARG$ (and~$\Meps$). After updating~$\vrho$, 
%
we set $\x^{(0)}$ to the approximate minimizer in 
the correction polytope.
%
%
The intuition is that changing $\vrho$ by a small amount
may not substantially modify the optimal $\x^*$ (for the new $\vrho$) and that the new optimum might 
be in the convex hull of the
vertices found thus far. 
If so, $\CORRECTION$ will be able to find it without
resorting to any additional MAP calls. 
This encourages the MAP solver to search 
for new, unique vertices instead of rediscovering old ones. 

\textbf{Approximate MAP Solvers: }
We can swap out the exact MAP solver with an approximate MAP solver. 
%
The primal objective plus the (approximate) duality gap may no longer be an upper bound 
on the log-partition function (black-box MAP solvers could be
considered to optimize over an inner bound to the marginal polytope).
%
%
%
%
%
Furthermore, the gap over $\DOM$ may be negative
if the approximate MAP solver fails to find a direction of descent. Since adaptive-$\shrinkAmount$ requires 
that the gap be positive in Alg.~\ref{alg:adaptive_update},
we take the max over the last gap obtained over the correction
polytope (which is always non-negative) and the computed gap over $\DOM$ as a heuristic. 

Theoretically, one could get similar convergence rates 
as in Thm.~\ref{thm:convergence_fixed_eps_main}
and~\ref{thm:convergence_adaptive_eps_main} using an approximate
MAP solver that has a multiplicative guarantee on the gap (line~8 of Alg.~\ref{alg:algInfadaptive}),
as was done previously for FW-like algorithms (see, e.g., Thm.~C.1 in~\citet{lacoste2012block}).
With an $\epsilon$-additive error guarantee on the MAP solution, one can prove 
similar rates up to a suboptimality error of $\epsilon$.
Even if the approximate MAP solver does not provide an approximation
guarantee, if it returns an  {\em upper bound} on the value of the MAP
assignment (as do branch-and-cut solvers for integer linear programs,
or \cite{SontagEtAl_uai08}),
one can use this to obtain an upper bound on $\log Z$ (see App.~\ref{sec:approx_map_logz}).

%
%
%
%
%
%
\section{Bounding $\log Z$ with Approximate MAP Solvers\label{sec:approx_map_logz}}

Suppose that we use an approximate MAP solver for line 7 of Algorithm \ref{alg:algInfadaptive}.
We show in this section that if the solver returns an {\em upper bound} on the value of the MAP
assignment (as do branch-and-cut solvers for integer linear programs), we can use this to get an upper bound on $\log Z$.
%
%
For notational consistency, we consider using Algorithm
\ref{alg:algInfadaptive} for $\min_{\x\in\DOM}f(\x)$, where
$f(\x) = -\TRW$ is convex, $\x=\vmu$, and $\DOM = \MARG$.

The property that the duality gap may be used as a certificate of optimality \citep{jaggi2013revisiting} gives us:
\begin{equation}
	\label{eqn:dual_gap_cert}
f(\bm{x}^*)\geq\fk-\gk\implies -f(\bm{x}^*)\leq -\fk + \gk.
\end{equation}
Adding the gap onto the TRW objective yields an upper bound on the optimum (which from Equation \ref{eqn:upperBoundPartition} is an
upper bound on $\log Z$), i.e. $\log Z\leq -f(\bm{x}^*)$. From our definition of the duality gap $\gk$ (line 8 in Algorithm \ref{alg:algInfadaptive}) and \eqref{eqn:dual_gap_cert}, we have:
\begin{align*}
\log Z \leq -f(\bm{x}^*)     &\leq -\fk + \innerProd{-\nabla \fk}{\sk-\xk} \\
&= -\fk + \underbrace{\innerProd{-\nabla \fk}{\sk}}_{\text{MAP call}} - \underbrace{\innerProd{-\nabla \fk}{\xk}}_{\text{Can be computed efficiently}},
\end{align*}
where $\sk = \argmin_{\vv\in\DOM}\innerProd{\nabla \fk}{\vv} =
\argmax_{\vv\in\DOM}\innerProd{-\nabla \fk}{\vv}$ (line~7 in Algorithm~\ref{alg:algInfadaptive}). Thus, if the approximate MAP solver returns
an upper bound $\kappa$ such that $\max_{\vv\in\DOM}\innerProd{-\nabla
  \fk}{\vv} \leq \kappa$, then we get the following upper bound on the
log-partition function:
\begin{equation}
\log Z \leq -\fk + \kappa - \innerProd{-\nabla \fk}{\xk}.
\end{equation}
%
%

For example, we could use a linear programming relaxation or a
message-passing algorithm based on dual decomposition such as
\citet{SontagEtAl_uai08} to obtain the upper bound $\kappa$.
There is a subtle but important point to note about this approach.
Despite the fact that we may use a relaxation
of $\MARG$ such as $\LOCAL$ or the cycle relaxation to compute the upper bound, we evaluate it at $\vmu^{(k)}$
that is {\em guaranteed} to be within $\MARG$. This should be
contrasted to instead optimizing over a relaxation such as $\LOCAL$ directly with Algorithm \ref{alg:algInfadaptive}. 
In the latter setting, the moment we move towards a fractional vertex (in line ~14)
we would immediately take $\vmu^{(k+1)}$ out of $\MARG$. Because
of this difference, we expect that this approach will typically result in
significantly tighter upper bounds on $\log Z$.

\section{Conclusion and future work}
\vspace{-2mm}
We have described a joint model that relates object states and manipulation actions.
Given a set of input videos, our model both localizes the manipulation actions \emph{and} discovers the corresponding object states. 
We have demonstrated that our joint approach improves performance of both object state recognition and action recognition. 
More generally, our work provides evidence that actions should be modeled in the larger context of goals and effects.
Finally, our work opens up the possibility of Internet-scale learning of manipulation actions from narrated video sequences.
%
\section{Experiments}
\label{sec:experiments}

In this section, we first describe our dataset, the object tracking pipeline and the feature representation for object tracklets and videos (Section~\ref{sec:dataset}). 
We consider two experimental set-ups. 
In the first weakly-supervised set-up (Section~\ref{sec:exp_res}), we apply our method on a set of video clips which we know contain the action of interest but do not know its precise temporal localization. 
In the second, more challenging ``in the wild" set-up (Section~\ref{sec:exp_weak}), the input set of weakly-supervised clips is obtained by automatic processing of text associated with the videos and hence may contain erroneous clips that do not contain the manipulation action of interest.         
The data and code are available online~\cite{Alayrac16ObjectStatesWeb}. 
%
%
%
%
%


\subsection{Dataset and features}
\label{sec:dataset}

%
%
%
%
%
%
\noindent\textbf{Dataset of manipulation actions.}
We build a dataset of manipulation actions by collecting videos from different sources: the instructional video dataset introduced in~\cite{Alayrac15Unsupervised}, the Charades dataset from~\cite{varol16hollywood}, and some additional videos downloaded from YouTube. We focus on ``third person" videos (rather than egocentric) as such videos depict a variety of people in different settings and can be obtained on a large scale from YouTube.
We annotate the precise temporal extent of seven different actions\footnote{\textit{put the wheel on the car (47 clips)}, \textit{withdraw the wheel from the car (46)}, \textit{place a plant inside a pot (27)}, \textit{open an oyster (28)}, \textit{open a refrigerator (234)}, \textit{close a refrigerator (191)} and \textit{pour coffee (57)}.} applied to five distinct objects\footnote{\textit{car wheel}, \textit{flower pot}, \textit{oyster}, \textit{refrigerator} and \textit{coffee cup}.}. 
This results in 630 annotated occurrences of ground truth manipulation action. 

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
To evaluate object state recognition, we define a list of two states for each object. %
We then run automatic object detector for each involved object, track the detected object occurrences throughout the video and then subdivide the resulting long tracks into short tracklets.
%
%
%
Finally, we label ground truth object states for tracklets within $\pm$40 seconds of each manipulation action.  
We label four possible states: \textit{state 1}, \textit{state 2}, \textit{ambiguous state} or \textit{false positive detection}.
The ambiguous state covers the (not so common) in-between cases, such as cup half-full.  
In total, we have 19,499 fully annotated tracklets out of which: $35\%$ cover \textit{state 1} or \textit{state 2}, $25\%$ are ambiguous, and $40\%$ are false positives.
Note that this annotation is only used for evaluation purpose, and not by any of our models.
Detailed statistics of the dataset are given in Appendix~\ref{app:dataset}.

\noindent\textbf{Object detection and tracking.}
In order to obtain detectors for the five objects, we finetune the FastRCNN network~\cite{girsh15fastrcnn} with training data from ImageNet~\cite{imagenet09}.
We use bounding box annotations from ImageNet when available (\eg the ``wheel" class).
For the other classes, we manually labeled more than 500 instances per class.
%
In our set-up with only moderate amount of training data, we observed that class-agnostic object proposals combined with FastRCNN performed better than FasterRCNN~\cite{ren2015faster}.
In detail, we use geodesic object proposals~\cite{kra2014gop} and set a relatively low object detection threshold ($0.4$) to have good recall.
We track objects using a generic KLT tracker from~\cite{Bojanowski13finding}.
The tracks are then post-processed into shorter tracklets that last about one second and thus are likely to have only one object state.
%

\noindent\textbf{Object tracklet representation.}
For each detected object, represented by a set of bounding boxes over the course of the tracklet, we compute a CNN feature from each (extended) bounding box that we then average over the length of the tracklet to get the final representation.
The CNN feature is extracted with a ROI pooling~\cite{ren2015faster} of ResNet50~\cite{he16resnet}.
The ROI pooling notably allows to capture some context around the object which is important for some cases (\eg \textit{wheel} ``on" or ``off" the car).
The resulting feature descriptor of each object tracklet is 8,192 dimensional.

\noindent\textbf{Representing video for recognizing actions.}
Following the approach of~\cite{Alayrac15Unsupervised,Bojanowski14weakly,Bojanowski15weakly}, each video is divided into chunks of 10 frames that are represented by a motion and appearance descriptor averaged over 30 frames.
For the motion we use a 2,000 dimensional bag-of-word representation of histogram of local optical flow (HOF) obtained from Improved Dense Trajectories~\cite{Wang13action}.
Following~\cite{Alayrac15Unsupervised}, we add an appearance vector that is obtained from a 1,000 dimensional bag-of-word vector of conv5 features from VGG16~\cite{Simonyan14vggnets}.
This results in a 3,000 dimensional feature vector for each chunk of 10 frames.

\begin{table*}[t!]
	\centering
	\resizebox{0.74\textwidth}{!}{
		\begin{tabular}{clccccccc|c}
			\noalign{\hrule height 1.3pt}
			\multicolumn{1}{l}{}                                                                                     & \multicolumn{1}{c}{}                         & put           & remove        & fill          & open          & fill          & open          & close         & \multicolumn{1}{l}{}                          \\
			\multicolumn{1}{l}{}                                                                                     & \multicolumn{1}{c}{\multirow{-2}{*}{Method}} & wheel         & wheel         & pot           & oyster        & coff.cup      & fridge        & fridge        & \multicolumn{1}{l}{\multirow{-2}{*}{\textbf{Average}}} \\ \noalign{\hrule height 1pt}
			
			& (\textbf{a}) Chance                        & 0.10         & 0.11          & 0.10          & 0.07          & 0.06 & 0.10         & 0.10          & 0.09                                          \\
			
			& (\textbf{b}) Kmeans                                   & 0.25          & 0.12          & 0.11          & 0.23          & 0.14          & 0.19          & 0.22          & 0.18                                         \\
			& (\textbf{c}) Constraints only                       & 0.35          & 0.38          & 0.35          & 0.36          & 0.31 & 0.29         & 0.42          & 0.35                                          \\
			
			& (\textbf{d}) Salient state only                       & 0.35          & 0.48          & 0.35          & 0.38          & 0.30          & 0.40          & 0.37          & 0.38                                          \\
			
			& (\textbf{e}) At least one state only                  & 0.43 & 0.55 & 0.46          & 0.52          & 0.29          & 0.43          & 0.39          & 0.44                                          \\
			
			
			
			& (\textbf{f}) Joint model                              & \textbf{0.52}          & 0.59          & \textbf{0.50} & 0.45 & 0.39          & \textbf{0.47} & \textbf{0.47} & 0.48                                 \\
			
			&(\textbf{g}) Joint model + det. scores.                                             & 0.47          & \textbf{0.65}          & \textbf{0.50}          & \textbf{0.61}        & \textbf{0.44}          & 0.46          & 0.43          & \textbf{0.51}                                         \\ \cline{2-10} 
			
			\multirow{-8}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}State\\ discovery\end{tabular}}} & (\textbf{h}) Joint + GT act. feat.                    & 0.55          & 0.56          & 0.56          & 0.52          & 0.46          & 0.45          & 0.49          & 0.51                                          \\ \noalign{\hrule height 1pt}
			& (\textbf{i})\phantom{ii} Chance                                   & 0.31          & 0.20          & 0.15          & 0.11          & 0.40          & 0.23          & 0.17          & 0.22                                          \\
			& (\textbf{ii})\phantom{i} \cite{Bojanowski15weakly}      & 0.24          & 0.13          & 0.11          & 0.14          & 0.26          & 0.29          & 0.23          & 0.20                                          \\
			& (\textbf{iii}) \cite{Bojanowski15weakly} + object cues                          & 0.24          & 0.13          & 0.26          & 0.07          & \textbf{0.84} & 0.33          & 0.37          & 0.32                                          \\
			& (\textbf{iv})\phantom{i} Joint model                             & \textbf{0.67} & \textbf{0.57} & \textbf{0.48} & \textbf{0.32} & 0.82          & \textbf{0.57} & \textbf{0.44} & \textbf{0.55}                                 \\ \cline{2-10} 
			\multirow{-5}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Action \\ localization\end{tabular}}}                      & (\textbf{v})\phantom{i\textbf{i}} Joint + GT stat. feat.                   & 0.72          & 0.66          & 0.44          & 0.46          & 0.86          & 0.55          & 0.44          & 0.59                                          \\ \noalign{\hrule height 1.3pt}
		\end{tabular}
	}
	\vspace{-2mm}
	\caption{\small State discovery (top) and action localization results (bottom). \label{tab:quantres}}
\vspace*{-3mm}	
\end{table*}


\subsection{Weakly supervised object state discovery}
%
\label{sec:exp_res}

\noindent\textbf{Experimental setup.}
We first apply our method in a weakly supervised set-up where for each action we provide an input set of clips, where we know the action occurs somewhere in the clip
but we do not provide the precise temporal localization.  Each clip may contain other actions that affect other objects or actions that do not affect any object at all (e.g. walking / jumping). The input clips are about 20s long and are obtained by taking approximately $\pm$ 10s of each annotated manipulation action.  
%
%

%
%
%
%
%


\noindent\textbf{Evaluation metric: average precision.}
For all variants of our method, we use the rounded solution that reached the smallest objective during optimization.
We evaluate these predictions with a precision score averaged over all the videos.
A temporal action localization is said to be correct if it falls within the ground truth time interval.
Similarly, a state prediction is correct if it matches the ground truth state.\footnote{In particular, we count ``\textit{ambiguous}" labels as incorrect.}
Note that a ``precision" metric is reasonable in our set-up as our method is forced to predict in all videos, i.e. the recall level is fixed to all videos and the method cannot produce high precision with low recall. 

\noindent\textbf{Hyperparameters.}
In all methods that involve a discriminative clustering objective, we used $\lambda=10^{-2}$ (action localization) and $\mu=10^{-4}$ (state discovery) for all 7 actions. 
For joint methods that optimize~\eqref{eq:jointstateactionprob}, we set the weight $\nu$ of the distortion measure~\eqref{eq:distGlobal} to $1$.

\noindent\textbf{State discovery results.}
Results are shown in the top part of Table~\ref{tab:quantres}.
In the following, we refer to ``State only" whenever we use our method without looking at the action cost or the distortion measure~\eqref{eq:jointstateactionprob}.
We compare to two baselines for the state discovery task.
Baseline~(\textbf{a}) evaluates chance performance.
Baseline~(\textbf{b}) performs K-means clustering of the tracklets with $K=3$ (2 clusters for the states and 1 for false positives).
We report performance of the best assignment for the solution with the lowest objective after 10 different initializations.
Baseline~(\textbf{c}) is obtained by running our ``State only" method while using random features for tracklet representation as well as "at least one ordering" and "non overlap" constraints. We use random features to avoid non-trivial analytic derivation for the "Constraints only" performance. This baseline reveals the difficulty of the problem and quantifies improvement brought by the ordering constraints.
The next two methods are ``State only" variants.
Method~(\textbf{d}) corresponds to a replacement of the ``at least one constraint" by an ``exactly one constraint" while the method~(\textbf{e}) uses our new constraint.
Finally, we report three joint methods that use our new joint rounding technique~\eqref{eq:jcrrounding} for prediction.
Method~(\textbf{f}) corresponds to our joint method that optimizes~\eqref{eq:jointstateactionprob}.
Method~(\textbf{g}) is a simple improvement taking into account object detection score in the objective (details below). %
Finally, method~(\textbf{h}) is our joint method but using the action ground truth labels as video features in order to test the effect of having perfect action localization for the task of object state discovery. 

We first note that method~(\textbf{e}) outperforms~(\textbf{d}), thus highlighting the importance of the ``at least one" constraint for modeling object states.
While the saliency approach (taking only the most confident detection per video) was useful for action modeling in~\cite{Bojanowski15weakly}, it is less suitable for our set-up where multiple tracklets can be in the same state.
%
The joint approach with actions~(\textbf{f}) outperforms the ``State only" method~(\textbf{e}) on 6 out of 7 actions and obtains better average performance,  confirming the benefits of joint modeling of actions and object states.
Using ground truth action locations further improves results (cf.~(\textbf{h}) against (\textbf{f})).
Our weakly supervised approach~(\textbf{f}) performs not much lower compared to using ground truth actions (\textbf{h}), except for the states of the coffee cup (empty/full). In this case we observe that a high number of false positive detections confuses our method.
A simple way to address this issue is to add the object detection score into the objective of our method, which then prefers to assign object states to higher scoring object candidates further reducing the effect of false positives.  
This can be done easily by adding a linear cost reflecting the object detection score to objective~\eqref{eq:jointstateactionprob}.
We denote this modified method ``(\textbf{g}) Joint model + det. scores". 
This method achieves the best average performance and highlights that additional information can be easily added to our model.

\noindent\textbf{Action localization results.}
We compare our method to three different baselines and give results in the bottom part of Table~\ref{tab:quantres}.
Baseline~(\textbf{i}) corresponds to chance performance, where the precision for each clip is simply the proportion of the entire clip taken by the ground truth time interval.
Baseline~(\textbf{ii}) is the method introduced in~\cite{Bojanowski15weakly} used here with only one action. It also corresponds to a special case of our method where the object state part of the objective in equation~\eqref{eq:jointstateactionprob} is turned off (salient action only).
Interestingly, this baseline is actually worse than chance for several actions. This is because without additional information about objects, this method localizes  other common actions in the clip and not the action manipulating the object of interest. This also demonstrates the difficulty of our experimental set-up where the input video clips often contain multiple different actions.
%
%
To address this issue, we also evaluate baseline~(\textbf{iii}), which complements~\cite{Bojanowski15weakly} with the additional constraint that the action prediction has to be within the first and the last frame where the object of interest is detected, improving the overall performance above chance.
%
Our joint approach~(\textbf{iv}) consistently outperforms these baselines on all actions, thus showing again the strong link between object states and actions.
Finally, the approach~(\textbf{v}) is the analog of method~(\textbf{g}) for action localization where we use ground truth state labels as tracklet features in our joint formulation showing that the action localization can be further improved with better object state descriptors.
In addition, we also compare to a supervised baseline.
The average obtained performance is 0.58 which is not far from our method. 
This demonstrates the potential of using object states for action localization. 
More details on this experiment are provided in Appendix~\ref{app:supervised}.

\noindent\textbf{Benefits of joint object-action modeling.}
We observe that the joint modeling of object states and actions benefits both tasks.
This effect is even stronger for actions.  
Intuitively, knowing perfectly the object states reduces a lot the search space for action localization. 
Moreover, despite the recent major progress in object recognition using CNNs, action recognition still remains a hard problem with much room for improvement. 
Qualitative results are shown in Fig.~\ref{fig:qualres} and failure cases of our method are discussed in~\ref{app:failcases}.
	
\begin{figure}
	\raggedleft    
	%
	\includegraphics[width=0.99\linewidth]{figs/qual_res_slim.pdf}
	\caption{\small Qualitative results for joint action localization (middle) and state discovery (left and right) (see Fig.~\ref{fig:teaser} for ``\textit{fill coffee cup}").}
	\vspace*{-.5cm}
	\label{fig:qualres}
\end{figure}

\subsection{Object state discovery in the wild}
\label{sec:exp_weak}
Towards the discovery of a large number of manipulation actions and state changes, we next apply our method in an automatic setting, where action clips have been obtained using automatic text-based retrieval.
%
%
%

%
 
%
%
%
%
%

\noindent\textbf{Clip retrieval by text.}
Instructional videos~\cite{Alayrac15Unsupervised,Malmaud15what,Sener15unsupervised} usually come with a narration provided by the speaker describing the performed sequence of actions. In this experiment, we keep only such {\em narrated instructional videos} from our dataset. This results in the total of 140 videos that are 3 minutes long in average. 
We extract the narration in the form of subtitles associated with the video. 
These subtitles have been directly downloaded from YouTube and have been obtained either by Youtube's Automatic Speech Recognition (ASR) or provided by the users.%

We use the resulting text to retrieve clip candidates that may contain the action modifying the state of an object. 
Obtaining the approximate temporal location of actions from the transcribed narration is still very challenging due to ambiguities in language (``undo bolt" and ``loosen nut" refer to the same manipulation) and only coarse temporal localization of the action provided by the narration. %
%
%
Given a manipulation action such as ``remove tire", we first find positive and negative sentences relevant for the action from an instruction website such as Wikihow.
We then train a linear SVM classifier~\cite{cortes95SVM} on bigram text features. 
Finally, we use the learned classifier to score clips from the input instructional videos.
In detail, the classifier is applied in a sliding window of 10 words finding the best scoring window in each input video. The clip candidates are then obtained by trimming the input videos 5 seconds before and 15 seconds after the timing of the best scoring text window to account for the fact that people usually perform the action after having talked about it.
We apply our method on the top 20 video clips based on the SVM score for each manipulation action.
More details about this process are provided in Appendix~\ref{app:svm}.
%
%
%
%

%
%
%
%
%

\noindent\textbf{Results.}
As shown in Table~\ref{tab:weakres}, the pattern of results, where our joint method performs the best, is similar to the weakly supervised set-up described in Sec.~\ref{sec:exp_res}. This highlights the robustness of our model to noisy input data -- an important property for scaling-up the method to Internet scale datasets.
To assess how well our joint method could do with perfect retrieval,  
we also report results for a ``Curated" set-up where we replace the automatically retrieved clips with the 20s clips used in Sec.~\ref{sec:exp_res} for the corresponding videos. 
%
%
%
%
%



\begin{table}[t!]
	\centering
	\resizebox{\columnwidth}{!}{
		\begin{tabular}{@{}clccccc|r@{}}
			\noalign{\hrule height 1.3pt}
			\multicolumn{1}{l}{}                                                               & \multicolumn{1}{c}{\textbf{Method}} & \begin{tabular}[c]{@{}c@{}}put \\ wheel\end{tabular} & \begin{tabular}[c]{@{}c@{}}remove\\ wheel\end{tabular} & \begin{tabular}[c]{@{}c@{}}fill \\ pot\end{tabular} & \begin{tabular}[c]{@{}c@{}}open \\ oyster\end{tabular} & \begin{tabular}[c]{@{}c@{}}fill\\ coff.cup\end{tabular} & \multicolumn{1}{c}{\textbf{Ave.}} \\ \midrule
			\multirow{4}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}State \\ disc.\end{tabular}}}   & \textbf{(c)} Cstrs only                              & 0.23                                                & 0.34                                                  & 0.25                                               & 0.29                                                   & 0.11                                                    & 0.24                              \\
			& State + det. sc.                         & 0.33                                                 & 0.48                                                  & \textbf{0.28}                                       &  0.40                                         & 0.13                                                    & 0.32                              \\
			& \textbf{(g)} Joint                               & \textbf{0.38}                                        & \textbf{0.53}                                          & 0.25                                       & \textbf{0.43 }                                                  & \textbf{0.20}                                           & \textbf{0.36}                     \\ \cmidrule(l){2-8} 
			& \textbf{(g)} Curated                             & 0.63                                                & 0.68                                                   & 0.63                                                & 0.63                                                   & 0.53                                                    & 0.62                             \\ \noalign{\hrule height 1pt}
			\multirow{4}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Action \\ local.\end{tabular}}} & \textbf{(i)} Chance                              & 0.14                                                 & 0.10                                                   & 0.06                                                & 0.10                                                   & 0.15                                                    & 0.11                             \\
			& \textbf{(iii)} Action                         & 0.05                                                & 0.10                                                   & 0.00                                                & 0.15                                          & \textbf{0.25}                                                    & 0.11                              \\
			& (\textbf{iv}) Joint                               & \textbf{0.30}                                        & \textbf{0.30}                                          & \textbf{0.20}                                       & \textbf{0.20}                                                   & 0.20                                           & \textbf{0.24}                              \\ \cmidrule(l){2-8} 
			& (\textbf{iv}) Curated                             & 0.53                                                 & 0.35                                                   & 0.32                                                & 0.40                                                   & 0.59                                                    & 0.44                              \\ \noalign{\hrule height 1.3pt}
		\end{tabular}
	}
	\vspace*{-2mm}
	\caption{\small Results on noisy clips automatically retrieved by text. \label{tab:weakres}}
	\vspace{-4mm}
\end{table}


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\vspace*{-3mm}
\section{Introduction}
\label{sec:intro}

Many of our activities involve changes in object states.
We need to open a book to read it, to cut bread before eating it and to lighten candles before taking out a birthday cake.
Transitions of object states are often coupled with particular manipulation actions (open, cut, lighten).
Moreover, the success of an action is often signified by reaching the desired state of an object (whipped cream, ironed shirt) and avoiding other states (burned shirt).
Recognizing object states and manipulation actions is, hence, expected to become a key component of future systems such as wearable automatic assistants or home robots helping people in their daily tasks. 


Human visual system can easily distinguish different states of objects, such as open/closed bottle or full/empty coffee cup~\cite{Brady06}.
%
%
%
Automatic recognition of object states and state changes, however, presents challenges as it requires distinguishing subtle changes in object appearance such as the presence of a cap on the bottle or screws on the car tire. %
Despite much work on object recognition and localization, recognition of object states has received only limited attention in computer vision~\cite{Isola15State}.


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/teaser_new.pdf}
    %
    \caption{We automatically discover object states such as empty/full coffee cup along with their corresponding manipulation actions by observing people interacting with the objects.}
    \label{fig:teaser}
    \vspace*{-5mm}
\end{figure}

One solution to recognizing object states would be to manually annotate states for different objects,
%
and treat the problem as a supervised fine-grained object classification task~\cite{duan2012discovering,Farhadi09ObjectsAttrib}. This approach, however, presents two problems. First, we would have to decide {\em a priori} on the set of state labels for each object, which can be ambiguous and not suitable for future tasks. Second, for each label we would need to collect a large number of examples, which can be very costly.

In this paper we propose to {\em discover} object states directly from videos with object manipulations.
As state changes are often caused by specific actions, we attempt to jointly discover object states and corresponding manipulations.
%
%
%
%
%
%
%
%
%
%
%
In our setup we assume that two distinct object states are temporally separated by a manipulation action.
For example, the empty and full states of a coffee cup are separated by the ``pouring coffee" action, as shown in Figure~\ref{fig:teaser}.  
Equipped with this constraint, %
we develop a clustering approach that jointly (i)~groups object states with similar appearance and consistent temporal locations with respect to the action and (ii) finds similar manipulation actions separating those object states in the input videos.
Our approach exploits the complementarity of both subproblems and finds a joint solution for states and actions.
%
%
We formulate our problem by adopting a discriminative clustering loss~\cite{Bach07diffrac} and a joint consistency cost between states and actions.  
We introduce an effective optimization solution in order to handle the resulting non-convex loss function and the set of spatial-temporal constraints.
To evaluate our method, we collect a new video dataset depicting real-life object manipulation actions in realistic videos.
Given this dataset for training, our method demonstrates successful discovery of object states and manipulation actions.
We also demonstrate that our joint formulation gives an improvement of object state discovery by action recognition and vice versa.


\section{Related work}
\label{sec:rel_work}

Below we review related work on person-object interaction, recognizing object states, action recognition and discriminative clustering that we employ in our model. 

\noindent\textbf{Person-object interactions.}
Many daily activities involve person-object interactions. %
Modeling co-occurrences of objects and actions have shown benefits for recognizing actions in~\cite{delaitre11personaction,gupta2009observing,kjellstrom2011visual,pirsiavash2012detecting,yao2011human}.
Recent work has also focused on building realistic datasets with people manipulating objects, \eg~in instructional videos~\cite{Alayrac15Unsupervised,Malmaud15what,Sener15unsupervised} or while performing daily activities~\cite{varol16hollywood}.
We build on this work but focus on joint modeling and recognition of actions and {\em object states}.

\noindent\textbf{States of objects.}
Prior work has addressed recognition of object attributes~\cite{Farhadi09ObjectsAttrib,Parikh2011Relattrib,patterson2014sun}, which can be seen as different object states in some cases. 
%
%
Differently from our approach, these works typically focus on classifying still images, do not consider human actions and assume an {\em a priori} known list of possible attributes. %
Closer to our setting, Isola~\etal\cite{Isola15State} discover object states and transformations between them by analyzing large collections of still images downloaded from the Internet. 
In contrast, our method does not require annotations of object states. %
Instead, we use the dynamics of consistent manipulations to discover object states in the video with minimal supervision. %
In~\cite{dima2014youdo}, the authors use consistent manipulations to discover task relevant objects.
However, they do not consider object states, rely mostly on first person cues (such as gaze) and take advantage of the fact that videos are taken in a single controlled environment.


\noindent\textbf{Action recognition.}
Most of the prior work on action recognition has focused on designing features to describe time intervals of a video using motion and appearance~\cite{laptev08learning,simonyan2014two,tran2015learning,Wang13action}. This is effective for actions  such as \textit{dancing} or \textit{jumping},
%
however, many of our daily activities are best distinguishable by their effect on the environment.
For example, \textit{opening door}  and \textit{closing door} can look very similar using only motion and appearance descriptors but their outcome is completely different.
This observation has been used to design action models in~\cite{fathi11modeling,fernando15vidDarwin,Wang16Transformation}.
In~\cite{Wang16Transformation}, for example, the authors propose to learn an embedding in which a given action acts as a transformation of  features of the video.
In our work we localize objects and recognize  changes of their states using manipulation actions as a supervisory signal.
Related to ours is also the work of Fathi~\etal~\cite{fathi11modeling} who represent actions in egocentric videos by changes of appearance of objects (also called object states), however, their method requires manually annotated precise temporal localization of actions in training videos. 
In contrast, we focus on (non-egocentric) Internet videos depicting real-life object manipulations where actions are performed by different people in a variety of challenging indoor/outdoor environments. In addition, our model jointly learns to recognize both actions and object states with only minimal supervision.

\noindent\textbf{Discriminative clustering.}
Our model builds on unsupervised discriminative clustering methods~\cite{Bach07diffrac,singh12discPat,Xu2004maximum} that group data samples according to a simultaneously learned classifier. 
Such methods can incorporate (weak) supervision that helps to steer the clustering towards a preferred solution~\cite{Bojanowski13finding,doersch2012what,feifei2016connectionist,jain2013representing,tang14coloc}.
In particular, we build on the discriminative clustering approach of~\cite{Bach07diffrac} that has been shown to perform well in a variety of computer vision problems~\cite{Bojanowski13finding}. %
It leads to a quadratic optimization problem where different forms of supervision can be  incorporated in the form of (typically) linear constraints.
Building on this formalism, we develop a model that jointly finds object states and temporal locations of actions in the video.
Part of our object state model is related to~\cite{Joulin14efficient}, while our action model is related to~\cite{Bojanowski15weakly}.
However, we introduce new spatial-temporal constraints together with a novel joint cost function linking object states and actions, as well as new effective optimization techniques.  


\noindent\textbf{Contributions.}
\label{sec:contrib}
The contributions of this work are three-fold. 
First, we develop a new discriminative clustering model that jointly discovers object states and temporally localizes associated manipulation actions in video. 
Second, we introduce an effective optimization algorithm to handle the resulting non-convex constrained optimization problem. 
Finally, we experimentally demonstrate that our model discovers key object states and manipulation actions from input videos with minimal supervision.

\begin{figure*}[t!]
   \centering
     \includegraphics[width=\linewidth]{figs/overview_color.pdf}
     \caption{
     Given a set of clips that depict a manipulated object, we wish to automatically discover the main states that the object can take along with localizing the associated manipulation action.
     In this example, we show one video of someone filling a coffee cup.
     The video starts with an empty cup (\textit{state 1}), which is filled with coffee (\textbf{action}) to become full (\textit{state 2}).
     Given imperfect object detectors, we wish to assign to the valid object candidates either the initial state or the final state (encoded in~$Y$).
	 We also want to localize the manipulating action in time (encoded in~$Z$) while maintaining a joint action-state consistency.
     }
     \vspace{-2mm} %
     \label{fig:mainpaper}
 \end{figure*}	

\section{Modeling manipulated objects}
\label{sec:approach}

We are given a set of $N$ clips that contain a common \textbf{manipulation} of the same \textit{object} (such as ``\textbf{open} an \textit{oyster}").
%
We also assume that we are given an \emph{a priori} model of the corresponding object in the form of a pre-trained object detector~\cite{girsh15fastrcnn}.
Given these inputs, our goal is twofold: (i)~localize the temporal extent of the action and (ii)~spatially/temporally localize the manipulated object and identify its states over time.
%
This is achieved by jointly clustering the appearances of an object (such as an ``oyster") appearing in all clips into two classes, corresponding to the two different states (such as ``closed" and ``open"), while at the same time temporally localizing a consistent ``opening" action that separates the two states consistently in all clips.   
More formally, we formulate the problem as a minimization of a joint cost function that ties together the action prediction in time, encoded in the assignment variable~$Z$, with the object state discovery in space and time, defined by the assignment variable~$Y$:


\begin{align}
\label{eq:jointstateactionprob}
\underset{\substack{Y\in\{0,1\}^{M\times 2}\\Z\in\{0,1\}^T}}{\text{minimize}} \!\! & & & \phantom{aai} f(Z) \; +  \; g(Y) \; + \; d(Z, Y) \\[-5mm]
& & \text{ s.t. }   & \underbrace{Z \in \mathcal{Z}}_{\substack{\text{saliency of action} \\ \\ \text{\textbf{Action localization}}}} \ \text{ and }  \  \underbrace{Y \in \mathcal{Y}}_{\substack{\text{ordering + non overlap}\\ \\ \text{\textbf{Object state labeling}}}}   \nonumber
\end{align}
where $f(Z)$ is a discriminative clustering cost to temporally localize the action in each clip, $g(Y)$ is a discriminative clustering cost to identify and localize the different object states and $d(Z,Y)$ is a joint cost that relates object states and actions together. $T$ denotes the total length of all video clips and $M$ denotes the total number of tracked object candidate boxes (tracklets).
In addition, we impose constraints~$\mathcal{Y}$ and~$\mathcal{Z}$ that encode additional structure of the problem: we localize the action with its most salient time interval per clip (``saliency"); we assume that the ordering of object states is consistent in all clips (``ordering") and that only one object is manipulated at a time (``non overlap").

In the following, we proceed with describing different parts of the model~\eqref{eq:jointstateactionprob}.
In Sec.~\ref{sec:statemodel} we describe the cost function for the discovery of object states. In Sec.~\ref{sec:actionmodel} we detail our model for action localization. Finally,  in Sec.~\ref{sec:joint} we describe and motivate the joint cost $d$.
%
\subsection{Discovering object states}
\label{sec:statemodel}

The goal here is to both (i) spatially localize the manipulated object and (ii) temporally identify its individual states.
To address the first goal, we employ pre-trained object detectors. To address the second goal, we formulate the discovery of object states as a discriminative clustering task with constraints.
We obtain candidate object detections using standard object detectors pre-trained on large scale existing datasets such as ImageNet~\cite{imagenet09}. We assume that each clip $n$ is accompanied with a set of $M_n$ tracklets\footnote{In this work, we use short tracks of objects (less than one second) that we call tracklet. We want to avoid long tracks that continue across a state change of objects. By using the finer granularity of tracklets, our model has the ability to correct for detection mistakes within a track as well as identify more precisely the state change. 
%
} of the object of interest. %

We formalize the task of localizing the states of objects as a discriminative clustering problem where the goal is to find an assignment matrix~$Y_n\in\{0,1\}^{M_n\times 2}$, where
 $(Y_n)_{mk}=1$ indicates that the $m$-th tracklet represents the object in state $k$. 
 We also allow a complete row of~$Y_n$ to be zero to encode that no state was assigned to the corresponding tracklet. 
 This is to model the possibility of false positive detections of an object, or that another object of the same class appears in the video, but is not manipulated and thus is not undergoing any state change. 
 In detail, we minimize the following discriminative clustering cost~\cite{Bach07diffrac}:\footnote{We concatenate all the variables~$Y_n$ into one $M\!\times\!2$ matrix $Y\!$.}  
\begin{align}
    g(Y) &= \min_{W_{s} \in \mathbb{R}^{d_s \times 2}} \ \underbrace{\frac{1}{2M} \|Y - X_{s} W_{s}\|_F^2}_\text{Discriminative loss on data} + \underbrace{\frac{\mu}{2} \|W_{s}\|_F^2}_\text{Regularizer}
    \label{eq:discr_state} 
    %
\end{align}
where $W_s$ is the object state classifier that we seek to learn, $\mu$ is a regularization parameter and $X_s$ is a $M\!\times\!d_s$ matrix of features, where each row is a $d_s$-dimensional (state) feature vector storing features for one particular tracklet. 
The minimization in $W_s$ actually leads to a convex quadratic cost function in~$Y$ (see~\cite{Bach07diffrac}). 
The first term in~\eqref{eq:discr_state} is the discriminative loss on the data that measures how easily the input data~$X_{s}$ is classified by the linear classifier~$W_{s}$ when the object state assignment is given by matrix~$Y$. 
In other words, we wish to find a labeling~$Y$ for given object tracklets into two states (or no state) so that their appearance features~$X$ are easily classified by a linear classifier. To steer the cost towards the right solution, we employ the following constraints (encoded by $Y \in \mathcal{Y}$ in \eqref{eq:jointstateactionprob}). 

\noindent\textbf{Only one object is manipulated at a time : non overlap constraint.}
As it is common in instructional videos, we assume that only one object can be manipulated at a given time.
However, in practice, it is common to have multiple (spatially diverse) tracklets that occur at the same time, for example, due to a false positive detection in the same frame.
To overcome this issue, we impose that at most one tracklet can be labeled as belonging to \emph{state 1} or \emph{state 2} at any given time.
We refer to this constraint as ``\emph{non overlap}" in problem~\eqref{eq:jointstateactionprob}.

\noindent\textbf{\emph{state 1}  $\rightarrow$ \emph{Action} $\rightarrow$  \emph{state 2}: ordering constraints.}
%
We assume that the manipulating action transforms the object from an initial state to a final state and that both states are present in each video.
This naturally introduces two constraints.
The first one is the ordering constraints on the labeling~$Y_n$, \ie the \emph{state 1} should occur before \emph{state 2} in each video.
The second constraint imposes that we have \emph{at least one} tracklet labeled as \emph{state 1} and at least one tracklet labeled as \emph{state 2}.
We call this last constraint the ``\textbf{at least one}" constraint in contrast to forcing ``exactly one" ordered prediction as previously proposed in a discriminative clustering approach on video for action localization~\cite{Bojanowski15weakly}.
This new type of constraint brings additional optimization challenges that we address in Section~\ref{subsec:frank-wolfe}.


\subsection{Action localization}
\label{sec:actionmodel}

Our action model is equivalent to the one of~\cite{Bojanowski15weakly} applied to only \emph{one action}. 
More precisely, the goal is to find an assignment matrix~$Z_n\in\{0,1\}^{T_n}$ for each clip $n$, where
$Z_{nt}=1$  encodes that the $t$-th time interval of video is assigned to an action and $Z_{nt}=0$ encodes that no action is detected in interval $t$.
The cost that we minimize for this problem is similar to the object states cost:
\vspace{-0.6mm}
\begin{align}
	f(Z) = \min_{W_v \in \mathbb{R}^{d_v}} \ \underbrace{\frac{1}{2T} \|Z - X_{v} W_{v}\|_F^2}_\text{Discriminative loss on data} + \underbrace{\frac{\lambda}{2} \|W_{v}\|_F^2}_\text{Regularizer} ,  
	\label{eq:actioncost} 
\end{align}
where $W_v$ is the action classifier, $\lambda$ is a regularization parameter and $X_v$ is a matrix of visual features.
We constrain our model to predict \emph{exactly} one time interval for an action per clip, an approach for actions that was shown to be beneficial in a weakly supervised setting~\cite{Bojanowski15weakly} (referred to as ``\emph{action saliency}" constraint).
%
As will be shown in experiments, this model \emph{alone} is incomplete because the clips in our dataset can contain other actions that do not manipulate the object of interest. 
Our central contribution is to propose a \emph{joint formulation} that links this action model with the object state prediction model, thereby resolving the ambiguity of actions.
We detail the joint model next.

\subsection{Linking actions and object states}
\label{sec:joint}
Actions in our model are directly related to changes in object states.
We therefore want to enforce consistency between the two problems.
To do so, we design a novel joint cost function that operates on the action video labeling~$Z_n$ and the state tracklet assignment~$Y_n$ for each clip.
We want to impose a constraint that the action occurs in between the presence of the two different object states.
In other words, we want to penalize the fact that state~$1$ is detected $\emph{after}$ the action happens, or the fact that state~$2$ is triggered  $\emph{before}$ the action occurs.
%
%

\noindent\textbf{Joint cost definition.}
We propose the following joint symmetric cost function for each clip:
\vspace{-0.6mm}
\begin{align}
d(Z_n,Y_n) =   \hspace{-0.5em}\sum_{y\in\mathcal{S}_1(Y_n)} \!\!\! [t_y-t_{Z_n}]_{+}  + \hspace{-0.5em}\sum_{y\in\mathcal{S}_2(Y_n)}  \! [t_{Z_n} \!\!-t_y]_{+},
\label{eq:dist}
\end{align}
where $t_{Z_n}$ and $t_y$ are the times when the action~$Z_n$ and the tracklet $y$ occur in a clip~$n$, respectively.
$\mathcal{S}_1(Y_n)$ and $\mathcal{S}_2(Y_n)$ are the tracklets in the $n$-th clip that have been assigned to state $1$ and state $2$, respectively.
Finally $[x]_{+}$ is the positive part of $x$.
%
In other words, the function penalizes the inconsistent assignment of objects states~$Y_n$  by the
amount of time that separates the incorrectly assigned tracklet and the manipulation action in the clip.
%
The overall joint cost is the sum over all clips weighted by a scaling hyperparameter $\nu>0$:
\vspace{-0.7mm}
\begin{equation} 
d(Z,Y) = \nu \frac{1}{T} \sum_{n=1}^N d(Z_n,Y_n). \label{eq:distGlobal}
%
\end{equation}\documentclass[twoside]{article} 
\usepackage[accepted]{aistats2017}

% If your paper is accepted, change the options for the package
% aistats2017 as follows:
%
%\usepackage[accepted]{aistats2017}
%
% This option will print headings for the title of your paper and
% headings for the authors names, plus a copyright note at the end of
% the first column of the first page.

\usepackage[round]{natbib} %SLJ: round is the convention for AISTATS, ICML, etc.
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

% Other custom packages:
\usepackage{enumitem} % to control list environment

\usepackage{color}
\hypersetup{ %
    pdftitle={},
    pdfauthor={},
    pdfkeywords={},
    pdfborder=0 0 0,
	pdfpagemode=UseNone,
    colorlinks=true,
    linkcolor=blue, %mydarkblue,
    citecolor=blue, %mydarkblue,
    filecolor=blue, %mydarkblue,
    urlcolor=blue, %mydarkblue,
    pdfview=FitH,
    pdfauthor={Rémi Leblond, Fabian Pedregosa, Simon Lacoste-Julien},
} 

% For figures
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{float}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

% for table
\usepackage{mdframed}

% For equations
\usepackage{amssymb, amsmath, amsthm}
\usepackage{bbm, bm}

% Variable names
\newcommand{\stepsize}{\gamma}
\newcommand{\strongconvex}{\mu}
\newcommand{\overlap}{\tau}
\newcommand{\contraction}{\rho}
\newcommand{\sparsity}{\Delta}
\newcommand{\sparsityr}{\Delta_r}
\newcommand{\lipschitz}{L}
\newcommand{\lyapunov}{\mathcal{L}}

\newcommand{\E}{\mathbb{E}}
\newcommand{\Econd}{\mathbf{E}}
\newcommand{\ind}{\mathbbm{1}}

% Commands
\newcommand{\blue}{\color{blue}}
\newcommand{\note}[1]{{\textbf{\color{red}#1}}}
\newcommand{\noteb}[1]{{\textbf{\color{blue}#1}}}
\newcommand{\ASAGA}{\textsc{Asaga}}
\newcommand{\SAGA}{\textsc{Saga}}
\newcommand{\SAG}{\textsc{Sag}}
\newcommand{\SSAGA}{\textsc{Sparse Saga}}
\newcommand{\SVRG}{\textsc{Svrg}}
\newcommand{\Hogwild}{\textsc{Hogwild}}
\newcommand{\SDCA}{\textsc{Sdca}}
\newcommand{\SGD}{\textsc{Sgd}}
\newcommand{\HSAG}{\textsc{Hsag}}
\newcommand{\KROMAGNON}{\textsc{Kromagnon}}

\newtheorem{asm}{Assumption}
\newtheorem{prop}{Property}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}

\renewcommand{\algorithmicloop}{\textbf{keep doing in parallel}} % for parallel algorithm
\renewcommand{\algorithmicendloop}{\algorithmicend\ \textbf{parallel loop}}

\renewcommand{\baselinestretch}{0.995} % last resort

\makeatletter
\makeatother


\begin{document}
\twocolumn[
	
\aistatstitle{\ASAGA: Asynchronous Parallel \SAGA}

%\aistatsauthor{ Anonymous Author 1 \And Anonymous Author 2 \And Anonymous Author 3 }

%\aistatsaddress{ Unknown Institution 1 \And Unknown Institution 2 \And Unknown Institution 3 }
	
\aistatsauthor{Rémi Leblond
\And
Fabian Pedregosa
\And
Simon Lacoste-Julien}

\aistatsaddress{ INRIA - Sierra Project-team\\
\'Ecole normale sup\'erieure, Paris 
\And INRIA - Sierra Project-team\\
\'Ecole normale sup\'erieure, Paris
\And
Department of CS \& OR (DIRO) \\
Universit\'e de Montr\'eal, Montr\'eal}

]

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{abstract}
\vspace{-2mm}
We describe \ASAGA, an asynchronous parallel version of the incremental gradient algorithm \SAGA\ that enjoys fast linear convergence rates. 
Through a novel perspective, we revisit and clarify a subtle but important technical issue present in a large fraction of the recent convergence rate proofs for asynchronous parallel optimization algorithms, and propose a simplification of the recently introduced ``perturbed iterate'' framework that resolves it. 
We thereby prove that \ASAGA\ can obtain a theoretical linear speedup on multi-core systems even without sparsity assumptions. 
We present results of an implementation on a 40-core architecture illustrating the practical speedup as well as the hardware overhead. 
\end{abstract} 

\vspace{-2mm}
\section{Introduction}
We consider the unconstrained optimization problem of minimizing a \emph{finite sum} of smooth convex functions:
\vspace{-1mm}
\begin{equation} \label{eq:finiteSum}
\min_{x \in \mathbb{R}^d} f(x), \quad f(x) := \frac{1}{n} \sum_{i=1}^{n} f_i(x),
\end{equation}
where each $f_i$ is assumed to be convex with $\lipschitz$-Lipschitz continuous gradient, $f$ is $\mu$-strongly convex and $n$ is large (for example, the number of data points in a regularized empirical risk minimization setting).
We define a condition number for this problem as $\kappa := \nicefrac{\lipschitz}{\mu}$.
A flurry of randomized incremental algorithms (which at each iteration select $i$ at random and process only one gradient $f'_i$) have recently been proposed to solve~\eqref{eq:finiteSum} with a fast\footnote{Their complexity in terms of gradient evaluations to reach an accuracy of $\epsilon$ is $O((n+\kappa)\log(\nicefrac{1}{\epsilon}))$, in contrast to $O(n\kappa\log(\nicefrac{1}{\epsilon}))$ for batch gradient descent in the worst case.} linear convergence rate, such as \SAG~\citep{SAG}, \SDCA~\citep{SDCA}, \SVRG~\citep{svrg} and \SAGA~\citep{SAGA}.
These algorithms can be interpreted as variance reduced versions of the stochastic gradient descent (\SGD) algorithm, and they have demonstrated both theoretical and practical improvements over \SGD\ (for the \emph{finite sum} optimization problem~\eqref{eq:finiteSum}).

In order to take advantage of the multi-core architecture of modern computers, the aforementioned optimization algorithms need to be adapted to the asynchronous parallel setting, where multiple threads work concurrently.
Much work has been devoted recently in proposing and analyzing asynchronous parallel variants of algorithms such as \SGD~\citep{hogwild}, \SDCA~\citep{asyncSDCA2015} and \SVRG~\citep{smola,mania,asySVRG}.
Among the incremental gradient algorithms with fast linear convergence rates that can optimize~\eqref{eq:finiteSum} in its general form, only \SVRG\ has had an asynchronous parallel version proposed.\footnote{We note that \SDCA\ requires the knowledge of an explicit $\mu$-strongly convex regularizer in~\eqref{eq:finiteSum}, whereas \SAG~/ \SAGA\ are adaptive to any local strong convexity of $f$~\citep{laggedsaga,SAGA}. This is also true for a variant of \SVRG~\citep{qsaga}.}
No such adaptation has been attempted yet for \SAGA, even though one could argue that it is a more natural candidate as, contrarily to \SVRG, it is not epoch-based and thus has no synchronization barriers at all.

\vspace{-2mm}
\paragraph{Contributions.} In Section~\ref{scs:sparse_saga}, we present a novel sparse variant of \SAGA\ that is more adapted to the parallel setting than the original \SAGA\ algorithm. 
In Section~\ref{sec:ASAGA}, we present \ASAGA, a lock-free asynchronous parallel version of Sparse \SAGA\ that does not require consistent reads. 
We propose a simplification of the ``perturbed iterate'' framework from~\citet{mania} as a basis for our convergence analysis. 
At the same time, through a novel perspective, we revisit and clarify a technical problem present in a large fraction of the literature on randomized asynchronous parallel algorithms (with the exception of~\citet{mania}, which also highlights this issue): namely, they all assume unbiased gradient estimates, an assumption that is inconsistent with their proof technique without further synchronization assumptions. 
In Section~\ref{ssec:convergence}, we present a tailored convergence analysis for \ASAGA. Our main result states that \ASAGA\ obtains the same geometric convergence rate per update as \SAGA\ when the overlap bound $\overlap$ (which scales with the number of cores) satisfies
$\overlap \leq \mathcal{O}(n)$ and $\overlap \leq \mathcal{O}({\scriptstyle \frac{1}{\sqrt{\sparsity}}} \max\{1,\frac{n}{\kappa} \})$, where $\sparsity \leq 1$ is a measure of the sparsity of the problem, notably implying that a linear speedup is theoretically possible even without sparsity in the well-conditioned regime where $n \gg \kappa$.
In Section~\ref{sec:results}, we provide a practical implementation of \ASAGA\ and illustrate its performance on a 40-core architecture, showing improvements compared to asynchronous variants of \SVRG\ and \SGD.

\vspace{-2mm}
\paragraph{Related Work.}
The seminal textbook of~\citet{bertsekasParalle1989} provides most of the foundational work for parallel and distributed optimization algorithms. 
An asynchronous variant of \SGD\ with constant step size called \Hogwild\ was presented by~\citet{hogwild}; part of their framework of analysis was re-used and inspired most of the recent literature on asynchronous parallel optimization algorithms with convergence rates, including asynchronous variants of coordinate descent~\citep{asyncCD2015}, \SDCA~\citep{asyncSDCA2015}, \SGD\ for non-convex problems~\citep{taming,asyncSGDNonConvex2015}, \SGD\ for stochastic optimization~\citep{duchi} and \SVRG~\citep{smola,asySVRG}. 
These papers make use of an unbiased gradient assumption that is not consistent with the proof technique, and thus suffers from technical problems\footnote{Except~\citet{duchi} that can be easily fixed by incrementing their global counter \emph{before} sampling.} that we highlight in Section~\ref{ssec:labelingIssue}. 

The ``perturbed iterate'' framework presented in~\citet{mania} is to the best of our knowledge the only one that does not suffer from this problem, and our convergence analysis builds heavily from their approach, while simplifying it.
In particular, the authors assumed that $f$ was both strongly convex and had a bound on the gradient, two \emph{inconsistent} assumptions in the unconstrained setting that they analyzed.
We overcome these difficulties by using tighter inequalities that remove the requirement of a bound on the gradient. We also propose a more convenient way to label the iterates (see Section~\ref{ssec:labelingIssue}).
The sparse version of \SAGA\ that we propose is also inspired from the sparse version of \SVRG\ proposed by~\citet{mania}. 
\citet{smola} presents a hybrid algorithm called \HSAG\ that includes \SAGA\ and \SVRG\ as special cases. 
Their asynchronous analysis is epoch-based though, and thus does not handle a fully asynchronous version of \SAGA\ as we do. 
Moreover, they require consistent reads and do not propose an efficient sparse implementation for \SAGA, in contrast to \ASAGA.  

\vspace{-2mm}
\paragraph{Notation.}  We denote by $\E$ a full expectation with respect to all the randomness, and by $\Econd$ the \emph{conditional} expectation of a random~$i$ (the index of the factor $f_i$ chosen in \SGD-like algorithms), conditioned on all the past, where ``past'' will be clear from the context. 
$[x]_v$ is the coordinate~$v$ of the vector~$x \in \mathbb{R}^d$.
$x^+$ represents the updated parameter vector after one algorithm iteration.

\vspace{-1mm}
\section{Sparse \SAGA}\label{scs:sparse_saga}
\vspace{-3mm}
Borrowing our notation from~\citet{qsaga}, we first present the original \SAGA\ algorithm and then describe a novel sparse variant that is more appropriate for a parallel implementation. 

\vspace{-2mm}
\paragraph{Original \SAGA\ Algorithm.} The standard \SAGA\ algorithm~\citep{SAGA} maintains two moving quantities to optimize~\eqref{eq:finiteSum}: the current iterate~$x$ and a table (memory) of historical gradients $(\alpha_i)_{i=1}^n$.\footnote{For linear predictor models, the memory $\alpha_i^0$ can be stored as a scalar. Following~\cite{qsaga}, $\alpha_i^0$ can be initialized to any convenient value (typically $0$), unlike the prescribed $f'_i(x_0)$ analyzed in~\citet{SAGA}.}
At every iteration, the \SAGA\ algorithm samples uniformly at random an index $i \in \{1,\ldots, n\}$, and then executes the following update on~$x$ and~$\alpha$ (for the unconstrained optimization version):
\begin{equation}\label{eq:SAGAupdate}
%\text{pick } i \text{ uniformly at random in } \{1,...,n\}; \quad  
x^{+} = x - \stepsize \big(f'_i(x) - \alpha_i + \bar \alpha\big); \qquad  \alpha_i^+ = f'_i(x),
\end{equation}
where $\stepsize$ is the step size and $\bar \alpha := \nicefrac{1}{n} \sum_{i=1}^n \alpha_i$ can be updated efficiently in an online fashion. Crucially, $\Econd \alpha_i = \bar \alpha$ and thus the update direction is unbiased ($\Econd x^{+} = x - \stepsize f'(x)$). 
Furthermore, it can be proven (see~\citet{SAGA}) that under a reasonable condition on $\stepsize$, the update has vanishing variance, which enables the algorithm to converge linearly with a constant step size.

\vspace{-2mm}
\paragraph{Motivation for a Variant.}
In its current form, every \SAGA\ update is dense even if the individual gradients are sparse due to the historical gradient ($\bar \alpha$) term. 
\citet{laggedsaga} introduced a special implementation with lagged updates where every iteration has a cost proportional to the size of the support of $f_i'(x)$. 
However, this subtle technique is not easily adaptable  to the parallel setting (see App.~\ref{apx:DifficultyLagged}). 
We therefore introduce Sparse \SAGA, a novel variant which explicitly takes sparsity into account and is easily parallelizable. 

\vspace{-2mm}
\paragraph{Sparse \SAGA\ Algorithm.}
As in the Sparse \SVRG\ algorithm proposed in~\citet{mania}, we obtain Sparse \SAGA\ by a simple modification of the parameter update rule in~\eqref{eq:SAGAupdate} where $\bar{\alpha}$ is replaced by a sparse version equivalent in expectation: 
\begin{equation} \label{eq:SparseSAGA}
x^{+} = x - \stepsize (f'_i(x) - \alpha_i + D_i \bar \alpha),
\end{equation}
where $D_i$ is a diagonal matrix that makes a weighted projection on the support of $f'_i$.
More precisely, let $S_i$ be the support of the gradient $f_i'$ function (i.e., the set of coordinates where $f_i'$ can be nonzero). Let $D$ be a $d\times d$ diagonal reweighting matrix, with coefficients~$\nicefrac{1}{p_v}$ on the diagonal, where $p_v$ is the probability that dimension~$v$ belongs to $S_i$ when $i$ is sampled uniformly at random in $\{1,...,n\}$.
We then define $D_i := P_{S_i} D$, where $P_{S_i}$ is the projection onto $S_i$. The normalization from $D$
ensures that $\Econd D_i \bar \alpha = \bar{\alpha}$, and thus that the update is still unbiased despite the projection.

\vspace{-2mm}
\paragraph{Convergence Result for (Serial) Sparse \SAGA.}
For clarity of exposition, we model our convergence result after the simple form of~\citet[Corollary~3]{qsaga} (note that the rate for Sparse \SAGA\ is the same as \SAGA). The proof is given in Appendix~\ref{apxA}.

\begin{theorem}\label{th1}
Let $\stepsize = \frac{a}{5\lipschitz}$ for any $a\leq1$. 
Then \textnormal{Sparse \SAGA} converges geometrically in expectation with a rate factor of at least $\rho(a) = \frac{1}{5} \min\big\{\frac{1}{n}, a\frac{1}{\kappa}\big\}$, i.e., for $x_t$ obtained after $t$ updates, we have ${\E \|x_t - x^*\|^2} \leq {(1-\rho)}^t \,  C_0$, where $C_0 := \|x_0 - x^*\|^2 + \frac{1}{5L^2} \sum_{i=1}^n {\|\alpha_i^0 - f'_i(x^*)\|^2}$.
\end{theorem}

\vspace{-2mm}
\paragraph{Comparison with Lagged Updates.}
The lagged updates technique in \SAGA\ is based on the observation that the updates for component $[x]_v$ can be delayed until this coefficient is next accessed. 
Interestingly, the expected number of iterations between two steps where a given dimension $v$ is involved in the partial gradient is $p_v^{-1}$, where $p_v$ is the probability that $v$ is involved. 
$p_v^{-1}$ is precisely the term which we use to multiply the update to $[x]_v$ in Sparse \SAGA. 
Therefore one may view the Sparse \SAGA\ updates as \textit{anticipated} \SAGA\ updates, whereas those in the~\citet{laggedsaga} implementation are \textit{lagged}. 

Although Sparse \SAGA\ requires the computation of the $p_v$ probabilities, this can be done during a first pass through the data (during which constant step size \SGD\ may be used) at a negligible cost. 
In our experiments, both Sparse \SAGA\ and \SAGA\ with lagged updates had similar convergence in terms of number of iterations, with the Sparse \SAGA\ scheme being slightly faster in terms of runtime. 
We refer the reader to~\citet{laggedsaga} and Appendix~\ref{apxC} for more details.

\section{Asynchronous Parallel Sparse \SAGA} \label{sec:ASAGA}
\vspace{-2mm}
As most recent parallel optimization contributions, we use a similar hardware model to~\citet{hogwild}. 
We have multiple cores which all have read and write access to a shared memory. 
They update a central parameter vector in an asynchronous and lock-free fashion. 
Unlike~\citet{hogwild}, we \emph{do not} assume that the vector reads are consistent: multiple cores can read and write different coordinates of the shared vector at the same time. This means that a full vector read for a core might not correspond to any consistent state in the shared memory at any specific point in time. 

\vspace{-1mm}
\subsection{Perturbed Iterate Framework}
\vspace{-2mm}
We first review the ``perturbed iterate'' framework recently introduced by~\citet{mania} which will form the basis of our analysis. 
In the sequential setting, stochastic gradient descent and its variants can be characterized by the following update rule:
\begin{equation}
x_{t+1} = x_t -\stepsize g(x_t, i_t),
\end{equation}
where $i_t$ is a random variable independent from $x_t$ and we have the unbiasedness condition $\Econd g(x_t, i_t) = f'(x_t)$ (recall that $\Econd$ is the relevant-past conditional expectation with respect to $i_t$).

Unfortunately, in the parallel setting, we manipulate stale, inconsistent reads of shared parameters and thus we do not have such a straightforward relationship. 
Instead, \citet{mania} proposed to separate $\hat x_t$, the actual value read by a core to compute an update, with $x_t$, a ``virtual iterate'' that we can analyze and is \emph{defined} by the update equation:
$
x_{t+1} := x_t -\stepsize g(\hat x_t, i_t).
$ We can thus interpret $\hat x_t$ as a noisy (perturbed) version of $x_t$ due to the effect of asynchrony. In the specific case of (Sparse) \SAGA, we have to add the additional read memory argument $\hat{\alpha}^t$ to our update:
\begin{equation}  \label{eq:PIupdate}
\begin{aligned}
x_{t+1} &:= x_t -\stepsize g(\hat x_t, \hat \alpha^t, i_t);
\\
g(\hat x_t, \hat \alpha^t, i_t) &:= f'_{i_t}(\hat x_t) - \hat \alpha_{i_t}^t + D_{i_t} \left({\textstyle \nicefrac{1}{n} \sum_{i=1}^n \hat{\alpha}_i^t }\right).
\end{aligned}
\end{equation}
We formalize the precise meaning of $x_t$ and $\hat x_t$ in the next section. 
We first note that all the papers mentioned in the related work section that analyzed asynchronous parallel randomized algorithms assumed that the following unbiasedness condition holds:
\begin{equation} \label{eq:unbiasedness}
\left[{\text{unbiasedness} \atop \text{condition}}\right] \quad \Econd [g(\hat x_{t}, i_t) | \hat x_t] = f'(\hat x_t).
\end{equation}
This condition is at the heart of most convergence proofs for randomized optimization methods.\footnote{A notable exception is \SAG~\citep{SAG} which has biased updates, yielding a significantly more complex convergence proof. Making \SAG\ unbiased leads to \SAGA~\citep{SAGA} and a much simpler proof.}
\citet{mania} correctly pointed out that most of the literature thus made the often implicit assumption that $i_t$ is independent of $\hat{x}_t$. 
But as we explain below, this assumption is incompatible with a non-uniform asynchronous model in the analysis approach used in most of the recent literature.

\vspace{-1mm}
\subsection{On the Difficulty of Labeling the Iterates} \label{ssec:labelingIssue}
\vspace{-2mm}
Formalizing the meaning of $x_t$ and $\hat x_t$ highlights a subtle but important difficulty arising when analyzing \emph{randomized} parallel algorithms: what is the meaning of $t$? 
This is the problem of \emph{labeling} the iterates for the purpose of the analysis, and this labeling can have randomness itself that needs to be taken in consideration when interpreting the meaning of an expression like $\E[x_t]$. 
In this section, we contrast three different approaches in a unified framework. 
We notably clarify the dependency issues that the labeling from~\citet{mania} resolves and propose a new, simpler labeling which allows for much simpler proof techniques.
We consider algorithms that execute in parallel the following four steps, where $t$ is a global labeling that needs to be defined:
\begin{equation} \label{eq:updates}
\begin{minipage}{0.43\textwidth}
\begin{enumerate}
\setlength\itemsep{-0.2em}
\item Read the information in shared memory ($\hat{x}_t$).
\item Sample $i_t$.
\item Perform some computations using ($\hat{x}_t, i_t$).
\item Write an update to shared memory.
\end{enumerate}
\end{minipage}
\end{equation}

\vspace{-4mm}
\paragraph{The ``After Write'' Approach.} We call the ``after write'' approach the standard global labeling scheme used in~\citet{hogwild} and re-used in all the later papers that we mentioned in the related work section, with the notable exceptions of~\citet{mania} and~\citet{duchi}. In this approach, $t$ is a (virtual) global counter recording the number of \emph{successful writes} to the shared memory $x$ (incremented after step~4 in~\eqref{eq:updates}); $x_t$ thus represents the (true) content of the shared memory after $t$ updates. 
The interpretation of the crucial equation~\eqref{eq:PIupdate} then means that $\hat{x}_t$ represents the (delayed) local copy value of the core that made the $(t+1)^{\mathrm{th}}$ successful update; $i_t$ represents the factor sampled by this core for this update. 
Notice that in this framework, the value of $\hat x_t$ and $i_t$ is unknown at ``time~$t$''; we have to wait to the later time when the next core writes to memory to finally determine that its local variables are the ones labeled by $t$.
We thus see that here~$\hat x_t$ and~$i_t$ are not necessarily independent -- they share dependence through the $t$ label assignment. 
In particular, if some values of~$i_t$ yield faster updates than others, it will influence the label assignment defining $\hat x_t$. We illustrate this point with a concrete problematic example in Appendix~\ref{apx:ProblematicExample} that shows that in order to preserve the unbiasedness condition~\eqref{eq:unbiasedness}, the ``after write'' framework makes the implicit assumption that the computation time for the algorithm running on a core is independent of the sample $i$ chosen.
This assumption seems overly strong in the context of potentially heterogeneous factors $f_i$'s, and is thus a fundamental flaw for analyzing non-uniform asynchronous computation.

\vspace{-2mm}
\paragraph{The ``Before Read'' Approach.}
\citet{mania} addresses this issue by proposing instead to increment the global~$t$~counter just \emph{before} a new core starts to \emph{read} the shared memory (before step~1 in~\eqref{eq:updates}).  
In their framework, $\hat{x}_t$ represents the (inconsistent) read that was made by this core in this computational block, and $i_t$ represents the picked sample. 
The update rule~\eqref{eq:PIupdate} represents a \emph{definition} of the meaning of $x_t$, which is now a ``virtual iterate'' as it does not necessarily correspond to the content of the shared memory at any point. 
The real quantities manipulated by the algorithm in this approach are the $\hat{x}_t$'s, whereas $x_t$ is used only for the analysis -- the critical quantity we want to see vanish is $\E \|\hat x_t - x^*\|^2$.
The independence of~$i_t$ with~$\hat{x}_t$ can be simply enforced in this approach by making sure that the way the shared memory $x$ is read does not depend on $i_t$ (e.g. by reading all its coordinates in a fixed order). Note that this means that we have to read all of $x$'s coordinates, regardless of the size of $f_{i_t}$'s support. 
This is a much weaker condition than the assumption that all the computation in a block does not depend on $i_t$ as required by the ``after write'' approach, and is thus more reasonable.

\vspace{-2mm}
\paragraph{A New Global Ordering: the ``After Read'' Approach.}
The ``before read'' approach gives rise to the following complication in the analysis: $\hat{x}_t$ can depend on $i_r$ for $r > t$. 
This is because $t$ is a global time ordering only on the assignment of computation to a core, not on when  $\hat{x}_t$ was finished to be read. 
This means that we need to consider both the ``future'' and the ``past'' when analyzing $x_t$. 
To simplify the analysis (which proved crucial for our \ASAGA\ proof), we thus propose a third way to label the iterates: $\hat{x}_t$ represents the $(t+1)^{\mathrm{th}}$ \emph{fully completed read} ($t$ incremented after step~1 in~\eqref{eq:updates}). 
As in the ``before read'' approach, we can ensure that $i_t$ is independent of $\hat{x}_t$ by ensuring that how we read does not depend on $i_t$. 
But unlike in the ``before read'' approach, $t$ here now does represent a global ordering on the $\hat{x}_t$ iterates -- and thus we have that $i_r$ is independent of $\hat{x}_t$ for $r > t$. 
Again using~\eqref{eq:PIupdate} as the definition of the virtual iterate $x_t$ as in the perturbed iterate framework, we then have a very simple form for the value of $x_t$ and $\hat{x}_t$ (assuming atomic writes, see Property~\ref{eventconst} below):
\begin{equation} \label{eq:xhatUpdates}
\begin{aligned}
x_t &= x_0 - \stepsize \sum_{u = 0}^{t-1} g(\hat x_u, \hat \alpha^u, i_u) \, ;
\\
[\hat{x}_t]_v &= [x_0]_v - \stepsize  
\mkern-36mu \sum_{\substack{u = 0 \\ 
		\text{u s.t. coordinate $v$ was written}\\ 
		\text{for $u$ before $t$} } 
}^{t-1} \mkern-36mu  [g(\hat x_u, \hat \alpha^u, i_u)]_v \, .
\end{aligned}
\end{equation}
The main idea of the perturbed iterate framework is to use this handle on $\hat x_t - x_t$ to analyze the convergence for $x_t$. 
In this paper, we can instead give directly the convergence of $\hat x_t$, and so unlike in~\citet{mania}, we do not require that there exists a $T$ such that $x_T$ lives in shared memory.


\begin{figure*}[ttt!]
	\vspace{-5mm}
 \begin{minipage}[t]{0.49\textwidth}
   \begin{algorithm}[H]
     \caption{\ASAGA\ (analyzed algorithm)}
     \label{alg:theoretical}
     \label{theoreticalgo}
     \begin{algorithmic}[1]
	   \STATE Initialize shared variables $x$ and $(\alpha_i)_{i=1}^n$
	   \LOOP
	      \STATE $\hat x = $ inconsistent read of $x$
		  \STATE $\forall j$, $\hat \alpha_j = $ inconsistent read of $\alpha_j$
	   	  \STATE \textcolor{blue}{Sample $i$}  uniformly at random in $\{1,...,n\}$
	      \STATE Let $S_i$ be $f_i$'s support
	      \STATE $[\bar \alpha]_{S_i} = \nicefrac{1}{n} \sum_{k=1}^n [\hat \alpha_k]_{S_i}$
    		  \STATE $[\delta x]_{S_i} = -\stepsize (f'_i(\hat x) - \hat \alpha_i + D_{i} [\bar \alpha]_{S_i})$
    		  \STATE
        	  \FOR{$v$ {\bfseries in} $S_i$}
        		 \STATE $[x]_v \leftarrow [x]_v + [\delta x]_v$      \hfill // atomic
	         \STATE $[\alpha_i]_v \leftarrow [f'_i( \hat x )]_v$
	         \STATE // {\small(`$\gets$' denotes a shared memory update.)}
    		  \ENDFOR
	   \ENDLOOP
	  \end{algorithmic}
    \end{algorithm}
 \end{minipage}
 \hfill
 \begin{minipage}[t]{0.5\textwidth}
    \begin{algorithm}[H]
      \caption{\ASAGA\ (implementation)}
      \label{alg:sagasync}
      \begin{algorithmic}[1]
	    \STATE Initialize shared variables $x$, $(\alpha_i)_{i=1}^n$ and $\bar \alpha$
	    \LOOP
	      \STATE \textcolor{blue}{Sample $i$} uniformly at random in $\{1,...,n\}$
  	      \STATE Let $S_i$ be $f_i$'s support
 	      \STATE $[\hat x]_{S_i} = $ inconsistent read of $x$ on $S_i$
	      \STATE $\hat \alpha_i = $ inconsistent read of $\alpha_i$
	      \STATE $[\bar \alpha]_{S_i} = $ inconsistent read of $\bar \alpha$ on $S_i$
	      \STATE $[\delta \alpha]_{S_i} = f'_i([\hat x]_{S_i}) - \hat \alpha_i$
	      \STATE $[\delta x]_{S_i} = - \gamma ([\delta\alpha]_{S_i} + D_i [\bar \alpha]_{S_i})$
	      \FOR{$v$ {\bfseries in} $S_i$}
	        \STATE $[x]_v \gets [x]_v + [\delta x]_v$  \hfill // atomic
	        \STATE $[\alpha_i]_v \gets [\alpha_i]_v + [\delta \alpha]_v$ \hfill // atomic
	        \STATE $[\bar \alpha]_v \gets [\bar \alpha]_v + \nicefrac{1}{n}[\delta \alpha]_v$ \hfill // atomic
	     \ENDFOR
	   \ENDLOOP
      \end{algorithmic}
    \end{algorithm}
 \end{minipage}
 	\vspace{-5mm}
\end{figure*}

\vspace{-1mm}
\subsection{Analysis setup} \label{ssec:convergence}
\vspace{-2mm}
We describe \ASAGA, a sparse asynchronous parallel implementation of Sparse \SAGA, in Algorithm~\ref{alg:theoretical} in the theoretical form that we analyze, and in Algorithm~\ref{alg:sagasync} as its practical implementation.
%The shared memory updates are denoted by.
Before stating its convergence, we highlight some properties of Algorithm~\ref{alg:theoretical} and make one central assumption. 

\begin{prop} [independence]
Given the ``after read'' global ordering, $i_r$ is independent of $\hat{x}_t$ $\forall r \geq t$.
\label{independence}
\end{prop}
\vspace{-2mm}
We enforce the independence for $r = t$ in Algorithm~\ref{alg:theoretical} by having the core read all the shared data parameters and historical gradients before starting their iterations. 
Although this is too expensive to be practical if the data is sparse, this is required by the theoretical Algorithm~\ref{theoreticalgo} that we can analyze. 
As~\citet{mania} stress, this independence property is assumed in most of the parallel optimization literature. The independence for $r > t$ is a consequence of using the ``after read'' global ordering instead of the ``before read'' one.

\begin{prop}[Unbiased estimator]
The update, $g_t := g(\hat x_t, \hat \alpha^t, i_t)$, is an unbiased estimator of the true gradient at $\hat x_t$ (i.e.~\eqref{eq:PIupdate} yields~\eqref{eq:unbiasedness} in conditional expectation).
\end{prop}
\vspace{-2mm}
This property is crucial for the analysis, as in most related literature. 
It follows by the independence of $i_t$ with $\hat{x}_t$ and from the computation of $\bar \alpha$ on line~7 of Algorithm~\ref{theoreticalgo}, which ensures that $\E \hat \alpha_i = 1/n \sum_{k=1}^n [\hat \alpha_k]_{S_i} = [\bar \alpha]_{S_i}$, making the update unbiased. In practice, recomputing $\bar \alpha$ is not optimal, but storing it instead introduces potential bias issues in the proof (as detailed in Appendix~\ref{apx:Bias}). 


\begin{prop} [atomicity]
The shared parameter coordinate update of $[x]_v$ on line~11 is atomic.
\label{eventconst}
\end{prop}
\vspace{-2mm}
Since our updates are additions, this means that there are no overwrites, even when several cores compete for the same resources. 
In practice, this is enforced by using \textit{compare-and-swap} semantics, which are heavily optimized at the processor level and have minimal overhead. 
Our experiments with non-thread safe algorithms (i.e. where this property is not verified, see Figure~\ref{fig:cas_comparison} of Appendix~\ref{apxE}) show that compare-and-swap is necessary to optimize to high accuracy.

Finally, as is standard in the literature, we make an assumption on the maximum delay that asynchrony can cause -- this is the \emph{partially asynchronous} setting as defined in~\citet{bertsekasParalle1989}:
\begin{asm}[bounded overlaps]\label{boundedoverlap}
We assume that there exists a uniform bound, called~$\overlap$, on the maximum number of iterations that can overlap together. We say that iterations $r$ and $t$ overlap if at some point they are processed concurrently. 
One iteration is being processed from the start of the reading of the shared parameters to the end of the writing of its update. 
The bound~$\overlap$ means that iterations $r$ cannot overlap with iteration $t$ for $r \geq t + \tau+1$,  and thus that every coordinate update from iteration $t$ is successfully written to memory before the iteration $t + \overlap+1$ starts.
\label{boundedoverlaps}
\end{asm}

\vspace{-2mm}
Our result will give us conditions on $\overlap$ subject to which we have linear speedups. 
$\overlap$ is usually seen as a proxy for $p$, the number of cores (which lowerbounds it). 
However, though $\overlap$ appears to depend linearly on $p$, it actually depends on several other factors (notably the data sparsity distribution) and can be orders of magnitude bigger than $p$ in real-life experiments.
We can upper bound $\overlap$ by $(p-1)R$, where $R$ is the ratio of the maximum over the minimum iteration time (which encompasses theoretical aspects as well as hardware overhead).
More details can be found in Appendix~\ref{apxD}.

\vspace{-2mm}
\paragraph{Explicit effect of asynchrony.}
By using the overlap Assumption~\ref{boundedoverlaps} in the expression~\eqref{eq:xhatUpdates} for the iterates, we obtain the following explicit effect of asynchrony that is crucially used in our proof:
\begin{align}\label{eq:async}
\hat x_t - x_t = \stepsize \sum_{u=(t - \overlap)_+}^{t-1}G_{u}^t g(\hat x_{u}, \hat \alpha^u, i_{u}),
\end{align}
where $G_{u}^t$ are $d\times d$ diagonal matrices with terms in $\{0, +1\}$. 
We know from our definition of~$t$ and~$x_t$ that every update in $\hat x_t$ is already in $x_t$ -- this is the $0$ case. 
Conversely, some updates might be late: this is the $+1$ case. 
$\hat x_t$ may be lacking some updates from the ``past" in some sense, whereas given our global ordering definition, it cannot contain updates from the ``future".

\vspace{-1mm}
\subsection{Convergence and speedup results}
\vspace{-2mm}
We now state our main theoretical results. We give an outline of the proof in Section~\ref{proofoutline} and its full details in Appendix~\ref{apxB}.
We first define a notion of problem sparsity, as it will appear in our results.

\begin{definition}[Sparsity]
As in~\citet{hogwild}, we introduce $\sparsityr := \max_{v=1..d} |\{i : v \in S_i\}|$. $\sparsityr$ is 
the maximum right-degree in the bipartite graph of the factors and the dimensions, i.e., 
the maximum number of data points with a specific feature. For succinctness, we also define $\sparsity := \sparsityr / n$. We have $1 \leq \sparsityr \leq n$, and hence $1/n \leq \sparsity \leq 1$.

\end{definition}

\begin{theorem}[Convergence guarantee and rate of \ASAGA]\label{thm:convergence}
Suppose $\overlap < n/10$.\footnote{\ASAGA\ can actually converge for any $\overlap$, but the maximum step size then has a term of $\exp(\overlap/n)$ in the denominator with much worse constants. See Appendix~\ref{apxB:lma3}.} Let
\vspace{-1mm}
\begin{equation}\label{eq:condition}
\begin{aligned}
&a^*(\overlap) := \frac{1}{32 \left(1+ \overlap  \sqrt \sparsity \right) \xi(\kappa, \sparsity, \overlap)}
\\
&\text{where } \xi(\kappa, \sparsity, \overlap) := \sqrt{1 + \frac{1}{8 \kappa}  \min\{\frac{1}{\sqrt{\sparsity}}, \overlap\} } \\
&\text{\small{(note that $\xi(\kappa, \sparsity, \overlap) \approx 1$ unless $\kappa < \nicefrac{1}{\sqrt{\sparsity}}  \,\, (\leq \sqrt{n})$)}}.
\end{aligned}
\end{equation}
For any step size $\stepsize = \frac{a}{L}$ with $a \leq a^*(\overlap)$, the inconsistent read iterates of Algorithm~\ref{alg:theoretical} converge in expectation at a geometric rate of at least: $\contraction(a) = \frac{1}{5} \min \big\{\frac{1}{n},  a \frac{1}{\kappa}\big\},$
i.e., $\E f(\hat x_t)-f(x^*) \leq (1-\rho)^t \,  \tilde C_0$, where $\tilde C_0$ is a constant independent of $t$  ($\approx \frac{n}{\stepsize}C_0$ with $C_0$ as defined in Theorem~\ref{th1}).
\end{theorem}
\vspace{-2mm}
This result is very close to \SAGA's original convergence theorem, but with the maximum step size divided by an extra $1+ \overlap \sqrt{\sparsity}$ factor. Referring to~\citet{qsaga} and our own Theorem~\ref{th1}, the rate factor for \SAGA\ is $\min\{1/n, 1/\kappa\}$ up to a constant factor. Comparing this rate with Theorem~\ref{thm:convergence} and inferring the conditions on the maximum step size $a^*(\overlap)$, we get the following conditions on the overlap $\overlap$ for \ASAGA\ to have the same rate as \SAGA\ (comparing upper bounds).
\begin{corollary}[Speedup condition]\label{thm:bigdata}\label{thm:illcondition}
Suppose $\overlap \leq \mathcal{O}(n)$ and $\overlap \leq \mathcal{O}({\scriptstyle \frac{1}{\sqrt{\sparsity}}} \max\{1,\frac{n}{\kappa} \})$. Then using the step size $\stepsize = \nicefrac{a^*(\overlap)}{L}\,$ from~\eqref{eq:condition}, \ASAGA\ converges geometrically with rate factor $\Omega( \min\{\frac{1}{n}, \frac{1}{\kappa}\})$ (similar to \SAGA), and is thus linearly faster than its sequential counterpart up to a constant factor. Moreover, if $\overlap \leq \mathcal{O}(\frac{1}{\sqrt{\sparsity}})$, then a universal step size of $\Theta(\frac{1}{L})$ can be used for \ASAGA\ to be adaptive to local strong convexity with a similar rate to \SAGA\ (i.e., knowledge of $\kappa$ is not required).
\end{corollary}

\vspace{-2mm}
Interestingly, in the well-conditioned regime ($n > \kappa$, where \SAGA\ enjoys a range of stepsizes which all give the same contraction ratio), \ASAGA\ can get the same rate as \SAGA\ even in the non-sparse regime ($\sparsity = 1$) for $\overlap < \mathcal{O}(n/\kappa)$. This is in contrast to the previous work on asynchronous incremental gradient methods which required some kind of sparsity to get a theoretical linear speedup over their sequential counterpart~\citep{hogwild,mania}. 
In the ill-conditioned regime ($\kappa > n$), sparsity is required for a linear speedup, with a bound on $\overlap$ of $\mathcal{O}(\sqrt{n})$ in the best-case (though degenerate) scenario where $\sparsity = 1/n$.

\vspace{-2mm}
\paragraph{Comparison to related work.}
\begin{itemize}[leftmargin=1.5em, topsep=-1mm, itemsep=-0.7mm]
\item We give the first convergence analysis for an asynchronous parallel version of \SAGA\ (note that \citet{smola} only covers an epoch based version of \SAGA\ with random stopping times, a fairly different algorithm).
\item Theorem~\ref{thm:convergence} can be directly extended to a parallel extension of the \SVRG\ version from~\citet{qsaga}, which is adaptive to the local strong convexity with similar rates (see Appendix~\ref{apx:SVRGext}).
\item In contrast to the parallel \SVRG\ analysis from~\citet[Thm. 2]{smola}, our proof technique handles inconsistent reads and a non-uniform processing speed across $f_i$'s. 
Our bounds are similar (noting that $\sparsity$ is equivalent to theirs), except for the adaptivity to local strong convexity: \ASAGA\ does not need to know $\kappa$ for optimal performance, contrary to parallel \SVRG\ (see App.~\ref{apx:SVRGext} for more details).
\item In contrast to the \SVRG\ analysis from~\citet[Thm. 14]{mania}, we obtain a better dependence on the condition number in our rate ($1/\kappa$ vs. $1/\kappa^2$ for them) and on the sparsity (they get $\overlap \leq \mathcal{O}(\sparsity^{\nicefrac{-1}{3}})$), while we remove their gradient bound assumption.
We also give our convergence guarantee on $\hat{x}_t$ \emph{during} the algorithm, whereas they only bound the error for the ``last'' iterate $x_T$.
\end{itemize}

\vspace{-2mm}
\subsection{Proof outline}\label{proofoutline}
\vspace{-2mm}
We give here the outline of our proof. Its full details can be found in Appendix~\ref{apxB}.

Let $g_t := g(\hat x_t, \hat \alpha^t, i_t)$. By expanding the update equation~\eqref{eq:PIupdate} defining the virtual iterate $x_{t+1}$ and introducing $\hat x_t$ in the inner product term, we get:
\begin{equation}\label{eq:initrec0}
\begin{aligned}
\|x_{t+1} - x^*\|^2
&= \|x_t -x^*\|^2 
-2\stepsize\langle \hat x_t -x^*,  g_t\rangle 
\\
&\quad+2\stepsize\langle \hat x_t -x_t,  g_t\rangle 
+ \stepsize^2 \|g_t\|^2 .
\end{aligned}
\end{equation}
In the sequential setting, we require $i_t$ to be independent of $x_t$ to get unbiasedness. 
In the perturbed iterate framework, we instead require that $i_t$ is independent of $\hat x_t$ (see Property~\ref{independence}). 
This crucial property enables us to use the unbiasedness condition~\eqref{eq:unbiasedness} to write:
$\E \langle \hat x_t -x^*,  g_t\rangle 
= \E \langle \hat x_t -x^*,  f'(\hat x_t)\rangle$. We thus take the expectation of~\eqref{eq:initrec0} that allows us to use the $\mu$-strong convexity of $f$:\footnote{Here is our departure point with \citet{mania} who replaced the $f(\hat{x}_t)-f(x^*)$ term with the lower bound $\frac{\strongconvex}{2}\|\hat x_t - x^*\|^2$ in this relationship (see their Equation (2.4)), yielding an inequality too loose to get fast rates for \SVRG.} 
\begin{align}
\langle \hat x_t -x^*,  f'(\hat x_t)\rangle &\geq f(\hat x_t) -f(x^*) +\frac{\strongconvex}{2}\|\hat x_t - x^*\|^2 . \notag
\end{align}
With further manipulations on the expectation of~\eqref{eq:initrec0}, including the use of the standard inequality $\|a + b\|^2 \leq 2\|a\|^2 + 2\|b\|^2$ (see Section~\ref{app:RecursiveDerivation}), we obtain our basic recursive contraction inequality:
\begin{equation}
\begin{aligned} \label{eq:RecursiveIneq1}
a_{t+1} \leq 
&(1 -\frac{\stepsize \strongconvex}{2}) a_t 
+ \stepsize^2 \E \|g_t\|^2 -2\stepsize e_t
\\
&\underbrace{
	+ \stepsize\strongconvex \E\|\hat x_t - x_t\|^2 
	+ 2\stepsize \E \langle \hat x_t -x_t,  g_t\rangle
}_{\text{additional asynchrony terms}}\,,
\end{aligned}
\end{equation}
where $a_t := \E \|x_t - x^*\|^2$ and $e_t := \E f(\hat x_t) - f(x^*)$.

In the sequential setting, one crucially uses the negative suboptimality term $-2\stepsize e_t$ to cancel the variance term $\stepsize^2 \E \|g_t\|^2$ (thus deriving a condition on $\stepsize$).
Here, we need to bound the additional asynchrony terms using the same negative suboptimality in order to prove convergence and speedup for our parallel algorithm -- thus getting stronger constraints on the maximum step size.

The rest of the proof then proceeds as follows:
\vspace{-2mm}
\begin{itemize}[leftmargin=1.5em, topsep=1mm, itemsep=0mm]
	\item Lemma~\ref{lma:1}: we first bound the additional asynchrony terms in~\eqref{eq:RecursiveIneq1} in terms of past updates ($\E \|g_u\|^2, u\leq t$). 
	We achieve this by crucially using the expansion~\eqref{eq:async} for $x_t - \hat{x}_t$, together with the sparsity inequality~\eqref{sparseproduct} (which is derived from Cauchy-Schwartz, see Appendix~\ref{apxB:lma1}).
	
	\item Lemma~\ref{lma:suboptgt}: we then bound the updates $\E \|g_u\|^2$ with respect to past suboptimalities $(e_v)_{v\leq u}$. From our analysis of Sparse \SAGA\ in the sequential case:
	\begin{equation}
	\E\|g_t\|\sp{2} 
	\leq 2 \E \|f'_{i_t}(\hat x_t)-f'_{i_t}(x\sp{*})\|^2 
	+ 2 \E \|\hat \alpha_{i_t}\sp{t} - f'_{i_t}(x\sp{*})\|^2 \notag
	\end{equation}
	We bound the first term by $4Le_t$ using~\citet[Equation (8)]{qsaga}.
	To express the second term in terms of past suboptimalities, we note that it can be seen as an expectation of past first terms with an adequate probability distribution which we derive and bound.
	
	\item By substituting Lemma~\ref{lma:suboptgt} into Lemma~\ref{lma:1}, we get a master contraction inequality~\eqref{master} in terms of $a_{t+1}$, $a_t$ and $e_u, u\leq t$.
	
	\item We define a novel Lyapunov function $\lyapunov_t = \sum_{u=0}^t (1-\contraction)^{t-u}a_u$ and manipulate the master inequality to show that $\lyapunov_t$ is bounded by a contraction, subject to a maximum step size condition on~$\stepsize$ (given in Lemma~\ref{lma:3}, see Appendix~\ref{apxB:outline}).
	
	\item Finally, we unroll the Lyapunov inequality to get the convergence Theorem~\ref{thm:convergence}.
\end{itemize}

\section{Empirical results}\label{sec:results}
\begin{figure*}[ttt!]
	\centering
	\begin{subfigure}[t]{0.48\linewidth}
		\centering
		\includegraphics[width = 1.05\linewidth]{figures/figure_2.pdf}
		\caption{Suboptimality as a function of time.}
		\label{fig:fig_2}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.48\linewidth}
		\centering
		\includegraphics[width = 1\linewidth]{figures/figure_3.pdf}
		\caption{Speedup as a function of the number of cores}
		\label{fig:fig_3}
	\end{subfigure}
	\caption{ {\bf Convergence and speedup for asynchronous stochastic gradient descent methods}.
		We display results for RCV1 and URL. Results for Covtype can be found in Appendix~\ref{apx:speedup}. } 
	\vspace{-3.5mm}
\end{figure*}
\vspace{-2mm}
We now present the main results of our empirical comparison of asynchronous \SAGA , \SVRG\ and \Hogwild. 
Additional results, including convergence and speedup figures with respect to the number of iteration and measures on the $\overlap$ constant are available in the appendix.
\vspace{-6mm}
\subsection{Experimental setup}\label{ImplDetails}
\vspace{-2mm}
{\bf Models.} 
Although \ASAGA\ can be applied more broadly, we focus on logistic regression, a model of particular practical importance. 
The associated objective function takes the following form:
$
{\frac{1}{n} \sum_{i=1}^n \log\big(1 + \exp(- b_i a_i^\intercal x)\big)} + \frac{\lambda}{2} \|x\|^2,
$
where $a_i \in \mathbb{R}^p$ and $b_i \in \{-1,+1\}$ are the data samples.

{\bf Datasets.}
We consider two sparse datasets: RCV1~\citep{RCV1} and URL~\citep{URL}; and a dense one, Covtype~\citep{Covtype}, with statistics listed in the table below. 
As in~\citet{SAG}, Covtype is standardized, thus $100\%$ dense.  
$\sparsity$ is $\mathcal{O}(1)$ in all datasets, hence not very insightful when relating it to our theoretical results.
Deriving a less coarse sparsity bound remains an open problem.

\begin{table}[h]
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{lcccc}
\toprule
{} & $n$ & $d$ & density & $\lipschitz$\\
\midrule
{\bf RCV1} & \hfill 697,641 & \hfill 47,236 & \hfill 0.15\% & \hfill 0.25\\ 
{\bf URL} & \hfill 2,396,130 & \hfill 3,231,961 & \hfill 0.004\% & \hfill 128.4\\
{\bf Covtype} & \hfill 581,012 & \hfill 54 & \hfill 100\% & \hfill 48428\\
\bottomrule
\end{tabular}
}
\end{table}

\vspace{-1mm}
{\bf Hardware and software}. 
Experiments were run on a 40-core machine with 384GB of memory. 
All algorithms were implemented in Scala. We chose this high-level language despite its typical 20x slowdown compared to C (when using standard libraries, see Appendix~\ref{scalavsc}) because our primary concern was that the code may easily be reused and extended for research purposes (to this end, we have made all our code available at \url{https://github.com/RemiLeblond/ASAGA}).
%The code we used to run all the experiments is available at \url{https://github.com/RemiLeblond/ASAGA}.

\vspace{-1mm}
\subsection{Implementation details}
\vspace{-2mm}
{\bf Exact regularization.} 
Following~\citet{laggedsaga}, the amount of regularization used was set to $\lambda=1 / n$.
In each update, we project the gradient of the regularization term (we multiply it by $D_i$ as we also do with the vector $\bar{\alpha}$) to preserve the sparsity pattern while maintaining an unbiased estimate of the gradient.
For squared $\ell_2$, the Sparse \SAGA\ updates becomes:
$
x^+ = x - \gamma (f_i'(x) - \alpha_i + D_i \bar{\alpha} + \lambda D_i x).
$

{\bf Comparison with the theoretical algorithm.} 
The algorithm we used in the experiments is fully detailed in Algorithm~\ref{alg:sagasync}. 
There are two differences with Algorithm~\ref{alg:theoretical}. 
First, in the implementation we pick $i_t$ at random \textit{before} we read data. 
This enables us to only read the necessary data for a given iteration (i.e.~$[\hat x_t]_{S_i}, [\hat \alpha_i^t], [\bar \alpha^t]_{S_i}$). 
Although this violates Property~\ref{independence}, it still performs well in practice. 

Second, we maintain $\bar \alpha^t$ in memory. 
This saves the cost of recomputing it at every iteration (which we can no longer do since we only read a subset data). 
Again, in practice the implemented algorithm enjoys good performance. 
But this design choice raises a subtle point: the update is not guaranteed to be unbiased in this setup (see Appendix~\ref{apx:Bias} for more details). 

\vspace{-2mm}
\subsection{Results}\label{ssec:results}
\vspace{-2mm}
We first compare three different asynchronous variants of stochastic gradient methods on the aforementioned datasets: \ASAGA, presented in this work, \KROMAGNON, the asynchronous sparse \SVRG\ method described in~\citet{mania} and \Hogwild~\citep{hogwild}.
Each method had its step size chosen so as to give the fastest convergence (up to $10^{-3}$ in the special case of \Hogwild). 
The results can be seen in Figure~\ref{fig:fig_2}: for each method we consider its asynchronous version with both one (hence sequential) and ten processors. 
This figure reveals that the asynchronous version offers a significant speedup over its sequential counterpart.

We then examine the speedup relative to the increase in the number of cores. 
The speedup is measured as time to achieve a suboptimality of $10^{-5}$ ($10^{-3}$ for \Hogwild) with one core divided by time to achieve the same suboptimality with several cores, averaged over 3 runs. 
Again, we choose step size leading to fastest convergence (see Appendix~\ref{scalavsc} for information about the step sizes). Results are displayed in Figure~\ref{fig:fig_3}. 

As predicted by our theory, we observe linear ``theoretical'' speedups (i.e. in terms of number of iterations, see Appendix~\ref{apx:speedup}).
However, with respect to running time, the speedups seem to taper off after $20$ cores.
This phenomenon can be explained by the fact that our hardware model is by necessity a simplification of reality. 
As noted in~\citet{duchi}, in a modern machine there is no such thing as \textit{shared memory}. 
Each core has its own levels of cache (L1, L2, L3) in addition to RAM. 
The more cores are used, the lower in the memory stack information goes and the slower it gets. 
More experimentation is needed to quantify that effect and potentially increase performance.

\vspace{-1.5mm}
\section{Conclusions and future work}
\vspace{-2.5mm}
We have described \ASAGA, a novel sparse and fully asynchronous variant of the incremental gradient algorithm \SAGA.
Building on the recently proposed ``perturbed iterate'' framework, we have introduced a novel analysis of the algorithm and proven that under mild conditions \ASAGA\ is linearly faster than \SAGA.
Our empirical benchmarks confirm speedups up to 10x.

Our proof technique accommodates more realistic settings than is usually the case in the literature (e.g. inconsistent reads/writes and an unbounded gradient); we obtain tighter conditions than in previous work. 
In particular, we show that sparsity is not always necessary to get linear speedups.
Further, we have proposed a novel perspective to clarify an important technical issue present in most of the recent convergence rate proofs for asynchronous parallel optimization algorithms.

\citet{laggedsaga} have shown that \SAG\ enjoys much improved performance when combined with non-uniform sampling and line-search.
We have also noticed that our $\sparsityr$ constant (being essentially a maximum) sometimes fails to accurately represent the full sparsity distribution of our datasets.
Finally, while our algorithm can be directly ported to a distributed master-worker architecture, its communication pattern would have to be optimized to avoid prohibitive costs. Limiting communications can be interpreted as artificially increasing the delay, yielding an interesting trade-off between delay influence and communication costs.

A final interesting direction for future analysis is the further exploration of the $\overlap$ term, which we have shown encompasses more complexity than previously thought.


%SLJ: no acknowledgment in submission
\subsubsection*{Acknowledgments}
%\note{Put something in there}
%
We would like to thank Xinghao Pan for sharing with us their implementation of \KROMAGNON, as well as Alberto Chiappa for spotting a typo in the proof.
%
This work was partially supported by a Google Research Award and the MSR-Inria Joint Center.
FP acknowledges financial support from
from the chaire {\em \'Economie des nouvelles donn\'ees} with the {\em data science} joint research initiative with the {\em fonds AXA pour la recherche}.

\bibliography{Asaga}
\bibliographystyle{abbrvnat}

%----------------------------------------------------------------------------
\clearpage
\appendix
\onecolumn
\fontsize{11}{13}
\selectfont

\paragraph{Appendix Outline:}
\begin{itemize}
\item In Appendix~\ref{apx:ProblematicExample}, we give a simple example illustrating why the ``After Write'' approach can break the crucial unbiasedness condition~\eqref{eq:unbiasedness} needed for standard convergence proofs.
\item In Appendix~\ref{apxA}, we adapt the proof from~\citet{qsaga} to prove Theorem~\ref{th1}, our convergence result for serial Sparse \SAGA.
\item In Appendix~\ref{apxB}, we first give a detailed outline and then the complete details for the proof of convergence for~\ASAGA\ (Theorem~\ref{thm:convergence}) as well as its linear speedup regimes  (Corollary~\ref{thm:bigdata}).
\item In Appendix~\ref{apx:AER}, we analyze additional experimental results, including a comparison of serial \SAGA\ algorithms and a look at ``theoretical speedups'' for \ASAGA.
\item In Appendix~\ref{apxD}, we take a closer look at the $\overlap$ constant. We argue that it encompasses more complexity than is usually implied in the literature, as additional results that we present indicate.
\item In Appendix~\ref{apxC}, we compare the lagged updates implementation of \SAGA\ with our sparse algorithm, and explain why adapting the former to the asynchronous setting is difficult.
\item In Appendix~\ref{apxE}, we give additional details about the datasets and our implementation.
\end{itemize}

\section{Problematic Example for the ``After Write'' Approach} \label{apx:ProblematicExample}
We provide a concrete example to illustrate the non-independence issue arising from the ``after write'' approach. 
Suppose that we have two cores and that $f$ has two factors: $f_1$ which has support on only one variable, and $f_2$ which has support on $10^6$ variables and thus yields a gradient step that is significantly more expensive to compute. 
In the ``after write'' approach, $x_0$ is the initial content of the memory, and we do not officially know yet whether $\hat{x}_0$ is the local copy read by the first core or the second core, but we are sure that $\hat{x}_0 = x_0$ as no update can occur in shared memory without incrementing the counter. 
There are four possibilities for the next step defining $x_1$ depending on which index $i$ was sampled on each core. 
If any core samples $i=1$, we know that $x_1 = x_0 - \stepsize f'_1(x_0)$ as it will be the first (much faster update) to complete. 
This happens in 3 out of 4 possibilities; we thus have that $\E x_1 = x_0 - \stepsize (\frac{3}{4} f'_1(x_0) + \frac{1}{4} f '_2(x_0))$ -- we see that this analysis scheme \emph{does not} satisfy the crucial unbiasedness condition~\eqref{eq:unbiasedness}. 

To understand this subtle point better, note that in this very simple example, $i_0$ and $i_1$ are not independent. We can show that $P(i_1=2 \mid i_0=2) =1$. They share dependency through the labeling assignment.
 
The only way we can think to resolve this issue and ensure unbiasedness in the ``after write'' framework is to assume that the computation time for the algorithm running on a core is independent of the sample $i$ chosen.
This assumption seems overly strong in the context of potentially heterogeneous factors $f_i$'s, and is thus a fundamental flaw in the ``after write'' framework that has mostly been ignored in the recent asynchronous optimization literature. 

We note that \citet{bertsekasParalle1989} briefly discussed this issue in Section~7.8.3 of their book, stressing that their analysis for SGD required that the scheduling of computation was independent from the randomness from SGD, but they did not offer any solution if this assumption was not satisfied. Both the ``before read'' labeling from~\citet{mania} and our proposed ``after read'' labeling resolve this issue.

\clearpage

\section{Proof of Theorem~\ref{th1}}\label{apxA}
\paragraph{Proof sketch for~\citet{qsaga}.}
As we will heavily reuse the proof technique from~\citet{qsaga}, we start by giving its sketch.

First, the authors combine classical strong convexity and Lipschitz inequalities to derive the inequality~\citet[Lemma~1]{qsaga}:
\begin{align}\label{eq:sparse}
\Econd \|x^{+} \! - \!x^*\|^2 \leq 
&(1 \! - \! \stepsize\strongconvex) \|x \! -\! x^*\|^2 
+ 2\stepsize^2 \Econd \|\alpha_i - f'_i(x^*)\|^2
+ (4 \stepsize^2 \lipschitz-2\stepsize)\big(f(x) - f(x^*)\big).
\end{align}
This gives a contraction term, as well as two additional terms; $2\stepsize^2 \Econd \|\alpha_i - f'_i(x^*)\|^2$ is a positive variance term, but $(4 \stepsize^2 \lipschitz-2\stepsize)\big(f(x) - f(x^*)\big)$ is a negative suboptimality term (provided $\stepsize$ is small enough).
The suboptimality term can then be used to cancel the variance one.

Second, the authors use a classical smoothness upper bound to control the variance term and relate it to the suboptimality.
However, since the $\alpha_i$ are partial gradients computed at previous time steps, the upper bounds of the variance involve suboptimality at previous time steps, which are not directly relatable to the current suboptimality.

Third, to circumvent this issue, a Lyapunov function is defined to encompass both current and past terms.
To finish the proof,~\citet{qsaga} show that the Lyapunov function is a contraction.

\paragraph{Proof outline.}\label{sparseoutline}
Fortunately, we can reuse most of the proof from~\citet{qsaga} to show that Sparse \SAGA\ converges at the same rate as regular \SAGA.
In fact, once we establish that \citet[Lemma~1]{qsaga} is still verified we are done.

To prove this, we derive close variants of equations $(6)$ and $(9)$ in their paper, which we remind the reader of here:
\begin{align}
\Econd \|f'_i(x) - \bar \alpha_i\|^2 &\leq 2 \Econd \|f'_i(x) - f'_i(x^*)\|^2 + 2 \Econd \|\bar \alpha_i - f'_i(x^*)\|^2 \, ,
\tag*{\citet[Eq.(6)]{qsaga}}
\\
\Econd \|\bar \alpha_i - f'_i(x^*)\|^2 &\leq \Econd \|\alpha_i - f'_i(x^*)\|^2 \, .
\tag*{\citet[Eq.(9)]{qsaga}}
\end{align} 

\paragraph{Deriving~\citet[Equation (6)]{qsaga}.}
We first show that the update estimator is unbiased.
The estimator is unbiased if:
\begin{align}\label{eq:apxBias}
\Econd D_i \bar \alpha = \Econd \alpha_i = \frac{1}{n}\sum_{i=1}^n \alpha_i \, .
\end{align}

We have:
\begin{align*}
\Econd D_i \bar \alpha 
= \frac{1}{n} \sum_{i=1}^n D_i \bar \alpha
= \frac{1}{n} \sum_{i=1}^n P_{S_i} D \bar \alpha
= \frac{1}{n} \sum_{i=1}^n \sum_{v \in S_i} \frac{[\bar \alpha]_v e_v}{p_v}
= \sum_{v=1}^{d} \left( \sum_{i\, | \, v \in S_i} 1 \right) \frac{[\bar \alpha]_v e_v}{n p_v}  \, ,
\end{align*}
where $e_v$ is the vector whose only nonzero component is the $v$ component which is equal to $1$.

By definition, $\sum_{i|v \in S_i} 1 = n p_v,$ which gives us Equation~\eqref{eq:apxBias}.

We define $\bar \alpha_i := \alpha_i - D_i\bar \alpha$ (contrary to~\citet{qsaga} where the authors define $\bar \alpha_i := \alpha_i - \bar \alpha$ since they do not concern themselves with sparsity).
Using the inequality $\|a+b\|^2 \leq 2 \|a\|^2 + 2 \|b\|^2$, we get:
\begin{align}
\Econd \|f'_i(x) - \bar \alpha_i\|^2 
\leq 2\Econd \|f'_i(x) -f'_i(x^*)\|^2 + 2\Econd \|\bar\alpha_i -f'_i(x^*)\|^2,
\end{align}
which is our equivalent to~\citet[Eq.(6)]{qsaga}, where only our definition of $\bar \alpha_i$ differs.

\paragraph{Deriving~\citet[Equation (9)]{qsaga}.}
We want to prove~\citet[Eq.(9)]{qsaga}:
\begin{align}
\Econd \|\bar \alpha_i -f'_i(x^*)\|^2
\leq \Econd \|\alpha_i -f'_i(x^*)\|^2 .
\end{align}

We have:
\begin{align} \label{eq:alphaiBarVariance}
\Econd \|\bar \alpha_i -f'_i(x^*)\|^2
&= \Econd \|\alpha_i -f'_i(x^*)\|^2
	-2\Econd \langle \alpha_i - f'_i(x^*), D_i \bar \alpha \rangle + \Econd\|D_i\bar \alpha\|^2 .
\end{align}

Let $D_{\neg i} := P_{S_i^c} D$; we then have the orthogonal decomposition $D \alpha = D_i \alpha + D_{\neg i} \alpha$ with $D_i \alpha \perp D_{\neg i} \alpha$, as they have disjoint support. We now use the orthogonality of $D_{\neg i} \alpha$ with any vector with support in $S_i$ to simplify the expression~\eqref{eq:alphaiBarVariance} as follows:
\begin{align}
\Econd \langle \alpha_i - f'_i(x^*), D_i\bar \alpha \rangle
&= \Econd \langle \alpha_i - f'_i(x^*), D_i \bar \alpha + D_{\neg i} \bar \alpha \rangle 
	\tag*{$(\alpha_i - f'_i(x^*) \perp D_{\neg i} \alpha)$}
\nonumber \\
&= \Econd \langle \alpha_i - f'_i(x^*), D \bar \alpha \rangle
\nonumber \\
&= \langle \Econd \big(\alpha_i - f'_i(x^*)\big), D \bar \alpha \rangle
\nonumber \\
&= \langle \Econd \alpha_i, D \bar \alpha \rangle
	\tag*{($f'(x^*) = 0$)}
\nonumber \\
&=\bar \alpha^\intercal D \bar \alpha \,  .
\end{align}

Similarly, 
\begin{align}
\Econd \|D_i\bar \alpha\|^2 
&= \Econd\langle D_i\bar \alpha, D_i\bar \alpha \rangle
\nonumber \\
&= \Econd\langle D_i\bar \alpha, D \bar \alpha \rangle
	\tag*{($D_i \alpha \perp D_{\neg i} \alpha$)}
\nonumber \\
&= \langle \Econd D_i\bar \alpha, D \bar \alpha \rangle
\nonumber \\
&= \bar \alpha^\intercal D \bar \alpha  \, .
\end{align}

Putting it all together,
\begin{align}
\Econd \|\bar \alpha_i -f'_i(x^*)\|^2 
= \Econd \|\alpha_i -f'_i(x^*)\|^2 - \bar \alpha^\intercal D \bar \alpha
\leq \Econd \|\alpha_i -f'_i(x^*)\|^2 .
\end{align}

This is our version of~\citet[Equation (9)]{qsaga}, which finishes the proof of~\citet[Lemma~1]{qsaga}. 
The rest of the proof from~\citet{qsaga} can then be reused without modification to obtain Theorem~\ref{th1}.
\qed

\section{Proof of Theorem~\ref{thm:convergence} and Corollary~\ref{thm:bigdata}}\label{apxB}
\subsection{Detailed outline}\label{apxB:outline}

We first give a detailed outline of the proof. The complete proof is given in the rest of Appendix~\ref{apxB}.

\paragraph{Initial recursive inequality.}
Let $g_t := g(\hat x_t, \hat \alpha^t, i_t)$. From the update equation~\eqref{eq:PIupdate} defining the virtual iterate $x_{t+1}$, the perturbed iterate framework~\citep{mania} gives:
\begin{align}\label{eq:initrec}
\|x_{t+1} - x^*\|^2 
&= \|x_t -\stepsize g_t -x^*\|^2 
\nonumber\\
&= \|x_t -x^*\|^2 + \stepsize^2 \|g_t\|^2  -2\stepsize\langle x_t -x^*,  g_t\rangle
\nonumber\\
&= \|x_t -x^*\|^2 + \stepsize^2 \|g_t\|^2 
	-2\stepsize\langle \hat x_t -x^*,  g_t\rangle +2\stepsize\langle \hat x_t -x_t,  g_t\rangle  \, .
\end{align}

Note that we have introduced $\hat x_t$ in the inner product because $g_t$ is a function of $\hat x_t$, not $x_t$.

In the sequential setting, we require $i_t$ to be independent of $x_t$ to get unbiasedness. 
In the perturbed iterate framework, we instead require that $i_t$ is independent of $\hat x_t$ (see Property~\ref{independence}). 
This crucial property enables us to use the unbiasedness condition~\eqref{eq:unbiasedness} to write:
$\E \langle \hat x_t -x^*,  g_t\rangle 
= \E \langle \hat x_t -x^*,  f'(\hat x_t)\rangle$. We thus take the expectation of~\eqref{eq:initrec} that allows us to use the $\mu$-strong convexity of $f$:\footnote{Note that here is our departure point with \citet{mania} who replaced the $f(\hat{x}_t)-f(x^*)$ term with the lower bound $\frac{\strongconvex}{2}\|\hat x_t - x^*\|^2$ in this relationship (see their Equation (2.4)), thus yielding an inequality too loose afterwards to get the fast rates for \SVRG.} 
\begin{align} \label{eq:strongconvexity}
\langle \hat x_t -x^*,  f'(\hat x_t)\rangle &\geq f(\hat x_t) -f(x^*) +\frac{\strongconvex}{2}\|\hat x_t - x^*\|^2 . 
\end{align}
With further manipulations on the expectation of~\eqref{eq:initrec}, including the use of the standard inequality $\|a + b\|^2 \leq 2\|a\|^2 + 2\|b\|^2$ (see Section~\ref{app:RecursiveDerivation}), we obtain our basic recursive contraction inequality:
\begin{align} \label{eq:RecursiveIneq2}
a_{t+1} &\leq 
	(1 -\frac{\stepsize \strongconvex}{2}) a_t 
	+ \stepsize^2 \E \|g_t\|^2 
	\underbrace{
		+ \stepsize\strongconvex \E\|\hat x_t - x^*\|^2 
		+ 2\stepsize \E \langle \hat x_t -x_t,  g_t\rangle
	}_{\text{additional asynchrony terms}}
	-2\stepsize e_t  \, ,
\end{align}
where $a_t := \E \|x_t - x^*\|^2$ and $e_t := \E f(\hat x_t) - f(x^*)$.

Inequality~\eqref{eq:RecursiveIneq2} is a midway point between the one derived in the proof of Lemma~1 in~\citet{qsaga} and Equation~(2.5) in~\citet{mania}, because we use the tighter strong convexity bound~\eqref{eq:strongconvexity} than in the latter (giving us the important extra term $-2\stepsize e_t$). 

In the sequential setting, one crucially uses the negative suboptimality term $-2\stepsize e_t$ to cancel the variance term $\stepsize^2 \E \|g_t\|^2$ (thus deriving a condition on $\stepsize$).
In our setting, we need to bound the additional asynchrony terms using the same negative suboptimality in order to prove convergence and speedup for our parallel algorithm -- this will give stronger constraints on the maximum step size.

The rest of the proof then proceeds as follows:
\begin{enumerate}
\item By using the expansion~\eqref{eq:async} for $\hat{x}_t-x_t$, we can bound the additional asynchrony terms in~\eqref{eq:RecursiveIneq2} in terms of the past updates ($\E \|g_u\|^2, u\leq t$). This gives Lemma~\ref{lma:1} below.
\item We then bound the updates $\E \|g_t\|^2$ in terms of past suboptimalities $(e_u)_{u \leq v}$ by using standard \SAGA\ inequalities and carefully analyzing the update rule for $\alpha_i^+$~\eqref{eq:SAGAupdate} in expectation. This gives Lemma~\ref{lma:suboptgt} below.
\item By substituting Lemma~\ref{lma:suboptgt} into Lemma~\ref{lma:1}, we get a master contraction inequality~\eqref{master} in terms of $a_{t+1}$, $a_t$ and $e_u, u\leq t$.
\item We define a novel Lyapunov function $\lyapunov_t = \sum_{u=0}^t (1-\contraction)^{t-u}a_u$ and manipulate the master inequality to show that $\lyapunov_t$ is bounded by a contraction, subject to a maximum step size condition on $\stepsize$ (given in Lemma~\ref{lma:3} below).
\item Finally, we unroll the Lyapunov inequality to get the convergence Theorem~\ref{thm:convergence}.
\end{enumerate}
We list the key lemmas below with their proof sketch, and give the detailed proof in the later sections of Appendix~\ref{apxB}.

\begin{lemma}[Inequality in terms of $g_t := g(\hat x_{t}, \hat \alpha^t, i_{t})$]\label{lma:1}
For all $t \geq 0$:
\begin{equation} \label{eq:recursivegt}
a_{t+1} \leq 
	(1 - \frac{\stepsize\strongconvex}{2}) a_t + \stepsize^2 C_1\E\|g_t\|^2 
	+ \stepsize^2 C_2\sum_{u=(t-\overlap)_+}^{t-1}\E\|g_{u}\|^2 - 2\stepsize e_t  \, ,
\end{equation}
where 
$C_1 := 1 + \sqrt{\sparsity}\overlap$ and
$C_2 :=  \sqrt{\sparsity} + \stepsize\strongconvex C_1$.
\end{lemma}

To prove this lemma we need to bound both $\E\|\hat x_t - x^*\|^2$ and $\E\langle \hat x_t -x_t,  g_t\rangle$ with respect to $(g_u, u\leq t)$. 
We achieve this by crucially using Equation~\eqref{eq:async}, together with the following proposition, which we derive by a combination of Cauchy-Schwartz and our sparsity definition (see Section~\ref{apxB:lma1}).
\begin{equation}
\E \langle G_{u}^t g_{u}, g_t \rangle \leq \frac{\sqrt{\sparsity}}{2}(\E\|g_{u}\|^2 + \E\|g_{t}\|^2)  \, .
\end{equation}

\begin{lemma} [Suboptimality bound on $\E \|g_t\|^2$]\label{lma:suboptgt}
For all $t \geq 0$,
\begin{equation}\label{gtbound}
\E\|g_t\|^2 
\leq 4\lipschitz e_t 
	+ \frac{4\lipschitz}{n} \sum_{u=1}^{t-1} (1 - \frac{1}{n})^{(t-2\overlap-u -1)_+} e_u
	+ 4\lipschitz (1 - \frac{1}{n})^{(t-\overlap)_+} \tilde e_0  \, .
\end{equation}
where $\tilde e_0 := \frac{1}{2\lipschitz} \E\|\alpha_i^0 - f'_i(x^*)\|^2$.\footnote{We introduce this quantity instead of $e_0$ so as to be able to handle the arbitrary initialization of the $\alpha_i^0$.}
\end{lemma}

From our Sparse \SAGA\ proof we know that (see Appendix~\ref{apxA}):
\begin{align}
\E\|g_t\|^2 
&\leq 2 \E \|f'_{i_t}(\hat x_t)-f'_{i_t}(x^*)\|^2
	+ 2 \E \|\hat \alpha_{i_t}^t - f'_{i_t}(x^*)\|^2 .
\end{align}
We can handle the first term by taking the expectation over a Lipschitz inequality (\citet[Equations (7) and (8)]{qsaga}. 
All that remains to prove the lemma is to express the $\E \|\hat \alpha_{i_t}^t - f'_{i_t}(x^*)\|^2$ term in terms of past suboptimalities.
We note that it can be seen as an expectation of past first terms with an adequate probability distribution which we derive and bound.

From our algorithm, we know that each dimension of the memory vector $[\hat \alpha_i]_v$ contains a partial gradient computed at some point in the past $[f'_i(\hat x_{u_{i, v}^{t}})]_v$\footnote{More precisely: $\forall t, i, v \hspace{0.5em}\exists u_{i, v}^t<t$ s.t. $[\hat \alpha_{i}^t]_v = [f'_{i}(\hat x_{u_{i, v}^{t}})]_v$.} (unless $u=0$, in which case we replace the partial gradient with $\alpha_i^0$).
We then derive bounds on $P(u_{i,v}^t = u)$ and sum on all possible $u$. 
Together with clever conditioning, we obtain Lemma~\ref{lma:suboptgt} (see Section~\ref{apxB:lma2}).

\paragraph{Master inequality.}
Let $H_t$ be defined as ${H_t: = \sum_{u=1}^{t-1} (1 - \frac{1}{n})^{(t-2\overlap-u -1)_+} e_u}$. 
Then, by setting~\eqref{gtbound} into Lemma~\ref{lma:1}, we get (see Section~\ref{apxB:master}):
\begin{equation}\label{master}
\begin{aligned}
a_{t+1} 
\leq &(1 - \frac{\stepsize\strongconvex}{2}) a_t 
	- 2\stepsize e_t 
	+ 4\lipschitz\stepsize^2 C_1 \big(e_t  + (1 - \frac{1}{n})^{(t-\overlap)_+} \tilde e_0 \big)
	+ \frac{4\lipschitz\stepsize^2 C_1}{n} H_t 
\\
	&+4\lipschitz\stepsize^2 C_2\sum_{u=(t-\overlap)_+}^{t-1} (e_u +  (1 - \frac{1}{n})^{(u - \overlap)_+} \tilde e_0 \big)
	+\frac{4\lipschitz\stepsize^2 C_2}{n} \sum_{u=(t-\overlap)_+}^{t-1} H_u  \, .
\end{aligned}
\end{equation}

\paragraph{Lyapunov function and associated recursive inequality.}
We now have the beginning of a contraction with additional positive terms which all converge to $0$ as we near the optimum, as well as our classical negative suboptimality term.
This is not unusual in the variance reduction literature. One successful approach in the sequential case is then to define a Lyapunov function which encompasses all terms and is a true contraction (see~\citet{SAGA, qsaga}).
We emulate this solution here.
However, while all terms in the sequential case only depend on the current iterate, $t$, in the parallel case we have terms ``from the past'' in our inequality.
To resolve this issue, we define a more involved Lyapunov function which also encompasses past iterates: 
\begin{align} \label{eq:LyapunovDefinition}
\lyapunov_t = \sum_{u=0}^t (1-\contraction)^{t-u}a_u, \quad 0<\contraction < 1,
\end{align}
where $\contraction$ is a target contraction rate that we define later.

Using the master inequality~\eqref{master}, we get (see Appendix~\ref{apxB:lyapunov}):
\begin{align}\label{Lyapunov}
\lyapunov_{t+1} 
&= (1 - \contraction)^{t+1}a_0 
	+ \sum_{u=0}^t(1 - \contraction)^{t-u}a_{u+1}
\nonumber \\
&\leq (1 - \contraction)^{t+1}a_0 + (1-\frac{\stepsize\strongconvex}{2})\lyapunov_t + \sum_{u=1}^t r_u^t e_u + r_0^t \tilde e_0  \, .
\end{align}

The aim is to prove that $\lyapunov_t$ is bounded by a contraction. 
We have two promising terms at the beginning of the inequality, and then we need to handle the last term.
Basically, we can rearrange the sums in~\eqref{master} to expose a simple sum of $e_u$ multiplied by factors $r_u^t$. 

Under specific conditions on $\contraction$ and $\stepsize$, we can prove that $r_u^t$ is negative for all $u \geq 1$, which coupled with the fact that each $e_u$ is positive means that we can safely drop the sum term from the inequality. 
The $r_0^t$ term is a bit trickier and is handled separately.

In order to have a bound on $e_t$ directly rather than on $\E \|\hat x_t - x^*\|^2$, we then introduce an additional $\stepsize e_t$ term on both sides of~\eqref{Lyapunov}.
The bound on $\stepsize$ under which the modified $r_t^t + \stepsize$ is negative is then twice as small (we could have used any multiplier between $0$ and $2\stepsize$, but chose $\stepsize$ for simplicity's sake).
This condition is given in the following Lemma.

\begin{lemma} [Sufficient condition for convergence]\label{lma:3}
Suppose $\overlap < n/10$ and $\contraction \leq 1/4n$. If 
\begin{align}\label{eq:lmacondition}
\stepsize \leq \stepsize^* = \frac{1}{32\lipschitz (1 + \sqrt{\sparsity} \overlap) \sqrt{1 + \frac{1}{8\kappa} \min(\overlap, \frac{1}{\sqrt{\sparsity}})}}
\end{align}
then for all $u \geq 1$, the $r_u^t$ from~\eqref{Lyapunov} verify:
\begin{align}
r_u^t \leq 0 \,; \quad r_t^t + \stepsize \leq 0 \,,
\end{align}
and thus we have:
\begin{align}
\stepsize  e_t + \lyapunov_{t+1} \leq (1 - \contraction)^{t+1}a_0 + (1-\frac{\stepsize\strongconvex}{2})\lyapunov_t + r_0^t \tilde e_0 \,.
\end{align}

\end{lemma}

We obtain this result after carefully deriving the $r_u^t$ terms. 
We find a second-order polynomial inequality in $\stepsize$, which we simplify down to~\eqref{eq:lmacondition} (see Appendix~\ref{apxB:lma3}).

We can then finish the argument to bound the suboptimality error $e_t$. We have:
\begin{align}
\lyapunov_{t+1} \leq
\stepsize e_t + \lyapunov_{t+1} 
&\leq (1-\frac{\stepsize\strongconvex}{2})\lyapunov_t  +(1 - \contraction)^{t+1} (a_0 + A \tilde e_0)  \, .
\end{align}

We have two linearly contracting terms. 
The sum contracts linearly with the worst rate between the two (the smallest geometric rate factor).
If we define $\contraction^* := \nu \min(\contraction, \stepsize \strongconvex / 2)$, with $0 <\nu < 1$,\footnote{$\nu$ is introduced to circumvent the problematic case where $\contraction$ and  $\stepsize \strongconvex / 2$ are too close together.} then we get:
\begin{align}
\stepsize e_t + \lyapunov_{t+1} 
&\leq (1-\frac{\stepsize\strongconvex}{2})^{t+1}\lyapunov_0 + (1 - \contraction^*)^{t+1} \frac{a_0 + A \tilde e_0}{1 -\eta} 
\\
\stepsize e_t &\leq (1 - \contraction^*)^{t+1} \big(\lyapunov_0 + \frac{1}{1 -\eta} (a_0 + A  \tilde e_0) \big) \, ,
\end{align}
where $\eta := \frac{1-M}{1-\contraction^*}$ with $M :=\max(\contraction, \stepsize\strongconvex/2)$. Our geometric rate factor is thus $\contraction^*$ (see Appendix~\ref{apxB:th2}).

\subsection{Extension to \SVRG}\label{apx:SVRGext}
Our proof can easily be adapted to accommodate the \SVRG\ variant introduced in~\citet{qsaga}, which is closer to \SAGA\ than the initial \SVRG\ algorithm and which is adaptive to local strong convexity (it does not require the inner loop epoch size~$m = \Omega(\kappa)$ as a hyperparameter). 
In this variant, instead of computing a full gradient every $m$ iterations, a random binary variable $U$ with probability $P(U=1)=1/n$ is sampled at the beginning of every iteration to determine whether a full gradient is computed or a normal \SVRG\ step is made. 
If $U=1$, then a full gradient is computed.
Otherwise the algorithm takes a normal inner \SVRG\ step.\footnote{Note that the parallel implementation is not very straightforward, as it requires a way to communicate to cores when they should start computing a batch gradient instead of inner steps.}

To prove convergence, all one has to do is to modify Lemma~\ref{lma:suboptgt} very slightly (the only difference is that the $(t -2\overlap -u -1)_+$ exponent is replaced by $(t - u)$ and the rest of the proof can be used as is).
The justification for this small tweak is that the batch steps in \SVRG\ are fully synchronized. More details can be found in Section~\ref{apxB:lma2} (see footnote~\ref{footnote}).

By using our ``after read'' labeling, we were also able to derive a convergence and speedup proof for the original \SVRG\ algorithm, but the proof technique diverges after Lemma~\ref{lma:1}. 
This is beyond the scope of this paper, so we omit it here. 
Using the ``after read'' labeling and a different proof technique from~\citet{mania}), we obtain an epoch size in $\mathcal{O}(\kappa)$ instead of $\mathcal{O}(\kappa^2)$ and a dependency in our overlap bound in $\mathcal{O}(\sparsity^{-1/2})$ instead of $\mathcal{O}(\sparsity^{-1/3})$.

\paragraph{\ASAGA\ vs. asynchronous \SVRG.}
There are several scenarios in which \ASAGA\ can be practically advantageous over its closely related cousin, asynchronous \SVRG\ (note though that ``asynchronous'' \SVRG\ still requires a synchronization step to compute the full gradients).

First, while \SAGA\ trades memory for less computation, in the case of generalized linear models the memory cost can be reduced to $\mathcal{O}(n)$, which is the same as for $\SVRG$.
This is of course also true for their asynchronous counterparts.

Second, as \ASAGA\ does not require any synchronization steps, it is better suited to heterogeneous computing environments (where cores have different clock speeds or are shared with other applications).

Finally, \ASAGA\ does not require knowing the condition number $\kappa$ for optimal convergence in the sparse regime.
It is thus adaptive to local strong convexity, whereas \SVRG\ is not.
Indeed, \SVRG\ and its asynchronous variant require setting an additional hyper-parameter -- the epoch size $m$ -- which needs to be at least $\Omega(\kappa)$ for convergence but yields a slower effective convergence rate than \ASAGA\ if it is set much bigger than $\kappa$.
\SVRG\ thus requires tuning this additional hyper-parameter or running the risk of either slower convergence (if the epoch size chosen is much bigger than the condition number) or even not converging at all (if $m$ is chosen to be much smaller than $\kappa$).\footnote{Note that as \SAGA\ (and contrary to the original \SVRG),  the \SVRG\ variant from~\citet{qsaga} does not require knowledge of $\kappa$ and is thus adaptive to local strong convexity, which carries over to its asynchronous adaptation.}

\subsection{Initial recursive inequality derivation} \label{app:RecursiveDerivation}
\vspace{-1mm}
We start by proving Equation~\eqref{eq:RecursiveIneq2}.
Let $g_t := g(\hat x_t, \hat \alpha^t, i_t)$. From~\eqref{eq:PIupdate}, we get:
\begin{align*}
\|x_{t+1} - x^*\|^2 \nonumber
= \|x_t -\stepsize g_t -x^*\|^2 
&= \|x_t -x^*\|^2 + \stepsize^2 \|g_t\|^2 -2\stepsize\langle x_t -x^*,  g_t\rangle
\\
&= \|x_t -x^*\|^2 + \stepsize^2 \|g_t\|^2 
	- 2 \stepsize\langle \hat x_t -x^*,  g_t\rangle +2\stepsize\langle \hat x_t -x_t,  g_t\rangle .
\end{align*}

In order to prove Equation~\eqref{eq:RecursiveIneq2}, we need to bound the $- 2 \stepsize\langle \hat x_t -x^*,  g_t\rangle$ term.
Thanks to Property~\ref{independence}, we can write:
\vspace{-2mm}
\begin{align*}
\E \langle \hat x_t -x^*,  g_t\rangle 
= \E \langle \hat x_t -x^*, \Econd g_t \rangle
= \E \langle \hat x_t -x^*,  f'(\hat x_t)\rangle  \, .
\end{align*}

We can now use a classical strong convexity bound as well as a squared triangle inequality to get:
\begin{align}
- \langle \hat x_t -x^*,  f'(\hat x_t)\rangle &\leq - \big(f(\hat x_t) -f(x^*)\big) -\frac{\strongconvex}{2}\|\hat x_t - x^*\|^2
\tag*{(Strong convexity bound)} \nonumber \\
- \|\hat x_t - x^*\|^2 &\leq \|\hat x_t - x_t\|^2 - \frac{1}{2} \|x_t - x^*\|^2
\tag*{($\|a+b\|^2 \leq 2 \|a\|^2 + 2 \|b\|^2$)} \nonumber \\
- 2 \stepsize \E \langle \hat x_t -x^*,  g_t\rangle &\leq
	- \frac{\stepsize \strongconvex}{2} \E \|x_t - x^*\|^2
	+ \stepsize \strongconvex \E \|\hat x_t - x_t\|^2
	-2 \stepsize \big(\E f(\hat x_t) - f(x^*)\big)  \, .
\end{align}

Putting it all together, we get the initial recursive inequality~\eqref{eq:RecursiveIneq2}, rewritten here explicitly:
\begin{align}
a_{t+1} \leq 
	(1 -\frac{\stepsize \strongconvex}{2}) a_t 
	+ \stepsize^2 \E \|g_t\|^2 
	+ \stepsize\strongconvex \E\|\hat x_t - x_t\|^2 
	+ 2\stepsize \E \langle \hat x_t -x_t,  g_t\rangle
	-2\stepsize e_t  \, ,
\end{align}
where $a_t := \E \|x_t - x^*\|^2$ and $e_t := \E f(\hat x_t) - f(x^*)$.

\subsection{Proof of Lemma~\ref{lma:1}} \label{apxB:lma1}
To prove Lemma~\ref{lma:1}, we now bound both $\E\|\hat x_t - x_t\|^2$ and $\E\langle \hat x_t -x_t,  g_t\rangle$ with respect to $\E\|g_u\|^2, u\leq t$.

We start by proving a relevant property of $\sparsity$, which enables us to derive an essential inequality for both these terms, given in Proposition~\ref{prop:1} below.
We reuse the sparsity constant introduced in~\citet{smola} and relate it to the one we have defined earlier, $\sparsityr$:
\begin{remark} \label{rmk:1}
Let $D$ be the smallest constant such that:
\begin{align} \label{sparsitycondition}
\Econd \|x\|_i^2 = \frac{1}{n} \sum_{i=1}^n \|x\|_i^2 \leq D \|x\|^2 \quad  \forall x \in \mathbb{R}^d,
\end{align}
where $\|.\|_i$ is defined to be the $\ell_2$-norm restricted to the support $S_i$ of $f_i$. 
We have:
\begin{equation}
D = \frac{\sparsityr}{n} = \sparsity  \, .
\end{equation}
\end{remark}

\begin{proof}
We have:
\begin{align}
\Econd \|x\|_i^2 = \frac{1}{n} \sum_{i=1}^n \|x\|_i^2
= \frac{1}{n} \sum_{i=1}^n \sum_{v \in S_i} [x]_v^2
= \frac{1}{n} \sum_{v=1}^d \sum_{i \mid v \in S_i} [x]_v^2
= \frac{1}{n} \sum_{v=1}^d \delta_v [x]_v^2 \, ,
\end{align}
where $\delta_v := \mathbf{card}(i \mid v \in S_i)$.

This implies:
\begin{align}
D \geq \frac{1}{n} \sum_{v=1}^d \delta_v \frac{[x]_v^2}{\|x\|^2}  \, .
\end{align}

Since $D$ is the minimum constant satisfying this inequality, we have:
\begin{align}
D = \max_{x \in \mathbb{R}^d} \frac{1}{n} \sum_{v=1}^d \delta_v \frac{[x]_v^2}{\|x\|^2}  \, .
\end{align}
We need to find $x$ such that it maximizes the right-hand side term.
Note that the vector $([x]_v^2 / \|x\|^2)_{v=1..d}$ is in the unit probability simplex, which means that an equivalent problem is the maximization over all convex combinations of $(\delta_v)_{v=1..d}$.
This maximum is found by putting all the weight on the maximum $\delta_v$, which is $\sparsityr$ by definition.

This means that $\sparsity = \sparsityr / n$ is indeed the smallest constant satisfying~\eqref{sparsitycondition}.
\end{proof}

\begin{proposition}\label{prop:1}
For any $u \neq t$,
\begin{align}\label{sparseproduct}
\E |\langle g_{u}, g_t \rangle | &\leq \frac{\sqrt{\sparsity}}{2}(\E\|g_{u}\|^2 + \E\|g_{t}\|^2)  \, .
\end{align}
\end{proposition}

\begin{proof}
Let $u \neq t$. Without loss of generality, $u < t$.\footnote{One only has to switch $u$ and $t$ if $u>t$.}
Then:

\begin{align}
\E |\langle g_{u}, g_t \rangle |
&\leq \E\|g_{u}\|_{i_t}\|g_t\| 
\tag*{(Sparse inner product; support of $g_t$ is $S_{i_t}$)} \nonumber \\ 
&\leq \sqrt{\E\|g_{u}\|_{i_t}^2}\sqrt{\E\|g_{t}\|^2}
\tag*{(Cauchy-Schwarz for expectations)} \nonumber \\ 
&\leq \sqrt{\sparsity \E\|g_{u}\|^2}\sqrt{\E\|g_{t}\|^2}
\tag*{(Remark~\ref{rmk:1} and $i_t \perp\!\!\!\perp g_u, \forall u < t$)} \nonumber \\ 
&\leq \frac{\sqrt{\sparsity}}{2}(\E\|g_{u}\|^2 + \E\|g_{t}\|^2)  \, .
\tag*{(AM-GM inequality)}
\end{align}
All told, we have:
\begin{align}
\E |\langle g_{u}, g_t \rangle | &\leq \frac{\sqrt{\sparsity}}{2}(\E\|g_{u}\|^2 + \E\|g_{t}\|^2)  \, .
\end{align}
\end{proof}

\paragraph{Bounding $\E\langle \hat x_t -x_t,  g_t\rangle$ in terms of $g_u$.}
\begin{align}
\frac{1}{\stepsize} \E\langle \hat x_t - x_t, g_t \rangle 
&= \sum_{u=(t - \overlap)_+}^{t-1} \E \langle G_u^t g_u, g_t \rangle
\tag*{(by Equation~\eqref{eq:async})} \nonumber \\
&\leq \sum_{u=(t - \overlap)_+}^{t-1} \E | \langle g_u, g_t \rangle |
\tag*{($G_u^t$ diagonal matrices with terms in $\{0, 1\}$)}\nonumber \\
&\leq \sum_{u=(t - \overlap)_+}^{t-1} \frac{\sqrt{\sparsity}}{2}(\E\|g_{u}\|^2 + \E\|g_{t}\|^2) 
\tag*{(by Proposition~\ref{prop:1})}\nonumber \\
&\leq \frac{\sqrt{\sparsity}}{2} \sum_{u=(t - \overlap)_+}^{t-1}\E\|g_{u}\|^2 + \frac{\sqrt{\sparsity}\overlap}{2}\E\|g_{t}\|^2 .
\end{align}

\paragraph{Bounding $\E\|\hat x_t - x_t\|^2$ with respect to $g_u$}
Thanks to the expansion for $\hat x_t - x_t$~\eqref{eq:async}, we get:
\begin{align*}
\|\hat x_t - x_t\|^2 
\leq \stepsize^2 \sum_{u, v=(t -\overlap)_+}^{t-1}|\langle G_u^t g_{u}, G_v^t g_{v}\rangle | 
\leq \stepsize^2 \sum_{u=(t -\overlap)_+}^{t-1}\|g_{u}\|^2 
	+ \stepsize^2 \sum_{\substack{u, v=(t-\overlap)_+ \\u\neq v}}^{t-1} |\langle G_u^t g_{u}, G_v^t g_{v}\rangle |  \, .
\end{align*}
Using~\eqref{sparseproduct} from Proposition~\ref{prop:1}, we have that for $u \neq v$:

\begin{equation} \label{supersparse}
\E |\langle G_u^t g_{u}, G_v^t g_{v}\rangle | 
\leq \E |\langle g_{u}, g_{v}\rangle | 
\leq \frac{\sqrt{\sparsity}}{2}(\E\|g_{u}\|^2 + \E\|g_{v}\|^2) \, .
\end{equation}

By taking the expectation and using~\eqref{supersparse}, we get:
\begin{align}
\E\|\hat x_t - x_t\|^2
&\leq \stepsize^2 \sum_{u=(t-\overlap)_+}^{t-1}\E\|g_{u}\|^2 
	+ \stepsize^2 \sqrt{\sparsity}(\overlap-1)_+ \sum_{u=(t-\overlap)_+}^{t-1}\E\|g_{u}\|^2 
\nonumber \\
&= \stepsize^2 \big(1+\sqrt{\sparsity}(\overlap-1)_+ \big)\sum_{u=(t-\overlap)_+}^{t-1}\E\|g_{u}\|^2 
\nonumber \\
&\leq \stepsize^2 \big(1+\sqrt{\sparsity}\overlap \big)\sum_{u=(t-\overlap)_+}^{t-1}\E\|g_{u}\|^2 .
\end{align}

We can now rewrite~\eqref{eq:RecursiveIneq2} in terms of $\E\|g_t\| ^2$, which finishes the proof for Lemma~\ref{lma:1} (by introducing $C_1$ and $C_2$ as specified in Lemma~\ref{lma:1}):
\begin{align}
a_{t+1} &\leq 
	(1 - \frac{\stepsize\strongconvex}{2}) a_t 
	- 2\stepsize e_t
	+ \stepsize^2 \E\|g_t\|^2
	+ \stepsize^3 \strongconvex(1+\sqrt{\sparsity}\overlap)\sum_{u=(t-\overlap)_+}^{t-1}\E\|g_{u}\|^2
\nonumber \\
	&\quad + \stepsize^2 \sqrt{\sparsity}\sum_{u=(t-\overlap)_+}^{t-1}\E\|g_{u}\|^2
	+ \stepsize^2 \sqrt{\sparsity}\overlap\E\|g_t\|^2
\nonumber \\
&\leq (1 - \frac{\stepsize\strongconvex}{2}) a_t 
	- 2\stepsize e_t
	+ \stepsize^2 C_1 \E\|g_t\|^2
	+ \stepsize^2 C_2 \sum_{u=(t-\overlap)_+}^{t-1}\E\|g_{u}\|^2 .
\end{align}
\qed

\subsection{Proof of Lemma~\ref{lma:suboptgt}} \label{apxB:lma2}
We now derive our bound on $g_t$ with respect to suboptimality.
From Appendix~\ref{apxA}, we know that:
\begin{align}
\E\|g_t\|^2
&\leq 2 \E \|f'_{i_t}(\hat x_t)-f'_{i_t}(x^*)\|^2 
	+ 2 \E \|\hat \alpha_{i_t}^t - f'_{i_t}(x^*)\|^2 \label{eq:classicsaga}
\\
\E \|f'_{i_t}(\hat x_t)-f'_{i_t}(x^*)\|^2 
&\leq 2\lipschitz\big(\E f(\hat x_t) - f(x^*)\big)
= 2\lipschitz e_t  \, . \label{eq:classicsaga2}
\end{align}
%\tag*{(\citep[Equation (8)]{qsaga})}

\textbf{N. B.: In the following, $i_t$ is a random variable picked uniformly at random in $\{1,...,n\}$, whereas $i$ is a fixed constant.} 

We still have to handle the $\E \|\hat \alpha_{i_t}^t - f'_{i_t}(x^*)\|^2$ term and express it in terms of past suboptimalities. 
We know from our definition of $t$ that $i_t$ and $\hat x_u$ are independent $\forall u<t$.
Given the ``after read'' global ordering, $\Econd$ -- the expectation on $i_t$ conditioned on $\hat x_t$ and all ``past" $\hat x_u$ and $i_u$ -- is well defined, and we can rewrite our quantity as:
\begin{align*}
\E \|\hat \alpha_{i_t}^t - f'_{i_t}(x^*)\|^2 
= \E \big( \Econd \|\hat \alpha_{i_t}^t - f'_{i_t}(x^*)\|^2 \big)
&= \E \frac{1}{n} \sum_{i=1}^n  \|\hat \alpha_i^t - f'_i(x^*)\|^2
\\
&=  \frac{1}{n} \sum_{i=1}^n \E \|\hat \alpha_i^t - f'_i(x^*)\|^2 .
\end{align*}

Now, with $i$ fixed, let $u_{i,l}^t$ be the time of the iterate last used to write the $[\hat \alpha_{i}^t]_l$ quantity, i.e. $[\hat \alpha_{i}^t]_l = [f'_{i}(\hat x_{u_{i, l}^t})]_l$.
We know\footnote{In the case where $u=0$, one would have to replace the partial gradient with $\alpha_i^0$. We omit this special case here for clarity of exposition.} that $0 \leq u_{i,l}^t \leq t - 1$.
To use this information, we first need to split $\hat \alpha_i$ along its dimensions to handle the possible inconsistencies among them:
\begin{align*}
\E \|\hat \alpha_i^t - f'_i(x^*)\|^2
=
\E \sum_{l=1}^d \big([\hat \alpha_i^t]_l-[f'_i(x^*)]_l\big)^2
= 
\sum_{l=1}^d \E \Big[ \big([\hat \alpha_i^t]_l-[f'_i(x^*)]_l\big)^2 \Big] .
\end{align*}

This gives us:
\begin{align}
\E \|\hat \alpha_i^t - f'_i(x^*)\|^2
&= 
\sum_{l=1}^d \E \Big[ \big(f'_{i}(\hat x_{u_{i, l}^t})_l-f'_i(x^*)_l\big)^2 \Big] \notag \\
&=
\sum_{l=1}^d\E \Big[\sum_{u=0}^{t-1} \ind_{\{u_{i, l}^t = u\}} \big(f'_i(\hat x_u)_l-f'_i(x^*)_l\big)^2 \Big] \notag \\
&=
\sum_{u=0}^{t-1} \sum_{l=1}^d\E \Big[\ind_{\{u_{i, l}^t = u\}} \big(f'_i(\hat x_u)_l-f'_i(x^*)_l\big)^2 \Big] 
\label{eq:IndicatorsAppearance}.
\end{align}

We will now rewrite the indicator so as to obtain independent events from the rest of the equality. 
This will enable us to distribute the expectation. 
Suppose $u>0$ ($u=0$ is a special case which we will handle afterwards). $\{u_{i, l}^t = u\}$ requires two things:
\begin{enumerate}
\item at time $u$, $i$ was picked uniformly at random,
\item (roughly) $i$ was not picked again between $u$ and $t$.
\end{enumerate}
We need to refine both conditions because we have to account for possible collisions due to asynchrony. 
We know from our definition of $\overlap$ that the $t^\mathrm{th}$ iteration finishes before at $t + \overlap + 1$, but it may still be unfinished by time $t + \overlap$.
This means that we can only be sure that an update selecting $i$ at time $v$ has been written to memory at time $t$ if $v \leq t -\overlap -1$.
Later updates may not have been written yet at time $t$.
Similarly, updates before $v = u + \overlap +1$ may be overwritten by the $u^\mathrm{th}$ update so we cannot infer that they did not select $i$. From this discussion, we conclude that $u_{i, l}^t = u$ implies that $i_v \neq i$ for all $v$ between $u+\overlap+1$ and $t-\overlap-1$, though it can still happen that $i_v = i$ for $v$ outside this range.
%SLJ: much clearer, thanks!
%Note that these two events are independent, and that they are both independent from $\hat x_u$. -> mention this later anyway

Using the fact that $i_u$ and $i_v$ are independent for $v \neq u$, we can thus upper bound the indicator function appearing in~\eqref{eq:IndicatorsAppearance} as follows:\footnote{\label{footnote}In the simpler case of the variant of \SVRG\ from~\citet{qsaga} as described in~\ref{apx:SVRGext}, the batch gradient computations are fully synchronized. This means that we can write much the same inequality without having to worry about possible overwrites, thus replacing  $\ind_{\{i_v \neq i\ \forall v\ \text{s.t.}\ u+\overlap+1 \leq v \leq t-\overlap-1\}}$ by $\ind_{\{i_v \neq i\ \forall v\ \text{s.t.}\ u+1 \leq v \leq t\}}$.}
\begin{align}\label{eq:indicatrices}
\ind_{\{u_{i,l}^t = u\}} 
\leq \ind_{\{i_u=i\}} 
	 \ind_{\{i_v \neq i\ \forall v\ \text{s.t.}\ u+\overlap+1 \leq v \leq t-\overlap-1\}} .
\end{align}
This gives us:
\begin{align}
\E \Big[ &\ind_{\{u_{i,l}^t = u\}} \big(f'_i(\hat x_u)_l-f'_i(x^*)_l\big)^2 \Big]
\nonumber \\ 
&\leq \E \Big[\ind_{\{i_u=i\}} 
	\ind_{\{i_v \neq i\ \forall v\ \text{s.t.}\ u+\overlap+1 \leq v \leq t-\overlap-1\}} \big(f'_i(\hat x_u)_l-f'_i(x^*)_l\big)^2\Big]
\nonumber \\ 
&\leq P\{i_u=i\}
	P\{i_v \neq i\ \forall v\ \text{s.t.}\ u+\overlap+1 \leq v \leq t-\overlap-1\}
	\E\big(f'_i(\hat x_u)_l-f'_i(x^*)_l\big)^2
\tag*{($i_v \perp\!\!\! \perp \hat x_u, \forall v \geq u$)} \nonumber \\ 
&\leq \frac{1}{n}(1 - \frac{1}{n})^{(t-2\overlap-u -1)_+}
	\E\big(f'_i(\hat x_u)_l-f'_i(x^*)_l\big)^2 .
\end{align}
Note that the third line used the crucial independence assumption $i_v \perp\!\!\! \perp \hat x_u, \forall v \geq u$ arising from our ``After Read'' ordering.
Summing over all dimensions $l$, we then get:
\begin{align}
\E \Big[ \ind_{\{u_{i,l}^t = u\}} \|f'_i(\hat x_u)-f'_i(x^*)\|^2 \Big]
\leq \frac{1}{n}(1 - \frac{1}{n})^{(t-2\overlap-u-1)_+}
	\E \|f'_i(\hat x_u)-f'_i(x^*)\|^2 .
\end{align}

So now:
\begin{align}
\E \|\hat \alpha_{i_t}^t - f'_{i_t}(x^*)\|^2 - \lambda \tilde e_0
&\leq \frac{1}{n}\sum_{i=1}^n \sum_{u=1}^{t-1} \frac{1}{n}(1 - \frac{1}{n})^{(t-2\overlap-u-1)_+} \E \|f'_i(\hat x_{u}) - f'_i(x^*)\|^2 
\nonumber \\ 
&= \sum_{u=1}^{t-1} \frac{1}{n}(1 - \frac{1}{n})^{(t-2\overlap-u -1)_+} \frac{1}{n}\sum_{i=1}^n \E \|f'_i(\hat x_{u}) - f'_i(x^*)\|^2
\nonumber \\ 
&= \sum_{u=1}^{t-1} \frac{1}{n}(1 - \frac{1}{n})^{(t-2\overlap-u -1)_+} \E \Big(\Econd \|f'_{i_u}(\hat x_{u}) - f'_{i_u}(x^*)\|^2 \Big)
\tag*{($i_u \perp\!\!\!\perp \hat x_u$)} \nonumber \\ 
&\leq \frac{2\lipschitz}{n} \sum_{u=1}^{t-1} (1 - \frac{1}{n})^{(t-2\overlap-u -1)_+} e_u
\tag*{(by Equation~\eqref{eq:classicsaga2})} \nonumber \\ 
&= \frac{2\lipschitz}{n} \sum_{u=1}^{(t-2\overlap -1)_+} (1 - \frac{1}{n})^{t-2\overlap-u -1} e_u 
	+ \frac{2\lipschitz}{n} \sum_{u=\max(1, t-2\overlap)}^{t-1} e_u  \, .\label{eq:51}
\end{align}

Note that we have excluded $\tilde e_0$ from our formula, using a generic $\lambda$ multiplier. 
We need to treat the case $u=0$ differently to bound $\ind_{\{u_{i,l}^t = u\}}$.
Because all our initial $\alpha_i$ are initialized to a fixed $\alpha_i^0$, $\{u_i^t = 0\}$ just means that $i$ has not been picked between $0$ and $t-\overlap -1$, i.e. $\{i_v \neq i\ \forall\ v\ \text{s.t.}\ 0 \leq v \leq t - \overlap -1\}$. 
This means that the $\ind_{\{i_u=i\}}$ term in~\eqref{eq:indicatrices} disappears and thus we lose a $\frac{1}{n}$ factor compared to the case where $u>1$.

Let us now evaluate $\lambda$.
We have:
\begin{align}\label{eq:52}
\E \Big[\ind_{\{u_i^t = 0\}} \|\alpha_i^0-f'_i(x^*)\|^2 \Big]
&\leq \E \Big[\ind_{\{i_v \neq i\ \forall\ v\ \text{s.t.}\ 0 \leq v \leq t - \overlap -1\}} \|\alpha_i^0-f'_i(x^*)\|^2 \Big]
\nonumber \\ 
&\leq P\{i_v \neq i\ \forall\ v\ \text{s.t.}\ 0 \leq v \leq t - \overlap -1\} \E \|\alpha_i^0-f'_i(x^*)\|^2
\nonumber \\ 
&\leq (1 - \frac{1}{n})^{(t-\overlap)_+} \E \|\alpha_i^0-f'_i(x^*)\|^2 .
\end{align}

Plugging~\eqref{eq:51} and~\eqref{eq:52} into~\eqref{eq:classicsaga}, we get Lemma~\ref{lma:suboptgt}:
\begin{align}\label{eq:53}
\E\|g_t\|^2
\leq 4\lipschitz e_t 
	+ \frac{4\lipschitz}{n} \sum_{u=1}^{t-1} (1 - \frac{1}{n})^{(t-2\overlap-u -1)_+} e_u 
	+ 4\lipschitz (1 - \frac{1}{n})^{(t-\overlap)_+} \tilde e_0 \,  ,
\end{align}
where we have introduced $\tilde e_0 := \frac{1}{2\lipschitz} \E\|\alpha_i^0 - f'_i(x^*)\|^2$.
Note that in the original \SAGA\ algorithm, a batch gradient is computed to set the $\alpha_i^0 = f'_i(x_0)$. 
In this setting, we can write Lemma~\ref{lma:suboptgt} using $\tilde e_0 \leq e_0$ thanks to~\eqref{eq:classicsaga2}.
In the more general setting where we initialize all $\alpha_i^0$ to a fixed quantity, we cannot use~\eqref{eq:classicsaga2} to bound $\E\|\alpha_i^0 - f'_i(x^*)\|^2$ which means that we have to introduce $\tilde e_0$.

\subsection{Master inequality derivation}\label{apxB:master}
Now, if we combine the bound on $\E\|g_{t}\|^2$ which we just derived (i.e. Lemma~\ref{lma:suboptgt}) with Lemma~\ref{lma:1}, we get:
\begin{equation}
\begin{aligned}
a_{t+1} 
\leq &(1 - \frac{\stepsize\strongconvex}{2}) a_t 
	- 2\stepsize e_t
\\
	&+ 4\lipschitz\stepsize^2 C_1e_t 
	+ \frac{4\lipschitz\stepsize^2 C_1}{n} \sum_{u=1}^{t-1} (1 - \frac{1}{n})^{(t-2\overlap-u -1)_+} e_u
	+ 4\lipschitz \stepsize^2 C_1(1 - \frac{1}{n})^{(t-\overlap)_+} \tilde e_0
\\
	&+4\lipschitz\stepsize^2 C_2\sum_{u=(t-\overlap)_+}^{t-1} e_u 
	+4\lipschitz\stepsize^2 C_2 \sum_{u=(t-\overlap)_+}^{t-1} (1 - \frac{1}{n})^{(u - \overlap)_+} \tilde e_0
\\
	&+ \frac{4\lipschitz\stepsize^2 C_2}{n} \sum_{u=(t-\overlap)_+}^{t-1} \sum_{v=1}^{u-1} (1-\frac{1}{n})^{(u - 2\overlap - v -1)_+}e_v  \, .
\end{aligned}
\end{equation}

If we define $H_t := \sum_{u=1}^{t-1} (1 - \frac{1}{n})^{(t-2\overlap-u-1)_+} e_u$, then we get:
\begin{equation}\label{eq:master}
\begin{aligned} 
a_{t+1} 
\leq &(1 - \frac{\stepsize\strongconvex}{2}) a_t 
	- 2\stepsize e_t
\\
	&+ 4\lipschitz\stepsize^2 C_1 \big(e_t  + (1 - \frac{1}{n})^{(t-\overlap)_+} \tilde e_0 \big)
	+ \frac{4\lipschitz\stepsize^2 C_1}{n} H_t
\\
	&+4\lipschitz\stepsize^2 C_2\sum_{u=(t-\overlap)_+}^{t-1} (e_u +  (1 - \frac{1}{n})^{(u - \overlap)_+} \tilde e_0 \big)
	+\frac{4\lipschitz\stepsize^2 C_2}{n} \sum_{u=(t-\overlap)_+}^{t-1} H_u  \, ,
\end{aligned}
\end{equation}
which is the master inequality~\eqref{master}.

\subsection{Lyapunov function and associated recursive inequality}\label{apxB:lyapunov}
We define $\lyapunov_t := \sum_{u=0}^t (1-\contraction)^{t-u}a_u$ for some target contraction rate $\contraction < 1$ to be defined later.
We have:
\begin{align}
\lyapunov_{t+1} 
&= (1 - \contraction)^{t+1}a_0 
	+ \sum_{u=1}^{t+1}(1 - \contraction)^{t+1-u}a_u
= (1 - \contraction)^{t+1}a_0 
	+ \sum_{u=0}^t(1 - \contraction)^{t-u}a_{u+1} \,  .
\end{align}

We now use our new bound on $a_{t+1}$,~\eqref{eq:master}:
\begin{align} \label{eq:rutMaster}
\lyapunov_{t+1} 
&\leq (1 - \contraction)^{t+1}a_0 
	+ \sum_{u = 0}^t(1 - \contraction)^{t-u} \Big[
		(1 - \frac{\stepsize\strongconvex}{2}) a_u 
		- 2\stepsize e_u
		+ 4\lipschitz\stepsize^2 C_1 \big(e_u  + (1 - \frac{1}{n})^{(u-\overlap)_+} \tilde e_0 \big)
\notag \\		
		&\qquad \qquad \qquad \qquad \qquad \qquad \qquad 
		+ \frac{4\lipschitz\stepsize^2 C_1}{n} H_u
		+\frac{4\lipschitz\stepsize^2 C_2}{n} \sum_{v=(u-\overlap)_+}^{u-1} H_v	
\notag \\		
		&\qquad \qquad \qquad \qquad \qquad \qquad \qquad 
		+4\lipschitz\stepsize^2 C_2\sum_{v=(u-\overlap)_+}^{u-1} (e_v +  (1 - \frac{1}{n})^{(v - \overlap)_+} \tilde e_0 \big)
	\Big]
\notag \\
&\leq (1 - \contraction)^{t+1}a_0 
	+ (1-\frac{\stepsize\strongconvex}{2})\lyapunov_t 
\notag \\
	&\qquad \qquad + \sum_{u = 0}^t(1 - \contraction)^{t-u} \Big[
		- 2\stepsize e_u
		+ 4\lipschitz\stepsize^2 C_1 \big(e_u  + (1 - \frac{1}{n})^{(u-\overlap)_+} \tilde e_0 \big)
\notag \\		
		&\qquad \qquad \qquad \qquad \qquad \qquad
		+ \frac{4\lipschitz\stepsize^2 C_1}{n} H_u
		+\frac{4\lipschitz\stepsize^2 C_2}{n} \sum_{v=(u-\overlap)_+}^{u-1} H_v	
\notag \\
		&\qquad \qquad \qquad \qquad \qquad \qquad
		+4\lipschitz\stepsize^2 C_2\sum_{v=(u-\overlap)_+}^{u-1} (e_v +  (1 - \frac{1}{n})^{(v - \overlap)_+} \tilde e_0 \big)
	\Big] .
\end{align}
%SLJ: I would suggest to use multline here instead in the future!

We can now rearrange the sums to expose a simple sum of $e_u$ multiplied by factors $r_u^t$:
\begin{align}\label{apx:Lyapunov}
\lyapunov_{t+1} \leq (1 - \contraction)^{t+1}a_0 + (1-\frac{\stepsize\strongconvex}{2})\lyapunov_t + \sum_{u=1}^t r_u^t e_u + r_0^t \tilde e_0  \, .
\end{align}

\subsection{Proof of Lemma~\ref{lma:3}}\label{apxB:lma3}
We want to make explicit what conditions on $\contraction$ and $\stepsize$ are necessary to ensure that $r_u^t$ is negative for all $u \geq 1$. 
Since each $e_u$ is positive, we will then be able to safely drop the sum term from the inequality. 
The $r_0^t$ term is a bit trickier and is handled separately. 
Indeed, trying to enforce that $r_0^t$ is negative results in a significantly worse condition on $\stepsize$ and eventually a convergence rate smaller by a factor of $n$ than our final result.
Instead, we handle this term directly in the Lyapunov function.

\paragraph{Computation of $r_u^t$.}
Let's now make the multiplying factor explicit. 
We assume $u \geq 1$.

We split $r_u^t$ into five parts coming from~\eqref{eq:rutMaster}: 
\begin{itemize}
\item $r_1$, the part coming from the $-2\stepsize e_u$ terms;
\item $r_2$, coming from $4\lipschitz\stepsize^2 C_1 e_u$;
\item $r_3$, coming from $\frac{4\lipschitz\stepsize^2 C_1}{n} H_u$;
\item $r_4$, coming from $4\lipschitz\stepsize^2 C_2\sum_{v=(u-\overlap)_+}^{u-1} e_v$;
\item $r_5$, coming from $\frac{4\lipschitz\stepsize^2 C_2}{n} \sum_{v=(u-\overlap)_+}^{u-1} H_v$.
\end{itemize}

$r_1$ is easy to derive. Each of these terms appears only in one inequality. 
So for $u$ at time $t$, the term is:
\begin{equation}\label{eq:r1}
r_1 = -2 \stepsize (1- \contraction)^{t-u} .
\end{equation}

For much the same reasons, $r_2$ is also easy to derive and is: 
\begin{equation}\label{eq:r2}
r_2 = 4\lipschitz\stepsize^2 C_1(1- \contraction)^{t-u} .
\end{equation}

$r_3$ is a bit trickier, because for a given $v > 0$ there are several $H_u$ which contain $e_v$. 
The key insight is that we can rewrite our double sum in the following manner:
\begin{align}
\sum_{u=0}^t(1 - \contraction)^{t-u} &\sum_{v=1}^{u-1} (1-\frac{1}{n})^{(u - 2\overlap -v -1)_+} e_v 
\nonumber \\
&= \sum_{v=1}^{t-1} e_v \sum_{u=v+1}^{t}(1 - \contraction)^{t-u}  (1-\frac{1}{n})^{(u - 2\overlap -v -1)_+}
\nonumber \\
&\leq \sum_{v=1}^{t-1} e_v \Big[
		\sum_{u=v+1}^{\min(t, v + 2\overlap)}(1 - \contraction)^{t-u} 
		+ \sum_{u=v + 2 \overlap +1}^{t}(1 - \contraction)^{t-u}  (1-\frac{1}{n})^{u - 2\overlap -v -1}
	\Big]
\nonumber \\
&\leq \sum_{v=1}^{t-1} e_v \Big[
		2\overlap (1 - \contraction)^{t-v-2\overlap}
		+ (1 - \contraction)^{t-v-2\overlap-1} \sum_{u=v + 2 \overlap +1}^{t}q^{u - 2\overlap -v -1}
	\Big]
\nonumber \\
&\leq \sum_{v=1}^{t-1} (1 - \contraction)^{t-v} e_v (1 - \contraction)^{-2\overlap -1} \big[
		2\overlap
		+ \frac{1}{1-q}
	\big],
\end{align}
where we have defined: 
\begin{equation} \label{eq:qDefinition}
q :=\frac{1-1/n}{1-\contraction}, \quad \text{with the assumption $\contraction < \frac{1}{n}$}  \, .
\end{equation}
Note that we have bounded the $\min(t, v+2\overlap)$ term by $v+2\overlap$ in the first sub-sum, effectively adding more positive terms. 

This gives us that at time $t$, for $u$: 
\begin{equation}\label{eq:r3}
r_3 \leq \frac{4\lipschitz\stepsize^2 C_1}{n}(1 - \contraction)^{t-u} (1 - \contraction)^{-2\overlap -1} \big[2\overlap + \frac{1}{1-q}\big] .
\end{equation}

For $r_4$ we use the same trick:
\begin{align}
\sum_{u=0}^t (1-\contraction)^{t-u} \sum_{v=(u-\overlap)_+}^{u-1} e_v
&= \sum_{v=0}^{t-1} e_v  \sum_{u=v+1}^{\min(t, v+\overlap)} (1-\contraction)^{t-u}
\nonumber \\
&\leq \sum_{v=0}^{t-1} e_v  \sum_{u=v+1}^{v+\overlap} (1-\contraction)^{t-u}
\leq \sum_{v=0}^{t-1} e_v \overlap (1-\contraction)^{t-v-\overlap} .
\end{align}

This gives us that at time $t$, for $u$: 
\begin{equation}\label{eq:r4}
r_4 \leq 4\lipschitz\stepsize^2 C_2(1 - \contraction)^{t-u} \overlap (1 - \contraction)^{-\overlap}\,  .
\end{equation}

Finally we compute $r_5$ which is the most complicated term. 
Indeed, to find the factor of $e_w$ for a given $w > 0$, one has to compute a triple sum, $\sum_{u = 0}^t(1 - \contraction)^{t-u} \sum_{v=(u-\overlap)_+}^{u-1} H_v$.
We start by computing the factor of $e_w$ in the inner double sum, $\sum_{v=(u-\overlap)_+}^{u-1} H_v$.

\begin{align}
\sum_{v=(u-\overlap)_+}^{u-1} \sum_{w=1}^{v-1}(1-\frac{1}{n})^{(v -2\overlap -w -1)_+} e_w
= \sum_{w=1}^{u-2} e_w \sum_{v=\max(w+1, u-\overlap)}^{u-1} (1-\frac{1}{n})^{(v -2\overlap -w -1)_+}  \, .
\end{align}
Now there are at most $\overlap$ terms for each $e_w$. 
If $w \leq u - 3\overlap -1$, then the exponent is positive in every term and it is always bigger than $u -3\overlap -1 -w$, which means we can bound the sum by $\overlap (1-\frac{1}{n})^{u -3\overlap -1 -w}$. 
Otherwise we can simply bound the sum by $\overlap$. We get:
\begin{align}
\sum_{v=(u-\overlap)_+}^{u-1} H_v
\leq \sum_{w=1}^{u-2} 
	\big[
		\ind_{\{u -3\overlap \leq w \leq u -2\}}\overlap	
		+ \ind_{\{w \leq u -3\overlap -1\}}\overlap (1-\frac{1}{n})^{u -3\overlap -1 -w}
	\big] e_w  \, .
\end{align}

This means that for $w$ at time $t$:
\begin{align}\label{eq:r5}
r_5 
&\leq \frac{4\lipschitz\stepsize^2 C_2}{n} \sum_{u=0}^t (1 -\contraction)^{t-u}
	\big[
		\ind_{\{u -3\overlap \leq w \leq u -2\}}\overlap	
		+ \ind_{\{w \leq u -3\overlap -1\}}\overlap (1-\frac{1}{n})^{u -3\overlap -1 -w}
	\big]
\nonumber \\
&\leq \frac{4\lipschitz\stepsize^2 C_2}{n}
	\Big[
		\sum_{u=w+2}^{\min(t, w +3\overlap)} \overlap (1 -\contraction)^{t-u}
		+ \sum_{u=w +3\overlap +1}^t \overlap (1-\frac{1}{n})^{u -3\overlap -1 -w} (1 -\contraction)^{t-u}
	\Big]
\nonumber \\
&\leq \frac{4\lipschitz\stepsize^2 C_2}{n} \overlap
	\Big[
		(1 -\contraction)^{t-w}(1 -\contraction)^{-3\overlap} 3\overlap
\nonumber \\ &\qquad \qquad \qquad
		+ (1 -\contraction)^{t-w}(1 -\contraction)^{-1 -3\overlap}\sum_{u=w +3\overlap +1}^t (1-\frac{1}{n})^{u -3\overlap -1 -w} (1 -\contraction)^{-u +3\overlap +1 +w}
	\Big]
\nonumber \\
&\leq \frac{4\lipschitz\stepsize^2 C_2}{n} \overlap (1 -\contraction)^{t-w}(1 -\contraction)^{-3\overlap-1}
	\big(
		3\overlap
		+ \frac{1}{1-q}
	\big) \,  .
\end{align}

By combining the five terms together (\eqref{eq:r1},~\eqref{eq:r2},~\eqref{eq:r3},~\eqref{eq:r4} and~\eqref{eq:r5}), we get that $\forall u$ s.t. $1 \leq u \leq t$:
\begin{equation}\label{eq:rut}
\begin{aligned}
r_u^t \leq (1- \contraction)^{t-u} 
	\Big[
		&-2 \stepsize
		+4\lipschitz\stepsize^2 C_1
		+\frac{4\lipschitz\stepsize^2 C_1}{n}
			(1 - \contraction)^{-2\overlap -1} \big(
				2\overlap 
				+ \frac{1}{1-q}
			\big)
\\
		&+4\lipschitz\stepsize^2 C_2 \overlap (1-\contraction)^{-\overlap}
		+\frac{4\lipschitz\stepsize^2 C_2}{n} \overlap (1 -\contraction)^{-3\overlap -1}
			\big(
				3\overlap
				+ \frac{1}{1-q}
		\big)
	\Big] .
\end{aligned}
\end{equation}

\paragraph{Computation of $r_0^t$.}
Recall that we treat the $\tilde e_0$ term separately in Section~\ref{apxB:lma2}.
The initialization of \SAGA\ creates an initial synchronization, which means that the contribution of $\tilde e_0$ in our bound on $\E\|g_t\|^2$~\eqref{eq:53} is roughly $n$ times bigger than the contribution of any $e_u$ for $1 < u < t$.\footnote{This is explained in details right before~\eqref{eq:52}.}
In order to safely handle this term in our Lyapunov inequality, we only need to prove that it is bounded by a reasonable constant.
Here again, we split $r_0^t$ in five contributions coming from~\eqref{eq:rutMaster}:
\begin{itemize}
\item $r_1$, the part coming from the $-2\stepsize e_u$ terms;
\item $r_2$, coming from $4\lipschitz\stepsize^2 C_1 e_u$;
\item $r_3$, coming from $4\lipschitz\stepsize^2 C_1 (1 -\frac{1}{n})^{(u -\overlap)_+} \tilde e_0$;
\item $r_4$, coming from $4\lipschitz\stepsize^2 C_2\sum_{v=(u-\overlap)_+}^{u-1} e_v$;
\item $r_5$, coming from $4\lipschitz\stepsize^2 C_2\sum_{v=(u-\overlap)_+}^{u-1} (1 -\frac{1}{n})^{(v -\overlap)_+} \tilde e_0$.
\end{itemize}
Note that there is no $\tilde e_0$ in $H_t$, which is why we can safely ignore these terms here.

We have $r_1 = -2\stepsize (1 -\contraction)^t$ and $r_2=4\lipschitz\stepsize^2 C_1 (1 -\contraction)^t$.

Let us compute $r_3$.
\begin{align}
\sum_{u=0}^t (1 -\contraction)^{t -u} &(1 -\frac{1}{n})^{(u -\overlap)_+}
\nonumber \\
&= \sum_{u=0}^{\min(t, \overlap)} (1 -\contraction)^{t -u} 
	+ \sum_{u=\overlap +1}^t (1 -\contraction)^{t -u} (1 -\frac{1}{n})^{u -\overlap}	
\nonumber \\
&\leq (\overlap + 1) (1 -\contraction)^{t -\overlap} 
	+ (1 -\contraction)^{t -\overlap} \sum_{u=\overlap +1}^t (1 -\contraction)^{\overlap -u} (1 -\frac{1}{n})^{u -\overlap}	
\nonumber \\
&\leq (1 -\contraction)^t (1 -\contraction)^{-\overlap}
	\big(
		\overlap + 1 + \frac{1}{1 -q}
	\big)  \, .
\end{align}
This gives us:
\begin{align}
r_3 \leq (1 -\contraction)^t 4\lipschitz\stepsize^2 C_1 (1 -\contraction)^{-\overlap} \big(\overlap + 1 + \frac{1}{1 -q} \big)  \, .
\end{align}

We have already computed $r_4$ for $u>0$ and the computation is exactly the same for $u=0$. $r_4 \leq (1 - \contraction)^t 4\lipschitz\stepsize^2 C_2 \overlap(1-\contraction)^{-\overlap}$ .

Finally we compute $r_5$.
\begin{align}
\sum_{u=0}^t (1 -\contraction)^{t -u} &\sum_{v=(u -\overlap)_+}^{u -1} (1 -\frac{1}{n})^{(v -\overlap)_+}
\nonumber \\
&=\sum_{v=1}^{t -1} \sum_{u=v +1}^{\min(t, v +\overlap)} (1 -\contraction)^{t -u} (1 -\frac{1}{n})^{(v -\overlap)_+}
\nonumber \\
&\leq \sum_{v=1}^{\min(t-1, \overlap)} \sum_{u=v +1}^{v +\overlap} (1 -\contraction)^{t -u}
	+ \sum_{v=\overlap + 1}^{t -1} \sum_{u=v +1}^{\min(t, v +\overlap)} (1 -\contraction)^{t -u} (1 -\frac{1}{n})^{v -\overlap}
\nonumber \\
&\leq \overlap^2 (1 -\contraction)^{t -2\overlap}
	+ \sum_{v=\overlap +1}^{t -1} (1 -\frac{1}{n})^{v -\overlap} \overlap (1 -\contraction)^{t -v -\overlap} 
\nonumber \\
&\leq \overlap^2 (1 -\contraction)^{t -2\overlap}
	+ \overlap (1 -\contraction)^t (1 -\contraction)^{-2\overlap}\sum_{v=\overlap +1}^{t -1} (1 -\frac{1}{n})^{v -\overlap} \overlap (1 -\contraction)^{-v +\overlap} 
\nonumber  \\
&\leq (1 -\contraction)^t (1 -\contraction)^{-2\overlap}\big(\overlap^2 + \overlap \frac{1}{1 -q}\big)  \, .
\end{align}
Which means:
\begin{align}
r_5 \leq (1 -\contraction)^t 4\lipschitz\stepsize^2 C_2 (1 -\contraction)^{-2\overlap}\big(\overlap^2 + \overlap \frac{1}{1 -q}\big) .
\end{align}

Putting it all together, we get that: $\forall t \geq 0$
\begin{equation} \label{r0t}
\begin{aligned}
r_0^t \leq (1 -\contraction)^t 
	\Big[    \Big( &-2 \stepsize + 
		4\lipschitz\stepsize^2 C_1+4\lipschitz\stepsize^2 C_2 \overlap(1-\contraction)^{-\overlap} \Big) \frac{e_0}{\tilde e_0} \\
		&+4\lipschitz\stepsize^2 C_1 (1 -\contraction)^{-\overlap} \big(\overlap + 1 + \frac{1}{1 -q} \big) 
		+4\lipschitz\stepsize^2 C_2 \overlap (1 -\contraction)^{-2\overlap}\big(\overlap + \frac{1}{1 -q}\big)
	\Big] .
\end{aligned}
\end{equation}

\paragraph{Sufficient condition for convergence.}
We need all $r_u^t, u \geq 1$ to be negative so we can safely drop them from~\eqref{apx:Lyapunov}. 
Note that for every $u$, this is the same condition. 
We will reduce that condition to a second-order polynomial sign condition.
We also remark that since $\stepsize \geq 0$, we can upper bound our terms in $\stepsize$ and $\stepsize^2$ in this upcoming polynomial, which will give us sufficient conditions for convergence.

Now, as $\stepsize$ is part of $C_2$, we need to expand it once more to find our conditions. 
We have:
\begin{align*}
C_1 &= 1 + \sqrt{\sparsity}\overlap ; \qquad
C_2 = \sqrt{\sparsity} + \stepsize\strongconvex C_1  \, .
\end{align*}

Dividing the bracket in~\eqref{eq:rut} by~$\stepsize$ and rearranging as a second degree polynomial, we get the condition:
\begin{align} 
4\lipschitz &\Bigg(
	C_1 
	+ \frac{C_1}{n}(1-\contraction)^{-2\overlap -1}\Big[2\overlap + \frac{1}{1-q}\Big]
	+ \Big[\sqrt{\sparsity}\overlap(1-\contraction)^{-\overlap} + \frac{\sqrt{\sparsity}\overlap}{n}(1 -\contraction)^{-3\overlap -1}(3\overlap + \frac{1}{1 -q}) \Big]
	\Bigg) \stepsize
\nonumber \\
	&+ 8\strongconvex C_1 \lipschitz \overlap \Big[(1-\contraction)^{-\overlap} + \frac{1}{n}(1 -\contraction)^{-3\overlap -1}(3\overlap + \frac{1}{1 -q})
	\Big] \stepsize^2
	+ 2
\leq 0  \, . \label{eq:ConvergenceCondition}
\end{align}
The discriminant of this polynomial is always positive, so $\stepsize$ needs to be between its two roots. 
The smallest is negative, so the condition is not relevant to our case (where $\stepsize > 0$). 
By solving analytically for the positive root~$\phi$, we get an upper bound condition on~$\stepsize$ that can be used for any overlap~$\overlap$ and guarantee convergence. 
Unfortunately, for large~$\overlap$, the upper bound becomes exponentially small because of the presence of~$\overlap$ in the exponent in~\eqref{eq:ConvergenceCondition}. 
More specifically, by using the bound $1/(1-\contraction) \leq \exp(2\contraction)$\footnote{This bound can be derived from the inequality $(1-x/2) \geq \exp(-x)$ which is valid for $0 \leq x \leq 1.59$.} and thus $(1-\contraction)^{-\overlap} \leq \exp(2 \overlap \contraction)$ in~\eqref{eq:ConvergenceCondition}, we would obtain factors of the form $\exp(\overlap/n)$ in the denominator for the root~$\phi$ (recall that $\contraction < 1/n$). 

Our Lemma~\ref{lma:3} is derived instead under the assumption that $\overlap \leq \mathcal{O}(n)$, with the constants chosen in order to make the condition~\eqref{eq:ConvergenceCondition} more interpretable and to relate our convergence result with the standard SAGA convergence (see Theorem~\ref{th1}). As explained in Appendix~\ref{apxD}, the assumption that $\overlap \leq \mathcal{O}(n)$ appears reasonable in practice. 
First, by using Bernoulli's inequality, we have:
\begin{equation} \label{eq:Bernouilli}
(1 - \contraction)^{k \overlap} \geq 1 - k\overlap \contraction \qquad \textnormal{for integers} \quad k\overlap \geq 0 \, .
\end{equation}

To get manageable constants, we make the following slightly more restrictive assumptions on the target rate~$\contraction$\footnote{Note that we already expected $\contraction < 1/n$.} and overlap~$\overlap$:\footnote{This bound on $\overlap$ is reasonable in practice, see Appendix~\ref{apxD}.}
\begin{align}
\contraction &\leq \frac{1}{4n} \label{eq:assumptionContraction}
\\
\overlap &\leq \frac{n}{10} \, . \label{eq:assumptionOverlap}
\end{align}

We then have:
\begin{align}
\frac{1}{1-q} &\leq \frac{4n}{3} & &
\\
\frac{1}{1-\contraction} &\leq \frac{4}{3} & &
\\
k\overlap\contraction &\leq \frac{3}{40} & &
\text{for $1 \leq k \leq 3$}
\\
(1 -\contraction)^{-k\overlap} &\leq \frac{1}{1-k\overlap \contraction} \leq \frac{40}{37} & &
\text{for $1 \leq k \leq 3$ and by using~\eqref{eq:Bernouilli}}.
\end{align}

We can now upper bound loosely the three terms in brackets appearing in~\eqref{eq:ConvergenceCondition} as follows:
\begin{align}
(1-\contraction)^{-2\overlap -1} \big[2\overlap + \frac{1}{1-q}\big] &\leq 3n \label{eq:Simp1}
\\
\sqrt{\sparsity}\overlap(1-\contraction)^{-\overlap} + \frac{\sqrt{\sparsity}\overlap}{n}(1 -\contraction)^{-3\overlap -1}(3\overlap + \frac{1}{1 -q}) 
\leq 4 \sqrt{\sparsity}\overlap &\leq 4C_1
\\
(1-\contraction)^{-\overlap} + \frac{1}{n}(1 -\contraction)^{-3\overlap -1}(3\overlap + \frac{1}{1 -q}) &\leq 4 \, .
\label{eq:Simp2}
\end{align}

By plugging~\eqref{eq:Simp1}--\eqref{eq:Simp2} into~\eqref{eq:ConvergenceCondition}, we get the simpler sufficient condition on~$\stepsize$:
\begin{align}
-1 + 16\lipschitz C_1 \stepsize + 16\lipschitz C_1\strongconvex\overlap\stepsize^2 \leq 0 \, .
\end{align}

The positive root $\phi$ is:
\begin{align}\label{eq:phi}
\phi = \frac{16\lipschitz C_1(\sqrt{1 + \frac{\strongconvex\overlap}{4\lipschitz C_1}} -1)}{32 \lipschitz C_1\strongconvex\overlap}
= \frac{\sqrt{1 + \frac{\strongconvex\overlap}{4\lipschitz C_1}} -1}{2\strongconvex\overlap}  \, .
\end{align}

We simplify it further by using the inequality:\footnote{This inequality can be derived by using the concavity property $f(y) \leq f(x) + (y-x) f'(x)$ on the differentiable concave function $f(x)=\sqrt{x}$ with $y=1$.}
\begin{equation}\label{eq:concavesqrt}
\sqrt{x} - 1 \geq \frac{x - 1}{2 \sqrt{x}}  \qquad \forall x > 0 \, .
\end{equation}

Using~\eqref{eq:concavesqrt} in~\eqref{eq:phi}, and recalling that $\kappa := \lipschitz / \strongconvex$, we get:
\begin{align}
\phi \geq \frac{1}{16\lipschitz C_1 \sqrt{1 + \frac{\overlap}{4\kappa C_1}}} \, .
\end{align}

Since $\frac{\overlap}{C_1} = \frac{\overlap}{1 + \sqrt{\sparsity}\overlap} \leq \min(\overlap, \frac{1}{\sqrt{\sparsity}})$, we get that a sufficient condition on our stepsize is:
\begin{equation}
\stepsize \leq \frac{1}{16\lipschitz (1 + \sqrt{\sparsity} \overlap) \sqrt{1 + \frac{1}{4\kappa} \min(\overlap, \frac{1}{\sqrt{\sparsity}})}}  \, .
\end{equation}

Subject to our conditions on $\stepsize$, $\contraction$ and $\overlap$, we then have that: $r_u^t \leq 0\ \text{for all}\ u\ \text{s.t.}\ 1 \leq u \leq t$. 
This means we can rewrite~\eqref{apx:Lyapunov} as:
\begin{align}\label{eq:lya2}
\lyapunov_{t+1} \leq (1 - \contraction)^{t+1}a_0 + (1-\frac{\stepsize\strongconvex}{2})\lyapunov_t + r_0^t \tilde e_0  \, .
\end{align}

Now, we could finish the proof from this inequality, but it would only give us a convergence result in terms of $a_t = \E \|x_t - x^*\|^2$. A better result would be in terms of the suboptimality at $\hat x_t$ (because $\hat x_t$ is a real quantity in the algorithm whereas $x_t$ is virtual). Fortunately, to get such a result, we can easily adapt~\eqref{eq:lya2}.

We make $e_t$ appear on the left side of~\eqref{eq:lya2}, by adding $\stepsize$ to $r_t^t$ in~\eqref{apx:Lyapunov}:\footnote{We could use any multiplier from $0$ to $2\stepsize$, but choose $\stepsize$ for simplicity. For this reason and because our analysis of the $r_t^t$ term was loose, we could derive a tighter bound, but it does not change the leading terms.}
\begin{align}\label{eq:newlyapunov}
\stepsize e_t + \lyapunov_{t+1} \leq (1 - \contraction)^{t+1}a_0 + (1-\frac{\stepsize\strongconvex}{2})\lyapunov_t + \sum_{u=1}^{t-1} r_u^t e_u + r_0^t \tilde e_0 + (r_t^t + \stepsize)e_t .
\end{align}

We now require the stronger property that $\stepsize +r_t^t \leq 0$, which translates to replacing $-2\stepsize$ with $-\stepsize$ in~\eqref{eq:rut}:
\begin{equation}
\begin{aligned}
0 \geq 
	\Big[
		&- \stepsize
		+4\lipschitz\stepsize^2 C_1
		+\frac{4\lipschitz\stepsize^2 C_1}{n}
			(1 - \contraction)^{-2\overlap - 1} \big(
				2\overlap
				+\frac{1}{1-q}
			\big) 
\\
		&+4\lipschitz\stepsize^2  C_2 \overlap(1-\contraction)^{-\overlap}
		+\frac{4\lipschitz\stepsize^2  C_2}{n} \overlap (1 -\contraction)^{-3\overlap-1}
			\big(
				3\overlap
				+ \frac{1}{1-q}
		\big)
	\Big] .
\end{aligned}
\end{equation}

We can easily derive a new stronger condition on $\stepsize$ under which we can drop all the $e_u, u > 0$ terms in~\eqref{eq:newlyapunov}:
\begin{align}
\stepsize \leq \stepsize^* = \frac{1}{32\lipschitz (1 + \sqrt{\sparsity} \overlap) \sqrt{1 + \frac{1}{8\kappa} \min(\overlap, \frac{1}{\sqrt{\sparsity}})}},
\end{align}
and thus under which we get:
\begin{align}\label{eq:lyapu3}
\stepsize  e_t + \lyapunov_{t+1} \leq (1 - \contraction)^{t+1}a_0 + (1-\frac{\stepsize\strongconvex}{2})\lyapunov_t + r_0^t \tilde e_0 .
\end{align}

This finishes the proof of Lemma~\ref{lma:3}. 
\qed

\subsection{Proof of Theorem~\ref{thm:convergence}}\label{apxB:th2}
\paragraph{End of Lyapunov convergence.}
We continue with the assumptions of Lemma~\ref{lma:3} which gave us~\eqref{eq:lyapu3}.
Thanks to~\eqref{r0t}, we can also rewrite $r_0^t \leq (1 -\contraction)^{t+1} A$, where $A$ is a constant which depends on $n$, $\sparsity$, $\stepsize$ and $\lipschitz$ but is finite and crucially does not depend on $t$.
In fact, by reusing similar arguments as in~\ref{apxB:lma3}, we can show the bound $A \leq \stepsize n$ under the assumptions of Lemma~\ref{lma:3} (including $\stepsize \leq \stepsize^*$).\footnote{In particular, note that $e_0$ does not appear in the definition of $A$ because it turns out that the parenthesis group multiplying $e_0$ in~\eqref{r0t} is negative. Indeed, it contains less positive terms than~\eqref{eq:rut} which we showed to be negative under the assumptions from Lemma~\ref{lma:3}.}
We then have:
\begin{align}
\lyapunov_{t+1}  \leq \stepsize e_t + \lyapunov_{t+1} 
&\leq (1-\frac{\stepsize\strongconvex}{2})\lyapunov_t  +(1 - \contraction)^{t+1} (a_0 + A \tilde e_0) 
\nonumber \\
&\leq (1-\frac{\stepsize\strongconvex}{2})^{t+1}\lyapunov_0 + (a_0 + A \tilde e_0) \sum_{k=0}^{t+1} (1 - \contraction)^{t+1 -k} (1-\frac{\stepsize\strongconvex}{2})^k .
\end{align}

We have two linearly contracting terms. 
The sum contracts linearly with the minimum geometric rate factor between $\stepsize \strongconvex / 2$ and $\contraction$. 
If we define $m := \min(\contraction, \stepsize\strongconvex/2)$, $M := \max(\contraction, \stepsize\strongconvex/2)$ and $\contraction^* := \nu m$ with $0 <\nu < 1$,\footnote{$\nu$ is introduced to circumvent the problematic case where $\contraction$ and $\stepsize \strongconvex / 2$ are too close together, which does not prevent the geometric convergence, but makes the constant $\frac{1}{1-\eta}$ potentially very big (in the case both terms are equal, the sum even becomes an annoying linear term in t).} we then get:\footnote{Note that if $m \neq \contraction$, we can perform the index change $t+1-k \rightarrow k$ to get the sum.}
\begin{align}
\stepsize e_t \leq 
\stepsize e_t + \lyapunov_{t+1} 
&\leq (1-\frac{\stepsize\strongconvex}{2})^{t+1}\lyapunov_0 + (a_0 + A \tilde e_0) \sum_{k=0}^{t+1} (1 - m)^{t+1-k} (1-M)^k
\nonumber \\
&\leq (1-\frac{\stepsize\strongconvex}{2})^{t+1}\lyapunov_0 + (a_0 + A \tilde e_0) \sum_{k=0}^{t+1} (1 - \contraction^*)^{t+1-k} (1-M)^k
\nonumber \\
&\leq (1-\frac{\stepsize\strongconvex}{2})^{t+1}\lyapunov_0 + (a_0 + A \tilde e_0) (1 - \contraction^*)^{t+1} \sum_{k=0}^{t+1} (1 - \contraction^*)^{-k} (1-M)^k
\nonumber \\
&\leq (1-\frac{\stepsize\strongconvex}{2})^{t+1}\lyapunov_0 + (1 - \contraction^*)^{t+1} \frac{1}{1 -\eta} (a_0 + A \tilde e_0)
\nonumber \\
&\leq (1 - \contraction^*)^{t+1} \big(a_0 + \frac{1}{1 -\eta} (a_0 + A \tilde e_0) \big) ,
\end{align}
where $\eta := \frac{1-M}{1-\contraction^*}$. 
We have $\frac{1}{1 - \eta} = \frac{1 - \contraction^*}{M - \contraction^*}$.

By taking $\nu = \frac{4}{5}$ and setting $\contraction = \frac{1}{4n}$ -- its maximal value allowed by the assumptions of Lemma~\ref{lma:3} -- we get $M \geq \frac{1}{4n}$ and $\contraction^* \leq \frac{1}{5n}$, which means $\frac{1}{1 - \eta} \leq 20n$.

All told, using $A \leq \stepsize n$, we get:
\begin{equation}
e_t \leq (1 - \contraction^*)^{t+1} \tilde C_0,
\end{equation}
where
\begin{equation}
\tilde C_0 := \frac{21n}{\stepsize}\Big(\|x_0 - x^*\|^2 + \stepsize \frac{n}{2L}\E\|\alpha_i^0 - f'_i(x^*)\|^2 \Big) \, .
\end{equation}
%SLJ: note that I removed the f(x_0)-f(x_0) as I did not like that it was multiplied by n -- this term did not appear in our statement of the SAGA convergence result, so comparison could have become (somewhat) problematic if, say, E||alpha_i^0 - f_i'(x*)||^2 is much much smaller than e_0 (it could happen!).

Since we set $\contraction = \frac{1}{4n}, \nu = \frac{4}{5}$, we have $\nu \contraction = \frac{1}{5n}$.
Using a stepsize $\stepsize = \frac{a}{\lipschitz}$ as in Theorem~\ref{thm:convergence}, we get $\nu \frac{\stepsize \strongconvex}{2} = \frac{2a}{5 \kappa}$.
We thus obtain a geometric rate of $\contraction^* = \min \{\frac{1}{5n}, a\frac{2}{5\kappa}\}$, which we simplified to $\frac{1}{5} \min \{\frac{1}{n}, a\frac{1}{\kappa}\}$ in Theorem~\ref{thm:convergence}, finishing the proof. We also observe that $\tilde C_0 \leq \frac{60n}{\stepsize} C_0$, with $C_0$ defined in Theorem~\ref{th1}.
\qed

\subsection{Proof of Corollary~\ref{thm:bigdata} (speedup regimes)}
Referring to~\citet{qsaga} and our own Theorem~\ref{th1}, the geometric rate factor of \SAGA\ is $\frac{1}{5}\min\{\frac{1}{n}, \frac{a}{\kappa}\}$ for a stepsize of $\stepsize = \frac{a}{5L}$. We start by proving the first part of the corollary which considers the step size $\stepsize = \frac{a}{L}$ with $a = a^*(\overlap)$.
We distinguish between two regimes to study the parallel speedup our algorithm obtains and to derive a condition on $\overlap$ for which we have a linear speedup.

\paragraph{Big Data.} 
In this regime, $n > \kappa$ and the geometric rate factor of sequential \SAGA\ is $\frac{1}{5n}$. 
To get a linear speedup (up to a constant factor), we need to enforce $\contraction^* = \Omega(\frac{1}{n})$.
We recall that $\contraction^* = \min \{\frac{1}{5n}, a\frac{1}{5\kappa}\}$.

We already have $\frac{1}{5n} = \Omega(\frac{1}{n})$. This means that we need $\overlap$ to verify $\frac{a^*(\overlap)}{5\kappa} = \Omega(\frac{1}{n})$, where $a^*(\overlap) = \frac{1}{32 \left(1+ \overlap  \sqrt \sparsity \right) \xi(\kappa, \sparsity, \overlap)}$ according to Theorem~\ref{thm:convergence}.
Recall that $\xi(\kappa, \sparsity, \overlap) := \sqrt{1 + \frac{1}{8 \kappa}  \min\{\frac{1}{\sqrt{\sparsity}}, \overlap\} }$.
Up to a constant factor, this means we can give the following sufficient condition:

\begin{equation}
\frac{1}{\kappa \left(1+ \overlap  \sqrt \sparsity \right) \xi(\kappa, \sparsity, \overlap)}
= \Omega \Big(\frac{1}{n}\Big)
\end{equation}
i.e. 
\begin{equation} \label{eq:SufficientCondition}
\left(1+ \overlap  \sqrt \sparsity \right) \xi(\kappa, \sparsity, \overlap)
= \mathcal{O} \Big( \frac{n}{\kappa} \Big) \, .
\end{equation}

We now consider two alternatives, depending on whether $\kappa$ is bigger than $\frac{1}{\sqrt{\sparsity}}$ or not. If $\kappa \geq \frac{1}{\sqrt{\sparsity}}$, then $\xi(\kappa, \sparsity, \overlap) < 2$ and we can rewrite the sufficient condition~\eqref{eq:SufficientCondition} as:

\begin{align}
\overlap = \mathcal{O}(1) \frac{n}{\kappa\sqrt{\sparsity}}.
\end{align}

In the alternative case, $\kappa \leq \frac{1}{\sqrt{\sparsity}}$. 
Since $a^*(\overlap)$ is decreasing in $\overlap$, we can suppose $\overlap \geq \frac{1}{\sqrt{\sparsity}}$ without loss of generality and thus $\xi(\kappa, \sparsity, \overlap) = \sqrt{1 + \frac{1}{8 \kappa \sqrt{\sparsity}}}$.
We can then rewrite the sufficient condition~\eqref{eq:SufficientCondition} as:
\begin{align}\label{eq:sufcondcor3}
\frac{\overlap \sqrt{\sparsity}}{\sqrt{\kappa}\sqrt[4]{\sparsity}} &= \mathcal{O}(\frac{n}{\kappa})
\nonumber \\
\overlap &= \mathcal{O}(1)\frac{n}{\sqrt{\kappa}\sqrt[4]{\sparsity}} \, .
\end{align}

We observe that since we have supposed that $\kappa \leq \frac{1}{\sqrt{\sparsity}}$, we have $\sqrt{\kappa \sqrt{\sparsity}} \leq \kappa \sqrt{\sparsity} \leq 1$, which means that our initial assumption that $\overlap < \frac{n}{10}$ is stronger than condition~\eqref{eq:sufcondcor3}.

We can now combine both cases to get the following sufficient condition for the geometric rate factor of \ASAGA\ to be the same order as sequential \SAGA\ when $n > \kappa$:
\begin{align}
\overlap = \mathcal{O}(1) \frac{n}{\kappa\sqrt{\sparsity}}; \quad
\overlap = \mathcal{O}(n) \, . 
\end{align}

\paragraph{Ill-conditioned regime.}
In this regime, $\kappa > n$ and the geometric rate factor of sequential \SAGA\ is $a \frac{1}{\kappa}$. 
Here, to obtain a linear speedup, we need $\contraction^* = \mathcal{O}(\frac{1}{\kappa})$.
Since $\frac{1}{n} > \frac{1}{\kappa}$, all we require is that $\frac{a^*(\overlap)}{\kappa} = \Omega(\frac{1}{\kappa})$ where $a^*(\overlap) = \frac{1}{32 \left(1+ \overlap  \sqrt \sparsity \right) \xi(\kappa, \sparsity, \overlap)}$, which reduces to $a^*(\overlap) = \Omega(1)$.

We can give the following sufficient condition:
\begin{align}
\frac{1}{\left(1+ \overlap  \sqrt \sparsity \right) \xi(\kappa, \sparsity, \overlap)} = \Omega(1)
\end{align}

Using that $\frac{1}{n} \leq \sparsity \leq 1$ and that $\kappa > n$, we get that $\xi(\kappa, \sparsity, \overlap) \leq 2$, which means our sufficient condition becomes:

\begin{align}
\overlap \sqrt{\sparsity} &= \mathcal{O}(1)
\nonumber \\
\overlap &= \frac{\mathcal{O}(1)}{\sqrt{\sparsity}} .
\end{align}
This finishes the proof for the first part of Corollary~\ref{thm:illcondition}.	

\paragraph{Universal stepsize.} If $\overlap = \mathcal{O}(\frac{1}{\sqrt{\sparsity}})$, then $\xi(\kappa, \sparsity, \overlap) = \mathcal{O}(1)$ and $(1+\overlap \sqrt{\sparsity}) = \mathcal{O}(1)$, and thus $a^*(\overlap) = \Omega(1)$ (for any $n$ and $\kappa$). This means that the universal stepsize $\stepsize = \Theta(1/L)$ satisfies $\stepsize \leq a^*(\overlap)$ for any $\kappa$, giving the same rate factor $\Omega( \min\{\frac{1}{n}, \frac{1}{\kappa}\})$ that sequential \SAGA\ has, completing the proof for the second part of Corollary~\ref{thm:illcondition}. 
\qed

\section{Additional experimental results}\label{apx:AER}
\subsection{Effect of sparsity}
Sparsity plays an important role in our theoretical results, where we find that while it is necessary in the ``ill-conditioned'' regime to get linear speedups, it is not in the ``well-conditioned'' regime.
We confront this to real-life experiments by comparing the convergence and speedup performance of our three asynchronous algorithms on the Covtype dataset, which is fully dense after standardization.
The results appear in Figure~\ref{fig:covtype}.

While we still see a significant improvement in speed when increasing the number of cores, this improvement is smaller than the one we observe for sparser datasets.
The speedups we observe are consequently smaller, and taper off earlier than on our other datasets.
However, since the observed ``theoretical'' speedup is linear (see Section~\ref{apx:speedup}), we can attribute this worse performance to higher hardware overhead.
This is expected because each update is fully dense and thus the shared parameters are much more heavily contended for than in our sparse datasets.

\begin{figure}
\center \includegraphics[width=0.7\linewidth]{figures/figure_covtype.pdf}
\caption{Comparison on the Covtype dataset. Left: suboptimality. Right: speedup. The number of cores in the legend only refers to the left plot. }\label{fig:covtype}
\end{figure}

One thing we notice when computing the $\sparsity$ variable for our datasets is that it often fails to capture the full sparsity distribution, being essentially a maximum.
This means that $\sparsity$ can be quite big even for very sparse datasets.
Deriving a less coarse bound remains an open problem.

\subsection{Theoretical speedups}\label{apx:speedup}
In the main text of this paper, we show experimental speedup results where suboptimality is a function of the running time.
This measure encompasses both theoretical algorithmic properties and hardware overheads (such as contention of shared memory) which are not taken into account in our analysis.

In order to isolate these two effects, we plot our convergence experiments where suboptimality is a function of the number of iterations; thus, we abstract away any potential hardware overhead.\footnote{To do so, we implement a global counter which is sparsely updated (every $100$ iterations for example) in order not to modify the asynchrony of the system. 
This counter is used only for plotting purposes and is not needed otherwise.
}
The experimental results can be seen in Figure~\ref{fig:theoretical_speedups}. 

\begin{figure*}
\includegraphics[width=\linewidth]{figures/figure_2_iterations.pdf}
\caption{{\bf Theoretical speedups}. 
Suboptimality with respect to number of iterations for \ASAGA\, \SVRG\ and \Hogwild\ with 1 and 10 cores.  
Curves almost coincide, which means the theoretical speedup is almost the number of cores $p$, hence linear.}\label{fig:theoretical_speedups}
\end{figure*} 

For all three algorithms and all three datasets, the curves for $1$ and $10$ cores almost coincide, which means that we are indeed in the ``theoretical linear speedup'' regime.
Indeed, when we plotted the amount of iterations required to converge to a given accuracy as a function of the number of cores, we obtained straight  horizontal lines for our three algorithms.
 
The fact that the speedups we observe in running time are less than linear can thus be attributed to various hardware overheads, including shared variable contention -- the compare-and-swap operations are more and more expensive as the number of competing requests increases -- and cache effects as mentioned in Section~\ref{ssec:results}.

\section{A closer look at the $\overlap$ constant} \label{apxD}
\subsection{Theory}
In the parallel optimization literature, $\overlap$ is often referred to as a proxy for the number of cores.
However, intuitively as well as in practice, it appears that there are a number of other factors that can influence this quantity.
We will now attempt to give a few qualitative arguments as to what these other factors might be and how they relate to $\overlap$.

\paragraph{Number of cores.}
The first of these factors is indeed the number of cores. 

If we have $p$ cores, $\overlap \geq p - 1$.
Indeed, in the best-case scenario where all cores have exactly the same execution speed for a single iteration, $\overlap = p -1$.

To get more insight into what $\overlap$ really encompasses, let us now try to define the worst-case scenario in the preceding example.
Consider $2$ cores. 
In the worst case scenario, one core runs while the other is stuck. 
Then the overlap is $t$ for all $t$ and eventually grows to $+\infty$. 
If we assume that one core runs twice as fast as the other, then $\overlap = 2$. 
If both run at the same speed, $\overlap = 1$.

It appears then that a relevant quantity is $R$, the ratio between the fastest execution time to the slowest execution time for a single iteration. 
$\overlap \leq (p-1) R$, which can be arbitrarily bigger than $p$.

\paragraph{Length of an iteration.} 
There are several factors at play in $R$ itself. 
\begin{itemize}
\item The first is the speed of execution of the cores themselves (i.e. clock time). 
The dependency here is quite clear.

\item The second is the data matrix itself. If one $f_i$ has support of size $n$ while all the others have support of size $1$, $r$ may eventually become very big.

\item The third is the length of the computation itself. The longer our algorithm runs, the more likely it is to explore the potential corner cases of the data matrix.
\end{itemize}

The overlap is upper bounded by the number of cores times the maximum iteration time over the minimum iteration time (which is linked to the sparsity distribution of the data matrix).
This is an upper bound, which means that in some cases it will not really be useful. 
For example, in the case where one factor has support size $1$ and all others have support size $d$, the probability of the event which corresponds to the upper bound is exponentially small in $d$. 
We conjecture that a more useful indicator could be the maximum iteration time over the expected iteration time.

To sum up this preliminary theoretical exploration, the $\overlap$ term encompasses a lot more complexity than is usually implied in the literature. 
This is reflected in the experiments we ran, where the constant was orders of magnitude bigger than the number of cores.

\subsection{Experimental results}
In order to verify our intuition about the $\overlap$ variable, we ran several experiments on all three datasets, whose characteristics are reminded in Table~\ref{table:1}.
$\delta_l^i$ is the support size of $f_i$.

\begin{table}[ht]
\caption{Density measures including minimum, average and maximum support size $\delta_l^i$ of the factors.}
\centering
\label{dataset-table2}
\begin{tabular}{lccccccc} 
\toprule
{} & $n$ & $d$ & density & $\max(\delta_l^i)$ & $\min(\delta_l^i)$ & $\bar \delta_l$ & $\max(\delta_l^i) / \bar \delta_l$\\
\midrule
{\bf RCV1} & \hfill 697,641 & \hfill 47,236 & \hfill 0.15\% & \hfill 1,224 & \hfill 4 & \hfill 73.2 & \hfill 16.7\\ 
{\bf URL} & \hfill 2,396,130 & \hfill 3,231,961 & \hfill 0.003\% & \hfill 414 & \hfill 16 & \hfill 115.6 & \hfill 3.58 \\
{\bf Covtype} & \hfill 581,012 & \hfill 54 & \hfill 100\% & \hfill 12 & \hfill 8 & \hfill 11.88 & \hfill 1.01 \\
\bottomrule
\end{tabular}
\label{table:1}
\end{table}

To estimate $\overlap$, we compute the average overlap over $100$ iterations, which is a lower bound on the actual overlap (which is a maximum, not an average). 
We then take the maximum observed quantity.
We use an average because computing the overlap requires using a global counter, which we do not want to update every iteration since it would make it a heavily contentious quantity susceptible of artificially changing the asynchrony of our algorithm.

The results we observe are order of magnitude bigger than $p$, indicating that $\overlap$ can indeed not be dismissed as a mere proxy for the number of cores, but has to be more carefully analyzed.

First, we plot the maximum observed $\overlap$ as a function of the number of cores (see Figure~\ref{fig:overlap}).
We observe that the relationship does indeed seem to be roughly linear with respect to the number of cores until 30 cores.
After 30 cores, we observe what may be a phase transition where the slope increases significantly.

\begin{figure*}
\includegraphics[width=\linewidth]{figures/overlap.pdf}
\caption{{\bf Overlap}. 
Overlap as a function of the number of cores for both \ASAGA\ and \Hogwild\ on all three datasets.}\label{fig:overlap}
\end{figure*} 

Second, we measured the maximum observed $\overlap$ as a function of the number of epochs.
We omit the figure since we did not observe any dependency; that is, $\overlap$ does not seem to depend on the number of epochs.
We know that it must depend on the number of iterations (since it cannot be bigger, and is an increasing function with respect to that number for example), but it appears that a stable value is reached quite quickly (before one full epoch is done).

If we allowed the computations to run forever, we would eventually observe an event such that $\overlap$ would reach the upper bound mentioned in the last section, so it may be that $\overlap$ is actually a very slowly increasing function of the number of iterations.

\section{Lagged updates and Sparsity}\label{apxC}

\subsection{Comparison with Lagged Updates in the sequential case}
The lagged updates technique in \SAGA\ is based on the observation that the updates for component $[x]_v$ need not be applied until this coefficient needs to be accessed, that is, until the next iteration $t$ such that $ v \in S_{i_t}$. 
We refer the reader to~\citet{laggedsaga} for more details.

Interestingly, the expected number of iterations between two steps where a given dimension $v$ is involved in the partial gradient is $p_v^{-1}$, where $p_v$ is the probability that $v$ is involved in a given step. 
$p_v^{-1}$ is precisely the term which we use to multiply the update to $[x]_v$ in Sparse \SAGA. 
Therefore one may see the updates in Sparse \SAGA\ as \textit{anticipated} updates, whereas those in the~\citet{laggedsaga} implementation are \textit{lagged}. 
The two algorithms appear to be very close, even though Sparse \SAGA\ uses an expectation to multiply a given update whereas the lazy implementation uses a random variable (with the same expectation). Sparse \SAGA\ therefore uses a slightly more aggressive strategy, which gave faster run-time in our experiments below.

Although Sparse \SAGA\ requires the computation of the $p_v$ probabilities, this can be done during a first pass throughout the data (during which constant step size \SGD\ may be used) at a negligible cost. 

\begin{figure*}
\includegraphics[width=\linewidth]{figures/fig1_all.pdf}
\includegraphics[width=\linewidth]{figures/fig1_all_passes.pdf}
\caption{{\bf Lagged vs sparse \SAGA\ updates}. 
Suboptimality with respect to time for different \SAGA\ update schemes on various datasets. First row: suboptimality as a function of time. Second row: suboptimality as a the number of passes over the dataset.
For sparse datasets (RCV1 and Real-sim), lagged and sparse updates have a lower cost per iteration which result in faster convergence.}\label{fig:fig_1}
\end{figure*} 

In our experiments, we compare the Sparse \SAGA\ variant proposed in Section~\ref{scs:sparse_saga} to two other approaches: the naive (i.e. dense) update scheme and the lagged updates implementation described in~\citet{SAGA}.
Note that we use different datasets from the parallel experiments, including a subset of the RCV1 dataset and the realsim dataset. 
Figure~\ref{fig:fig_1} reveals that sparse and lagged updates have a lower cost per iteration, resulting in faster convergence for sparse datasets.
Furthermore, while the two approaches had similar convergence in terms of number of iterations, the Sparse \SAGA\ scheme is slightly faster in terms of runtime (and as previously pointed out, sparse updates are better adapted for the asynchronous setting).
For the dense dataset (Covtype), the three approaches exhibit a similar performance. 

\subsection{On the difficulty of parallel lagged updates} \label{apx:DifficultyLagged}

In the implementation presented in~\citet{laggedsaga}, the dense part ($\bar \alpha$) of the updates is deferred. 
Instead of writing dense updates, counters $c_d$ are kept for each coordinate of the parameter vector -- which represent the last time these variables were updated -- as well as the average gradient $\bar \alpha$ for each coordinate. 
Then, whenever a component $[\hat x]_d$ is needed (in order to compute a new gradient), we subtract $\stepsize (t-c_d) [\bar \alpha]_d$ from it and $c_d$ is set to $t$.
The reason we can do this without modifying the algorithm is that $[\bar \alpha]_d$ only changes when $[\hat x]_d$ also does.

In the sequential setting, this is strictly the same as doing the updates in a dense way, since the coordinates are only stale when they're not used. 
Note that at the end of an execution all counters have to be subtracted at once to get the true final parameter vector (and to bring every $c_d$ counter to the final $t$).

In the parallel setting, several issues arise:
\begin{itemize}
\item two cores might be attempting to correct the lag at the same time. 
In which case since updates are done as additions and not replacements (which is necessary to ensure that there are no overwrites), the lag might be corrected multiple times, i.e. overly corrected. 
\item we would have to read and write atomically to each $[\hat x_d], c_d, [\bar \alpha]_d$ triplet, which is highly impractical.
\item we would need to have an explicit global counter, which we do not in \ASAGA\ (our global counter~$t$ being used solely for the proof).
\item in the dense setting, updates happen coordinate by coordinate. 
So at time $t$ the number of $\bar \alpha$ updates a coordinate has received from a fixed past time $c_d$ is a random variable, which may differs from coordinate to coordinate. 
Whereas in the lagged implementation, the multiplier is always $(t-c_d)$ which is a constant (conditional to $c_d$), which means a potentially different $\hat x_t$.
\end{itemize}

All these points mean both that the implementation of such a scheme in the parallel setting would be impractical, and that it would actually yields a different algorithm than the dense version, which would be even harder to analyze.


\section{Additional empirical details} \label{apxE}
\subsection{Detailed description of datasets}
We run our experiments on four datasets. In every case, we run logistic regression for the purpose of binary classification.

\paragraph{RCV1 ($n=697,641$, $d=47,236$).}
The first is the Reuters Corpus Volume I (RCV1) dataset~\citep{RCV1}, an archive of over 800,000 manually categorized newswire stories made available by Reuters, Ltd. for research purposes. 
The associated task is a binary text categorization.

\paragraph{URL ($n=2,396,130$, $d=3,231,961$).}
Our second dataset was first introduced in~\citet{URL}. Its associated task is a binary malicious url detection.
This dataset contains more than 2 million URLs obtained at random from Yahoo's directory listing (for the ``benign'' URLs) and from a large Web mail provider (for the ``malicious'' URLs).
The benign to malicious ratio is 2.
Features include lexical information as well as metadata.
This dataset was obtained from the libsvmtools project.\footnote{\url{http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html}}

\paragraph{Covertype ($n=581,012$, $d=54$).}
On our third dataset, the associated task is a binary classification problem (down from 7 classes originally, following the pre-treatment of~\citet{Covtype}). The features are cartographic variables.
Contrarily to the first two, this is a dense dataset.

\paragraph{Realsim ($n=73,218$, $d=20,958$).}
We only use our fourth dataset for non-parallel experiments and a specific compare-and-swap test.
It constitutes of UseNet articles taken from four discussion groups (simulated auto racing, simulated aviation, real autos, real aviation).


\subsection{Implementation details}
\paragraph{Hardware.} 
All experiments were run on a Dell PowerEdge 920 machine with 4 Intel Xeon E7-4830v2 processors with 10 2.2GHz cores each and 384GB 1600 Mhz RAM.

\paragraph{Software.} \label{scalavsc}
All algorithms were implemented in the Scala language and the software stack consisted of a Linux operating system running Scala 2.11.7 and Java 1.6. 

We chose this expressive, high level language for our experimentation despite its typical 20x slower performance compared to C because our primary concern was that the code may easily be reused and extended for research purposes (which is harder to achieve with low level, heavily optimized C code; especially for error prone parallel computing). 

As a result our timed experiments exhibit sub-optimal running times, e.g. compared to~\citet{KR13}.
This is as we expected.
The observed slowdown is both consistent across datasets (roughly 20x) and with other papers that use Scala code (e.g.~\citet{mania}, ~\citet[Fig. 2]{cocoa}).

Despite this slowdown, our experiments show state-of-the-art results in convergence per number of iterations.
Furthermore, the speed-up patterns that we observe for our implementation of Hogwild and Kromagnon are similar to the ones given in [MN15], Niu et al.[2011] and Reddi et al.[2015] (in various languages).

%The code we used to run all the experiments is available at \url{http://www.di.ens.fr/sierra/research/asaga/}.
The code we used to run all the experiments is available at \url{https://github.com/RemiLeblond/ASAGA}.

\paragraph{Necessity of compare-and-swap operations.}
Interestingly, we have found necessary to use compare-and-swap instructions in the implementation of \ASAGA. 
In Figure~\ref{fig:cas_comparison}, we display suboptimality plots using non-thread safe operations and compare-and-swap (CAS) operations. The non-thread safe version starts faster but then fails to converge beyond a specific level of suboptimality, while the compare-and-swap version does converges linearly up to machine precision.

For \textit{compare-and-swap} instructions we used the \texttt{AtomicDoubleArray} class from the Google library \texttt{Guava}. This class uses an \texttt{AtomicLongArray} under the hood (from package \texttt{java.util.concurrent.atomic} in the standard Java library), which does indeed benefit from lower-level CPU-optimized instructions.

\begin{figure}
\center \includegraphics[width=0.5 \linewidth]{figures/compare_and_swap.pdf}
\caption{{\bf Compare and swap in the implementation of \ASAGA}. 
Suboptimality as a function of time for \ASAGA, both using compare-and-swap (CAS) operations and using standard operations. 
The graph reveals that CAS is indeed needed in a practical implementation to ensure convergence to a high precision.} \label{fig:cas_comparison}
\end{figure}

\paragraph{Efficient storage of the $\alpha_i$.}
Storing $n$ gradient may seem like an expensive proposition, but for linear predictor models, one can actually store a single scalar per gradient (as proposed in~\citet{laggedsaga}), which is what we do in our implementation of \ASAGA.

\paragraph{Step sizes.} For each algorithm, we picked the best step size among 10 equally spaced values in a grid, and made sure that the best step size was never at the boundary of this interval. 
For Covtype and RCV1, we used the interval $[\frac{1}{10L}, \frac{10}{L}]$, whereas for URL we used  the interval $[\frac{1}{L}, \frac{100}{L}]$ as it admitted larger step sizes.
It turns out that the best step size was fairly constant for different number of cores for both \ASAGA\ and \KROMAGNON, and both algorithms had similar best step sizes.

\subsection{Biased update in the implementation}\label{apx:Bias}
In the implementation detailed in Algorithm~\ref{alg:sagasync}, $\bar \alpha$ is maintained in memory instead of being recomputed for every iteration.
This saves both the cost of reading every data point for each iteration and of computing $\bar \alpha$ for each iteration.

However, this removes the unbiasedness guarantee.
The problem here is the definition of the expectation of $\hat \alpha_i$.
Since we are sampling uniformly at random, the average of the $\hat \alpha_i$ is taken at the precise moment when we read the $\alpha_i^t$ components. 
Without synchronization, between two reads to a single coordinate in $\alpha_i$ and in $\bar \alpha$, new updates might arrive in $\bar \alpha$ that are not yet taken into account in $\alpha_i$. 
Conversely, writes to a component of $\alpha_i$ might precede the corresponding write in $\bar \alpha$ and induce another source of bias. 

In order to alleviate this issue, we can use coordinate-level locks on $\alpha_i$ and $\bar \alpha$ to make sure they are always synchronized. 
Such low-level locks are quite inexpensive when $d$ is large, especially when compared to vector-wide locks.

However, as previously noted, experimental results indicate that this fix is not necessary.

\end{document}\vspace{-2mm}
\section{Background}
\vspace{-3mm}

{\bf Markov Random Fields}:
MRFs are undirected probabilistic graphical models 
where the probability distribution factorizes over cliques in the graph.
We consider marginal inference on pairwise MRFs with $N$ random variables $X_1,X_2,\ldots,X_N$
where each variable takes discrete states $x_i\in \VAL_i$. Let $G=(V,E)$ be the Markov 
network with an undirected edge $\edges$ for every two 
variables $X_i$ and $X_j$ that are connected together. Let $\nbrs{i}$ refer to the 
set of neighbors of variable $X_i$. We organize the edge log-potentials $\theta_{ij}(x_i, x_j)$
for all possible values of $x_i \in \VAL_i$, $x_j \in \VAL_j$ in the vector $\btheta_{ij}$,
and similarly for the node log-potential vector $\btheta_i$. We regroup these in the overall
vector~$\vtheta$. We introduce a similar grouping for the marginal vector $\vmu$:
for example, $\mu_i(x_i)$ gives the coordinate of the marginal vector corresponding
to the assignment $x_i$ to variable $X_i$.

{\bf Tree Re-weighted Objective} \citep{wainwright2005new}:
%
%
Let $Z(\vtheta)$ be the partition function for the MRF and $\MARG$ be the set of all valid marginal vectors (the marginal polytope). The maximization of the TRW objective gives the following
upper bound on the log partition function:
\begingroup 
\vspace{-1mm}
\setlength\belowdisplayskip{-12pt} %
\begin{equation}
\begin{split}
\label{eqn:upperBoundPartition}
\log Z(\vtheta) &\leq \min_{\vrho \in \TREEPOL} \max_{\vmu \in \MARG}\quad \underbrace{\brangle{\vtheta,\vmu} + \trwent}_{\TRW},
\end{split}
\end{equation}
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
where the TRW entropy is:
\endgroup
\begin{equation}
	\label{eqn:trw_entropy_like_bethe}
	\trwent := \sum_{i\in V}(1-\hspace{-3mm}\sum_{j\in \nbrs{i}}\hspace{-2mm}\rho_{ij})H(\bmu_i) 
	+\hspace{-2mm} \sum_{(ij)\in
          E}\hspace{-1mm}\rho_{ij}H(\bmu_{ij}), \quad H(\bmu_i) :=   -\sum_{x_i} \mu_i(x_i)\log\mu_i(x_i).
\end{equation}
%
$\TREEPOL$ is the spanning tree polytope, the convex hull
of edge indicator vectors of all possible spanning trees of the graph. 
Elements of $\vrho\in \TREEPOL$ specify the probability of an
edge being present under a specific distribution over spanning trees.
$\MARG$ is difficult to optimize over, and most
TRW algorithms optimize over a relaxation called
the local consistency polytope $\LOCAL \supseteq \MARG$:
\vspace{-1mm}
\begin{equation*}
%
%
\resizebox{\hsize}{!}{$\LOCAL:=\left\{\vmu\geq \bm{0},\;\sum_{x_i} \mu_i(x_i) = 1 \;\forall i\in V,\;\sum_{x_i} \mu_{ij}(x_i,x_j) = \mu_j(x_j), \sum_{x_j} \mu_{ij}(x_i,x_j) = \mu_i(x_i)\;\;\forall \edges \right\}.$}
%
%
%
%
%
%
\end{equation*}
\vspace{-3mm}

%
%
%
%
%
%

%
The TRW objective $\TRW$ is a globally concave function of $\vmu$ over $\LOCAL$,
assuming that $\vrho$ is obtained from a valid distribution over spanning
trees of the graph (i.e. $\vrho \in \TREEPOL$). 
%

{\bf Frank-Wolfe (FW) Algorithm:} 
In recent years, the Frank-Wolfe (aka conditional gradient) algorithm has gained 
popularity in machine learning \citep{jaggi2013revisiting} for the 
optimization of convex functions over compact domains (denoted $\DOM$). The algorithm is used to 
solve 
$\min_{\x\in \DOM} f(\x)$	
by iteratively finding a good descent vertex by solving the linear subproblem:
%
\begin{equation}
\label{eqn:lin_subproblem}
\sk = \argmin_{\s\in\mathcal{D}}\brangle{\nabla f(\xk), \s} \qquad \text{(FW oracle)},
\end{equation}
%
and then taking a convex step towards this vertex: $\x^{(k+1)} = (1-\stepsize) \x^{(k)} + \stepsize \sk$ for
a suitably chosen step-size $\stepsize \in [0,1]$.
The algorithm remains within the feasible set (is projection free), is invariant to affine transformations of the domain, and can be implemented in a memory efficient manner. Moreover, the FW gap $g(\x^{(k)}) := \brangle{-\nabla f(\xk), \sk - \x^{(k)}}$ provides an upper bound on the suboptimality of the iterate $\x^{(k)}$.
%
%
%
The primal convergence of the Frank-Wolfe algorithm is given by 
Thm.~$1$ in \citet{jaggi2013revisiting}, restated here for convenience: for $k\geq1$, the iterates $\xk$ satisfy:
\vspace{-3mm}
\begin{dmath}
	\label{eqn:fw_theorem_convergence_original}
f(\xk) - f(\xopt) \leq \frac{2\Cf}{k+2} ,
\end{dmath}
\vspace{-3mm}
where $\Cf$ is called the ``curvature constant''. Under the assumption 
that $\nabla f$ is $L$-Lipschitz continuous\footnote{I.e. $\|\nabla{f}(\x)-\nabla{f}(\x')\|_* \leq L \|\x-\x'\|$ for $\x, \x' \in \DOM$. Notice that the dual norm $\| \cdot \|_*$ is needed here.} on $\DOM$, we can bound it as
$\Cf \leq L \diam_{||.||}(\mathcal{D})^2$.
%
%
%
%
%
%
%
%
%

\textbf{Marginal Inference with Frank-Wolfe:}
To optimize $\max_{\vmu\in\MARG} \TRW$ with Frank-Wolfe,
the linear subproblem~\eqref{eqn:lin_subproblem} becomes $\argmax_{\vmu\in\mathcal{M}}\brangle{\tilde{\btheta},\vmu}$, where the perturbed potentials $\tilde{\btheta}$ 
correspond to the gradient of $\TRW$ with respect to $\vmu$. %
Elements of $\tilde{\btheta}$ are of the 
form $\theta_c(x_c) + K_c (1+\log\mu_c(x_c))$, evaluated at the pseudomarginals'
current location in $\mathcal{M}$, where 
$K_c$ is the coefficient of the entropy for the node/edge
term in~\eqref{eqn:trw_entropy_like_bethe}.
The FW linear subproblem here is thus equivalent to performing MAP inference in a graphical model with
potentials $\tilde{\btheta}$ \citep{belangerWorkshop2013}, as
the vertices of the marginal polytope are in 1-1 
correspondence with valid joint assignments to the random variables of the MRF,
and the solution of a linear program is always achieved at a
vertex of the polytope. 
%
%
The TRW objective does not have a Lipschitz continuous gradient over $\MARG$, and so standard convergence proofs for Frank-Wolfe do not hold.
%
\documentclass[twoside]{article}
\usepackage{proceed2e}

\usepackage[latin2]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage[
    pdfstartview={FitH},
    bookmarks=true,
    bookmarksnumbered=true,
    bookmarksopen=true,
    bookmarksopenlevel=\maxdimen,
    pdfborder={0 0 0},
    colorlinks=true,
    linkcolor = black,
    citecolor=black,
    urlcolor=black,
    filecolor=black,
    pdfauthor={Ferenc Huszar and Simon Lacoste-Julien},
    pdftitle={A Kernel Approach to Tractable Bayesian Nonparametrics},
    pdfdisplaydoctitle=true
]{hyperref}
\usepackage[pdftex]{graphicx}
\usepackage[numbers,square]{natbib}
\usepackage{tikz}
\usetikzlibrary{arrows,shapes}

% % % % % %
% Definition of macros
%
\newcommand{\ourmethod}{kst}
\newcommand{\figref}[1]{Fig.~\ref{#1}}
\newcommand{\subfigref}[2]{Fig.~\ref{#1}.#2}
\newcommand{\studentt}{student-$\mathcal{T}$\ }
\newcommand{\tr}{\mbox{\,tr\,}}
\newcommand{\trp}[1]{\mbox{\,tr\,}\left(#1\right)}
\newcommand{\transpose}[1]{#1^{\mbox{\sf \scriptsize T}}}
\newcommand{\quadform}[2]{\transpose{#1}#2#1}
\newcommand{\quadformp}[2]{\transpose{\left(#1\right)}#2\left(#1\right)}
\newcommand{\varconc}{\alpha}
\newcommand{\meanconc}{\beta}
\newcommand{\ie}{i.\,e.\ }
\newcommand{\eg}{e.\,g.\ }
\newcommand{\fix}[1]{$^\text{\textcolor{red}{?}}$\marginpar{\parbox[m]{3cm}{\tiny{#1}}}}

% % % % % %
% Title
%
\title{A Kernel Approach to Tractable Bayesian Nonparametrics}

\author{Ferenc Husz\'{a}r\\University of Cambridge \And Simon Lacoste-Julien\\University of Cambridge}

\begin{document}

\maketitle

\begin{abstract}
Inference in popular nonparametric Bayesian models typically relies on sampling or other approximations. This paper presents a general methodology for constructing novel tractable nonparametric Bayesian methods by applying the kernel trick to inference in a parametric Bayesian model. For example, Gaussian process regression can be derived this way from Bayesian linear regression. Despite the success of the Gaussian process framework, the kernel trick is rarely explicitly considered in the Bayesian literature. In this paper, we aim to fill this gap and demonstrate the potential of applying the kernel trick to tractable Bayesian parametric models in a wider context than just regression. As an example, we present an intuitive Bayesian kernel machine for density estimation that is obtained by applying the kernel trick to a Gaussian generative model in feature space.
\end{abstract}

\section{Introduction}

The popularity of nonparametric Bayesian methods has steadily risen in machine learning over the past decade. Bayesian inference in almost all current nonparametric models relies on approximations which typically involve Markov chain Monte Carlo, or more recently variational approximations~\cite{Blei2004}. There is an ever-growing interest in developing nonparametric Bayesian methods in which inference and prediction can be expressed in closed form and no approximations are needed. Such methods would be quite useful and desired in many applications, but are unfortunately rare at present. Perhaps the only known example is Gaussian process (GP) regression. In GP regression~\cite{Rasmussen2006}, the posterior predictive distribution is a tractable Gaussian with parameters that can be computed from data exactly in polynomial time. The method is widely adopted and also has favourable frequentist asymptotic properties~\cite{vanderVaart2008}. But what is the secret behind the remarkable algorithmic clarity of GP regression? What makes closed-form computations possible? We argue that the key is that GP regression is also a \emph{kernel machine}: the method is arrived at by applying the kernel trick in a parametric Bayesian model, namely linear regression (see \eg Chapter~2 in~\cite{Rasmussen2006})

Kernel methods~\cite{Hofman2008,Scholkopf2002} use a simple trick, widely known as the kernel trick, to overcome the limitations of a linear model: the observations ${\bm{x}_i\in\mathcal{X}}$ are first embedded in a feature space $\mathcal{F}$ using a nonlinear mapping ${\varphi:\mathcal{X} \mapsto \mathcal{F}}$. A linear algorithm is then applied on the embedded representations $\bm{\phi}_n = \varphi(\bm{x}_n)$ instead of the observations themselves. If the algorithm only makes use of scalar products $\langle\bm{\phi}_n,\bm{\phi}_m\rangle$, then by replacing all scalar products by tractable \emph{kernel} evaluations ${k(\bm{x}_n,\bm{x}_m) := \langle\bm{\phi}_n,\bm{\phi}_m\rangle}$, the expressive power can be substantially increased with only a minor increase in computational costs. Notably, the kernel trick allows one to construct nonparametric machine learning methods from parametric ones.

Despite its popularity in ``non-Bayesian'' studies, the kernel trick is rarely considered as a construction tool in Bayesian nonparametrics. GP regression is a rare, if not the only example. GPs are therefore often called \emph{Bayesian kernel machines}. In this paper,  we consider finding new examples of Bayesian kernel machines, \ie broadening the intersection between kernel machines and nonparametric Bayesian methods. For Bayesian nonparametrics, the kernel approach offers invaluable closed-form computations and a rigorous analysis framework associated with reproducing kernel Hilbert spaces (RKHS). For kernel machines, a Bayesian formulation offers benefits, such as novel probabilistic approaches to setting kernel hyperparameters and means of incorporating such methods in hierarchical Bayesian models~\cite{Rasmussen2002,Chu2007,Adams2009}.

In this paper, we present a methodology for finding novel Bayesian kernel machines, following the recipe of GP regression:
\begin{enumerate}
\addtolength{\itemsep}{-2mm}
 \item Start with a simple Bayesian model of observations.
 \item Derive exact Bayesian inference in the model.
 \item Express the posterior predictive distribution in terms of dot products and apply the kernel trick.
\end{enumerate}

The crucial point is finding the basic model in step 1, in which both steps 2 and 3 are possible. Fortunately, this search is guided by intuitive orthonormal invariance considerations that will be demonstrated in this paper. We present an example Bayesian kernel machine for density estimation, based on the linear Gaussian generative model underlying principal component analysis (PCA)~\cite{Roweis1999}. We show that the kernel trick can be applied in the Bayesian method by choosing prior distributions over the parameters which preserve invariance to orthonormal transformations.

The rest of the paper is organised as follows. In Sec.~\ref{sec:genmodel}, we review Bayesian inference in a Gaussian generative model and discuss consequences of applying the kernel trick to the predictive density. We consider the infinite dimensional feature spaces case in Subsec.~\ref{sec:infinite_dimensional}. In Sec.~\ref{sec:experiments}, we present experiments on high dimensional density estimation problems comparing our method to other Bayesian and non-Bayesian nonparametric methods.

\section{A Gaussian model in feature space\label{sec:genmodel}}

Assume that we have observations $\bm{x}_i$ in a $d$-dimensional Euclidean space and that our task is to estimate their density. In anticipation of the sequel, we embed the observations $\bm{x}_i$ into a high-dimensional feature space $\mathcal{F}$ with an injective smooth nonlinear mapping ${\varphi:\mathcal{X} \mapsto \mathcal{F}}$. Our density estimation method is based on a simple generative model on the ambient feature space of the embedded observations $\bm{\phi}_i=\varphi(\bm{x}_i)$. For now, think of $\mathcal{F}$ as a $D$-dimensional Euclidean space and $\bm{\phi}_i$ as \emph{arbitrary elements} of $\mathcal{F}$ (\ie they are not necessary constrained to lie on the \emph{observation manifold} $\mathcal{O} = \left\{\varphi(\bm{x}) : \bm{x}\in\mathcal{X}\right\}$). We suppose that $\bm{\phi}_i$ were sampled from a Gaussian distribution with unknown mean $\bm{\mu}$ and covariance $\bm{\Sigma}$:
\begin{equation}
 \bm{\phi}_{1:N}\vert \bm{\mu},\bm{\Sigma} \sim \mathcal{N}\left(\bm{\mu},\bm{\Sigma}\right)\mbox{, i.\,i.\,d.}\label{eqn:genmodel_obs}
\end{equation}
The notation $\bm{\phi}_{1:N}$ is used to denote the set of vectors $\bm{\phi}_{1}$ up to $\bm{\phi}_{N}$.

%Given this model, one can estimate parameters $\bm{\mu}$ and $\bm{\Sigma}$ via a maximum likelihood procedure. When the maximisation is done in the class of finite-rank approximations $\hat{\bm{\Sigma}}=\transpose{\bm{W}}\bm{W}$, the problem is equivalent to finding the eigenvectors of the empirical covariance matrix of $\bm{\phi}_i$'s, also known as principal component analysis~\cite[PCA]{Roweis1999}. Sch\"{o}lkopf \emph{et al.}\~\cite{Scholkopf1998} presented nonlinear principal component analysis as a kernel eigenvalue problem and introduced kernel PCA (kPCA), which has the ability to handle very large, even infinite dimensional feature spaces.
%Instead of maximising the likelihood in eq.\ \eqref{eqn:genmodel_obs},

Now we will consider estimating parameters of this model in a Bayesian way. In Bayesian inference, one defines prior distributions over the parameters and then uses Bayes' rule to compute the posterior over them. Importantly, now we also require that the resulting Bayesian procedure is still amenable to the kernel trick. We therefore start by discussing a necessary condition for kernelisation which can then guide our choice of prior distributions.

The kernel trick requires that the algorithm be expressed solely in terms of scalar products $\langle\bm{\phi}_i,\bm{\phi}_j\rangle$. Scalar products are invariant under orthonormal transformations of the space, \ie if $\mathcal{A}$ is an orthonormal transformation then $\left\langle \bm{u},\bm{v} \right\rangle = \left\langle \mathcal{A}\bm{u},\mathcal{A}\bm{v} \right\rangle$ for all $\bm{u}$, and $\bm{v}$ in the space. Thus, if one wants to express an algorithm in terms of scalar products, it has to be -- at least -- invariant under orthonormal transformations, such as
rotations, reflections and permutations. It is well known that PCA has this property, but, for example, factor analysis (FA) does not~\cite{Roweis1999}, thus one cannot expect a kernel version of FA without any restrictions.

Another desired property of the method is analytical convenience and tractability, which can be ensured by using conjugate priors. The conjugate prior of the Gaussian likelihood is the \emph{Normal-inverse-Wishart}, which in our case has to be restricted to meet the orthonormal invariance condition:
\begin{equation} \label{eqn:genmodel_mean}
\begin{split}
 \bm{\Sigma};\:\sigma_0^{2}, \varconc &\sim \mathcal{W}^{-1}\left(\sigma^{2}_0\bm{I},\varconc\right)  \\ \bm{\mu}\vert\bm{\Sigma},\meanconc &\sim \mathcal{N}\left(\bm{0},\frac{1}{\meanconc}\bm{\Sigma}\right) ,
\end{split}
\end{equation}
where $\mathcal{W}^{-1}$ and $\mathcal{N}$ denote the inverse-Wishart and Gaussian distributions with the usual parametrisation. The general Normal-inverse-Wishart family had to be restricted in two ways: firstly, the mean of $\bm{\mu}$ was set to zero; secondly, the scale matrix of the inverse-Wishart was set to be spherical. These sensible restrictions ensure that the marginal distribution of $\bm{\phi}$ is centered at the origin and is spherically symmetric and therefore orthonormal invariance holds, which is required for kernelisation.

Having defined the hierarchical generative model in eqns.\ \eqref{eqn:genmodel_obs}--\eqref{eqn:genmodel_mean}, our task is to estimate the density of $\bm{\phi}$'s given the previous observations $\bm{\phi}_{1:N}$, which in a Bayesian framework is done by calculating the following posterior predictive distribution:
\begin{multline*}
 p(\bm{\phi}\vert\bm{\phi}_{1:N};\sigma_0^{2},\varconc,\meanconc) = \\
 \int p(\bm{\phi}\vert \bm{\mu},\bm{\Sigma})p(\bm{\mu},\bm{\Sigma}\vert\bm{\phi}_{1:N};\sigma_0^{2},\varconc,\meanconc) d\bm{\mu}d\bm{\Sigma}
\end{multline*}

By straightforward computation, it can be shown that the posterior predictive distribution is a $D$-dimensional \studentt distribution of the form (with the dependence on hyper-parameters made implicit):
\begin{align}
  &p(\bm{\phi}\vert\bm{\phi}_{1:N}) \propto \label{eqn:postpredictive} \\
  &\left( \gamma + \quadform{\tilde{\bm{\phi}}}{\left(\sigma_0^{2}\bm{I} + \tilde{\bm{\Phi}}\left(\bm{I} - \frac{\bm{1}\transpose{\bm{1}}}{N+\meanconc}\right)\transpose{\tilde{\bm{\Phi}}}\right)^{-1}}\right)^{-\frac{1 + N + \varconc}{2}} . \notag
\end{align}
where $\gamma = \frac{1+\meanconc+N}{\meanconc+N}$,  $\tilde{\bm{\phi}} = \bm{\phi} - \frac{N\bar{\bm{\phi}}}{N+\meanconc}$, $\tilde{\bm{\Phi}} = \left[\bm{\phi}_1 - \bar{\bm{\phi}},\ldots,\bm{\phi}_N - \bar{\bm{\phi}}\right]$, $\bar{\bm{\phi}} = \frac{1}{N}\sum_{n=1}^{N}\bm{\phi}_n$ is the empirical mean in feature space and $\bm{1}$ is a $N\times1$ vector of ones.

In order to obtain an expression which only contains scalar products, we invoke Woodbury's matrix inversion formula~\cite{Hager1989}:
\begin{align}
  &p(\bm{\phi}\vert\bm{\phi}_{1:N}) \propto \left( \gamma + \frac{\quadform{\tilde{\bm{\phi}}}{}}{\sigma_0^2} \:- \right. \label{eqn:postpredictive_Woodbury} \\
  &\left. \quadform{\tilde{\bm{\phi}}}{ \tilde{\bm{\Phi}} \left(\sigma_0^{4}\left(\bm{I} + \frac{\bm{1}\transpose{\bm{1}}}{\meanconc}\right) + \sigma_0^2\quadform{\tilde{\bm{\Phi}}}{}\right)^{-1}\transpose{\tilde{\bm{\Phi}}}} \right)^{-\frac{1 + N + \varconc}{2}}. \notag
\end{align}

\subsection{Kernel trick}

Until now, we assumed that $\bm{\phi}$ was an arbitrary point in $D$-dimensional space. In reality, however, we only want to assign probabilities~\eqref{eqn:postpredictive_Woodbury} to points on the so-called observation manifold, $\mathcal{O} = \left\{\varphi(\bm{x}) : \bm{x}\in\mathcal{X}\right\}$, \ie to points that can be realised by mapping an observation $\bm{x}\in\mathcal{X}$ to feature space $\bm{\phi} = \varphi(\bm{x})$. Restricted to $\mathcal{O}$, we can actually make use of the kernel trick, assuming as well that $\bm{\phi}_i=\varphi(\bm{x}_i)$ for the previous observations. Indeed, Eq.~\eqref{eqn:postpredictive_Woodbury} then only depends on $\bm{x}_{1:N}$ and $\bm{x}$ through $\quadform{\tilde{\bm{\phi}}}{}$, $\transpose{\tilde{\bm{\phi}}}\tilde{\bm{\Phi}}$ and $\quadform{\tilde{\bm{\Phi}}}{}$, which can all be expressed in terms of pairwise scalar products $\transpose{\bm{\phi}_n}\bm{\phi}_m = \langle\varphi(\bm{x}_n),\varphi(\bm{x}_m)\rangle = k(\bm{x}_n,\bm{x}_m)$ as follows:

\begin{align}
 \quadform{\tilde{\bm{\phi}}}{} &= k(\bm{x},\bm{x}) - 2\frac{\sum_i k(\bm{x},\bm{x}_i)}{N + \meanconc} + \frac{\sum_{i,j} k(\bm{x}_i,\bm{x}_j)}{\left(N + \meanconc\right)^{2}} \label{eqn:kernelexpressions} \\
 \left[\transpose{\tilde{\bm{\phi}}}\tilde{\bm{\Phi}}\right]_{n} &=
     \begin{aligned}[t]
     k(\bm{x},\bm{x}_n) &- \frac{\sum_i k(\bm{x},\bm{x}_i)}{N} \\
     &+ \frac{\sum_{i,j} k(\bm{x}_i,\bm{x}_j) - N\sum_i k(\bm{x}_n,\bm{x}_i)}{N\left(N+\meanconc\right)} \\
     \end{aligned} \notag \\
 \left[\quadform{\tilde{\bm{\Phi}}}{}\right]_{n,m} &=
    \begin{aligned}[t]
    k(\bm{x}_n,\bm{x}_m) &- \frac{\sum_i k(\bm{x_n},\bm{x}_i) + \sum_i k(\bm{x_m},\bm{x}_i)}{N} \\
    &+ \frac{\sum_{i,j} k(\bm{x}_i,\bm{x}_j)}{N^2}
    \end{aligned} \notag
\end{align}

As said previously, we are only interested in points in the (curved) observation manifold. The restriction of the predictive distribution~\eqref{eqn:postpredictive_Woodbury} to the observation manifold induces the same density function $q(\bm{\varphi}(\bm{x}) \vert \bm{x}_{1:N}) = p(\bm{\varphi}(\bm{x}) \vert\bm{\phi}_{1:N})$ (which is now unnormalised), but with respect to a $d$-dimensional Lebesgue base measure in the geodesic coordinate system of the manifold. Finally, we map the density $q(\bm{\varphi}(\bm{x}) \vert \bm{x}_{1:N})$ back to the original input space by implicitly inverting the mapping $\varphi$, yielding the (unnormalised) predictive density $q_k(\bm{x} \vert \bm{x}_{1:N})$ defined on the input space. By doing so, we need to include a multiplicative Jacobian correction term which relates volume elements in the tangent space of the manifold to volume elements in the input space $\mathcal{X}$:
 %weights the change of variable from the geodesic coordinate system to the $\mathcal{X}$ space:
\begin{multline} \label{corrected_density}
q_k(\bm{x} \vert \bm{x}_{1:N}) = \\
q(\bm{\varphi}(\bm{x})\vert\bm{x}_{1:N}) \cdot \det\left( \left\langle\frac{\partial\varphi(\bm{x})}{\partial x^i} ,  \frac{\partial\varphi(\bm{x})}{\partial x^j}\right\rangle \right)^{\frac{1}{2}}_{i,j=1,\ldots, d} .
\end{multline}
This formula can be be derived from the standard change-of-variable formula for densities, as we show in the Appendix \ref{sec:change_of_variable}.

\begin{figure*}
    \begin{center}
        \begin{tikzpicture}
            \node[anchor = south west, inner sep = 0in,] at (10pt,10pt) {\includegraphics[trim = 1.3in 2.4in 0.93in 2in, clip, width=6.45in,keepaspectratio]{figures/illustration_2D}};
            \def\offsetB{1.70in}
            \def\offsetC{3.40in}
            \def\offsetD{5.11in}

            \node at (0.85in,1.5in) {\textbf{A}};

            \node at (10pt,5pt) {-1};
            \node at (1.02in,0in) {$x$};
            \node at (1.43in,5pt) {1};

            \node at (5pt,15pt) {0};
            \node at (0in,0.82in) [rotate = 90]{$p(x)$};
            \node at (5pt,1.47in) {3};

            \node at (\offsetB+0.85in,1.5in) {\textbf{B}};

            \node at (\offsetB+10pt,5pt) {-1};
            \node at (\offsetB+0.82in,0in) {$\phi^1$};
            \node at (\offsetB+1.43in,5pt) {1};

            \node at (\offsetB+5pt,15pt) {0};
            \node at (\offsetB+0in,0.82in) [rotate = 90]{$\phi^2$};
            \node at (\offsetB+5pt,1.47in) {1};

            \node at (\offsetC+0.85in,1.5in) {\textbf{C}};

            \node at (\offsetC+10pt,5pt) {-1};
            \node at (\offsetC+0.82in,0in) {$x$};
            \node at (\offsetC+1.47in,5pt) {1};

            \node at (\offsetC+5pt,15pt) {0};
            \node at (\offsetC+5pt,1.47in) {3};

            \node at (\offsetD+0.85in,1.5in) {\textbf{D}};

            \node at (\offsetD+10pt,5pt) {-1};
            \node at (\offsetD+0.82in,0in) {$x$};
            \node at (\offsetD+1.43in,5pt) {1};

            \node at (\offsetD+5pt,15pt) {0};
            \node at (\offsetD+0in,0.82in) [rotate = 90]{$p(x)$};
            \node at (\offsetD+5pt,1.47in) {3};
        \end{tikzpicture}
    \end{center}

    \caption{
        \label{fig:illustration_2D}
        Illustration of kernel \studentt density estimation on a toy problem.
        \textbf{A,\ }Fourteen data points $x_{1:14}$ (\emph{crosses}) drawn from from a mixture distribution (\emph{solid curve}).
        \textbf{B,\ }Embedded observations $\bm{\phi}_{1:14}$(\emph{crosses}) in the two dimensional feature space and contours of the predictive density $q(\bm{\phi}_{15}\vert x_{1:14})$. The observation manifold is a parabola (\emph{solid curve}), and we are interested in the magnitude of the predictive distribution to this manifold (\emph{colouring}).
        \textbf{C,\ }The restricted posterior predictive distribution pulled back to the real line (\emph{solid curve}) gets multiplied by the Jacobian term $\sqrt{1 + 4x^2}$ (\emph{dotted curve}) to give
        \textbf{D,\ }the predictive distribution $q_{k}(x_{15}\vert x_{1:14})$ (\emph{solid curve}) in observation space.
         All distributions are scaled so that they normalise to one.}
\end{figure*}

So what exactly did we gain by applying the kernel trick? Despite the fact that the simple density model in the whole feature space is unimodal, in the local coordinate system of the non-linear observation manifold, the predictive density may appear multimodal and hence possess interesting nonlinear features. This is illustrated in Fig.~\ref{fig:illustration_2D}. Assume that we observe draws from a complicated distribution, which cannot be conveniently modelled with a linear Gaussian model (Fig.~\ref{fig:illustration_2D}.A). We map observations to a two dimensional feature space using the mapping $\varphi^1(x)=x,\varphi^2(x)=(x)^2$ hoping that the embedded observations are better fitted by a Gaussian, and carry out inference there (Fig.~\ref{fig:illustration_2D}.B). Note how all possible observations in the original space get mapped to the observation manifold~$\mathcal{O}$, which is now the parabola $\phi^2=(\phi^1)^2$. The outcome of inference is a posterior predictive distribution in feature space, which takes a \studentt form. To obtain the predictive distribution in the observation space we have to look at the magnitude of the predictive distribution along the observation manifold. This is illustrated by the contour lines in panel B. The restricted predictive distribution is then ``pulled back'' to observation space and has to be multiplied by the Jacobian term (Fig.~\ref{fig:illustration_2D}.C) to yield the final estimation of the density of the observed samples (Fig.~\ref{fig:illustration_2D}.D). Remarkably, our method computes the unnormalised density estimate (before the Jacobian term is added, Fig.~\ref{fig:illustration_2D}.C) directly from the data (Fig.~\ref{fig:illustration_2D}.A). All intermediate steps, including the inversion of the mapping, are \emph{implicit} in the problem formulation, and we never have to work with the feature space directly. As the method essentially estimates the density by a \studentt in feature space, we shall call it \emph{kernel \studentt density estimation}.

\subsection{Infinite dimensional feature spaces \label{sec:infinite_dimensional}}

Of course, we constructed the previous toy example so that the simple normal model in this second order polynomial feature space is able to model the bimodal data distribution. Should the data distribution be more complicated, \eg would have three or more modes, the second order features would hardly be enough to pick up all relevant statistical properties. Intuitively, adding more features results in higher potential expressive power, while the Bayesian integration safeguards us from overfitting. In practice, therefore, we will use the method in conjunction with rich, perhaps infinite dimensional feature spaces, which will allow us to relax these parametric restrictions and to apply our method to model a wide class of probability distributions.

However, moving to infinite dimensional feature spaces introduces additional technical difficulties. As for one, Lebesgue measures do not exist in these spaces, therefore our derivations based on densities with respect to Euclidean base measure should be reconsidered. For a fully rigorous description of the method in infinite spaces, one may consider using Gaussian base measures instead. In this paper, we address two specific issues that arise when the feature space is large or infinite dimensional.

Firstly, because of the Jacobian correction term, the predictive density $q_k$ in input space is still not fully kernelised in~\eqref{corrected_density}. We note though that this term is the determinant of a small $d\times d$ matrix of inner products, which can actually be computed efficiently for some kernels. For example, the all-subsets kernel~\cite{Cristianini2004} has a feature space with exponential dimension $D=2^d$, but each inner product of derivatives can be computed in $\mathcal{O}(d)$. But what about nonparametric kernels with infinite $D$? Interestingly, we found that shift invariant kernels -- for which $k(\bm{x},\bm{y}) = k(\bm{x}+\bm{z},\bm{y}+\bm{z})$ for all $\bm{z}$ -- actually yield a \emph{constant Jacobian term} (see proof in the Appendix \ref{sec:Jacobian_shift}) so it can be safely ignored to obtain a fully kernelised -- although unnormalised -- predictive density $q_k(\bm{x} \vert \bm{x}_{1:N}) = q(\bm{\varphi}(\bm{x})\vert\bm{x}_{1:N})$. %, where the RHS can be computed using only kernel evaluations from equations~\eqref{eqn:postpredictive_Woodbury}--\eqref{eqn:kernelexpressions}. 
This result is important, as the most commonly used nonparametric kernels, such as the squared exponential, or the Laplacian fall into this category.

The second issue is normalisation: Eq.~\eqref{corrected_density} only expresses the predictive distribution up to a multiplicative constant. Furthermore, the derivations implicitly assumed that $\alpha>D-1$, where $D$ is the dimensionality of the feature space, otherwise the probability densities involved become improper. This assumption clearly cannot be satisfied when $D$ is infinite.
However, one can argue that even if the densities in the feature space are improper, the predictive distributions that we use may still be normalisable when restricted to the observation manifold for $\alpha>d-1$. Indeed, the two-dimensional \studentt is normalisable when restricted to the parabola as in Fig.~\ref{fig:illustration_2D} for all $\alpha>0$ rather than $\alpha>1$ (to show this, consider $N=0$ so that~\eqref{eqn:postpredictive} takes the simple form $q(\bm{\phi}) = (\textrm{cnst.}+ \transpose{\bm{\phi}}\bm{\phi})^{-\frac{1+\alpha}{2}}$). So although distributions in feature space may be improper, the predictive densities may remain well-defined even when nonparametric kernels are used.

Given these observations, we thus decide to \emph{formally} apply equation~\eqref{corrected_density} for our predictive density estimation algorithm, ignoring the normalisation constant and also the Jacobian term when shift-invariant nonparametric kernels are applied.

%Moving to infinite dimensional feature spaces introduces additional technical difficulties, as for example there is no analog to the Lebesgue measure in infinite dimensions, and so one often has to resort to Gaussian process measures~\cite{Bogachev1991} instead. We conjecture that the change of variable expression~\eqref{corrected_density} still holds, but it is unclear whether the full Bayesian procedure outlined from~\eqref{eqn:genmodel_obs}--\eqref{eqn:postpredictive_Woodbury} can be made rigorous. Orbanz~\cite{Orbanz2010} recently brought the difficulties of Bayesian conditioning in infinite dimensions to light and made the construction of several nonparametric conjugate priors rigorous. We leave it for future work to develop a similar construction for our generative model. In this paper, our algorithm applies equation~\eqref{corrected_density} formally and uses the Bayesian model to help interpretation.  In particular, consider the case of the squared exponential (SE) kernel with length scale $l$, $k_{SE}(\bm{x},\bm{x}')=\exp(-\frac{1}{2l}\|\bm{x} - \bm{x}'\|^2)$. Embeddings $\bm{\phi}_i$ will now be functions, and not finite dimensional vectors. In the SE case, each observation $\bm{x}_i$ gets mapped to a bell-shaped `bump' $\bm{\phi}_i(\cdot) = \varphi_{SE}(\bm{x}_i) = \exp(-\frac{1}{2l}\|\cdot-\bm{x}_i\|^2)$. The observation manifold is the set of SE bumps positioned all over the observation space. The infinite dimensional analogue to our generative model~ \eqref{eqn:genmodel_mean}--\eqref{eqn:genmodel_obs} is that these bumps were sampled from a Gaussian process of unknown mean function $m(\cdot)$ and covariance function $c(\cdot,\cdot)$. The method implicitly estimates $m$ and $c$ from the observations in a Bayesian way, using a nonparametric conjugate prior analogous to the Normal-inverse-Wishart one in~\eqref{eqn:genmodel_mean}\footnote{The ``inverse-Wishart process'' appears in \eg~\cite{Li2009} as priors over covariance or kernel functions. However it is yet to be shown that they indeed constitute valid nonparametric priors~\cite{Orbanz2010}.}. The posterior predictive distribution will be a \studentt process, which is restricted to the observation manifold.

\begin{figure*}[t]
 \begin{center}
  \begin{tikzpicture}
    \node[anchor = south west, inner sep = 0in] at (0in,4pt) {\includegraphics[width=2.1in,keepaspectratio]{figures/fantasy_lowalpha}};
    \draw[ultra thick,draw = red] (0in,0pt) -- (0.15in,0pt);
    \node[inner sep = 0in] at (1.05in,2.25in) {\textbf{A}: $\alpha = 0.01,\beta = 0.01$};
    \node[anchor = south west, inner sep = 0in] at (2.15in,4pt) {\includegraphics[width=2.1in,keepaspectratio]{figures/fantasy_medalpha}};
    \draw[ultra thick,draw = red] (2.15in,0pt) -- (2.85in,0pt);
    \node[inner sep = 0in] at (3.2in,2.25in) {\textbf{B}: $\alpha = 3,\beta = 0.01$};
    \node[anchor = south west, inner sep = 0in] at (4.3in,4pt) {\includegraphics[width=2.1in,keepaspectratio]{figures/fantasy_largealpha}};
    \draw[ultra thick,draw = red] (4.3in,0pt) -- (6in,0pt);
    \node[inner sep = 0in] at (5.35in,2.25in) {\textbf{C}: $\alpha = 10, \beta = 0.01$};
  \end{tikzpicture}
 \end{center}
 \caption{\label{fig:fantasy}Fantasy data drawn from a two dimensional kernel \studentt model with SE kernel. The  length-scale $\ell=1$ (\emph{bars below panels}) and $\meanconc=0.01$ are fixed, $\varconc$ varies between $0.01$ and $10$ (\textbf{A} to \textbf{C}). Panels show 50 points (\emph{blue dots}) drawn from the generative model and the predictive density (\emph{gray level}) of the $51^{\mbox{st}}$ draw.}
\end{figure*}

Finally, we want to emphasize that the method does not imply nor does it require that arbitrary distributions, when embedded in rich Hilbert spaces, will look exactly like Gaussian or \studentt measures. Instead, the method uses the potentially infinitely flexible family of Gaussian measures to approximately model distributions in the feature space, and uses Bayesian integration to address model-misspecification via explicit representation of uncertainty. As demonstrated in Fig.~\ref{fig:illustration_2D}, the richness and flexibility of the density model stems from projecting the distribution onto a nonlinear observation manifold, allowing us to overcome limitations of Gaussian measures, such as unimodality.

\section{Related work}

Our method is closely related to kernel principal component analysis~\cite[kPCA]{Scholkopf1998} as both of them essentially use the same underlying generative model (Eq.~\ref{eqn:genmodel_obs}). While our method is based on Bayesian inference, kPCA performs constrained maximum likelihood estimation~\cite{Rosipal2001} of parameters $\bm{\mu}$ and $\bm{\Sigma}$. kPCA is a very effective and popular method for nonlinear feature extraction. Its pitfall from a density estimation perspective is that it does not generally induce a sensible probability distribution in observation space. To understand this, consider performing kPCA of order one (\ie recover just a single nonlinear component) on the toy data in~\subfigref{fig:illustration_2D}{A}. The solution defines a degenerate Gaussian distribution in feature space which is concentrated~ along a line, roughly the long axes of the equiprobability ellipses in \subfigref{fig:illustration_2D}{B}. As such a distribution can intersect the observation manifold at most twice, the predictive distribution in observation space will be a mixture of two delta distributions. This degeneracy can be ruled out by recovering at least as many nonlinear components as the dimensionality of feature space, but this is clearly not a viable option in infinite or large dimensional feature spaces. The Bayesian approach sidesteps the degeneracy problem by integrating out the mean and covariance, thereby averaging many, possibly degenerate distributions to obtain a non-degenerate, smooth \studentt distribution in feature space, which results in a smooth density estimate in observation space.

Another particularly interesting related topic is that of \emph{characteristic kernels}~\cite{Sriperumbudur2008}. A positive definite kernel $k(x,x') = \langle\varphi(x),\varphi(x')\rangle$ is called characteristic if the mean element $\mu_\pi = \mathbb{E}_{\mathbb{X} \sim \pi}[\varphi(\mathbb{X})]$ uniquely characterises any probability measure $\pi$. From a density estimation perspective, this suggests that estimating a distribution in observation space boils down to estimating the mean in a characteristic kernel space. The kernel moment matching framework~\cite{Song2008} tries to exploit characteristic kernels in a very direct way: parameters of an approximating distribution are chosen by minimising maximum mean discrepancy in feature space. Our method can be interpreted as performing Bayesian estimation of first and second moments in feature space, therefore one may hope that work on characteristic kernels will help us further study and understand properties of the algorithm.

The Gaussian process has been used as a nonparametric prior over functions for unsupervised learning tasks in several studies, such as in Gaussian process latent variable models~\cite[GPLVMs]{Lawrence2004} or the Gaussian process density sampler~\cite[GPDS]{Adams2009}. What sets the present approach apart from these is that while they incorporated Gaussian processes as a building block in an unsupervised model, we re-used the construction of GP regression and explicitly applied the kernel trick in an parametric unsupervised method. The GPDS method relies on Markov chain Monte Carlo (MCMC) methods for so called doubly-intractable distributions~\cite{Murray2006}. These methods can be used to sample from the posterior over hyper-parameters of probabilistic methods where normalisation is intractable. In particular, they could be used to integrate out the hyper-parameters $\varconc,\meanconc$ and $\sigma_0$, of our method. We emphasise that while MCMC is a crucial component of the GPDS, inference in our model is essentially closed-form.

\section{Experiments\label{sec:experiments}}

A common way to examine properties of unsupervised learning models is to draw \emph{fantasy datasets} from the generative model. The kernel \studentt model can be simulated as follows: first, we fix $\bm{x}_1=\bm{0}$ for convenience. Our generative model with shift-invariant kernels define an improper prior over the whole space when there is no observation (as it should since only difference between points is meaningful for shift invariant kernels). Then we draw each subsequent $\bm{x}_n$ from the kernel \studentt predictive distribution $q_{k}(\bm{x}_n\vert\bm{x}_{1:n-1};\sigma_o,\varconc,\meanconc)$ conditioned on previous draws. Sampling from the predictive distribution is possible via hybrid Monte Carlo~\cite{Neal2010}.

Fig.~\ref{fig:fantasy} shows example fantasy datasets for different settings of parameters. We observe that the generative process has an approximate clustering property, in which it behaves similarly to the Chinese restaurant process. We also note that the ``checkerboard - like'' eigenfunctions of the squared exponential kernel induce a tree like refinement of the space, in which the method is similar to Dirichlet diffusion trees.

\begin{figure}[t]
	\begin{center}
	\resizebox{\columnwidth}{!}{
	\begin{tikzpicture}
		\node[anchor = south west, inner sep = 0in] at (18pt,16pt) {\includegraphics[width=0.88\columnwidth,trim = 1.2in 0.8in 0.9in 0.6in, clip]{figures/AUC_and_Confusion}};
		\node[rotate=90] at (-3pt,0.76in) {confusion$[\%]$};
		\node[rotate=90] at (-3pt,2.15in) {AuC$[\%]$};
		\node at (1.63in,0pt) {number of training data};
		\node[anchor = east] at (20pt,0.28in) {0};
		\node[anchor = east] at (20pt,0.60in) {25};
		\node[anchor = east] at (20pt,0.92in) {50};
		\node[anchor = east] at (20pt,1.24in) {75};
		\node[anchor = west] at (2.5in,1.18in) {\emph{\ourmethod}};
		\node[anchor = west] at (2.5in,1.01in) {\emph{kde}};
		\node at (1.63in,1.4in) {label reconstruction};

		\node[anchor = east] at (20pt,1.65in) {50};
		\node[anchor = east] at (20pt,2.15in) {75};
		\node[anchor = east] at (20pt,2.65in) {100};
		\node[anchor = west] at (2.5in,1.94in) {\emph{\ourmethod}};
		\node[anchor = west] at (2.5in,1.77in) {\emph{kde}};
		\node at (1.63in,2.8in) {novelty detection};
	
		\node at (20pt,10pt) {0};
		\node at (0.94in,10pt) {100};
		\node at (1.63in,10pt) {200};
		\node at (2.32in,10pt) {300};
		\node at (3.01in,10pt) {400};
	\end{tikzpicture}
	}
	\end{center}
	\caption{\label{fig:AUC_and_Confusion} Performance of \emph{\ourmethod} and \emph{kde} as a function of the number of training examples  on USPS data. Errorbars show $\pm$ one standard deviation.}
\end{figure}

\begin{table}[t]
 \begin{center}
 \resizebox{\columnwidth}{!}{
  \begin{tabular}{|l||c|c|}
	\hline \textsc{method}& \textsc{novelty}(AuC[\%]) & \textsc{recons.}(conf[\%])\\
	\hline\hline \emph{\ourmethod}	& $\bm{96.82}\pm0.8$ & $\bm{2.15}\pm{1.0}$\\
	\hline	\emph{kde}	& $94.56\pm0.6$ & $9.24\pm1.9$\\
	\hline	\emph{dpm}	& $73.56\pm4.3$ & $23.85\pm8.2$\\
	\hline
  \end{tabular}
 }
 \end{center}
 \caption{\label{tab:digits_results}Performance of \emph{\ourmethod}, \emph{kde} and \emph{dpm} in novelty detection (\emph{novelty, left}) and label reconstruction (\emph{recons., right}) on the USPS data. The table shows average AuC and confusion values $\pm$ one standard deviation measured over 10 randomised experiments.}
\end{table}

The performance of density models where the normalisation constants are not available -- similar examples in the past included Markov random fields~\cite{Haluk1987} and deep belief nets -- is hard to evaluate directly. These density models are typically used in unsupervised tasks, such as reconstruction of missing data, novelty detection and image denoising, where the important quantity is the shape of the distribution, rather than the absolute density values.  In the following, we consider three such unsupervised tasks that are indicative of the model's performance: novelty detection, assessment of reconstruction performance and the recently introduced task of relative novelty detection~\cite{Smola2009}. The purpose of these experiments is to demonstrate, as proof of concept, that the method is able to model relevant statistical properties in high dimensional data.

\subsection{Novelty detection}

Novelty detection, recognition of patterns in test data that are unlikely under the distribution of the training data, is a common unsupervised task for which density estimation is used. Here, we considered the problem of detecting mislabelled images of handwritten digits given a training set of correctly labelled images from the USPS dataset. The dataset consists of 7291 training and 2007 test images, each of which is a labelled, $16\times 16$ gray-scale image of a handwritten digit (0-9).

We modelled the joint distribution of the image (a 256 dimensional vector) and the label (a ten dimensional sparse vector, where the $l^{\mbox{th}}$ element is $1$ and the rest are $0$'s if the label is $l$) by a kernel \studentt density (\emph{\ourmethod}). We used a squared exponential kernel with length-scale chosen to be the median distance between data-points along digit dimensions and length-scale $1$ along the label dimensions. We trained the algorithm on a randomly selected subset of available training data and calculated predictive probabilities on a test dataset composed of 100 correctly labelled and 100 mislabelled test points. Digit-label pairs with predictive probability under a threshold were considered \emph{mislabelled}. We compared the performance of our method to two baseline methods, kernel density estimation (\emph{kde}, also called Parzen window estimate) and Dirichlet process mixtures of Gaussians (\emph{dpm}), based on the area under the receiver operating characteristic curve (AuC) metric. Hyper-parameters of all three methods were chosen by grid search so that the average AuC was maximised on a separate validation set. Table \ref{tab:digits_results} summarises the results of this comparison based on ten randomised iteration of the experiment with 2000 training and 200 test samples. We found that \emph{\ourmethod} outperformed both competing methods significantly ($p<0.01$, two sample T-test). The performance of \emph{dpm} was substantially worse than that of the other two, which demonstrates that general mixture models are very inefficient in modelling high dimensional complex densities. To investigate the difference between the performance of \emph{kde} and \emph{\ourmethod}, we carried out a second sequence of experiments where the size of the training set ranged from 10 to 400. Fig.~\ref{fig:AUC_and_Confusion} shows average AuC values as a function of training set size. We observed that the two algorithms perform similarly for small amounts of training data, the difference in performance becomes significant ($p<0.05$) for 60 training points or more.

\subsection{Label reconstruction\label{sec:classification}}

In these experiments, we considered learning the classification of handwritten digits. In order to assess the density modelling capabilities of our method, we posed this problem as an image reconstruction problem \footnote{We could have considered more general image reconstruction tasks, but reconstructing labels equips us with intuitive measures of loss, such as confusion and AuC.}. Again, we augmented the gray-scale image with 10 additional `pixels' representing a sparse coding of the labels 0-9. We first trained our density model on the augmented images on randomly selected training subsets of various sizes. Then, on a separate subset of test images we computed the probability of each label 0-9 conditioned on the image by computing the joint probability of the image with that label and renormalising. For each image we assigned the label for which the conditional predictive probability was the highest (maximum \emph{a posteriori} reconstruction). Results of comparison to \emph{kde} and \emph{DPM} with 2000 training and 400 test points are shown in \figref{fig:AUC_and_Confusion}. As measure of performance, we used average confusion, \ie the fraction of misclassified test images. We again found that \emph{kst} outperformed both other methods, and now the advantage of our method compared to \emph{kde} is more substantial. Fig.~\ref{fig:AUC_and_Confusion} shows confusion of \emph{\ourmethod} and \emph{kde} as a function of the number of training examples. The difference between the two methods becomes significant ($p<0.01$) for 60 training examples or more.

% We note that we did not test our method against dedicated methods for classification such as SVMs or GPs, and we expect that such methods would substantially outperform all density estimation methods considered here.

\subsection{Relative novelty detection}

\begin{figure*}
 \begin{center}
  \resizebox{\textwidth}{!}{
  \begin{tikzpicture}
    \node[anchor = south west, inner sep = 0in] at (0in,8pt) {\includegraphics[width=2.1in,keepaspectratio]{figures/satellite_background}};
    \node[inner sep = 0in] at (1.05in,2.3in) {\textbf{A}: Background};
    \node[anchor = south west, inner sep = 0in] at (2.15in,8pt) {\includegraphics[width=2.1in,keepaspectratio]{figures/satellite_target}};
    \node[inner sep = 0in] at (3.2in,2.3in) {\textbf{B}: Target};
    \node[anchor = south west, inner sep = 0in] at (4.3in,8pt) {\includegraphics[width=2.1in,keepaspectratio]{figures/satellite_novelty}};
    \node[inner sep = 0in] at (5.35in,2.3in) {\textbf{C}: Relative novelty};
  \end{tikzpicture}
    }
 \end{center}

 \caption{\textbf{A,\ }Background and \textbf{B,\ }target images for relative novelty detection. \textbf{C,\ }Top 5\% novel patches in the target image are highlighted in \emph{red}. The results are qualitatively similar to those presented in~\cite[Fig. 2]{Smola2009}\label{fig:satellite}.}
\end{figure*}

Finally, we considered the problem of relative novelty detection~\cite{Smola2009}, for which we re-used the experimental setup in~\cite{Smola2009}. The data consists of two $200\times 200$ color satellite images, the \emph{background} and the \emph{target} (Fig.~\ref{fig:satellite}.A--B). The task is to detect novel objects on the target image relative to the background image. Treating RGB values in each non-overlapping $2\times 2$ patches as data points, we estimated the density over such patches in both the background ($\hat{q}(\bm{x})$) and in the target ($\hat{p}(\bm{x})$) images. A patch $\bm{x}$ was identified as relatively novel if the ratio $\hat{p}(\bm{x})/\hat{q}(\bm{x})$ was above a certain threshold. The results produced by our algorithm are similar to that obtained by using state of the art novelty detection algorithms (c.\,f.\ Fig.~2 in~\cite{Smola2009}), even though here we only used a 3\% random subsample of available image patches to estimate target and background densities.

\section{Discussion}

\paragraph{Summary} In this paper, we considered applying the kernel trick as a tool for constructing nonparametric Bayesian methods, and suggested a general recipe for obtaining novel analytically tractable \emph{Bayesian kernel machines}. The starting point is a suitably simple parametric Bayesian model in which inference is tractable. We have demonstrated how arguments based on orthonormal invariance can be used to guide our choice of models that are amenable to kernel trick. Having defined the basic model, one needs to express posterior predictive distributions in terms of kernel evaluations, then apply the kernel trick in those equations. By using kernels with infinite dimensional feature spaces, novel tractable nonparametric Bayesian methods may be obtained in this way.

To illustrate our general approach, we studied the problem of density estimation and presented kernel \studentt density estimation, a novel Bayesian kernel machine for density modelling. We followed our general methodology to arrive at a closed form expression that is exact up to a multiplicative constant. We also carried out numerical experiments to investigate the performance of the method. The results demonstrate that the method is capable of estimating the distribution of high dimensional real-world data, and can be used in various applications such as novelty detection and reconstruction. The most apparent limitation of the kernel \studentt density model is the intractability of its normalisation constant. Nevertheless, the unnormalised density can still be used in a variety of unsupervised learning tasks, such as image reconstruction or novelty detection. As other kernel methods, our algorithm has cubic complexity in the number of training points, though sparse approximations~\cite{Fine2001} can be used to construct faster algorithms for large-scale applications. Another advantage of the kernel approach is that the expressions can be formally extended to define non-trivial discrete distributions over \eg graphs, permutations or text.

\paragraph{Conclusions and future work} We believe that the present paper opens the door for developing a novel class of Bayesian kernel machines. The methodology presented in this paper can be applied in further problems to obtain nonparametric methods based on parametric Bayesian models. Investigating merits and limitations of this general approach in various statistical estimation and decision problems is certainly an interesting theme for future research. One particularly exciting direction that we plan to pursue is a Bayesian two-sample hypothesis test analogous to the frequentist test based on characteristic kernels~\cite{Sriperumbudur2008}. For hypothesis testing, one needs to apply the kernel trick to a \emph{ratio} of marginal likelihoods, rather than to a predictive density, which enables cancellations which simplify the issues with normalisation or Jacobians. Other potentially fruitful directions include semi-supervised regression and Bayesian numerical analysis~\cite{OHagan1992}.

\paragraph{Acknowledgements} We would like to thank Zoubin Ghahramani, Carl Rasmussen, Arthur Gretton, Peter Orbanz
and Le Song for helpful discussions. In particular, the proof in
Appendix \ref{sec:Jacobian_shift} is mainly due to Le Song. Part of this work was supported
under the EPSRC grant EP/F026641/1. Ferenc Husz\'{a}r is supported by Trinity College Cambridge.

\bibliographystyle{nature3}
\bibliography{bibliography/Bayesian_kernel}

\appendix

\twocolumn[
\section{Change-of-variable formula\label{sec:change_of_variable}}

\paragraph{Theorem} Let $\mathcal{X}$ and $\mathcal{F}$ be $d$ and $D$ dimensional Euclidean spaces respectively ($D>d$), ${\varphi:\mathcal{X} \mapsto \mathcal{F}}$ be a smooth injective mapping and let $q(\bm{\phi})$ define a density function with respect to the Lebesgue measure on $\mathcal{F}$. Then the restriction of distribution $q$ to the observation manifold $\varphi(\mathcal{X})$ (\ie conditioning on the event of being in the observation manifold) induces the following density in the input space $\mathcal{X}$:
\begin{equation} \label{eq:densityChange}
p(\bm{x}) \propto q(\bm{\varphi}(\bm{x})) \cdot \det\left( \left\langle\frac{\partial\varphi(\bm{x})}{\partial x^i} ,  \frac{\partial\varphi(\bm{x})}{\partial x^j}\right\rangle \right)^{\frac{1}{2}}_{i,j=1,\ldots, d}\mbox{.}
\end{equation}

We suspect this result may have appeared in several forms in the literature, but we didn't succeed to find a citation, and so we provide a proof here.

\paragraph{Proof} Note that we cannot apply the standard change-of-variable formula directly as the two spaces have different dimensions. So we augment the space $\mathcal{X}$ with an Euclidean space $\Xi$ of dimension $D-d$. We then define the following mapping $F:\mathcal{X}\times\Xi \mapsto \mathcal{F}$:
$$
F(\bm{x},\bm{\xi}) = \varphi(\bm{x}) + V_{\bm{x}} \bm{\xi},
$$
where $V_{\bm{x}}$ is a $D\times(D-d)$ matrix of mutually orthonormal vectors to the tangent space of the manifold at $\bm{x}$ (\ie a basis for the orthogonal space to the tangent space at $\bm{x}$). For small enough $\bm{\xi}$'s (which could depend on $\bm{x}$), $F$ will map small neighborhoods of $(\bm{x},\bm{0})$ to neighborhoods of $\varphi(\bm{x})$ in an injective manner. We thus apply the standard change of variable formula to map the density $q(\bm{\phi})$ to a density on an open set around $\mathcal{X}\times\{\bm{0}\}$:
$$
    p(\bm{x},\bm{\xi}) = q(F(\bm{x},\bm{\xi})) \left\vert\det J_F(\bm{x},\bm{\xi}) \right\vert ,
$$
where $J_F(\bm{x},\bm{\xi})$ is the Jacobian matrix of the $F$ mapping. To condition on being in the observation manifold, we simply need to condition on $\bm{\xi}=0$, and so the density we want to compute is:
$$
    p(\bm{x}) \propto p(\bm{x},\bm{0}) = q(F(\bm{x},\bm{0})) \left\vert\det J_F(\bm{x},\bm{0}) \right\vert .
$$
Now, $J_F(\bm{x},\bm{0})=(J_\varphi(\bm{x}), V_{\bm{x}})$ where $J_\varphi(\bm{x})=(\frac{\partial\varphi(\bm{x})}{\partial x^1}, \ldots, \frac{\partial\varphi(\bm{x})}{\partial x^d})$ is the Jacobian matrix of $\varphi$. Finally, we use the fact that $\vert\det(J_F)\vert$ = $(\det(\transpose{J_F} J_F))^{1/2}$ for any squared matrix $J_F$ to get that:
$$
\left\vert\det\left(J_F (\bm{x},\bm{0})\right)\right\vert = \det\left(\begin{array}{cc}
                        \transpose{J_\varphi(\bm{x})}J_\varphi(\bm{x}) & \bm{0} \\
                        \bm{0} & \bm{I}
                      \end{array} \right)^{\frac{1}{2}} = \det\left( \transpose{J_\varphi(\bm{x})}J_\varphi(\bm{x}) \right)^{\frac{1}{2}},
$$
where we used that $\transpose{J_\varphi(\bm{x})} V_{\bm{x}}=0$ and $\transpose{V_{\bm{x}}} V_{\bm{x}}=\bm{I}$ by orthonormality of the $V_{\bm{x}}$ with the tangent space spanned by the columns of $J_\varphi(\bm{x})$. Finally, we use the fact that:
$$
 \transpose{J_\varphi(\bm{x})}J_\varphi(\bm{x}) = \left( \left\langle\frac{\partial\varphi(\bm{x})}{\partial x^i} ,  \frac{\partial\varphi(\bm{x})}{\partial x^j}\right\rangle \right)_{i,j=1,\ldots, d}
$$
to obtain~\eqref{eq:densityChange}. $\square$
]

\twocolumn[
\section{Jacobian term for translation invariant kernel\label{sec:Jacobian_shift}}

We provide the sketch of a proof that $\left\langle\frac{\partial\varphi(\bm{x})}{\partial x^i} ,  \frac{\partial\varphi(\bm{x})}{\partial x^j}\right\rangle$ doesn't depend on $\bm{x}$ for a translation invariant kernel under suitable regularity conditions.

\paragraph{}By Bochner's theorem (\eg theorem 4.1 in~\cite{Rasmussen2006}), a translation invariant kernel can be expressed as the inverse Fourier transform of a positive finite measure. If moreover we assume that the Fourier transform of the kernel exists, this measure will have a density $p(\bm{s})$ with respect to the Lebesgue measure, and so we can write:
\begin{align*}
 k(\bm{x}-\bm{y}) & =  \int_{\mathbb{R}^d} p(\bm{s}) e^{2\pi i \transpose{\bm{s}} (\bm{x}-\bm{y})} d\bm{s} \\
 &= \int p(\bm{s}) \left( \cos(2\pi \transpose{\bm{s}} (\bm{x}-\bm{y}))+i
 \sin(2\pi \transpose{\bm{s}} (\bm{x}-\bm{y})) \right) d\bm{s} \\
 &= \int p(\bm{s}) \left( \cos(2\pi \transpose{\bm{s}} (\bm{x}-\bm{y})) \right) d\bm{s}
\end{align*}
as $\sin(\transpose{\bm{s}}\bm{\delta})$ is an odd function of $\bm{s}$ for any $\bm{\delta}$ and $p(\bm{s})$ is a symmetric function given that it is the inverse Fourier transform of the symmetric function $k$, so the Cauchy principal value of the second term vanishes. Now using a trigonometric identity, we get:
\begin{align*}
 k(\bm{x}-\bm{y}) & = \int p(\bm{s}) \left( \left( \cos(2\pi \transpose{\bm{s}}\bm{x})  \cos(2\pi \transpose{\bm{s}}\bm{y}) \right) +
 \left( \sin(2\pi \transpose{\bm{s}}\bm{x})  \sin(2\pi \transpose{\bm{s}}\bm{y}) \right) \right) d\bm{s} \\
  &= \int \left\langle \sqrt{p(\bm{s})} \left( {\cos(2\pi \transpose{\bm{s}}\bm{x}) \atop \sin(2\pi \transpose{\bm{s}}\bm{x})} \right) , \sqrt{p(\bm{s})} \left( {\sin(2\pi \transpose{\bm{s}}\bm{y}) \atop \sin(2\pi \transpose{\bm{s}}\bm{y})} \right) \right\rangle d\bm{s}
\end{align*}
So we can obtain an explicit feature mapping by defining:
$$
\varphi(\bm{x}) = \left( \sqrt{p(\bm{s})} \left( {\cos(2\pi \transpose{\bm{s}}\bm{x}) \atop \sin(2\pi \transpose{\bm{s}}\bm{x})} \right) \right)_{\bm{s} \in \mathbb{R}^d}.
$$
Assuming that $p(\bm{s})$ goes to zero sufficiently quickly, we have that:
$$
\frac{\partial \varphi(\bm{x})}{\partial x_k} = \left( 2\pi s_k \sqrt{p(\bm{s})} \left( {-\sin(2\pi \transpose{\bm{s}}\bm{x}) \atop \cos(2\pi \transpose{\bm{s}}\bm{x})} \right) \right)_{\bm{s} \in \mathbb{R}^d}
$$
and using a similar derivation as we used above, we get that:
\begin{align*}
\left\langle\frac{\partial\varphi(\bm{x})}{\partial x_k} ,  \frac{\partial\varphi(\bm{x})}{\partial x_l}\right\rangle &= 4\pi^2 \int
s_k s_l p(\bm{s}) e^{2\pi i \transpose{\bm{s}} (\bm{x}-\bm{x})} d\bm{s} \\
&=  4\pi^2 \int
s_k s_l p(\bm{s}) e^{0} d\bm{s}
\end{align*}
which doesn't depend on $\bm{x}$ as we wanted to show. $\square$
]
\end{document}
%
\documentclass{article}

%
%
%
%
%
%

%

%
%
\usepackage[final,nonatbib]{nips_2016}
\usepackage[numbers]{natbib}

%
\usepackage[utf8]{inputenc} %
\usepackage[T1]{fontenc}    %
\usepackage{hyperref}       %
\usepackage{url}            %
\usepackage{booktabs}       %
\usepackage{amsfonts}       %
\usepackage{nicefrac}       %
\usepackage{microtype}      %

%
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{color}
\usepackage{graphicx}
\usepackage{subfig}

\usepackage{BPB-definitions}
\newtheorem{thm}{Theorem}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{conj}[thm]{Conjecture}
\theoremstyle{definition}
\newtheorem{definition}[thm]{Definition}
\newtheorem{remark}[thm]{Remark}
\theoremstyle{plain}

%
\definecolor{darkgreen}{rgb}{0,.4,.2}
\definecolor{darkblue}{rgb}{.1,.2,.6}
\definecolor{brightblue}{rgb}{0,0.6,0.8}
\hypersetup{
  colorlinks=true,
  linkcolor=darkblue,
  citecolor=darkgreen,
  filecolor=darkblue,
  urlcolor=darkblue
}
%

\def\LONGVERSION{}

\ifdefined\LONGVERSION
  \newcommand{\citeA}{\cite}
  \newcommand{\citepA}{\citep}
  \newcommand{\citetA}{\citet}
\else
  \usepackage{multibib}
  \newcites{A}{References}
\fi

%
%
%
  \title{PAC-Bayesian Theory Meets Bayesian Inference}
%

%
%
%
%
%
%
%
%
%

\author{
	Pascal Germain$^\dagger$ \quad Francis Bach$^\dagger$  \quad Alexandre Lacoste$^\ddagger$ \quad Simon Lacoste-Julien$^\dagger$\\
	$^\dagger$ INRIA Paris - \'Ecole Normale Sup\'erieure, \ \texttt{firstname.lastname@inria.fr}\\
		$^\ddagger$ Google, \ \texttt{allac@google.com}
}
%
%
%
%
%
%
%
%
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
%

\begin{document}
%

\maketitle

\begin{abstract}
We exhibit a strong link between frequentist PAC-Bayesian risk bounds and the Bayesian marginal likelihood.
That is, for the negative log-likelihood loss function, we show that the minimization of PAC-Bayesian generalization risk bounds maximizes the Bayesian marginal likelihood.
This provides an alternative explanation to the Bayesian Occam's razor criteria, under the assumption that the data is generated by an \iid~distribution. 
Moreover, as the negative log-likelihood is an unbounded loss function, we motivate and propose a PAC-Bayesian theorem tailored for the sub-gamma loss family, and we show that our approach is sound on classical Bayesian linear regression tasks. 
%
\end{abstract}

\section{Introduction}

Since its early beginning \citep{mcallester-99, shawetaylor-97}, the PAC-Bayesian theory claims to provide ``PAC guarantees to \emph{Bayesian} algorithms'' (\citet{mcallester-99}).
However, despite the amount of work dedicated to this statistical learning theory---many authors improved the initial results
\citep{catoni-07,lever-13,mcallester-03a,seeger-thesis,tolstikhin-13}
%
%
and/or generalized them for various machine learning setups
\citep{graal-aistats14,graal-icml16-dalc,grunwald&mehta-16,langford-02,pentina-14,seldin-10,seldin-11,seldin-12}%
%
---it is 
%
mostly used as a \emph{frequentist} method. That is, under the assumptions that the learning samples are \iid-generated by a data-distribution, this theory expresses \emph{probably approximately correct} (PAC) bounds on the generalization risk. In other words, with probability $1{-}\delta$, the generalization risk is at most $\varepsilon$ away from the training risk.  The \emph{Bayesian} side of PAC-Bayes comes mostly from the fact that these bounds are expressed on the averaging/aggregation/ensemble of multiple predictors (weighted by a \emph{posterior} distribution) and  incorporate prior knowledge.  Although it is still sometimes referred as a theory that bridges the Bayesian and frequentist approach 
\cite[\eg,][]{guyon-10},
%
 it has been merely used to  justify Bayesian methods until now.\footnote{Some existing connections \citep{banerjee-06,bissiri-16,grunwald-2012,lacoste-thesis,seeger-02,seeger-thesis,zhang-06} are discussed in Appendix~\ref{appendix:related}.}
%
%

In this work, we provide a direct connection between Bayesian inference techniques \citep[summarized by][]{bishop-2006,zoubin-15} and PAC-Bayesian risk bounds in a general setup.
Our study is based on a simple but insightful connection between the Bayesian marginal likelihood and PAC-Bayesian bounds (previously mentioned by \citet{grunwald-2012}) obtained by considering the negative log-likelihood loss function (Section~\ref{section:marginal_likelihood}). By doing so, we provide an alternative explanation for the Bayesian Occam's razor criteria  \citep{jeffreys-92,mackay-92} in the  context of model selection, expressed as the complexity-accuracy trade-off appearing in most PAC-Bayesian results. In Section~\ref{sec:more-bounds}, we extend PAC-Bayes theorems to regression problems with unbounded loss, adapted to the negative log-likelihood loss function. Finally, we study the Bayesian model selection from a PAC-Bayesian perspective (Section~\ref{sec:bayesian_model_comp}), and illustrate our finding on classical Bayesian regression tasks (Section~\ref{section:linreg}).
%

%
%
%
%
%
%


\section{PAC-Bayesian Theory}

\newcommand{\fcmt}[1]{\red{\footnote{\blue{#1}}}}
\newcommand{\cmt}[1]{\blue{\small{#1}}}

%
We denote the learning sample 
%
$(X,Y){=}\{(x_i,y_i)\}_{i=1}^n{\in}(\Xcal{\times}\Ycal)^n$, 
that contains $n$ input-output pairs. 
The main assumption of frequentist learning theories---including PAC-Bayes---is that $(\Dsample)$ is randomly sampled from a data generating distribution that we denote~$\Dgen$. 
%
Thus, we denote $(\Dsample){\sim}\Dgen^n$ the \iid observation of $n$ elements.
From a frequentist perspective, we consider in this work loss functions  $\loss:\Fcal{\times}\Xcal{\times}\Ycal\to\Rbb$,
%
where $\Fcal$ is a (discrete or continuous) set of predictors $f:\Xcal\to\Ycal$, and we write the empirical risk on the sample ($\Dsample$) and the generalization error on  distribution $\Dgen$ as
\begin{equation*}
\emploss(f)
\ = \ \frac1n \sum_{i=1}^n  \loss(f, x_i,y_i) \,;
\quad
%
\genloss(f)
\ = \, \Esp_{(x,y)\sim\Dgen} \loss(f, x,y) 
\,.
\end{equation*} 
The PAC-Bayesian theory \citep{mcallester-99,mcallester-03a} studies an averaging of the above losses according to a \emph{posterior} distribution $\post$ over  $\Fcal$. That is, it provides \emph{probably approximately correct} generalization bounds on the (unknown) quantity
%
$\Esp_{f\sim\post} \genloss(f)
\ = \, \Esp_{f\sim\post}  \Esp_{(x,y)\sim\Dgen} \loss(f, x,y) 
\,,$
%
given the empirical estimate  $\Esp_{f\sim\post} \emploss(f)$ and some other parameters. Among these, most PAC-Bayesian theorems rely on the \emph{Kullback-Leibler} divergence 
%
$\KL(\post\|\prior) \, = \, \Esp_{f\sim\post} \ln [\post(f) / \prior(f)]$ 
between a \emph{prior} distribution $\prior$ over~$\Fcal$---specified before seeing the learning sample $\Dsample$---and the posterior~$\post$---typically obtained by feeding a learning process with $(\Dsample)$. 

Two appealing aspects of PAC-Bayesian theorems are that they provide data-driven generalization bounds that are computed on the training sample  (\ie, they do not rely on a testing sample), and that they are uniformly valid for all $\post$ over $\Fcal$. This explains why  many works study them as model selection criteria or as an inspiration for learning algorithm conception. Theorem~\ref{thm:pacbayescatoni}, due to \citet{catoni-07}, has been used to derive or study learning algorithms \citep{graal-icml09,hazan-13,mcallester&keshet-11,noy-14}. 
\begin{thm}[\citet{catoni-07}] 
	\label{thm:pacbayescatoni}
	Given a distribution $\Dcal$ over  $\Xcal   \times   \Ycal$, a hypothesis set $\Fcal$, a loss function $\losszo:\Fcal\times\Xcal\times\Ycal\to[0,1]$,   a prior distribution $\prior$ over $\Fcal$, a real number $\delta \in (0,1]$, and a real number $\beta>0$,  with probability at least $1 - \delta$ over the choice of $(\Dsample) \sim  \Dgen^n $,  we have
	\begin{equation} \label{eq:catoni07}
	\forall \mbox{$\post$ on $\Fcal$}\,:\quad
	\Esp_{f\sim\post} \genlosszo(f) \ \leq\ 
	\frac{1}{1 - e^{-\beta}}    \left[1 - e^{ -\beta\,\Esp_{f\sim\post} \emplosszo(f){-}\tfrac1n\big(\KL(\post\|\prior) {+} \ln  \tfrac{1}{\delta}\big)}\right] .
	\end{equation}
\end{thm}
%
%
Theorem~\ref{thm:pacbayescatoni} is limited to loss functions mapping to the range $[0,1]$.  
Through a straightforward rescaling we can extend it to any bounded loss, \ie, $\loss:\Fcal\times\Xcal\times\Ycal\to[a,b]$, where $[a,b]\subset\Rbb$. This is done by 
%
using 
$\beta\eqdots b-a$ and with the \emph{rescaled}  loss function
$\losszo (f,x,y) \,\eqdots\, {(\loss(f,x,y){-}a)}/{(b{-}a)}\in[0,1]\,.$
After few arithmetic manipulations, we can rewrite Equation~\eqref{eq:catoni07} as
\begin{eqnarray}\label{eq:catoni_ab}
	\forall \mbox{$\post$ on $\Fcal$} :\ \ 
	\Esp_{f\sim\post}\! \genloss(f) \,\leq\, a+\tfrac{b-a}{1 - e^{a-b}}    \Big[1 {-} \exp\Big({ -\Esp_{\mathclap{f\sim\post}}\, \emploss(f)  {+}a{-}\tfrac1n\big(\KL(\post\|\prior) {+} \ln  \tfrac{1}{\delta}\big)}\Big)\Big]\,.
\end{eqnarray}
%
From an algorithm design perspective, Equation~\eqref{eq:catoni_ab} suggests optimizing a trade-off between the empirical expected loss and the Kullback-Leibler divergence. Indeed,
for fixed $\prior$, $X$, $Y$, $n$, and $\delta$, minimizing Equation~$\eqref{eq:catoni_ab}$
%
is equivalent to find the distribution $\post$ that minimizes
\begin{equation}\label{eq:a_minimser_beta}
n\,\Esp_{\mathclap{f\sim\post}}\, \emploss(f)  +  \KL(\post\|\prior)\,.
\end{equation}
%
It is well known \citep{alquier-15,catoni-07,graal-icml09,lever-13} that the \emph{optimal Gibbs posterior} $\post^*$ is given by
%
\begin{equation}\label{eq:post_catoni}
\post^*(f) \, = \, \tfrac1{\ZDs} \prior(f) \,e^{-n\,\emploss(f)}\,,
\end{equation}
where $\ZDs$ is a normalization term. Notice that the constant $\beta$ of Equation~\eqref{eq:catoni07} is now absorbed in the loss function as the rescaling factor 
setting the trade-off between the expected empirical loss and~$\KL(\post\|\prior)$. 
%


%


\section{Bridging Bayes and PAC-Bayes}
\label{section:marginal_likelihood}


In this section, we show that by choosing the negative log-likelihood loss function,
%
minimizing the PAC-Bayes bound is equivalent to maximizing the Bayesian marginal likelihood.
%
%
%
%
To obtain this result, we first consider the Bayesian approach that starts by defining a prior $p(\theta)$ over the set of possible model parameters $\Theta$. This induces a set of probabilistic estimators $f_\theta \in \Fcal$, mapping $x$ to a probability distribution over $\Ycal$. Then, we can estimate the likelihood of observing $y$ given $x$ and $\theta$, \ie, $p(y|x,\theta) \equiv f_\theta(y|x)$.\footnote{To stay aligned with the PAC-Bayesian setup, we only consider the discriminative case in this paper. One can extend to the generative setup by considering the likelihood of the form $p(y,x|\theta)$ instead.} Using Bayes' rule, we obtain the posterior $p(\theta|X,Y)$:
\begin{samepage}
\begin{equation} \label{eq:bayes_updaterule}
p(\theta|\Dsample) 
\,=\, \frac{p(\theta)\,p(Y|X,\theta)}{p(Y|X)}
\, \propto\, 
p(\theta)\,p(Y|X,\theta)\,,
\end{equation}
where $p(Y|X,\theta) \, = \, \prod_{i=1}^n p(y_i|x_i,\theta)$ and 
%
$p(Y|X) = \int_\Theta p(\theta) \,p(Y|X,\theta)\, d\theta$.
\end{samepage}

To bridge the Bayesian approach with the PAC-Bayesian framework, we consider the \emph{negative log-likelihood} loss function 
%
\citep{banerjee-06}, 
denoted $\lossnll$ and defined by 
\begin{equation} \label{eq:loss_vs_likelihood}
\lossnll(f_\theta,x,y) \ \equiv\ -\ln{p(y|x,\theta)}\,.
\end{equation}
Then, we can relate the \emph{empirical loss} $\emploss$ of a predictor to its likelihood:
\begin{equation*}
	\emplossnll(\theta) 
	\ = \ \frac1n \sum_{i=1}^n \lossnll(\theta, x_i ,y_i)
	\ = \  -\frac1n \sum_{i=1}^n \ln p( y_i |x_i, \theta)
   \ = \  -\frac1n \ln p(Y|X,\theta) \,,
\end{equation*}
or, the other way around,
\vspace{-2mm}
\begin{equation} \label{eq:dataset_likelihhod}
p(Y|X,\theta) \, = \, e^{-n\, \emplossnll(\theta)}\,. 
\end{equation}

Unfortunately, existing PAC-Bayesian theorems work with bounded loss functions or in very specific contexts \citep[\eg,][]{dalalyan-08,zhang-06},  and $\lossnll$ spans the whole real axis in its general form. In Section~\ref{sec:more-bounds}, we explore PAC-Bayes bounds for unbounded losses. Meanwhile, we consider priors with bounded likelihood. This can be done by assigning a prior of zero to any $\theta$ yielding $\ln \frac{1}{p(y|x,\theta)} \notin [a,b]$. 
%

Now, using Equation~\eqref{eq:dataset_likelihhod} in the optimal posterior (Equation~\ref{eq:post_catoni}) simplifies to
\begin{eqnarray} \label{eq:post_catoni_theta} 
\post^*(\theta)
=
\frac{\prior(\theta) \,e^{-n\,\emplossnll(\theta)}}{\ZDs}\,
\,=\,  
\frac{p(\theta)\,p(Y|X,\theta)}{p(Y|X)}
 = 
p(\theta|\Dsample)\,,
\end{eqnarray}
%
where the normalization constant $\ZDs$ corresponds to the Bayesian \emph{marginal likelihood}:
\begin{equation} \label{eq:post_catoni_Zb} 
\ZDs \ \equiv \ p(Y|X) \ = \ 
\int_{\Theta}\prior(\theta) \,e^{-n\,\emplossnll(\theta)} d \theta\,.
\end{equation}
This shows that the optimal PAC-Bayes posterior given by the generalization bound of Theorem~\ref{thm:pacbayescatoni} coincides with the Bayesian posterior, when one chooses $\lossnll$ as loss function and $\beta\eqdots b{-}a$ (as in Equation~\ref{eq:catoni_ab}).
%
Moreover, using the posterior of Equation~\eqref{eq:post_catoni_theta} inside Equation~\eqref{eq:a_minimser_beta}, we obtain
%
%
\begin{eqnarray} \label{eq:putback} 
& & \hspace{-.8cm} n \Esp_{\theta\sim\post^*} \emplossnll(\theta)  +  \KL(\post^*\|\prior)\\[-1mm]
&=& \nonumber
n  \int_\Theta \tfrac{\prior(\theta) \,e^{-n\,\emplossnll(\theta)}}{\ZDs} \emplossnll(\theta)\, d\theta
+  \int_\Theta \tfrac{\prior(\theta) \,e^{-n\,\emplossnll(\theta)}}{\ZDs} 
\ln \Big[ \tfrac{\prior(\theta) \,e^{-n\,\emplossnll(\theta)}}{\prior(\theta)\, \ZDs} %
\Big]
 d\theta\\[-1mm]
%
%
%
%
%
&=& \nonumber
\int_\Theta \tfrac{\prior(\theta) \,e^{-n\,\emplossnll(\theta)}}{\ZDs} 
\left[\ln\tfrac1{\ZDs}\right]
 d\theta
%
\ = \ \tfrac{\ZDs}{\ZDs}  \ln\tfrac1{\ZDs} \ = \ -\ln{\ZDs}\,.
\end{eqnarray}
In other words, minimizing the PAC-Bayes bound is equivalent to maximizing the marginal likelihood.
Thus, from the PAC-Bayesian standpoint, the latter encodes  a trade-off between the averaged negative log-likelihood loss function and the prior-posterior Kullback-Leibler divergence.
%
%
Note that Equation~\eqref{eq:putback} has been mentioned by \citet{grunwald-2012}, based on an earlier observation of \citet{zhang-06}. However, the PAC-Bayesian theorems proposed by the latter do not bound the generalization loss directly, as the ``classical'' PAC-Bayesian results \cite{catoni-07,mcallester-99,seeger-02} that we extend to regression in forthcoming Section~\ref{sec:more-bounds} (see the corresponding remarks in Appendix~\ref{appendix:related}).

We conclude this section by proposing a compact form of Theorem~\ref{thm:pacbayescatoni} by expressing it in terms of the  marginal likelihood, 
as a direct consequence  of Equation~\eqref{eq:putback}. 
\begin{cor}%
	\label{thm:pacbayescatoni_maglike}
	Given a data distribution $\Dcal$, a parameter set $\Theta$,  a prior distribution $\prior$ over $\Theta$, a $\delta \in (0,1]$, if $\lossnll$ lies in $[a,b]$, we have, with probability at least $1 - \delta$ over the choice of  $(\Dsample) \sim  \Dgen^n $,
	\begin{equation*} %
	\Esp_{\theta\sim\post^*} \genlossnll(\theta) 
			\ \leq \ 
			a + \tfrac{ b-a}{1 - e^{a-b}} \left[ 1-e^a \,\sqrt[n]{\ZDs\,\delta}\,\right] ,
	\end{equation*}		
	where $\post^*$ is the Gibbs optimal posterior (Eq.~\ref{eq:post_catoni_theta}) 
	and $\ZDs$ is the marginal likelihood (Eq.~\ref{eq:post_catoni_Zb}).
\end{cor}
%
%
%
%
%
%
%
%
%
 In Section~\ref{sec:bayesian_model_comp}, we exploit the link between PAC-Bayesian bounds and Bayesian marginal likelihood to expose similarities between both frameworks in the context of model selection.
 Beforehand, next Section~\ref{sec:more-bounds} extends the PAC-Bayesian generalization guarantees to unbounded loss functions. This is mandatory to make our study fully valid, as  the negative log-likelihood loss function is in general unbounded (as well as other common regression losses).
 \section{PAC-Bayesian Bounds for Regression}
\label{sec:more-bounds}

This section aims to extend the PAC-Bayesian results of Section~\ref{section:marginal_likelihood} to real valued unbounded loss.
%
These results are used in forthcoming sections to study $\lossnll$, but they are valid  for broader classes of loss functions. 
Importantly, our new results are focused on regression problems, as opposed to the usual PAC-Bayesian classification framework. 

%
The new bounds are obtained through a recent theorem of \citet{alquier-15}, stated below
(we provide a proof in Appendix~\ref{appendix:pacbayes_proofs} for completeness).
%
%
%
%
\begin{thm}[\citet{alquier-15}] \label{thm:general-alquier}
	Given a distribution $\Dcal$ over  $\Xcal   \times   \Ycal$, a hypothesis set $\Fcal$, a loss function $\loss:\Fcal\times\Xcal\times\Ycal\to\Rbb$,   a prior distribution $\prior$ over $\Fcal$, a $\delta \in (0,1]$, and a real number $\lambda>0$,  
%
%
%
%
%
%
%
%
%
%
%
	with probability at least $1{-}\delta$ over the choice of $(\Dsample)\sim \Dgen^n$,
	we have
	\begin{align} \label{eq:alquier}
	\forall \post \text{ on } \Fcal\colon \quad &
	\Esp_{f\sim\post} \genloss(f) 
	\ \le \  \Esp_{f\sim\post} \emploss(f) +
	\dfrac{1}{\lambda}\!\left[ \KL(\post\|\prior) +
	\ln\dfrac{1}{\delta}
	+ \Psi_{\ell,\prior,\Dgen}(\lambda,n)  \right],\\[2mm]
%
\mbox{where }\quad &
	\label{eq:alquier_assumption}
\Psi_{\ell,\prior,\Dgen}(\lambda,n) \ = \ 
\ln	\Esp_{f\sim\prior} \Esp_{{\Dsampleprim\sim \Dgen^n}} 
	\exp\left[{\lambda\left(\genloss(f) - \emplossprim(f)\right)}\right].
	\end{align}
\end{thm}
%
%
%
\citeauthor{alquier-15} used Theorem~\ref{thm:general-alquier} to design a learning algorithm for $\{0,1\}$-valued classification losses.  
Indeed, a bounded loss function $\loss:\Fcal\times\Xcal\times\Ycal\to[a,b]$ can be used along with Theorem~\ref{thm:general-alquier} by applying the
Hoeffding's lemma to Equation~\eqref{eq:alquier_assumption}, that gives
$
%
\Psi_{\ell,\prior,\Dgen}(\lambda,n) \leq
\lambda^2(b{-}a)^2/(2n).
$
More specifically, with $\lambda\eqdots n$, we obtain the following bound
\begin{align} \label{eq:alquier_hoeffding_n}
\forall \post \text{ on } \Fcal\colon \quad 
\Esp_{f\sim\post} \genloss(f) 
\ \le \  \Esp_{f\sim\post} \emploss(f) +
\tfrac{1}{n}\!\left[  \KL(\post\|\prior) +
\ln\tfrac{1}{\delta}\right]+\tfrac12(b-a)^2.
\end{align}
Note that the latter bound leads to the same trade-off as Theorem~\ref{thm:pacbayescatoni} (expressed by Equation~\ref{eq:a_minimser_beta}).
However, the choice $\lambda\eqdots n$ has the inconvenience that the bound value is at least $\frac12(b-a)^2$, even at the limit $n\to\infty$. With $\lambda\eqdots \sqrt n$ the bound converges (a result similar to Equation~\eqref{eq:alquier_hoeffding_sqrtn} is also formulated by \citet{pentina-14}):
\begin{align} \label{eq:alquier_hoeffding_sqrtn}
\forall \post \text{ on } \Fcal\colon \quad 
\Esp_{f\sim\post} \genloss(f) 
\ \le \  \Esp_{f\sim\post} \emploss(f) +
\tfrac{1}{\sqrt n}\!\left[  \KL(\post\|\prior) +
\ln\tfrac{1}{\delta} +\tfrac12(b-a)^2 \right].
\end{align}
%

%
\paragraph{Sub-Gaussian losses.}
%
In a regression context, it may be restrictive to consider strictly bounded loss functions. Therefore, we extend Theorem~\ref{thm:general-alquier} to \emph{sub-Gaussian} losses.
We say that a loss function~$\loss$ is sub-Gaussian with variance factor $s^2$ under a prior $\prior$ and a data-distribution $\Dcal$ if it can be described by a sub-Gaussian random variable $V{=}\genloss(f){-}\loss(f,x,y)$, \ie, its moment generating function is upper bounded by the one of a normal distribution of variance $s^2$ (see \citet[][Section~2.3]{boucheron-13}):
\begin{equation}\label{eq:subG_assumption}
\psi_{_V}(\lambda)
	\ = \ \ln \Esp e^{\lambda V}%
	\ =  \ \ln	\Esp_{f\sim\prior} \Esp_{{(x,y)\sim \Dgen}} 
	\exp\left[{\lambda\left(\genloss(f) - \loss(f, x,y)\right)}\right]
	\ \leq \ {\tfrac{\lambda^2 s^2}{2}}\,
, \quad \forall \lambda  \in \Rbb\,.
%
%
%
\end{equation}
The above sub-Gaussian assumption corresponds to the \emph{Hoeffding assumption} of \citet{alquier-15}, and allows to obtain the following result.
\begin{cor} \label{cor:alquier-subG}
Given $\Dcal$, $\Fcal$,  $\loss$,  $\prior$ and $\delta$ defined in the statement of  Theorem~\ref{thm:general-alquier}, if the loss is sub-Gaussian with variance factor $s^2$, we have,
with probability at least $1{-}\delta$ over the choice of $(\Dsample)\sim \Dgen^n$,
\begin{align*}%
\forall \post \text{ on } \Fcal\colon \quad 
\Esp_{f\sim\post} \genloss(f) 
\ \le \  \Esp_{f\sim\post} \emploss(f) +
\tfrac{1}{n}\!\left[  \KL(\post\|\prior) +
\ln\tfrac{1}{\delta}\right]+\tfrac12\, s^2\,.
\end{align*}
\end{cor}
\begin{proof}
	For  $i = 1\ldots n$, we denote  $\ell_i$ a \iid realization of the random variable 
	$\genloss(f) - \loss(f, x, y) $. 
%
		\begin{align*}\textstyle
	\Psi_{\ell,\prior,\Dgen}(\lambda,n)
		= \ln \Esp \exp \left[ \tfrac\lambda n \sum_{i=1}^n \ell_i\right]%
		= \ln \prod_{i=1}^n \Esp \exp \left[ \tfrac\lambda n\ell_i\right]
		= \sum_{i=1}^n \psi_{\loss_i}(\tfrac{\lambda}{n})
		\, \leq\, n \tfrac{\lambda^2s^2}{2n^2}
		%
		\,=\,   \tfrac{\lambda^2s^2}{2n},
		\end{align*}
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
	where the inequality comes from the sub-Gaussian loss assumption (Equation~\ref{eq:subG_assumption}).
	The result is then obtained from Theorem~\ref{thm:general-alquier}, with $\lambda\eqdots n$.
%
%
%
%
%
%
%
%
%
%
%
\end{proof}
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


\paragraph{Sub-gamma losses.}
%
%
We say that an unbounded loss function $\loss$ is sub-gamma with a variance factor~$s^2$ and scale parameter $c$, under a prior $\prior$ and a data-distribution $\Dgen$, if it can be described by a 
%
sub-gamma random variable $V$ %
(see \citet[][Section 2.4]{boucheron-13}), that is 
\begin{equation}\label{eq:subgamma_assumption}
\psi_{_V}(\lambda)
\ \leq \ \tfrac{s^2}{c^2}({-}\ln(1{-}\lambda c) - \lambda c)
\ \leq \ {\tfrac{\lambda^2 s^2}{2(1-c\lambda)}}\,,  \qquad \forall \lambda  \in (0, \tfrac1c)\,.
\end{equation}
Under this sub-gamma assumption, we obtain the following new result, which is necessary to study linear regression in the next sections. 
\begin{cor} \label{cor:subgamma}
	Given $\Dcal$, $\Fcal$,  $\loss$,  $\prior$ and $\delta$ defined in the statement of Theorem~\ref{thm:general-alquier}, if the loss is sub-gamma with variance factor $s^2$ and scale $c<1$, we have,
	with probability at least $1{-}\delta$ over 
%
$(\Dsample)\sim \Dgen^n$,
	\begin{align} \label{eq:subgamma}
	\forall \post \text{ on } \Fcal\colon \quad 
	\Esp_{f\sim\post} \genloss(f) 
	\ \le \  \Esp_{f\sim\post} \emploss(f) +
	\tfrac{1}{n}\!\left[  \KL(\post\|\prior) +
	\ln\tfrac{1}{\delta}\right]+\tfrac1{2(1-c)}\, s^2\,.
	\end{align}
As a special case, with $\loss\eqdots\lossnll$ and $\post \eqdots \post^*$ (Equation~\ref{eq:post_catoni_theta}), we have
\begin{equation} \label{eq:subgammaZ} 
		\displaystyle\Esp_{\theta\sim\post^*} \genlossnll(\theta) 
		\, \leq\,
		\tfrac{s^2 }{2(1-c)}- \tfrac{1}{n}\ln\left( \ZDs\, \delta \right).
\end{equation}
\end{cor}
\begin{proof}
Following the same path as in the proof of Corollary~\ref{cor:alquier-subG} (with $\lambda\eqdots n$), we have
		\begin{align*}\textstyle
		\Psi_{\ell,\prior,\Dgen}(n,n)
		= \ln \Esp \exp \left[ \sum_{i=1}^n \ell_i\right]%
		= \ln \prod_{i=1}^n \Esp \exp \left[ \ell_i\right]
		= \sum_{i=1}^n \psi_{\loss_i}(1)
		\, \leq\, n \tfrac{s^2}{2(1-c)}
		%
		\,=\,   \tfrac{n\,s^2}{2(1-c)}\,,
		\end{align*}
	where the inequality comes from the sub-gamma loss assumption, with $ 1\in(0,\frac1c)$.
\end{proof}

\paragraph{Squared loss.}

The parameters $s$ and $c$ of Corollary~\ref{cor:subgamma} rely on the chosen loss function and prior, and the assumptions concerning the data distribution.
As an example, consider a regression problem where $\Xcal{\times}\Ycal \subset \Rbb^d{\times}\Rbb$, a family of linear predictors $f_\wb(\xb) = \wb\cdot\xb$, with $\wb\in\Rbb^d$, and a Gaussian prior 
%
$\Ncal(\zerobf, \sigma_\prior^2\,\Ib)$.  
%
Let us assume that the input examples are generated by $\xb\!\sim\!\Ncal(\zerobf,\sigx^2\,\Ib)$ with label $y=\wb^*\!\cdot \xb+\epsilon$, where $\wb^*\! \in\! \Rbb^d$ and  $\epsilon\!\sim\!\Ncal(0,\sigma_\epsilon^2)$ is a Gaussian noise.
Under the squared loss function
\begin{equation}\label{eq:sqrloss}
\losssqr(\wb,\xb,y) \, =\, (\wb\cdot\xb-y)^2\,,
\end{equation}
we show in Appendix~\ref{appendix:s} that Corollary~\ref{cor:subgamma}  is valid with 
 $s^2 \geq 2\left[\sigx^2(\sigprior^2d+ \|\wb^*\|^2)+ \sigeps^2(1- c)\right]$ and $c \geq 2\sigx^2\sigprior^2$.
As expected, the bound degrades when the noise increases
%
%
%


\paragraph{Regression versus classification.}
\label{section:discussion}


The classical PAC-Bayesian theorems are stated in a classification context and bound the generalization error/loss of the stochastic \emph{Gibbs predictor} $G_\post$.
%
%
%
%
In order to predict the label of an example $x\in\Xcal$, the Gibbs predictor first draws a hypothesis $h\in\Fcal$ according to $\post$, and then returns $h(x)$.
\citet{maurer-04} shows that we can generalize PAC-Bayesian bounds on the generalization risk of the Gibbs classifier to any loss function with output between zero and one. 
%
Provided that $y\in\{-1,1\}$ and $h(x)\in[-1,1]$, a common choice is to use the linear loss function  $\losszozo(h,x,y) = \frac12 - \frac12 y\,h(x)$.
The Gibbs generalization loss is then given by
$R_\Dgen(G_\post) = \Esp_{(x,y)\sim\Dgen}  \Esp_{h\sim\post}  \losszozo(h,x,y) $\,.
%
%
%
%
%
%
Many PAC-Bayesian works use $R_\Dgen(G_\post)$ as a surrogate loss to study the zero-one classification loss of the majority vote classifier  $R_\Dgen(B_\post)$:
%
%
%
\begin{equation} \label{eq:bayes_risk} %
%
R_\Dgen(B_\post) 
\,=\, \Pr_{(x,y)\sim\Dgen} \Big( y\,\Esp_{h\sim\post}h(x) < 0 \Big)
\,=\, \Esp_{(x,y)\sim\Dgen} I\Big[ y\,\Esp_{h\sim\post}h(x) < 0 \Big]\,,
%
%
%
%
\end{equation} 
where $I[\cdot]$ being the indicator function. Given a distribution $\post$, %
an upper bound on the Gibbs risk is converted to an upper bound on the majority vote risk by $R_\Dgen(B_\post)\leq 2 R_\Dgen(G_\post)$ \citep{langford-02}.
In some situations, this \emph{factor of two} may be reached, \ie, $R_\Dgen(B_\post)\simeq 2 R_\Dgen(G_\post)$.
In other situations, we may have $R_\Dgen(B_\post)=0$ even if $R_\Dgen(G_\post) = \frac12{-}\epsilon$ (see \citet{graal-neverending} for an extensive study). 
Indeed, these bounds obtained via the Gibbs risk are exposed to be loose and/or unrepresentative of the majority vote generalization error.\footnote{It is noteworthy that the best PAC-Bayesian empirical bound values are so far obtained by considering a majority vote of linear classifiers, where the prior and posterior are Gaussian \citep{ambroladze-06,graal-icml09,langford-02}, similarly to  the Bayesian linear regression analyzed in Section~\ref{section:linreg}.} 

In the current work, we study regression losses instead of classification ones.
That is, the provided results express upper bounds on $\Esp_{f\sim\post} \genloss(f)$ for any (bounded, sub-Gaussian, or sub-gamma) losses. %
%
%
Of course, one may want to bound the regression loss of the averaged regressor $F_\post(x) = \Esp_{f\sim\post} f(x)$.  In this case, if the loss function $\loss$ is convex (as the squared loss), Jensen's inequality gives
%
$\genloss(F_\post)  \, \leq\, \Esp_{f\sim\post} \genloss(f)\,.$
%
Note that a strict inequality 
%
 replaces the factor two mentioned above for the classification case, due to the non-convex indicator function of Equation~\eqref{eq:bayes_risk}.

Now that we have generalization bounds for real-valued loss functions, we can continue our study linking PAC-Bayesian results to Bayesian inference. 
In the next section, we focus on model selection.


\section{Analysis of Model Selection}

\label{sec:bayesian_model_comp}

We consider $L$ distinct models $\{\model_i\}_{i=1}^L$, each one defined by a set of parameters $\Theta_i$.
The PAC-Bayesian theorems naturally suggest selecting the model that is best adapted for the given task  by evaluating the bound for each model $\{\model_i\}_{i=1}^L$ and selecting the one with the lowest bound \citep{ambroladze-06,mcallester-03a,zhang-06}. 
This is closely linked with the Bayesian model selection procedure, as we showed in Section~\ref{section:marginal_likelihood} that minimizing the PAC-Bayes bound amounts to maximizing the marginal likelihood. %
Indeed, given a collection of $L$ optimal Gibbs posteriors---one for each model---given by Equation~\eqref{eq:post_catoni_theta},
\begin{equation} \label{eq:optpost_modelselect}
p(\theta|X,Y, \model_i) \ \equiv\  \post_i^*(\theta) \, = \, \tfrac1{\Zi} \prior_i(\theta) \,e^{-n\,\emplossnll(\theta)}, \ \mbox{ for } \theta \in \Theta_i\,,
\end{equation}
%
%
%
the Bayesian Occam's razor criteria  \citep{jeffreys-92,mackay-92}  chooses the one with the higher \emph{model evidence}
\begin{equation} \label{eq:model_evidence}
p(Y|X,\model_i) \ \equiv \ \Zi \,=\, 
\int_{\Theta_i} \, \prior_i(\theta)\, e^{-n \,\emploss(\theta)} \, d \theta\,.
\end{equation}
Corollary~\ref{cor:pacbayescatoni_modelselect} below formally links the PAC-Bayesian and the Bayesian model selection.
To obtain this result, we simply use the bound of Corollary~\ref{cor:subgamma} $L$ times, together with $\lossnll$ and Equation~\eqref{eq:putback}.
From the union bound (\emph{a.k.a.} Bonferroni inequality), it is mandatory to compute each bound with a confidence parameter of
$\delta/L$,
to ensure that the final conclusion is valid with probability at least $1{-}\delta$.
%
%
%
\begin{cor}%
	\label{cor:pacbayescatoni_modelselect}
	Given a data distribution $\Dcal$, a family of model parameters $\{\Theta_i\}_{i=1}^L$  and associated priors $\{\prior_i\}_{i=1}^L$---where $\prior_i$ is defined over $\Theta_i$--- , a $\delta \in (0,1]$,  if the loss is sub-gamma with parameters $s^2$ and $c<1$, then, with probability at least $1 - \delta$ over $(\Dsample) \sim  \Dgen^n $, 
	\begin{equation*} %
	\forall i \in \{1,\ldots,L\}\,:\qquad
		\Esp_{\theta\sim\post_i^*} \genlossnll(\theta) 
		\ \leq \ 
%
\tfrac1{2(1-c)}\, s^2 - \tfrac{1}{n}\ln\left( \Zi \tfrac\delta L\right).
	\end{equation*}
	where $\post^*_i$ is the Gibbs optimal posterior (Eq.~\ref{eq:optpost_modelselect})
%
%
%
%
	and $\Zi$ is the marginal likelihood (Eq.~\ref{eq:model_evidence}).
\end{cor}

%
%
%
%
%
%
%
%
%
Hence, under the uniform prior over the $L$ models, choosing the one with the best model evidence is equivalent to choosing the one with the lowest PAC-Bayesian bound. 
%

\paragraph{Hierarchical Bayes.}
To perform proper inference on hyperparameters, we have to rely on the \emph{Hierarchical Bayes} approach. This is done by considering an \emph{hyperprior} $p(\eta)$ over the set of hyperparameters $\Hrm$. Then, the prior $p(\theta|\eta)$ can be conditioned on a choice of hyperparameter $\eta$. 
%
%
%
%
The Bayes rule of Equation~\eqref{eq:bayes_updaterule} becomes
%
%
$p(\theta,\eta|X,Y) 
=\frac{p(\eta)\, p(\theta | \eta) \,p(Y|X,\theta)}{p(Y|X)}\,.$
%
%
%

Under the negative log-likelihood loss function, we can rewrite the results of Corollary~\ref{cor:subgamma} as a generalization bound 
on $\Esp_{\eta\sim\post_0} \Esp_{\theta\sim\post_\eta^*} \genlossnll(\theta) $, where $\post_0(\eta) \propto {\prior_0(\eta) \,\ZDeta}$ is the hyperposterior on~$\Hrm$ and $\prior_0$ the hyperprior. Indeed, Equation~\eqref{eq:subgammaZ} becomes
		\begin{equation} \label{eq:h_bound}
		\Esp_{\theta\sim\post^*} \genlossnll(\theta) 
		=  \Esp_{\eta\sim\post_0^*} \Esp_{\theta\sim\post_\eta^*} \genlossnll(\theta) 
		\ \leq\ 
\tfrac1{2(1-c)}\, s^2 - \tfrac{1}{n}\ln\left( \Esp_{\eta\sim\prior_0}  \ZDeta\,\delta \right). 
%
		\end{equation}

		
%
To relate to the bound obtained in Corollary~\ref{cor:pacbayescatoni_modelselect}, we consider the case of a discrete hyperparameter set $\Hrm = \{\eta_i\}_{i=1}^L$, with a uniform prior $\prior_0(\eta_i)=\frac1L$ (from now on, we regard each hyperparameter $\eta_i$ as the specification of a model $\Theta_i$). 
Then, Equation~\eqref{eq:h_bound} becomes 
		\begin{equation*} %
		 \Esp_{\theta\sim\post^*} \genlossnll(\theta) 
		=  \Esp_{\eta\sim\post_0^*} \Esp_{\theta\sim\post_\eta^*} \genlossnll(\theta) 
		\ \leq\ 
%
\tfrac1{2(1-c)}\, s^2 - \tfrac{1}{n}\ln\left(  \textstyle \sum_{i=1}^L  Z_{X,Y,\eta_i}\,\tfrac \delta L \right). 
		\end{equation*}
%
This bound is now a function of $\sum_{i=1}^L  Z_{X,Y,\eta_i}$ instead of $\max_i Z_{X,Y,\eta_i}$ as in the bound given by the ``best'' model in Corollary~\ref{cor:pacbayescatoni_modelselect}.
This yields a tighter bound, corroborating the Bayesian wisdom that model averaging performs best. %
%
Conversely, when selecting a single hyperparameter $\eta^*\in\Hrm$, the hierarchical representation is equivalent to choosing a deterministic hyperposterior, satisfying $\post_0(\eta^*)=1$ and $0$ for every other values. We then have
\begin{align*}
 \KL(\post||\prior) \ =\ \KL(\post_0||\prior_0) + \Esp_{\eta \sim \post_0}  \KL(\post_\eta||\prior_\eta) 
 \ =\ \ln(L) + \KL(\post_{\eta^*}||\prior_{\eta^*})\,.
\end{align*}
With the optimal posterior for the selected $\eta^*$, we have
\begin{align*}
n \Esp_{\theta \sim \post} \emplossnll(\theta) +\KL(\post||\prior) & \ =\  n \Esp_{\theta \sim \post_\eta^*} \emplossnll(\theta) +\KL(\post_{\eta^*}^*||\prior_{\eta^*}) +\ln(L) \\[-2mm]
&\ =\ -\ln(Z_{X,Y,\eta^*}) + \ln(L) \, =\, -\ln \left( \tfrac{Z_{X,Y,\eta^*}}{L} \right).
\end{align*}
Inserting this result into Equation~\eqref{eq:subgamma}, we fall back on the bound obtained in Corollary~\ref{cor:pacbayescatoni_modelselect}.
Hence, by comparing the values of the bounds, one can get an estimate on the consequence of performing model selection instead of model averaging.

%

%
%

%
%

%
%


%
%
%


%
%
%



\section{Linear Regression}
\label{section:linreg}

In this section, we perform \emph{Bayesian linear regression} using the parameterization of \citet{bishop-2006}.  The output space is $\Ycal\eqdots\Rbb$ and, for an arbitrary input space $\Xcal$, we use a mapping function $\phib:\!\Xcal{\to}\Rbb^d$.  
%

\paragraph{The model.}
Given $(x,y)\in\Xcal\times\Ycal$ and model parameters $\theta\eqdots\tuple{\wb, \sigma}\in\Rbb^d\times\Rbb^+$, we consider the likelihood
%
$p(y|x,\tuple{\wb, \sigma}) = \Ncal(y|\wb\cdot\phib(\xb), \sigma^2)$.
%
%
Thus, the negative log-likelihood loss is
\begin{equation}\label{eq:lossnll_ws}
\lossnll(\dtuple{\wb, \sigma},x,y) \, =\, -\ln{p(y|x,\dtuple{\wb, \sigma})}\
= \tfrac12 \ln(2\pi\sigma^2) + \tfrac1{2\sigma^2} (y - \wb\cdot\phib(x))^2\,.
\end{equation}
For a fixed $\sigma^2$, minimizing Equation~\eqref{eq:lossnll_ws} is equivalent to minimizing the squared loss function of Equation~\eqref{eq:sqrloss}.
%
We also consider an isotropic Gaussian prior of mean $\mathbf0$ and variance~$\sigmaprior^2$:
$p(\wb|\sigmaprior) = \Ncal(\wb|\mathbf0, \sigmaprior^2\Ib)$.
%
%
For the sake of simplicity, we consider fixed parameters~$\sigma^2$ and $\sigmaprior^2$. The Gibbs optimal posterior (see Equation~\ref{eq:post_catoni_theta}) is then given by 
%
%
%
%
%
%
%
%
\begin{equation}\label{eq:post_reg} 
\post^*(\wb) \, \equiv\,  
p(\wb|X,Y,\sigma, \sigmaprior) 
\ = \ 
\tfrac{p(\wb| \sigmaprior)\,p(Y|X,\wb, \sigma)}{p(Y|X,\sigma, \sigmaprior)}%
\ =\ 
\Ncal(\wb\,|\,\wpost, A^{-1})\,,
\end{equation}
where 
%
$A\eqdots\frac1{\sigma^2}\Phib^T\Phib+\frac1{\sigmaprior^2}\Ib$
 ;  $\wpost\eqdots\frac1{\sigma^2}A^{-1}\Phib^T \yb$ 
 ;  $\Phib$ is a $n{\times} d$ matrix such that the $i^{th}$ line is~$\phib(x_i)$\,; 
  $\yb \eqdots [y_1, \ldots y_n]$ is the labels-vector
 ;  and the negative log marginal likelihood is
\begin{align*}
-\ln &\,p(Y|X,\sigma, \sigmaprior)
%
	=
	\tfrac1{2\sigma^2}\|\yb-\Phib\wpost\|^2
	+\tfrac n 2\ln(2\pi \sigma^2)
	+ 
	\tfrac1{2\sigmaprior^2}\|\wpost\|^2
	+ \tfrac12 \log|A|
	+ d\ln\sigmaprior \\
	&=
	\underbrace{
%
%
n\,\emplossnll(\wpost)
		+     \tfrac{1}{2 \sigma^2}  \tr ( \Phib^T \Phib A^{-1})
	}_{n\Esp_{\wb\sim\post^*}\emplossnll(\wb)} 
	{+} 
	\underbrace{
		\tfrac1{2\sigmaprior^2}\tr(A^{-1})
		- \tfrac d2
		+ \tfrac1{2\sigmaprior^2}\|\wpost\|^2
		+ \tfrac12 \log|A|
		+ d\ln\sigmaprior 
	}_{\KL\big(\Ncal(\wpost,A^{-1})\,\|\,\Ncal(\mathbf{0},\sigmaprior^2\Ib)\big)}.
\end{align*}
%
To obtain the second equality, we substitute
$\tfrac1{2\sigma^2}\|\yb{-}\Phib\wpost\|^2{+}\tfrac n 2\ln(2\pi \sigma^2) = n\,\emplossnll(\wpost) $ and insert\\
%
\phantom.\hfill
	$\tfrac{1}{2\sigma^2}  \tr ( \Phib^T \Phib A^{-1}) + \tfrac1{2\sigmaprior^2} \tr(A^{-1}) 
= \tfrac12 \tr ( \tfrac{1}{\sigma^2} \Phib^T \Phib A^{-1} + \tfrac1{\sigmaprior^2} A^{-1}) 
=  \tfrac12 \tr ( A^{-1} A )
= \tfrac d2\,.$\hfill\phantom.\\
%
%
This exhibits how the Bayesian regression optimization problem is related to the minimization of a PAC-Bayesian bound, expressed by a trade-off between
$\Esp_{\wb\sim\post^*}\emplossnll(\wb)$
and
 $\KL\big(\Ncal(\wpost,A^{-1})\,\|\,\Ncal(\mathbf{0},\sigmaprior^2\,\Ib)\big)$.
 See Appendix~\ref{appendix:linreg} for detailed calculations.

\paragraph{Model selection experiment.}
To produce Figures~\ref{fig:a} and ~\ref{fig:b}, we reimplemented the toy experiment of \citet[Section~3.5.1]{bishop-2006}. 
That is, we generated a learning sample of $15$ data points according to $y=\sin(x)+\epsilon$, where $x$ is uniformly sampled in  the interval $[0,2\pi]$ and $\epsilon\sim\Ncal(0,\frac14)$ is a Gaussian noise. We then learn seven different polynomial models applying Equation~\eqref{eq:post_reg}. More precisely, for a polynomial model of degree $d$, we map input $x\in\Rbb$ to a vector $\phib(x) = [1,x^1,x^2,\ldots,x^d]\in\Rbb^{d+1}$, and we fix parameters $\sigmaprior^2=\frac1{0.005}$ and $\sigma^2=\frac12$.
Figure~\ref{fig:a} illustrates the seven learned models.
Figure~\ref{fig:b} shows the negative log marginal likelihood  computed for each polynomial model, and is designed to reproduce \citet[Figure 3.14]{bishop-2006}, where it is explained that the marginal likelihood correctly indicates that the polynomial model of degree $d=3$ is ``the simplest model which gives a good explanation for the observed data''. We show that this claim is well quantified by the trade-off intrinsic to our PAC-Bayesian approach: the complexity $\KL$ term keeps increasing with the parameter $d\in\{1,2,\ldots,7\}$, while the empirical risk drastically decreases from $d=2$ to $d=3$, and only slightly afterward.  Moreover, we show that the generalization risk (computed on a test sample of size $1000$) tends to increase with complex models (for $d\geq4$). 

\begin{figure}\centering
	%
	\subfloat[Predicted models. Black dots are the $15$ training samples.]
	{\includegraphics[width=.48\textwidth]{figure1a-crop.pdf}
		\label{fig:a}}\ \ \ 
	\subfloat[Decomposition of the marginal likelihood into the empirical loss and $\KL$-divergence.]
	{\includegraphics[width=.47\textwidth]{figure1b-crop.pdf}
		\label{fig:b}}
	\vspace{-4mm}
	
	\subfloat[Bound values on a synthetic dataset according to the number of training samples.]
	{\hspace{2mm}\includegraphics[width=.97\textwidth]{figure2november-crop.pdf}	
		\label{fig:c}}
	
	\caption{Model selection experiment (a-b); and comparison of bounds values (c).}
	\label{fig:MS_experiment}
	\vspace{-3.5mm}
\end{figure}


\paragraph{Empirical comparison of bound values.} Figure~\ref{fig:c} compares the values of the PAC-Bayesian bounds presented in this paper on a synthetic dataset, where each input $\xb{\in}\Rbb^{20}$ is generated by a Gaussian $\xb{\sim} \Ncal(\mathbf0,\Ib)$.
%
The associated output $y{\in}\Rbb$ is given by $y{=}\wb^*\!\cdot\xb+\epsilon$, with $\|\wb^*\|{=}\frac1{2}$,  $\epsilon{\sim}\Ncal(0,\sigma_\epsilon^2)$, and $\sigma_\epsilon^2 {=} \frac19$. We perform Bayesian linear regression in the input space, \ie, $\phib(\xb){=}\xb$, fixing $\sigma_\prior^2{=}\frac{1}{100}$ and $\sigma^2{=}2$.
That is, we compute the posterior of Equation~\eqref{eq:post_reg} for training samples of sizes from $10$ to $10^6$.
 For each learned model, we compute the empirical negative log-likelihood loss of Equation~\eqref{eq:lossnll_ws}, and the three PAC-Bayes bounds, with confidence parameter of $\delta{=}\frac1{20}$.
Note that this loss function 
%
is an affine transformation of the squared loss studied in Section~\ref{sec:more-bounds} (Equation~\ref{eq:sqrloss}), \ie,
%
%
%
%
%
%
$\lossnll(\dtuple{\!\wb, \sigma\!},\xb,y) {=} \tfrac12 \ln(2\pi\sigma^2) {+} \tfrac1{2\sigma^2} \losssqr(\wb,\xb,y).$ It turns out that $\lossnll$ is sub-gamma with parameters
$s^2\geq  \frac1{\sigma^2} \big[\sigx^2(\sigprior^2d+ \|\wb^*\|^2)+ \sigeps^2(1- c) \big] $ and 
$c \geq  \tfrac{1}{\sigma^2} (\sigx^2\sigprior^2),$ as shown in Appendix~\ref{appendix:linregsubgamma}. The bounds of 
Corollary~\ref{cor:subgamma} are computed using the above mentioned values of 
$\|\wb^*\|, d,\sigma, \sigx,\sigma_\epsilon, \sigma_\prior$,
leading to $s^2\simeq 0.280$ and $c \simeq 0.005$. As the two other bounds of Figure~\ref{fig:c} are not suited for unbounded loss, we compute their value using a cropped loss $[a,b]=[1,4]$. 
Different parameter values could have been chosen, sometimes leading to another picture: a large value of~$s$ degrades our sub-gamma bound, as a larger $[a,b]$ interval does for the other bounds.\\[1mm]
%
%
In the studied setting, 
%
the bound of Corollary~\ref{cor:subgamma}---that we have developed for (unbounded) sub-gamma losses---gives  tighter guarantees than the two results for $[a,b]$-bounded losses (up to $n{=}10^6$). However, our new bound always maintains a gap of 
%
$\frac1{2(1{-}c)} s^2$ 
between its value and the generalization loss.
The result of Corollary~\ref{thm:pacbayescatoni_maglike} (adapted from \citet{catoni-07}) for bounded losses suffers from a similar gap, while having higher values than our sub-gamma result. Finally, the result of Theorem~\ref{thm:general-alquier} (\citet{alquier-15}), combined with $\lambda=1/\sqrt{n}$ (Eq.~\ref{eq:alquier_hoeffding_sqrtn}), converges to the expected loss, but it provides good guarantees only for large training sample ($n\gtrsim 10^5$). Note that the latter bound is not directly minimized by our ``optimal posterior'', as opposed to the one with $\lambda=1/{n}$ (Eq.~\ref{eq:alquier_hoeffding_n}), for which we observe values 
between $5.8$ (for $n{=}10^6$)  and $6.4$ (for $n{=}10$)---not displayed on Figure~\ref{fig:c}.

\vspace{-1mm}

\section{Conclusion}
 

The first contribution of this paper is to bridge the concepts underlying the Bayesian and the PAC-Bayesian approaches;
%
under proper parameterization, the minimization of the PAC-Bayesian bound maximizes the marginal likelihood. 
This study 
%
motivates the second contribution of this paper, which is to prove PAC-Bayesian generalization bounds for regression with unbounded sub-gamma loss functions, including the squared loss used in regression tasks.\\[1mm]
%
In this work, we studied model selection techniques.  On a broader perspective, we would like to suggest that both Bayesian and PAC-Bayesian frameworks may have more to learn from each other than what has been done lately (even if other works paved the way \cite[\eg,][]{bissiri-16,grunwald-2012,seeger-thesis}). 
%
%
%
%
Predictors learned from the Bayes rule can benefit from strong PAC-Bayesian frequentist guarantees (under the \iid assumption).
Also, the rich Bayesian toolbox may be incorporated in PAC-Bayesian driven algorithms and risk bounding techniques.

\subsubsection*{Acknowledgments}

We thank Gabriel Dubé and Maxime Tremblay for having proofread the paper and supplemental. 
%
%
%

%

%
%
%
%
%
%
%

\newpage

\ifdefined\LONGVERSION\else
\begin{small}
	\setlength{\bibsep}{1.5pt} %
		\bibliographystyle{plainnat}
 %
\bibliography{BPB-refs}
\end{small}

%


\newpage
\fi

\appendix
\section{Supplementary Material}
\label{appendix}

\subsection{Related Work}
\label{appendix:related}

In this section, we discuss briefly other works containing (more or less indirect) links between Bayesian inference and PAC-Bayesian theory, and explain how they relate to the current paper.

\newcommand{\mycite}[2][]{\paragraph{\if&#1&\citeauthor{#2}\else#1\fi\ (\citeyear{#2}) \citep{#2}\,.}}
%
	\mycite[\citeauthor{seeger-02}]{seeger-02,seeger-thesis}
	 Soon after the initial  work of \citet{mcallester-99,mcallester-03a}, Seeger shows how to apply the PAC-Bayesian theorems to bound the generalization error of Gaussian Processes in a classification context.
	%
   By building upon the PAC-Bayesian theorem initially appearing in \citetA{langford&seeger-01-techreport}---where the divergence between the training error and the generalization one is given by the Kullback-Leibler divergence between two Bernoulli distributions---it achieves very tight generalization bounds.\footnote{The PAC-Bayesian results for Gaussian processes are summarized in \citetA[Section~7.4]{rasmussen-06-book}}
   Also, the thesis of \citet[Section 3.2]{seeger-thesis} foresees this by noticing  that ``the log marginal likelihood incorporates a \emph{similar trade-off} as the PAC-Bayesian theorem'', but using another variant of the PAC-Bayes bound and in the context of classification.
%

\mycite{banerjee-06}
This paper shows similarities between the early PAC-Bayesian results (\citet{mcallester-03a}, \citetA{langford&seeger-01-techreport}), and the \emph{Bayesian log-loss bound}  (\citetA{freund-97,kakade-04}). This is done by highlighting that the proof of all these results are strongly relying on the same \emph{compression lemma} \citep[Lemma~1]{banerjee-06}, which is equivalent to our \emph{change of measure}  used in the proof of Theorem~\ref{thm:general-alquier} (see forthcoming Equation~\ref{eq:changeofmeasure}). 
%
Note that the loss studied in the Bayesian part of \citet{banerjee-06} is the negative log-likelihood of Equation~\eqref{eq:loss_vs_likelihood}. Also, as in Equation~\eqref{eq:putback}, the \emph{Bayesian log-loss bound} contains the Kullback-Leibler divergence between the prior and the posterior. However, the latter result is not a generalization bound, but a bound on the training loss that is obtained by computing a surrogate training loss in the specific context of online learning.
Moreover, the marginal likelihood and the model selection techniques are not addressed in \mbox{\citet{banerjee-06}.}

\mycite{zhang-06}
This paper presents a family of information theoretical bounds for \emph{randomized estimators} that have a lot in common with PAC-Bayesian results (although the bounded quantity is not directly the generalization error). Minimizing these bounds leads to the same optimal Gibbs posterior of Equation~\eqref{eq:post_catoni}. The author noted that using the negative log-likelihood (Equation~\ref{eq:loss_vs_likelihood}) leads to the Bayesian posterior, but made no connection with the marginal likelihood.

\mycite{grunwald-2012} This paper proposes the \emph{Safe Bayesian} algorithm, which selects a proper Bayesian \emph{learning rate} --- that is analogous to the parameter $\beta$ of our Equation~\eqref{eq:catoni07}, and the parameter $\lambda$ of our Equation~\eqref{eq:alquier} --- in the context of \emph{misspecified models}.\footnote{The empirical model selection capabilities of the \emph{Safe Bayesian} algorithm has been further studied in \citetA{grunwald-14}.} 
The standard Bayesian inference method is obtained with a fixed learning rate, corresponding to the case $\lambda\eqdots n$ (that is the case we focus on the current paper, see Corollaries~\ref{cor:alquier-subG} and \ref{cor:subgamma}). 
%
The analysis of \citet{grunwald-2012} relies both on the Minimum Description Length principle \citepA{grunwald-07-book} and PAC-Bayesian theory. Building upon the work of \citet{zhang-06} discussed above, they formulate the result that we presented as Equation~\eqref{eq:putback}, linking the marginal likelihood to the inherent PAC-Bayesian trade-off.
However, they do not compute explicit bounds on the generalization loss, which required us to take into account the complexity term of Equation~\eqref{eq:alquier_assumption}.


\mycite{lacoste-thesis} In a binary classification context, it is shown that the parameter $\beta$ of Theorem~\ref{thm:pacbayescatoni} can be interpreted as a Bernoulli label noise model from a Bayesian likelihood standpoint. For more details, we refer the reader to Section~2.2 of this thesis.

\mycite{bissiri-16}
This recent work studies Bayesian inference through the lens of loss functions. When the loss function is the negative log-likelihood (Equation~\ref{eq:loss_vs_likelihood}), the approach of \citet{bissiri-16} coincides with the Bayesian update rule. As mentioned by the authors, there is some connection between their framework and the PAC-Bayesian one, but ``the motivation and construction are very different.''

\paragraph{Other references.}
See also \citetA{grunwald-07,lacostejulien-11,meir-03,ng-01,rousseau-16} for other studies drawing links between frequentist statistics and Bayesian inference, but outside the PAC-Bayesian framework.

\subsection{Proof of Theorem~\ref{thm:general-alquier}}
\label{appendix:pacbayes_proofs}

%

Recall that Theorem~\ref{thm:general-alquier} originally comes from~\citet[Theorem 4.1]{alquier-15}. We present below a different proof that follows the key steps of the very general PAC-Bayesian theorem presented in~\citetA[Theorem 4]{graal-aistats16}.

\begin{proof}[Proof of Theorem~\ref{thm:general-alquier}]
	 The \emph{Donsker-Varadhan's change of measure} states that, for any measurable function $\phi:\Fcal\to\Rbb$, we have
	 \begin{equation} \label{eq:changeofmeasure}
	 \Esp_{f\sim\post} \phi(f) \ \leq\ \KL(\post\|\prior) + \ln\left(\Esp_{f\sim\prior}e^{\phi(f)}\right).
	 \end{equation}
	Thus, with
	$\phi(f){\eqdots} \lambda\big(\genloss(f){-}\emploss(f)\big)$, we obtain
	$\forall\, \post \mbox{ on }\Fcal :$
	\begin{eqnarray*}
		\lambda\, \big(	\Esp_{f\sim\post} \genloss(f)-	\Esp_{f\sim\post} \emploss(f)\big) %
		&=&
		\Esp_{f\sim \post} \lambda\, \big(\genloss(f)-\emploss(f)\big)
		\\
		&\leq&
		\KL(\post\|\prior) + \ln \bigg(
		%
		\Esp_{f\sim \prior} e^{\lambda\, \big(\genloss(f)-\emploss(f)\big)}
		%
		\bigg)\,.
	\end{eqnarray*}
	Now, we apply Markov's inequality on the random variable $\displaystyle\zeta_\prior(\Dsample) \eqdots \Esp_{\mathclap{f\sim \prior}} e^{\lambda \big(\genloss(f)-\emploss(f)\big)}$:
%
\begin{equation*}
		\Pr_{\Dsample\sim \Dgen^n} \left(
		\zeta_\prior(\Dsample)\,\le\,
		\frac{1}{\delta}\Esp_{{\Dsampleprim\sim \Dgen^n}}\zeta_\prior(\Dsampleprim)
		\right)\, \geq\, 1-\delta\,.
\end{equation*}

This  implies that with probability at least $1{-}\delta$ over the choice of $\Dsample\sim \Dgen^n$,
	we have
	$\forall\, \post \mbox{ on }\Fcal :$
	\begin{equation*}\label{eq:presque}
	\Esp_{f\sim\post} \genloss(f) \ \leq\ 	\Esp_{f\sim\post} \emploss(f) 
	+ \frac1\lambda\left[
	\KL(\post\|\prior) + \ln \frac{\displaystyle\Esp_{\Dsampleprim\sim \Dgen^n} \zeta_\prior(\Dsampleprim)}{\delta}
	\right].
	\end{equation*}
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
	%
	%
	%
\end{proof}

\subsection{Proof of Equations~\eqref{eq:alquier_hoeffding_n} and \eqref{eq:alquier_hoeffding_sqrtn}}
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\begin{proof}%
	Given a loss function $\loss:\Fcal\times \Xcal\times \Ycal$, and a fixed predictor $f\in\Fcal$, we consider the random experiment of sampling $(x,y)\in\Dgen$.  We denote  $\ell_i$ a realization of the random variable $\genloss(f) - \loss(f, x, y)$, for  $i = 1\ldots n$. Each $\ell_i$ is \iid,  zero mean, and bounded by $a-b$ and $b-a$, as $\ell(f, x, y) \in [a,b]$.  Thus,
	{\allowdisplaybreaks[4]
		\begin{eqnarray*}
			%
%
			\Esp_{{\Dsampleprim\sim \Dgen^n}} 
			\exp\left[{\lambda\left(\genloss(f) - \emplossprim(f)\right)}\right]%
			&=& \Esp \exp \left[ \frac\lambda n \sum_{i=1}^n \ell_i\right]\\
			&=& \prod_{i=1}^n \Esp \exp \left[ \frac\lambda n\ell_i\right]\\
			&\leq& \prod_{i=1}^n  \exp \left[ \frac{\lambda^2(a-b-(b-a))^2}{8n^2}\right]
			%
			\\
			&=& \prod_{i=1}^n  \exp \left[ \frac{\lambda^2(b-a)^2}{2n^2}\right]\\
			&=&  \exp \left[ \frac{\lambda^2(b-a)^2}{2n}\right],
		\end{eqnarray*}
	}%
	where the inequality comes from Hoeffding's lemma.
	
	With $\lambda\eqdots n$, Equation~\eqref{eq:alquier} becomes Equation~\eqref{eq:alquier_hoeffding_n} :
	\begin{align*} 
	%
	\Esp_{f\sim\post} \genloss(f) 
	&\ \le \  \Esp_{f\sim\post} \emploss(f) +
	\dfrac{1}{n}\!\left[ \KL(\post\|\prior) +
	\ln\dfrac{1}{\delta} + \frac{n^2(b-a)^2}{2n}\right]\\
	&\ = \  \Esp_{f\sim\post} \emploss(f) +
	\dfrac{1}{n}\!\left[ \KL(\post\|\prior) +
	\ln\dfrac{1}{\delta} \right] + \frac12 (b-a)^2\,.%
	\end{align*}
	Similarly, with $\lambda\eqdots \sqrt n$, Equation~\eqref{eq:alquier} becomes Equation~\eqref{eq:alquier_hoeffding_sqrtn} .
\end{proof}



%
%
%
%
%
%
%
%
%
%
%
%
%
%


\subsection{Study of the Squared Loss}
\label{appendix:s}

\newcommand{\mzx}{\mu_{z|\xb}}
\newcommand{\szx}{\sigma_{z|\xb}}
\newcommand{\mbar}{\bar\mu}
\newcommand{\sbar}{\bar\sigma}


We consider a regression problem where $\Xcal\times\Ycal \subset \Rbb^d\times\Rbb$, a family of linear predictors $f_\wb(\xb) = \wb\cdot\xb$, with $\wb\in\Rbb^d$, and a Gaussian prior 
$\Ncal(\zerobf, \sigma_\prior^2\,\Ib)$.  
Let us assume that the input examples 
are generated according to $\Ncal(\zerobf, \sigx^2\Ib)$
%
and  $\epsilon\sim\Ncal(0,\sigma_\epsilon^2)$ is a Gaussian noise.

%
We study the squared loss $\losssqr(\wb,\xb,y) \, =\, (\wb\cdot\xb-y)^2$ such that:
\begin{itemize}
	\item $\wb \sim \Ncal(\zerobf, \sigma_\prior^2\,\Ib)$ is given by the prior $\prior$, 
	%
%
	\item $\xb\sim  \Ncal(\zerobf, \sigx^2\Ib)$ (and $\xb\in\Rbb^d$),
	%
	\item $y = \wb^* \!\cdot \xb + \epsilon$, where $\epsilon\sim\Ncal(0,\sigma_\epsilon^2)$, corresponds to the labeling function.\\
	Thus $y|\xb\sim\Ncal (\xb\cdot \wb^*, \sigma_\epsilon^2)$.
\end{itemize}


Let us consider the random variable $v = \big[ \Esp_\xb \Esp_{y|\xb} \losssqr(\wb,\xb,y)  \big] - \losssqr(\wb,\xb,y)$.
To show that $v$ is a sub-gamma random variable, we will find  values of $c$ and $s$ such that the criterion of Equation~\eqref{eq:subgamma_assumption} is fulfilled, \ie,
\begin{equation*}
\psi_{v}(\lambda)
\ = \ \ln \Esp e^{\lambda v}
%
\ \leq \ {\tfrac{\lambda^2 s^2}{2(1-c\lambda)}}\,,  \qquad \forall \lambda  \in (0, \tfrac1c)\,.
\end{equation*}

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

We have,
{%
	\begin{align} 
	\psi_v(\lambda) 
	\ &=\ \nonumber
	\ln \Esp_\xb \Esp_{y|\xb} \Esp_\wb \exp\Big( \lambda \big[ \Esp_\xb \Esp_{y|\xb} (y-\wb\cdot \xb )^2  \big] - \lambda(y-\wb\cdot \xb )^2  \Big)\\
	\ &\leq\ \nonumber
	\ln \Esp_\wb \exp\Big( \lambda  \Esp_\xb \Esp_{y|\xb} (y-\wb\cdot \xb )^2\Big) \\
	\ &=\ \nonumber
	\ln \Esp_\wb \exp\Big( \lambda  \Esp_\xb [\xb\cdot(\wb^*-\wb)]^2 + \lambda\sigeps^2 \Big) \\
	&=\ \nonumber
	\ln  \Esp_\wb \exp\Big( \lambda  \sigx^2\|\wb^*-\wb\|^2 + \lambda\sigeps^2 \Big) \\
	%
	\ & = \ \nonumber
	\ln  \frac1{(1-2\lambda\sigx^2\sigprior^2)^{\frac d2}}  \exp\Big(\frac{ \lambda  \sigx^2 \|\wb^*\|^2}{1-2\lambda\sigx^2\sigprior^2}  + \lambda\sigeps^2 \Big) \\
	%
	&=\ \nonumber
	- \frac d2 \ln  (1-2\lambda\sigx^2\sigprior^2)
	+ \frac{ \lambda  \sigx^2 \|\wb^*\|^2}{1-2\lambda\sigx^2\sigprior^2} + \lambda\sigeps^2 \\
	&\leq\ \nonumber
	\frac{\lambda\sigx^2\sigprior^2d}{1-2\lambda\sigx^2\sigprior^2}
	+ \frac{ \lambda  \sigx^2 \|\wb^*\|^2}{1-2\lambda\sigx^2\sigprior^2} + \lambda\sigeps^2 \\
	&=\ \nonumber
	\frac{\lambda(\sigprior^2\sigx^2 d+\sigx^2 \|\wb^*\|^2+ (1-2\lambda\sigx^2\sigprior^2)\sigeps^2)}{1-2\lambda\sigx^2\sigprior^2}\\
	&=\nonumber
	\frac{\lambda^2 s^2}{2(1-\lambda c)}\,,
	\end{align}
}%
with $s^2 = \dfrac2\lambda\left[\sigx^2(\sigprior^2d+ \|\wb^*\|^2)+ \sigeps^2(1-\lambda c)\right]$ and $c = 2\sigx^2\sigprior^2$\,. 

Recall that Corollary~\ref{cor:subgamma} is obtained with $\lambda\eqdots 1$. 

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\subsection{Linear Regression : Detailed calculations}
\label{appendix:linreg}

%
Recall that, from Equation~\eqref{eq:post_reg}, the Gibbs optimal posterior of the described model is given by 
$$p(\wb|X,Y,\sigma, \sigmaprior)  =\Ncal(\wb\,|\,\wpost, A^{-1})\,,$$
with
$A\eqdots\frac1{\sigma^2}\Phib^T\,\Phib+\frac1{\sigmaprior^2}\Ib$
;  $\wpost\eqdots\frac1{\sigma^2}A^{-1}\Phib^T \yb$ 
;  $\Phib$ is a $n{\times} d$ matrix such that the $i^{th}$ line is~$\phib(x_i)$\,; 
$\yb \eqdots [y_1, \ldots y_n]$ is the labels-vector. For the complete derivation leading to this posterior distribution, see \citet[Section~3.3]{bishop-2006} or \citetA[Section~2.1.1]{rasmussen-06-book}. 
 
 \paragraph{Marginal likelihood.}
 We decompose of the marginal likelihood into the PAC-Bayesian trade-off:
\begin{align}
&\!\!\!\!-\ln \,p(Y|X,\sigma, \sigmaprior) \nonumber \\ 
%
&\ = \tag{$\dagger$}\label{eq:classic}
\tfrac1{2\sigma^2}\|\yb-\Phib\wpost\|^2
+\tfrac n 2\ln(2\pi \sigma^2)
+ 
\tfrac1{2\sigmaprior^2}\|\wpost\|^2
+ \tfrac12 \log|A|
+ d\ln\sigmaprior \\
&\ =
\underbrace{
	%
	%
	n\,\emplossnll(\wpost)
	+   \blue{  \tfrac{1}{2 \sigma^2}  \tr ( \Phib^T \Phib A^{-1}) }
}_{\green{n\Esp_{\wb\sim\post^*}\emplossnll(\wb)} }
\blue{+} 
\underbrace{
	\blue{\tfrac1{2\sigmaprior^2}\tr(A^{-1})
	- \tfrac d2}
	+ \tfrac1{2\sigmaprior^2}\|\wpost\|^2
	+ \tfrac12 \log|A|
	+ d\ln\sigmaprior 
}_{\red{\KL\big(\Ncal(\wpost,A^{-1})\,\|\,\Ncal(\mathbf{0},\sigmaprior^2\Ib)\big)}}\,.
\tag{$\star$} 
\label{eq:star}
\end{align}
Line~(\ref*{eq:classic}) corresponds to the classic form of the negative log marginal likelihood in a Bayesian linear regression context (see \citet[Equation 3.86]{bishop-2006}).

\mbox{Line~(\ref*{eq:star}) introduces three terms that cancel out\,: 
%
$\blue{\tfrac{1}{2 \sigma^2}  \tr \left( \Phib^T \Phib A^{-1}\right) + \tfrac1{2\sigmaprior^2} \tr\left(A^{-1}\right) - \tfrac 12 d} = 0\,.$}
%
The latter equality follows from the trace operator properties and the definition of matrix $A$:
\begin{align*}
\blue{\tfrac{1}{2 \sigma^2}  \tr \left( \Phib^T \Phib A^{-1}\right) + \tfrac1{2\sigmaprior^2} \tr\left(A^{-1}\right) }
&=  \tr \left( \tfrac{1}{2 \sigma^2} \Phib^T \Phib A^{-1} + \tfrac1{2\sigmaprior^2} A^{-1}\right) \\
& =  \tr \left( \tfrac12 A^{-1} ( \tfrac{1}{ \sigma^2} \Phib^T \Phib +\tfrac1{\sigmaprior^2}\Ib)\right) \\
& =  \tr \left( \tfrac12 A^{-1} A \right)\\
& = \blue{\tfrac 12\, d} \,.
\end{align*}		

We show below that the expected loss \green{$\Esp_{\wb\sim\post^*}\emplossnll(\wb)$} corresponds to the left part of Line~(\ref*{eq:star}). Note that a proof of equality 
$ \Esp_{\wb \sim  \post}   \wb^T \Phib^T  \Phib \wb = \tr \left( \Phib^T \Phib A^{-1} \right) + \wpost^T \Phib^T  \Phib \wpost $ (Line~\ref*{eq:quadform} below),
known as the ``expectation of the quadratic form'',
can be found in \citetA[Theorem 1.5]{seber-03}.

{\allowdisplaybreaks[4]
	\begin{align*}
%
\green{n \Esp_{\wb\sim\post} \emplossnll(\wb) }
&= \Esp_{\wb \sim \post} \sum_{i=1}^n -\ln p(y_i|x_i,\wb) \\
& = \Esp_{\wb \sim \post} \left( \frac{n}{2} \ln(2 \pi \sigma^2 ) +  \frac{1}{2 \sigma^2} \sum_{i=1}^n \left(y_i -  \wb \cdot \phib(\xb_i) \right)^2 \right) \\
& =   \frac{n}{2} \ln(2 \pi \sigma^2 ) +   \frac{1}{2 \sigma^2} \Esp_{\wb \sim \post}\left\Vert \yb -  \Phib \wb  \right\Vert ^2  \\
& = \frac{n}{2} \ln(2 \pi \sigma^2 ) +    \frac{1}{2 \sigma^2}\Esp_{\wb \sim \post}\left ( \Vert \yb \Vert^2  - 2 \yb  \Phib \wb +  \wb^T \Phib^T  \Phib \wb  \right) \\
& = \frac{n}{2} \ln(2 \pi \sigma^2 ) +   \frac{1}{2 \sigma^2}\left( \Vert \yb \Vert^2  - 2 \yb  \Phib \widehat{\wb} + \Esp_{\wb \sim  \post}   \wb^T \Phib^T  \Phib \wb  \right) \\
& = \frac{n}{2} \ln(2 \pi \sigma^2 ) +    \frac{1}{2 \sigma^2} \left( \Vert \yb \Vert^2  - 2 \yb  \Phib \widehat{\wb} + \tr \left( \Phib^T \Phib A^{-1} \right) + \wpost^T \Phib^T  \Phib \wpost \right) \tag{$\clubsuit$} \label{eq:quadform} \\
& = \frac{n}{2} \ln(2 \pi \sigma^2 ) +    \frac{1}{2 \sigma^2} \left\Vert \yb -  \Phib \wpost  \right\Vert ^2 +    \frac{1}{2 \sigma^2}  \tr \left( \Phib^T \Phib A^{-1} \right)\\
& = n\,\emplossnll(\wpost) +    \frac{1}{2 \sigma^2}  \tr \left( \Phib^T \Phib A^{-1}\right).
\end{align*}
}

Finally, the right part of Line~(\ref*{eq:star}) is equal to the Kullback-Leibler divergence between the two multivariate normal distributions
$\Ncal(\wpost,A^{-1})$ and $\Ncal(\mathbf{0},\sigmaprior^2\Ib)$\,:
\begin{align*}
\red{\KL\big(\Ncal(\wpost,A^{-1})\,\|\,\Ncal(\mathbf{0},\sigmaprior^2\Ib)\big)}
&= \frac12\left(
\tr\left((\sigmaprior^2\Ib)^{-1}A^{-1}\right)
+ 				\frac1{\sigmaprior^2}\|\wpost\|^2
- d
+  \log\frac{|\sigmaprior^2\Ib|}{|A|}
%
\right)\\
&= \frac12\left(
\frac1{\sigmaprior^2}\tr\left(A^{-1}\right)
+ 				\frac1{\sigmaprior^2}\|\wpost\|^2
- d
+  \log{|A|}
+d\ln\sigmaprior^2 
\right).
\end{align*}		

\subsection{Linear Regression: PAC-Bayesian sub-gamma bound coefficients}
\label{appendix:linregsubgamma}

We follow then exact same steps as in Section~\ref{appendix:s}, except that we replace the random variable $v$ (giving the squared loss value) by a random variable $v'$ giving the value of the loss
\begin{equation*}%
\lossnll(\dtuple{\wb, \sigma},\xb,y) 
%
= \tfrac12 \ln(2\pi\sigma^2) + \tfrac1{2\sigma^2} (y - \wb\cdot\xb)^2\,,
\end{equation*}
where $\wb$, $\xb$ and $y$ are generated as described in Section~\ref{appendix:s}. We aim to find the values of $c$ and $s$ such that the criterion of Equation~\eqref{eq:subgamma_assumption} is fulfilled, \ie,
\begin{equation*}
\psi_{v'}(\lambda)
\ = \ \ln \Esp e^{\lambda v'}
%
\ \leq \ {\tfrac{\lambda^2 s^2}{2(1-c\lambda)}}\,,  \qquad \forall \lambda  \in (0, \tfrac1c)\,.
\end{equation*}
We obtain
	\begin{align} 
\psi_{v'}(\lambda) 
\ &=\ \nonumber
\ln \Esp_\xb \Esp_{y|\xb} \Esp_\wb \exp\Big( \tfrac{\lambda}{2\sigma^2} \big[ \Esp_\xb \Esp_{y|\xb} (y-\wb\cdot \xb )^2  \big] - \lambda(y-\wb\cdot \xb )^2  \Big)\\
\ &\leq\ \nonumber
\ln \Esp_\wb \exp\Big( \tfrac{\lambda}{2\sigma^2}  \Esp_\xb \Esp_{y|\xb} (y-\wb\cdot \xb )^2\Big) \\
&\vdots\\
&=\ \nonumber
\frac{\tfrac{\lambda}{2\sigma^2}(\sigprior^2\sigx^2 d+\sigx^2 \|\wb^*\|^2+ (1-2\tfrac{\lambda}{2\sigma^2}\sigx^2\sigprior^2)\sigeps^2)}{1-2\tfrac{\lambda}{2\sigma^2}\sigx^2\sigprior^2}\\
&=\nonumber
\frac{\lambda^2 s^2}{2(1-\lambda c)}\,,
\end{align}
with 
\begin{align*}
c &\,=\, \tfrac{1}{2\sigma^2}\Big[  2\sigx^2\sigprior^2\Big]
\ =\  \tfrac{1}{\sigma^2} (\sigx^2\sigprior^2)\,,\\
s^2 &\,=\,  \tfrac{1}{2\sigma^2} \Big[\dfrac2\lambda\left[\sigx^2(\sigprior^2d+ \|\wb^*\|^2)+ \sigeps^2(1-\lambda c)\right]\Big] \ = \ 
 \dfrac1{\lambda\sigma^2} \Big[\sigx^2(\sigprior^2d+ \|\wb^*\|^2)+ \sigeps^2(1-\lambda c) \Big] 
\end{align*}
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%



\ifdefined\LONGVERSION
\begin{small}
	\setlength{\bibsep}{2pt} %
	\bibliographystyle{plainnat}
	%
	\bibliography{BPB-refs}
\end{small}
\else
\begin{small}
	\setlength{\bibsep}{2pt} %
	\renewcommand\refnameA{Supplementary Material References}
	\bibliographystyleA{plainnat}
	%
	\bibliographyA{BPB-refs}
\end{small}
\fi

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


\end{document}



%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
\documentclass{article}

%
\usepackage{times}
%
\usepackage{graphicx} %
%
\usepackage{subfigure} 

%
\usepackage{natbib}

%
\usepackage{algorithm}
\usepackage{algorithmic}

%
%
%
%
%

%
%
\newcommand{\theHalgorithm}{\arabic{algorithm}}

%
%
%
%

%
%
%
\usepackage[accepted]{icml2016}


%
%
\icmltitlerunning{Beyond CCA: Moment Matching for Multi-View Models}





%
\newcommand{\ppp}{\textbf}
\usepackage{natbib}
\usepackage{multibib}
\newcites{sup}{Supplementary References} 

\usepackage{tikz}
\usepackage{tkz-base}
\usetikzlibrary{arrows}
\usetikzlibrary{fit}
\usetikzlibrary{decorations}

\usepackage{times}

\usepackage{graphicx} 
\usepackage{subfigure} 

\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{hyperref}
\usepackage{url}

\usepackage{color}
\definecolor{darkcerulean}{rgb}{0.03, 0.27, 0.49} 
\hypersetup{
    colorlinks=true, 
    linkcolor=darkcerulean,  
    citecolor=darkcerulean,  
    filecolor=darkcerulean,  
    urlcolor=darkcerulean,   
}
\definecolor{backcolor}{RGB}{188, 211, 229}
\newcommand{\piccolor}{backcolor!50}

\usepackage{amssymb,amsmath,amsthm}
\newtheorem{theorem}{Theorem}
\usepackage{bbm}

\newcommand{\one}{1}
\newcommand{\zero}{0}
\newcommand{\norm}[1]{\left\|#1\right\|}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\inner}[1]{\left\langle#1\right\rangle}
\newcommand{\innerp}[1]{\langle#1\rangle}
\newcommand{\Rra}[1]{\left(#1\right)}
\newcommand{\rbra}[1]{\left(#1\right)}
\newcommand{\sbra}[1]{\left[#1\right]}
\newcommand{\cbra}[1]{\left\{#1\right\}}
\newcommand{\wt}[1]{\widetilde{#1}}
\newcommand{\wh}[1]{\widehat{#1}}
\newcommand{\diag}{\mathrm{diag}}
\newcommand{\hess}[2]{\mathrm{Hess}_{#1}(#2)}
\newcommand{\off}{\mathrm{Off}}
\def\argmin{\mathop{\rm arg\,min}\limits}
\newcommand{\R}{\mathbb{R}}
\newcommand{\ebb}{\mathbb{E}}       
\newcommand{\webb}{\wh{\ebb}}       
\newcommand{\var}{\mathrm{var}}     
\newcommand{\wvar}{\wh{\var}}       
\newcommand{\cov}{\mathrm{cov}}     
\newcommand{\wcov}{\wh{\cov}}       
\newcommand{\cum}{\mathrm{cum}}     
\newcommand{\wcum}{\wh{\cum}}       
\newcommand{\sumk}{\sum\mathop{}_{k}}
\newcommand{\summ}{\sum\mathop{}_{m}}
\newcommand{\sumn}{\sum\mathop{}_{n}}
\newcommand{\sump}{\sum\mathop{}_{p}}
\newcommand{\poi}{\mathrm{Poisson}}
\newcommand{\gam}{\mathrm{Gamma}}
\newcommand{\dir}{\mathrm{Dirichlet}}
\newcommand{\acal}{\mathcal{A}}
\newcommand{\bcal}{\mathcal{B}}
\newcommand{\ccal}{\mathcal{C}}
\newcommand{\ecal}{\mathcal{E}}
\newcommand{\ncal}{\mathcal{N}}
\newcommand{\tcal}{\mathcal{T}}
\newcommand{\ci}{\perp\!\!\!\perp}

\makeatletter
\newenvironment{mi}{%
  \begin{list}{-}{}
  \let\olditem\item
}{%
  \end{list}
}
\makeatother


\begin{document} 

\twocolumn[
\icmltitle{Beyond CCA: Moment Matching for Multi-View Models}

\icmlauthor{Anastasia Podosinnikova}{anastasia.podosinnikova@inria.fr}
\icmlauthor{Francis Bach}{francis.bach@inria.fr}
\icmlauthor{Simon Lacoste-Julien}{firstname.lastname@inria.fr}
\icmladdress{INRIA - \'Ecole normale sup\'erieure, Paris}

\icmlkeywords{cca, canonical correlation analysis, multi-view models, moment matching, joint diagonalization, cumulant tensors, cp tensor decomposition}

\vskip 0.3in
]

\begin{abstract}
\noindent
We introduce three novel semi-parametric extensions of probabilistic canonical correlation analysis with identifiability guarantees. We consider moment matching techniques for estimation in these models.
For that, by drawing explicit links between the new models and a discrete version of independent component analysis~(DICA), we first extend the~DICA cumulant tensors to the new discrete version of CCA. 
By further using a close connection with independent component analysis, we introduce generalized covariance matrices, which can replace the cumulant tensors in the moment matching framework, and, therefore, improve sample complexity and simplify derivations and algorithms significantly. 
As the tensor power method or orthogonal joint diagonalization are not applicable in the new setting, we use non-orthogonal joint diagonalization techniques for matching the cumulants. We demonstrate performance of the proposed models and estimation techniques on experiments with both synthetic and real datasets.  
\end{abstract} 





\section{Introduction}
Canonical correlation analysis (CCA), originally introduced by \citet{Hot1936}, is a common statistical tool for the analysis of  multi-view data. Examples of such data include, for instance, representation of some text in two languages \citep[e.g.,][]{VinEtAl2003} or images aligned with text data \citep[e.g.,][]{HarEtAl2004,GonEtAl2014}.
Given two multidimensional variables (or datasets), CCA finds two linear transformations (factor loading matrices) that mutually maximize the correlations between the transformed variables (or datasets). 
Together with its kernelized version \citep[see, e.g.,][]{ChrSha2004,BacJor2002}, CCA has  a wide range of applications (see, e.g., \citet{HarEtAl2004} for an overview).

\citet{BacJor2005} provide a probabilistic interpretation of CCA: they show that the maximum likelihood estimators of a particular Gaussian graphical model, which we refer to as Gaussian CCA, is equivalent to the classical CCA by \citet{Hot1936}. 
The key idea of Gaussian CCA is to allow some of the covariance in the two observed variables to be  explained by a linear transformation of common independent sources, while the rest of the covariance of each view is explained by their own (unstructured) noises. Importantly, the dimension of the common sources is often significantly smaller than the dimensions of the observations and, potentially, than the dimensions of the noise.
Examples of applications and extensions of Gaussian CCA are the works by \citet{SocFei2010}, for mapping visual and textual features to the same latent space, and \citet{HagEtAl2008}, for  machine translation applications. 

Gaussian CCA is subject to some well-known unidentifiability issues, in the same way as the closely related factor analysis model~\citep[FA;][]{Bar1987,Bas1994} and its special case, the probabilistic principal component analysis model \citep[PPCA;][]{TipBis1999,Row1998}.
Indeed, as FA and PPCA are identifiable only up to multiplication by any rotation matrix, Gaussian CCA is only identifiable up to multiplication by any invertible matrix. 
Although this unidentifiability does not affect the predictive performance of the model, it does affect the factor loading matrices and hence the interpretability of the latent factors. In FA and PPCA, one can enforce additional constraints to recover unique factor loading matrices \citep[see, e.g.,][]{Mur2012}.
A notable identifiable version of FA is independent component analysis \citep[ICA;][]{Jut1987,JutHer1991,ComJut2010}.
One of our goals is to introduce identifiable versions of  CCA.

The main contributions of this paper are as follows. We first introduce  for the first time, to the best of our knowledge, three new formulations of CCA: \emph{discrete, non-Gaussian, and mixed}  (see Section~\ref{sec:models}). We then provide \emph{ identifiability guarantees} for the new models (see Section~\ref{sec:identifiability}). Then, in order to use a moment matching framework for estimation, we first derive a \emph{new set of cumulant tensors} for the discrete version of CCA (Section~\ref{sec:cumulants}). We further replace these tensors with their approximations by \emph{generalized covariance matrices} for all three new models (Section~\ref{sec:generalized-covariances}). Finally, as opposed to standard approaches, we use a particular type of \emph{non-orthogonal joint diagonalization algorithms} for extracting the model parameters from the cumulant tensors or their approximations (Section~\ref{sec:jd}).

\ppp{Models.}
The new CCA models are adapted to applications where one or both of the data-views are either counts, like in the bag-of-words representation for text, or continuous data, for instance, any continuous representation of images. A key feature of CCA compared to joint PCA is the focus on modeling the common variations of the two views, as opposed to modeling all variations (including joint and marginal ones).

\ppp{Moment matching.} Regarding parameter estimation, we use the method of moments, also known as ``spectral methods.'' It recently regained popularity as an alternative to other estimation methods for graphical models, such as approximate variational inference or MCMC sampling. 
Estimation of a wide range of models is possible within the moment matching framework: 
ICA \citep[e.g.,][]{CarCom1996,ComJut2010}, 
mixtures of Gaussians \citep[e.g.,][]{AroKan2005,HsuKak2013}, 
latent Dirichlet allocation and topic models \citep{AroEtAl2012,AroEtAl2013,AnaEtAl2012,PodEtAl2015}, 
supervised topic models \citep{WanZhu2014}, 
Indian buffet process inference \citep{TunSmo2014},
stochastic languages \citep{BalEtAl2014},
mixture of hidden Markov models \citep{SubEtAl2014},  
neural networks   \citep[see, e.g.,][]{AnaSed2015,JanEtAl2016},
and other models \citep[see, e.g.,][and references therein]{AnaEtAl2014}.

Moment matching algorithms for estimation in graphical models mostly consist of two main steps: (a) construction of moments or cumulants with a particular diagonal structure and (b) joint diagonalization of the sample estimates of the moments or cumulants to estimate the parameters. 

\ppp{Cumulants and generalized covariance matrices.} By using the close connection between ICA and CCA, we first derive in Section~\ref{sec:cumulants} the cumulant tensors for the discrete version of CCA from the cumulant tensors of a discrete version of ICA (DICA) proposed by \citet{PodEtAl2015}. Extending the ideas from the ICA literature~\citep{Yer2000,TodHer2013}, we further generalize in Section~\ref{sec:generalized-covariances} cumulants as the derivatives of the cumulant generating function. This allows us to replace cumulant tensors with ``generalized covariance matrices'', while preserving the rest of the framework. As a consequence of working with the second-order information only, the derivations and algorithms get significantly simplified and the sample complexity potentially improves.

\ppp{Non-orthogonal joint diagonalization.} When estimating model parameters, both CCA cumulant tensors and generalized covariance matrices for CCA lead to non-symmetric approximate joint diagonalization problems. Therefore, the workhorses of the method of moments in similar context --- orthogonal diagonalization algorithms, such as the tensor power method \cite{AnaEtAl2014},  and orthogonal joint diagonalization \citep{BunEtAl1993,CarSou1996} --- are not applicable. As an alternative, we use a particular type of non-orthogonal Jacobi-like joint diagonalization algorithms (see Section~\ref{sec:jd}). Importantly, the joint diagonalization problem we deal with in this paper is conceptually different from the one considered, e.g., by \citet{KulEtAl2015} (and references therein) and, therefore, the respective algorithms are not applicable here.









\section{Multi-view models}
\subsection{Extensions of Gaussian CCA}
\label{sec:models}
\ppp{Gaussian CCA.}
Classical CCA \citep{Hot1936} aims to find projections $D_1\in\R^{M_1\times K}$ and $D_2\in\R^{M_2\times K}$, of two observation vectors $x_1\in\R^{M_1}$ and $x_2\in\R^{M_2}$, each representing a data-view, such that the projected data, $D_1^{\top}x_1$ and $D_2^{\top}x_2$, are maximally correlated. Similarly to classical PCA, the solution boils down to solving a generalized SVD problem. 
The following probabilistic interpretation of CCA is well known \citep{Bro1979,BacJor2005,KlaEtAl2013}. 
Given that $K$ sources are i.i.d.~standard normal random variables, $\alpha\sim\ncal(0,I_K)$, the \emph{Gaussian CCA} model is given by
\\[-0.6em]
\begin{equation}\label{gcca}
\begin{aligned}
x_1\, |\, \alpha,\;\mu_1,\;\Psi_1 &\sim \ncal(D_1 \alpha + \mu_1, \; \Psi_1), \\ 
x_2 \, |\, \alpha,\;\mu_2,\;\Psi_2 &\sim \ncal(D_2 \alpha + \mu_2, \; \Psi_2), 
\end{aligned}
\end{equation}
\\[-0.4em]
where the matrices $\Psi_1\in\R^{M_1\times M_1}$ and $\Psi_2\in\R^{M_2\times M_2}$ are positive semi-definite.
Then, the maximum likelihood solution of~\eqref{gcca} coincides (up to permutation, scaling, and multiplication by any invertible matrix) with the classical CCA solution. 
The model~\eqref{gcca} is equivalent to
\\[-0.6em]
\begin{equation}\label{pcca}
\begin{aligned}
x_1 &= D_1 \alpha + \varepsilon_1,\\ 
x_2 &= D_2 \alpha + \varepsilon_2,
\end{aligned}
\end{equation}
\\[-0.6em]
where the noise vectors are normal random variables, i.e. $\varepsilon_1\sim \ncal(\mu_1,\Psi_1)$ and $\varepsilon_2 \sim \ncal(\mu_2,\Psi_2)$,
and the following independence assumptions are made:
\\[-0.7em]
\begin{equation}\label{indep}
\begin{aligned}
&\alpha_1,\dots,\alpha_K \; \mbox{ are mutually independent}, \\
&\alpha \ci \varepsilon_1,\;\varepsilon_2 \quad \mbox{and} \quad \varepsilon_1 \ci \varepsilon_2.
\end{aligned}
\end{equation}
\\[-0.7em]
The following three models are our novel semi-parametric extensions of Gaussian CCA~\eqref{gcca}--\eqref{pcca}.


\ppp{Multi-view models.}
The first new model follows 
by dropping the Gaussianity assumption on~$\alpha$,~$\varepsilon_1$, and~$\varepsilon_2$. In particular, the \emph{non-Gaussian CCA} model is defined as
\\[-0.7em]
\begin{equation}\label{ncca}
\begin{aligned}
x_1 &= D_1 \alpha + \varepsilon_1,\\
x_2 &= D_2 \alpha + \varepsilon_2,
\end{aligned}
\end{equation}
\\[-0.7em]
where, as opposed to~\eqref{pcca}, no assumptions are made on the sources $\alpha$ and the noise $\varepsilon_1$ and $\varepsilon_2$ except for the independence assumption~\eqref{indep}.

Similarly to
\citet{PodEtAl2015}, we further ``discretize'' non-Gaussian CCA~\eqref{ncca} by applying the Poisson distribution to each view (independently on each variable):
\\[-0.7em]
\begin{equation}\label{dcca}
\begin{aligned}
x_1\,|\,\alpha,\;\varepsilon_1 & \sim \poi (D_1\alpha + \varepsilon_1),\\
x_2\,|\,\alpha,\;\varepsilon_2 & \sim \poi (D_2\alpha + \varepsilon_2).
\end{aligned}
\end{equation}
\\[-0.7em]
We obtain the (non-Gaussian) \emph{discrete CCA} (DCCA) model, which is adapted to count data (e.g., such as word counts in the bag-of-words model of text). In this case, the sources $\alpha$, the noise $\varepsilon_1$ and $\varepsilon_2$, and the matrices $D_1$ and $D_2$ have non-negative components.

 
Finally, by combining non-Gaussian and discrete CCA, we also introduce the \emph{mixed CCA} (MCCA) model:
\\[-0.7em]
\begin{equation}\label{mcca}
\begin{aligned}
x_1 &=D_1\alpha + \varepsilon_1,\\
x_2\,|\,\alpha,\;\varepsilon_2 &\sim \poi (D_2\alpha + \varepsilon_2),
\end{aligned}
\end{equation}
\\[-0.7em]
which is adapted to a combination of discrete and continuous data (e.g., such as images represented as continuous vectors aligned with text represented as counts).  Note that no assumptions are made on distributions of the sources $\alpha$ except for independence~\eqref{indep}.



\setlength{\textfloatsep}{9pt}
\begin{figure}
\centering
\begin{tikzpicture}
[
observed/.style={minimum size=20pt,circle,draw=black,fill=\piccolor,inner sep=1pt, thick},
unobserved/.style={minimum size=17pt,circle,draw,inner sep=1pt, thick},
hyper/.style={minimum size=3pt,circle,fill=black},
arrow/.style={
    decoration={markings,mark=at position 1 with {\arrow[scale=2,#1]{>}}},
    postaction={decorate}, thick},
]
\node (alpha) [unobserved] at (0,0) {$\alpha_k$};
\node (betao) [unobserved] at (-2,0.2) {$\varepsilon_1$};
\node (betat) [unobserved] at (2, 0.2) {$\varepsilon_2$};
\node (xo)    [observed]   at (-1.2,-1.7) {$x_{1m}$};
\node (xt)    [observed]   at (1.2,-1.7) {$x_{2m}$};
\filldraw [black] (-1.0,-2.9) circle (2pt);
\node (Do) [label=below:$D_1$,inner sep=2pt, outer sep=0pt] at (-1.0, -2.9) {};
\filldraw [black] (1.4, -2.9) circle (2pt);
\node (Dt) [label=below:$D_2$,inner sep=2pt, outer sep=0pt] at (1.4, -2.9) {};

\path
(betao) edge [arrow] (xo)
(alpha) edge [arrow] (xo)
(alpha) edge [arrow] (xt)
(betat) edge [arrow] (xt)
(Do)    edge [arrow] (xo)
(Dt)    edge [arrow] (xt)
;

\node [draw,fit=(alpha),inner sep=9pt, outer sep=0pt, rounded corners=.3em, thick] (plate-alpha) {};
\node [below right,inner sep=1.5pt,outer sep=0pt] at (plate-alpha.north west) {$K$};
\node [draw,fit=(xo),inner sep=10pt, outer sep=0pt, rounded corners=.3em, thick] (plate-xo) {};
\node [above right,inner sep=1.1pt, outer sep=0pt] at (plate-xo.south west) {$M_1$};
\node [draw,fit=(xt),inner sep=10pt, outer sep=0pt, rounded corners=.3em, thick] (plate-xt) {};
\node [above right, inner sep=1.1pt, outer sep=0pt] at (plate-xt.south west) {$M_2$};
\node [draw,fit=(xo) (xt) (alpha) (betao) (betat) (plate-xo) (plate-alpha),inner sep=6pt,rounded corners=.3em, thick] (plate-doc) {};
\node [above right,inner sep=1.5pt, outer sep=0pt] at (plate-doc.south west) {$N$};
\end{tikzpicture}
\vspace*{-.5cm}
\caption{Graphical models for non-Gaussian~\eqref{ncca}, discrete~\eqref{dcca}, and mixed~\eqref{mcca} CCA.}
\label{fig:cca}
\end{figure}

The plate diagram for the models~\eqref{ncca}--\eqref{mcca} is presented in Fig.~\ref{fig:cca}. 
We call $D_1$ and $D_2$  \emph{factor loading matrices} (see a comment on this naming convention in Appendix~\ref{app:naming_convention}).


\ppp{Relation between PCA and CCA.}
The key difference between Gaussian CCA and the closely related FA/PPCA models is that the noise in each view of Gaussian CCA is not assumed to be isotropic unlike for FA/PPCA. 
In other words, the components of the noise are not assumed to be independent or, equivalently, the noise covariance matrix does not have to be diagonal and may exhibit a strong structure.
In this paper, we never assume any diagonal structure of the covariance matrices of the noises of the models~\eqref{ncca}--\eqref{mcca}.
The following example illustrates the mentioned relation. Assuming  a linear structure for the noise, (non-) Gaussian CCA (NCCA) takes the form
\\[-0.6em]
\begin{equation}
\label{linear-cca}
\begin{aligned}
x_1 & = D_1\alpha + F_1 \beta_1, \\
x_2 & = D_2\alpha + F_2 \beta_2,
\end{aligned}
\end{equation}
\\[-0.4em]
where $\varepsilon_1 = F_1\beta_1$ with $\beta_1\in\R^{K_1}$ and $\varepsilon_2 = F_2\beta_2$ with $\beta_2\in\R^{K_2}$. By stacking the vectors on the top of each other
\\[-1.5em]
\begin{equation}
\label{stacking}
x = \begin{pmatrix} x_1 \\ x_2 \end{pmatrix}, \; D = \begin{pmatrix} D_1 & F_1 & \zero \\ D_2 & \zero & F_2 \end{pmatrix}, \; z = \begin{pmatrix} \alpha \\ \beta_1 \\ \beta_2 \end{pmatrix},
\end{equation}
\\[-1.0em]
we rewrite the model as
$x = D z$. Assuming that the noise sources $\beta_1$ and $\beta_2$ have mutually independent components, ICA is recovered. If the sources $z$ are further assumed to be Gaussian, $x=Dz$ corresponds to PPCA.
However, we do not assume the noise in Gaussian CCA (and in~\eqref{ncca}--\eqref{mcca}) to have a very specific low dimensional structure.

 
\ppp{Related work.}
Some extensions of Gaussian CCA were proposed in the literature: exponential family CCA \citep{Vir2010,KlaEtAl2010} and Bayesian CCA \citep[see, e.g.,][and references therein]{KlaEtAl2013}. 
Although exponential family CCA can also be discretized, it assumes in practice that the prior of the sources is a specific combination of Gaussians.
Bayesian CCA models the factor loading matrices and the covariance matrix of Gaussian CCA.
Sampling or approximate variational inference are used for estimation and inference in both models. Both models, however, lack our identifiability guarantees and are quite different from the models~\eqref{ncca}--\eqref{mcca}.
\citet{SonEtAl2014} consider a multi-view framework to deal with non-parametric mixture components, while our approach is semi-parametric with an explicit linear structure (our loading matrices) and makes the explicit link with CCA. See also~\citet{GeZou2016} for a related approach.



\subsection{Identifiability}
\label{sec:identifiability}
In this section, the identifiability of the factor loading matrices $D_1$ and $D_2$ is discussed. In general, for the type of models considered, the unidentifiability to permutation and scaling cannot be avoided. In practice, this unidentifiability is however easy to handle
and, in the following, we only consider identifiability up to permutation and scaling.

ICA can be seen as an identifiable analog of FA/PPCA.
Indeed, it is known that the mixing matrix $D$ of ICA is identifiable if at most one source is Gaussian \citep{Com1994}. The factor loading matrix of FA/PPCA is unidentifiable since it is defined only up to
multiplication by any orthogonal rotation matrix.


Similarly, the factor loading matrices of Gaussian CCA~\eqref{gcca}, which can be seen as a multi-view extension of PPCA, are identifiable only up to multiplication by any invertible matrix \citep{BacJor2005}. We show the identifiability results for the new models~\eqref{ncca}--\eqref{mcca}: the factor loading matrices of these models are identifiable if at most one source is Gaussian (see Appendix~\ref{app:identifiability} for a proof).

\vspace{0em}
\begin{theorem}
Assume that matrices $D_1\in\R^{M_1\times K}$ and $D_2\in\R^{M_2\times K}$, where $K \le \min(M_1,\;M_2)$, have full rank. If the covariance matrices $\cov(x_1)$ and $\cov(x_2)$ exist and if at most one source $\alpha_k$, for $k=1,\dots,K$, is Gaussian and none of the sources are deterministic, then the models~\eqref{ncca}--\eqref{mcca} are identifiable (up to scaling and joint permutation).
\label{thm:identif}
\vspace{-1.5em}
\end{theorem}

Importantly, the permutation unidentifiability does not destroy the alignment in the factor loading matrices, that is, for some permutation matrix $P$, if $D_1P$ is the factor loading matrix of the first view, than $D_2P$ must be the factor loading matrix of the second view. 
This property is important for the interpretability of the factor loading matrices and, in particular, is used in our experiments in 
 Section~\ref{sec:experiments}.













%
\section{The cumulants and generalized covariances}
In this section, we first derive the cumulant tensors for the discrete CCA model (Section~\ref{sec:cumulants}) and then generalized covariance matrices (Section~\ref{sec:generalized-covariances}) for the models~\eqref{ncca}--\eqref{mcca}. We show that both cumulants and generalized covariances have a special diagonal form and, therefore, can be efficiently used within the moment matching framework (Section~\ref{sec:jd}).




\subsection{From discrete ICA to discrete CCA}
\label{sec:cumulants}
In this section, we derive the DCCA  cumulants as an extension of the cumulants of discrete independent component analysis \citep[DICA;][]{PodEtAl2015}.

\ppp{Discrete ICA.}
\citet{PodEtAl2015}  consider the discrete ICA model~\eqref{dica}, where $x\in\R^M$ has conditionally independent Poisson components with mean $D\alpha$ and $\alpha\in\R^K$ has independent non-negative components:
\\[-0.8em]
\begin{equation}\label{dica}
\begin{aligned}
x\,|\,\alpha & \sim \poi(D\alpha).
\end{aligned}
\end{equation}
\\[-1.3em]
For estimating the factor loading matrix $D$, \citet{PodEtAl2015} propose an algorithm based on the moment matching method with the cumulants of the DICA model.
In particular, they define the DICA S-covariance matrix and T-cumulant tensor as
\\[-1.5em]
\begin{equation}\label{STdica}
\begin{aligned}
S &:= \cov(x) - \diag\sbra{ \ebb x}, \\
\sbra{T}_{m_1m_2m_3} &:= \cum(x)_{m_1m_2m_3} + [\tau]_{m_1m_2m_3},
\end{aligned}
\end{equation}
\\[-0.9em]
where 
indices $m_1$, $m_2$, and $m_3$ take the values in $1,\dots, M$, and 
$\sbra{ \tau }_{m_1m_2m_3}  = 2\delta_{m_1m_2m_3} \ebb x_{m_1} -\delta_{m_2m_3} \cov(x)_{m_1m_2} - \delta_{m_1m_3} \cov(x)_{m_1m_2} - \delta_{m_1m_2} \cov(x)_{m_1m_3}$
with
 $\delta$ being the Kronecker delta. 
For completeness, we outline the derivation by \citet{PodEtAl2015} below.
Let $y := D\alpha$. By the law of total expectation $\ebb(x) = \ebb(x|y) = \ebb(y)$ and by the law of total covariance
\\[-0.4em]
\vspace{-1.1em}
\begin{equation*}
\begin{aligned}
\cov(x) & = \ebb[ \cov(x|y) ] + \cov[ \ebb(x|y), \; \ebb(x|y) ] \\
& = \diag[ \ebb(y) ] + \cov(y),
\end{aligned}
\end{equation*}
\\[-0.8em]
since all the cumulants of a Poisson random variable with parameter $y$ are equal to $y$.
Therefore, $S = \cov(y)$. Similarly, by the law of total cumulance $T = \cum(y)$.
Then, by the multilinearity property for cumulants, one obtains
\\[-0.8em]
\begin{equation}\label{diagSTdica}
\begin{aligned}
S&=D\,\cov(\alpha)D^{\top}, \\
T&= \cum(\alpha) \times_1 D^{\top} \times_2 D^{\top} \times_3 D^{\top},
\end{aligned}
\end{equation}
\\[-0.8em]
where $\times_i$ denotes the $i$-mode tensor-matrix product
\citep[see, e.g.,][]{KolBad2009}.
Since the covariance $\cov(\alpha)$ and cumulant $\cum(\alpha)$ of the independent sources are diagonal,~\eqref{diagSTdica} is called
the \emph{diagonal form}.  
This diagonal form is further used for estimation of $D$ (see Section~\ref{sec:jd}).




\ppp{Noisy discrete ICA.}
The following noisy version~\eqref{ndica} of the DICA model reveals the connection between DICA and DCCA. 
Noisy discrete ICA is obtained by adding non-negative noise $\varepsilon$, such that $\alpha \ci \varepsilon$, to discrete ICA~\eqref{dica}:
\\[-0.8em]
\begin{equation}\label{ndica}
x\,|\,\alpha,\;\varepsilon \sim \poi\Rra{ D\alpha + \varepsilon}.
\end{equation}
\\[-1.2em]
Let $y:=D\alpha+\varepsilon$ and $S$ and $T$ are defined as in~\eqref{STdica}.
Then a simple extension of the derivations from above gives $S= \cov(y)$ and $T=\cum(y)$.
Since the covariance matrix (cumulant tensor) of the sum of two independent multivariate random variables, $D\alpha$ and $\varepsilon$, is equal to the sum of the covariance matrices (cumulant tensors) of these variables, the ``perturbed'' version of the diagonal form~\eqref{diagSTdica} follows
\\[-1.4em]
\begin{equation}\label{diagSTndica}
\begin{aligned}
S &= D\cov(\alpha) D^{\top} + \cov(\varepsilon), \\
T &= \cum(\alpha) \times_1 D^{\top} \times_2 D^{\top} \times_3 D^{\top} + \cum(\varepsilon).
\end{aligned}
\end{equation}
\\[-1.9em]



\ppp{DCCA cumulants.}
By analogy with~\eqref{stacking}, stacking the observations $x=[x_1; \; x_2]$, the factor loading matrices $D=[D_1; \; D_2]$, and the noise vectors $\varepsilon=[\varepsilon_1; \; \varepsilon_2]$ of discrete CCA~\eqref{dcca} gives a noisy version of discrete ICA with a particular form of the covariance matrix of the noise:
\\[-0.5em]
\begin{equation}\label{cov-noise-stacked-cca}
\textstyle
\cov(\varepsilon) = \begin{pmatrix} \cov(\varepsilon_1) & \zero \\ \zero & \cov(\varepsilon_2) \end{pmatrix},
\end{equation}
\\[-0.5em]
which is due to the independence $\varepsilon_1 \ci \varepsilon_2$.
Similarly, the cumulant $\cum(\varepsilon)$ of the noise has only two diagonal blocks which are non-zero. Therefore, considering only those parts of the S-covariance matrix and T-cumulant tensor of noisy DICA that correspond to zero blocks of the covariance $\cov(\varepsilon)$ and cumulant $\cum(\varepsilon)$, gives immediately a matrix and tensor with a diagonal structure similar to the one in~\eqref{diagSTdica}. Those blocks are the cross-covariance and cross-cumulants of $x_1$ and $x_2$.

 
We define the \emph{S-covariance matrix of discrete CCA}%
\footnote{ Note that $S_{21} := \cov(x_2,x_1)$ is just the transpose of $S_{12}$. 
}
as the cross-covariance matrix of $x_1$ and $x_2$:
\\[-1.1em]
\begin{equation}\label{Sdcca}
S_{12} := \cov(x_1,x_2).
\end{equation}
\\[-1.2em]
From~\eqref{diagSTndica} and~\eqref{cov-noise-stacked-cca}, the matrix $S_{12}$ has the following diagonal form
\\[-1.6em]
\begin{equation}\label{diagSdcca}
S_{12} = D_1 \cov(\alpha) D_2^{\top}.
\end{equation}
\\[-1.2em]
Similarly, we define the \emph{T-cumulant tensors of discrete CCA} (
$T_{121}\in\R^{M_1\times M_2\times M_1}$ and $T_{122}\in\R^{M_1\times M_2\times M_2}$) through the cross-cumulants of $x_1$ and $x_2$, for $j=1,2$:
\\[-1.3em]
\begin{equation}\label{Tdcca}
\begin{aligned}
\sbra{ T_{12j} }_{m_1m_2\tilde{m}_j} & := [\cum(x_1,x_2,x_j)]_{m_1m_2\tilde{m}_j} \\
& - \delta_{m_j\tilde{m}_j}[\cov(x_1,x_2)]_{m_1m_2},
\end{aligned}
\end{equation}
\\[-0.9em]
where the indices $m_1$, $m_2$, and $\tilde{m}_j$ take the values $m_1\in 1,\dots,M_1$, $m_2 \in 1,\dots,M_2$, and $\tilde{m}_j\in 1,\dots, M_j$.
From~\eqref{diagSTdica} and the mentioned block structure~\eqref{cov-noise-stacked-cca} of $\cov(\varepsilon)$, the DCCA T-cumulants have the diagonal form:
\\[-0.6em]
\begin{equation}\label{diagTdcca}
\begin{aligned}
T_{121} &= \cum(\alpha) \times_1 D_1^{\top} \times_2 D_2^{\top} \times_3 D_1^{\top}, \\
T_{122} &= \cum(\alpha) \times_1 D_1^{\top} \times_2 D_2^{\top} \times_3 D_2^{\top}.
\end{aligned}
\end{equation}
\\[-0.6em]
In Section~\ref{sec:jd}, we show how to estimate the factor loading matrices $D_1$ and $D_2$ using the diagonal form~\eqref{diagSdcca} and~\eqref{diagTdcca}.
Before that, in Section~\ref{sec:generalized-covariances}, we first derive the generalized covariance matrices of discrete ICA and the CCA models~\eqref{ncca}--\eqref{mcca}  as an extension of the ideas by \citet{Yer2000,TodHer2013}. 

 




\subsection{Generalized covariance matrices}
\label{sec:generalized-covariances}
In this section, we introduce the generalization of the S-covariance matrix for both  DICA and the CCA models~\eqref{ncca}--\eqref{mcca},
which are obtained through the Hessian of the cumulant generating function. We show that (a) the generalized covariance matrices can be used for approximation of the T-cumulant tensors using generalized derivatives and (b) in the DICA case, these generalized covariance matrices have the diagonal form analogous to~\eqref{diagSTdica}, and, in the CCA case, they have the diagonal form analogous to~\eqref{diagSdcca}. 
Therefore, generalized covariance matrices can be seen as a substitute for the T-cumulant tensors in the moment matching framework. This (a) significantly simplifies derivations and the final expressions used for implementation of resulting algorithms and (b)  potentially improves the sample complexity, since only the second-order information is used.





\ppp{Generalized covariance matrices.} The idea of generalized covariance matrices is inspired by the similar extension of the ICA cumulants by \citet{Yer2000}. 

The cumulant generating function (CGF) of a multivariate random variable $x\in\R^M$ is defined as
\\[-0.9em]
\begin{equation}\label{cgf}
K_x(t) = \log { \ebb ( e^{t^{\top}x} ) },
\end{equation}
\\[-1.4em]
for $t\in\R^M$. The cumulants $\kappa_s(x)$, for $s=1,2,3,\dots$, are the coefficients of the Taylor series expansion of the CGF evaluated at zero. Therefore, the cumulants are the derivatives of the CGF evaluated at zero:
$\kappa_s(x) = \nabla^s K_x(0)$, $s=1,2,3,\dots$, where $\nabla^sK_x(t)$ is the $s$-th order derivative of $K_x(t)$ with respect to $t$. Thus, the expectation of $x$ is the gradient $\ebb(x) = \nabla K_x(0)$ and the covariance of $x$ is the Hessian $\cov(x)=\nabla^2 K_x(0)$ of the CGF evaluated at zero.

The extension of cumulants then follows immediately: for $t\in\R^M$, we refer to the derivatives $\nabla^sK_x(t)$ of the CGF as the \emph{generalized cumulants}. The respective parameter $t$ is called a \emph{processing point}. In particular, the gradient, $\nabla K_x(t)$, and Hessian, $\nabla^2 K_x(t)$, of the CGF are referred to as the \emph{generalized expectation} and \emph{generalized covariance matrix}, respectively:
\\[-1.3em]
\begin{align}
\textstyle
\label{gen-expectation}
\textstyle
\ecal_x(t) &:= \nabla K_x(t) = \frac{\ebb ({ x e^{t^{\top}x}})}{\ebb ( e^{t^{\top}x} ) }, \\[-0.1em]
\textstyle
\label{gen-covariance}
\ccal_x(t) &:= \nabla^2 K_x(t) = \frac{\ebb (xx^{\top} e^{t^{\top}x}) }{\ebb ( e^{t^{\top}x} ) } - \ecal_x(t) \ecal_x(t)^{\top}.
\end{align}
\\[-1.3em] 
We now outline the key ideas of this section. When a multivariate random variable $\alpha\in\R^K$ has independent components, its CGF $K_{\alpha}(h)=\log { \ebb(e^{h^{\top}\alpha}) }$, for some $h\in\R^K$, is equal to a sum of decoupled terms:
$
K_{\alpha}(h) = \sum_k \log \ebb(e^{h_k\alpha_k}).
$
Therefore, the Hessian $\nabla^2K_{\alpha}(h)$ of the CGF $K_{\alpha}(h)$ is diagonal (see Appendix~\ref{app:gen-cums-alpha}). Like   covariance matrices, these Hessians (a.k.a.~generalized covariance matrices) are subject to the multilinearity property for linear transformations of a vector, hence the resulting diagonal structure of the form~\eqref{diagSTdica}. This is essentially the previous ICA work \citep{Yer2000,TodHer2013}.
Below we generalize these ideas first to the discrete ICA case and then to the CCA models~\eqref{ncca}--\eqref{mcca}.



\ppp{Discrete ICA generalized covariance matrices.} 
Like covariance matrices, generalized covariance matrices of a vector with independent components are diagonal: they satisfy the multilinearity property $\ccal_{D\alpha}(h) = D\,\ccal_{\alpha}(h)D^{\top}$, and are equal to covariance matrices when $h=\zero$.
Therefore, we can expect that the derivations of the diagonal form~\eqref{diagSTdica} of the S-covariance matrices extends to the generalized covariance matrices case.
By analogy with~\eqref{STdica}, we define the \emph{generalized S-covariance matrix} of DICA:
\\[-0.8em]
\begin{equation}\label{GenSdica}
S(t) := \ccal_x(t) - \diag[\ecal_x(t)].
\end{equation}
\\[-1.2em]
To derive the analog of the diagonal form~\eqref{diagSTdica} for $S(t)$, we have to compute all the expectations in~\eqref{gen-expectation} and~\eqref{gen-covariance} for a Poisson random variable $x$ with the parameter $y=D\alpha$. 
To illustrate the intuition, we compute here one of these expectations (see Appendix~\ref{app:epxectations-of-poisson} for further derivations):
\\[-0.6em]
\[
\begin{aligned}
\ebb( xx^{\top} & e^{t^{\top}x} )  = \ebb[ \ebb( xx^{\top} e^{t^{\top} x} \; | \; y ) ]\\
& = \diag[e^t]\, \ebb (yy^{\top} e^{y^{\top}(e^t-\one)} ) \diag[e^t] \\
& = \rbra{\diag[e^t] D}\, \ebb ({\alpha\alpha^{\top} e^{\alpha^{\top} h(t)} }) \rbra{\diag[e^t]D}^{\top}, 
\end{aligned}
\]
\\[-0.4em]
where $h(t) = D^{\top} (e^t-1)$ and $e^t$ denotes an $M$-vector with the $m$-th component equal to $e^{t_m}$.
This gives
\\[-1.5em]
\begin{equation}\label{diagGenSdica}
S(t) = \Rra{\diag[e^t]{D}} \; \ccal_{\alpha}\Rra{h(t)} \; \Rra{\diag[e^t]{D}}^{\top},
\end{equation}
\\[-1.5em]
which is a diagonal form similar (and equivalent for $t=\zero$) to~\eqref{diagSTdica} since the generalized covariance matrix $\ccal_{\alpha}(h)$ of independent sources is diagonal (see~\eqref{gen-covariance-alpha} in Appendix~\ref{app:gen-cums-alpha}). Therefore, the generalized S-covariance matrices, estimated at different processing points $t$, can be used as a substitute of the T-cumulant tensors in the moment matching framework. Interestingly enough, the T-cumulant tensor~\eqref{STdica} can be approximated by the generalized covariance matrix via its directional derivative (see Appendix~\ref{app:dir-derivative-approx}).



\ppp{CCA generalized covariance matrices.} 
For the CCA models~\eqref{ncca}--\eqref{mcca}, straightforward generalizations of the ideas from Section~\ref{sec:cumulants} leads to the following definition of the \emph{generalized CCA S-covariance matrix}:
\\[-1.4em]
\begin{equation}\label{GenSdcca}
S_{12}(t) := \frac{ \ebb ({ x_1x_2^{\top} e^{t^{\top} x} }) }{ \ebb ({ e^{ t^{\top} x } }) } - 
\frac{ \ebb( x_1 e^{t^{\top} x} ) }{ \ebb( e^{t^{\top} x} ) } \frac{ \ebb( x_2^{\top} e^{t^{\top} x} ) }{ \ebb( e^{t^{\top} x} ) },
\end{equation}
\\[-1.4em]
where the vectors $x$ and $t$ are obtained by vertically stacking $x_1$ \& $x_2$ and $t_1$ \& $t_2$ as in~\eqref{stacking}. In the discrete CCA case, $S_{12}(t)$ is essentially the upper-right block of the generalized S-covariance matrix $S(t)$ of DICA and has the form 
\\[-1.4em]
\begin{equation}\label{diagGenSdcca}
S_{12}(t) = \Rra{\diag[e^{t_1}]D_1} \ccal_{\alpha}({ h(t) }) \Rra{ \diag[ e^{t_2}]D_2 }^{\top},
\end{equation}
\\[-1.4em]
where $h(t)=D^{\top}(e^t - \one)$ and the matrix $D$ is obtained by vertically stacking $D_1$ \& $D_2$ by analogy with~\eqref{stacking}.
For non-Gaussian CCA, the diagonal form is
\\[-0.8em]
\begin{equation}\label{diagGenSncca}
S_{12}(t) = D_1 \, \ccal_{\alpha} \rbra{{ h(t) }} \, D_2^{\top},
\end{equation} 
\\[-1.0em]
where $h(t) = D_1^{\top}t_1 + D_2^{\top}t_2$. 
Finally, for mixed CCA,
\\[-0.8em]
\begin{equation}\label{diagGenSmcca}
S_{12}(t) = D_1  \, \ccal_{\alpha} \rbra{{ h(t) }} \, \Rra{ \diag[e^{t_2}]D_2 }^{\top},
\end{equation}
\\[-1.0em]
where $h(t) = D_1^{\top}t_1 + D_2^{\top} (e^{t_2}-\one)$.
Since the generalized covariance matrix of the sources $\ccal_{\alpha}(\cdot)$ is diagonal, expressions~\eqref{diagGenSdcca}--\eqref{diagGenSmcca} have the desired diagonal form (see Appendix~\ref{app:derivations-cca-generalized-covariances} for detailed derivations).










\section{Joint diagonalization algorithms}
\label{sec:jd}

The standard algorithms such as TPM or orthogonal joint diagonalization cannot be used for the estimation of $D_1$ and $D_2$. Indeed, even after whitening, the matrices appearing in the diagonal form~\eqref{diagSdcca}\&\eqref{diagTdcca} or~\eqref{diagGenSdcca}--\eqref{diagGenSmcca} are \emph{not} orthogonal. 
As an alternative, we use Jacobi-like non-orthogonal diagonalization algorithms \citep{FuGao2006,IfeEtAl2009,LucAlb2010}. These algorithms are discussed in this section and in Appendix~\ref{app:nojd}.

The estimation of the factor loading matrices $D_1$ and $D_2$ of the CCA models~\eqref{ncca}--\eqref{mcca} via non-orthogonal joint diagonalization algorithms consists of the following steps: (a) construction of a set of matrices, called \emph{target matrices}, to be jointly diagonalized (using finite sample estimators), (b) a whitening step, (c) a non-orthogonal joint diagonalization step, and (d) the final estimation of the factor loading matrices (Appendix~\ref{app:est-D}).
 

\ppp{Target matrices.}
There are two ways to construct target matrices: either with the CCA S-matrices~\eqref{Sdcca} and T-cumulants~\eqref{Tdcca} (only DCCA) or the generalized covariance matrices~\eqref{GenSdcca} (D/N/MCCA). These matrices are estimated with finite sample estimators (Appendices~\ref{app:finite-sample-generalized-covariances} \&~\ref{app:finite-sample-cumulants}).

The (computationally efficient) construction of target matrices from S- and T-cumulants was discussed by \citet{PodEtAl2015} and we recall it in Appendix~\ref{app:S-and-T-cumulants}.
Alternatively, the target matrices can be constructed by estimating the generalized S-covariance matrices
at $P+1$ processing points $\zero, t_1,\dots, t_P \in \R^{M_1+M_2}$:
\\[-1.0em]
\begin{equation}\label{jdmatricesGenS}
\{S_{12} = S_{12}(\zero), \;\; S_{12}(t_1), \;\; \dots, \;\; S_{12}(t_P)\},
\end{equation}
\\[-1.3em]
which also have the diagonal form~\eqref{diagGenSdcca}--\eqref{diagGenSmcca}. It is interesting to mention the connection between the T-cumulants and the generalized S-covariance matrices. The T-cumulant can be approximated via the directional derivative of the generalized covariance matrix (see Appendix~\ref{app:dir-derivative-approx}).
However, in general, e.g., $S_{12}(t)$ with $t=[t_1;\zero]$ is not exactly the same as $T_{121}(t_1)$ and the former can be non-zero even when the latter is zero. This is important since order-4 and higher statistics are used with the method of moments when there is a risk that an order-3 statistic is zero like for symmetric sources. 
In general, the use of higher-order statistics increases the sample complexity and makes the resulting expressions quite complicated.
Therefore, replacing the T-cumulants with the generalized S-covariance matrices is potentially beneficial.


\ppp{Whitening.}
The matrices $W_1\in\R^{K\times M_1}$ and $W_2\in\R^{K\times M_2}$ are called \emph{whitening matrices} of $S_{12}$ if
\\[-0.9em]
\begin{equation}\label{whitening}
W_1 S_{12} W_2^{\top} = I_K,
\end{equation}
\\[-1.4em]
where $I_K$ is the $K$-dimensional identity matrix. 
 $W_1$ and $W_2$ are only defined up to multiplication by any invertible matrix $Q\in\R^{K\times K}$, since any pair of matrices $\wt{W}_1=Q W_1$ and $\wt{W}_2 = Q^{-\top} W_2$ also satisfy~\eqref{whitening}. In fact, using higher-order information (i.e. the T-cumulants or the generalized covariances for $t\ne\zero$) allows to solve this ambiguity. 

The whitening matrices can be computed via SVD of $S_{12}$ (see Appendix~\ref{app:compute-whitening}).
When $M_1$ and $M_2$ are too large, one can use a randomized SVD algorithm~\citep[see, e.g.,][]{HalEtAl2011} to avoid the construction of the large matrix $S_{12}$ and to decrease the computational time. 


\ppp{Non-orthogonal joint diagonalization (NOJD).}
Let us consider joint diagonalization of the generalized covariance matrices~\eqref{jdmatricesGenS} (the same procedure holds for the S- and T-cumulants~\eqref{jdMatricesS&T}; see Appendix~\ref{app:whiten-t-cumulants}).
Given the whitening matrices $W_1$ and $W_2$, 
the transformation of the generalized covariance matrices~\eqref{jdmatricesGenS} gives $P+1$ matrices
\vspace{-.4em}
\begin{equation}\label{jdmatricesGenSwhitened}
\{W_1S_{12} W_2^{\top}, \; W_1S_{12}(t_p)W_2^{\top}, \quad p=1,\dots,P \},
\vspace{-.4em}
\end{equation}
where each matrix is in $\R^{K\times K}$ and has reduced dimension since $K<M_1,M_2$. In practice, finite sample estimators are used to construct~\eqref{jdmatricesGenS} (see Appendices~\ref{app:finite-sample-generalized-covariances} and~\ref{app:finite-sample-cumulants}).

Due to the diagonal form~\eqref{diagSdcca} and~\eqref{diagGenSdcca}--\eqref{diagGenSmcca}, each matrix in~\eqref{jdmatricesGenS} has the form%
\footnote{
Note that when the diagonal form has terms $\diag[e^t]$, we simply multiply the expression by $\diag[e^{-t}]$.
}
$(W_1D_1) \, \diag(\cdot) \, (W_2D_2)^{\top}$. Both $D_1$ and $D_2$ are (full) $K$-rank matrices and $W_1$ and $W_2$ are $K$-rank by construction. Therefore, the square matrices $V_1 = W_1D_1$ and $V_2 = W_2D_2$ are invertible. From~\eqref{diagSdcca} and~\eqref{whitening}, we get $V_1 \cov(\alpha) V_2^{\top} = I$ and hence $V_2 = \diag[{ \var(\alpha)^{-1} }]V_1^{-1}$ (the covariance matrix of the sources is diagonal and we assume they are non-deterministic, i.e. $\var(\alpha)\ne \zero$). Substituting this into $W_1 S_{12}(t) W_2^{\top}$ and using the diagonal form
~\eqref{diagGenSdcca}--\eqref{diagGenSmcca}, we obtain that the matrices in~\eqref{jdmatricesGenS} have the form $V_1 \diag(\cdot) V_1^{-1}$. Hence, we deal with the problem of the following type:
Given $P$ non-defective (a.k.a.~diagonalizable) matrices
$
\bcal = \cbra{ B_1,\;\dots,\; B_P}$,
where each matrix $B_p \in\R^{K\times K}$, find and invertible matrix $Q\in\R^{K\times K}$ such that 
\\[-.8em]
\begin{equation}\label{nojd}
Q \bcal Q^{-1} = \{ QB_1Q^{-1},\; \dots,\;QB_PQ^{-1} \}
\end{equation}
\\[-1.5em]
are (jointly) as diagonal as possible. This can be seen as a joint non-symmetric eigenvalue problem. 
This problem should not be confused with the classical joint diagonalization problem by congruence (JDC), where $Q^{-1}$ is replaced by $Q^{\top}$, except when $Q$ is an orthogonal matrix \citep{LucAlb2010}. JDC is often used for ICA algorithms or moment matching based algorithms for graphical models when a whitening step is not desirable (see, e.g., \citet{KulEtAl2015} and references therein). However, neither JDC nor the orthogonal diagonalization-type algorithms \citep[such as, e.g., the tensor power method by][]{AnaEtAl2014}  are applicable for the problem~\eqref{nojd}.





\begin{figure*}[!t]
\centering 
\begin{tabular}{ccc}
\includegraphics[width=.3\textwidth,clip=true, trim=5 5 5 5]{pca_exp_2D_30Jan_1_l1.pdf} &
\includegraphics[width=.3\textwidth,clip=true, trim=5 5 5 5]{2D.pdf} &
\includegraphics[width=.3\textwidth,clip=true, trim=5 5 5 5]{pca_exp_30Jan_5_l1.pdf}
\end{tabular}
\vspace{-1em}
\caption{Synthetic experiment with discrete data. \textbf{Left (2D example)}: $M_1=M_2=K_1=K_2=2$, $K=1$, $c=c_1=c_2=0.1$, and $L_s = L_n = 100$; \textbf{middle (2D data)}: the $x_1$-observations and factor loading matrices for the 2D example ($F_{1j}$ denotes the $j$-th column of the noise factor matrix $F_1$); \textbf{right (20D example)}: $M_1=M_2=K_1=K_2=20$, $K=10$, $L_s = L_n = 1,000$, $c=0.3$, and $c_1=c_2=0.1$. \vspace{-1.1em}}
\label{fig:dica}
\end{figure*}
\setlength{\textfloatsep}{11pt}
\begin{table*}[!t] 
\centering 
\resizebox{0.85\textwidth}{!}{
\begin{tabular}{cc|cc|cc|cc} 
                 nato &                 otan &                  work &              travail &                 board &           commission &                 nisga &                nisga  \\ 
                kosovo &               kosovo &               workers &        n\'egociations &                 wheat &                 bl\'e &                treaty &          autochtones  \\ 
                forces &           militaires &                strike &         travailleurs &               farmers &         agriculteurs &            aboriginal &              trait\'e  \\ 
              military &               guerre &           legislation &               gr\`eve &                 grain &       administration &             agreement &               accord  \\ 
                   war &        international &                 union &               emploi &             producers &          producteurs &                 right &                droit  \\ 
                troops &                 pays &             agreement &                droit &             amendment &                grain &                  land &              nations  \\ 
               country &           r\'efugi\'es &                labour &             syndicat &                market &              conseil &               reserve &          britannique  \\ 
                 world &            situation &                 right &             services &             directors &                ouest &              national &            indiennes  \\ 
              national &                 paix &              services &               accord &               western &           amendement &               british &                terre  \\ 
                 peace &          yougoslavie &          negotiations &                 voix &              election &              comit\'e &              columbia &             colombie  \\ 
 \end{tabular}
 }
 \vspace{-2.5mm}
\caption{ Factor loadings (a.k.a. topics) extracted from the Hansard collection for $K=20$ with DCCA. 
} 
\label{tab:topics}
\vspace{-1.4em}
\end{table*}



To solve the problem~\eqref{nojd}, we use the Jacobi-like non-orthogonal joint diagonalization (NOJD) algorithms  \citep[e.g.,][]{FuGao2006,IfeEtAl2009,LucAlb2010}. 
These algorithms are an extension of the orthogonal joint diagonalization algorithms based on Jacobi (=Givens) rotations \citep{GolVan1996,BunEtAl1993,CarSou1996}.
Due to the space constraint, the description of the NOJD algorithms is moved to Appendix~\ref{app:nojd}.
Although these algorithms are quite stable in practice, we are not aware of any theoretical guarantees about their convergence or stability to perturbation.

\ppp{Spectral algorithm.} By analogy with the orthogonal case \citep{Car1989,AnaEtAl2012}, we can easily extend the idea of the spectral algorithm to the non-orthogonal one. Indeed, it amounts to performing whitening as before and constructing only one matrix with the diagonal structure, e.g., $B=W_1 S_{12}(t) W_2^{\top}$ for some $t$. Then, the matrix $Q$ is obtained as the matrix of the eigenvectors of $B$. The vector $t$ can be, e.g., chosen as $t=Wu$, where $W=[W_1; \,W_2]$ and $u\in\R^K$ is a vector sampled uniformly at random.

This spectral algorithm and the NOJD algorithms are closely connected. In particular, when $B$ has real eigenvectors, the spectral algorithm is equivalent to NOJD of $B$. Indeed, in such case, NOJD boils down to an algorithm for a non-symmetric eigenproblem \citep{Ebe1962,Ruh1968}. 
In practice, however, due to the presence of noise and finite sample errors, $B$ may have complex eigenvectors. In such case, the spectral algorithm is different from NOJD.
Importantly, the joint diagonalization type algorithms are known to be more stable in practice \citep[see, e.g.,][]{BacJor2002,PodEtAl2015}.

While deriving precise theoretical guarantees is beyond the scope of this paper, the techniques outlined by \citet{AnaEtAl2012} for the spectral algorithm for latent Dirichlet Allocation can potentially be extended. The main difference is obtaining the analogue of the SVD accuracy \citep[Lemma C.3,][]{AnaEtAl2013} for the eigen decomposition. This kind of analysis can potentially be extended with the techniques outlined in \citep[Chapter 4,][]{SteSun1990}.
Nevertheless, with appropriate parametric assumptions on the sources, we expect that the above described extension of the spectral algorithm should lead to similar guarantee as the spectral algorithm of \citet{AnaEtAl2012}.  

See Appendix~\ref{app:implementation-details} for some important implementation details, including the choice of the processing points.







\section{Experiments}\label{sec:experiments}

\ppp{Synthetic data.} 
We sample synthetic data to have ground truth information for comparison.
We sample from linear DCCA which extends linear CCA~\eqref{linear-cca} such that each view is $x_j \sim \poi (D_j \alpha + F_j \beta_j )$. The sources $\alpha \sim \gam(c,b)$ and the noise sources $\beta_j \sim \gam(c_j,b_j)$, for $j=1,2$, are sampled from the gamma distribution (where $b$ is the rate parameter). Let $s_j \sim \poi(D_j \alpha)$ be the part of the sample due to the sources and $n_j \sim \poi(F_j\beta_j)$ be the part of the sample due to the noise (i.e., $x_j = s_j + n_j$). Then we define the expected sample length due to the sources and noise, respectively, as $L_{js} := \ebb[\sum\mathop{}_{m} s_{jm}]$ and $L_{jn} := \ebb[ \sum\mathop{}_{m} n_{jm}]$. For sampling, the target values $L_s = L_{1s} = L_{2s}$ and $L_n = L_{1n} = L_{2n}$ are fixed and the parameters $b$ and $b_j$ are accordingly set to ensure these values: $b= Kc / L_s$ and $b_j = K_j c_j / L_n$ (see Appendix B.2 of \citet{PodEtAl2015}). For the larger dimensional example (Fig.~\ref{fig:dica}, right), each column of the matrices $D_j$ and $F_j$, for $j=1,2$, is sampled from the symmetric Dirichlet distribution with the concentration parameter equal to $0.5$. For the smaller 2D example (Fig.~\ref{fig:dica}, left), they are fixed: $D_1=D_2$ with $[D_1]_1=[D_1]_2=0.5$ and $F_1=F_2$ with $[F_1]_{11}=[F_1]_{22}=0.9$ and $[F_1]_{12} = [F_1]_{21}=0.1$.
For each experiment, $D_j$ and $F_j$, for $j=1,2$, are sampled once and, then, the $x$-observations are sampled for different sample sizes $N = \cbra{ 500, 1,000, 2,000, 5,000, 10,000}$, 5 times for each $N$.

\ppp{Metric.} The evaluation is performed on a matrix $D$ obtained by stacking $D_1$ and $D_2$ vertically (see also the comment after Thm.~\ref{thm:identif}). As in~\citet{PodEtAl2015}, we use as evaluation metric the normalized $\ell_1$-error between a recovered matrix $\wh{D}$ and the true matrix $D$ with the best permutation of columns $\mathrm{err}_1(\wh{D},D):=\min_{\pi\in\mathrm{PERM}} \frac{1}{2K} \sum_k \|{ \wh{d}_{\pi_k} - d_k }\|_1 \in [0,1]$. The minimization is over the possible permutations $\pi\in\mathrm{PERM}$ of the columns of $\wh{D}$ and can be efficiently obtained with the Hungarian algorithm for bipartite matching. The (normalized) $\ell_1$-error takes the values in $[0,\,1]$ and smaller values of this error indicate better performance of an algorithm.

\ppp{Algorithms.} We compare DCCA (implementation with the S- and T-cumulants) and DCCAg (implementation with the generalized S-covariance matrices and the processing points initialized as described in Appendix~\ref{app:processing-projection-points}) to DICA and the non-negative matrix factorization (NMF) algorithm with multiplicative updates for divergence \citep{LeeSeu2000}. To run DICA or NMF, we use the stacking trick~\eqref{stacking}. DCCA is set to estimate $K$ components. DICA is set to estimate either $K_0 = K+K_1+K_2$ or $M=M_1+M_2$ components (whichever is the smallest, since DICA cannot work in the over-complete case).  NMF is always set to estimate $K_0$ components. For the evaluation of DICA/NMF, the $K$ columns  with the smallest $\ell_1$-error are chosen. NMF$^{\circ}$ stands for NMF initialized with a matrix $D$ of the form~\eqref{stacking} with induced zeros; otherwise NMF is initialized with (uniformly) random non-negative matrices. The running times are discussed in Appendix~\ref{app:running-time}.

\ppp{Synthetic experiment.} We first perform an experiment with discrete synthetic data in 2D (Fig.~\ref{fig:dica}) and then repeat the same experiment when the size of the problem is 10 times larger. In practice, we observed that for $K_0<M$ all models work approximately equally well, except for NMF which breaks down in high dimensions. In the over-complete case as in Fig.~\ref{fig:dica}, DCCA works better. 
A continuous analogue of this experiment is presented in Appendix~\ref{sec:exp-continuous}.








\ppp{Real data (translation).}
Following \citet{VinEtAl2003}, we illustrate the performance of DCCA by extracting bilingual topics from the Hansard collection \citep{VinGir2002} with aligned English and French proceedings of the 36-th Canadian Parliament. 
In Table~\ref{tab:topics}, we present some of the topics extracted after running DCCA with $K=20$ (see all the details in Appendices~\ref{app:translation} and~\ref{app:translation-preprocess}). The (Matlab/C++) code for reproducing the experiments of this paper is available at \href{https://github.com/anastasia-podosinnikova/cca}{https://github.com/anastasia-podosinnikova/cca}.




\subsection*{Conclusion} We have proposed the first identifiable versions of CCA, together with moment matching algorithms which allow the identification of the loading matrices in a semi-parametric framework, where no assumptions are made regarding the distribution of the source or the noise. We also introduce new sets of moments (our generalized covariance matrices), which could prove useful in other settings.
 































\section*{Acknowledgements} 
This work was partially supported by the MSR-Inria Joint Center.  


\bibliography{lit}
\bibliographystyle{icml2016}












\cleardoublepage
\newpage
\section{Appendix}
\appendix
\renewcommand{\thesubsection}{\Alph{subsection}}

The appendix is organized as follows. 
\begin{mi}
\item
In Appendix~\ref{app:notation}, we summarize our notation.
\item
In Appendix~\ref{app:identifiability}, we present the proof of Theorem~\ref{thm:identif} stating the \emph{identifiability of the CCA models~\eqref{ncca}--\eqref{mcca}}.
\item
In Appendix~\ref{app:generalized-E-cov}, we provide some details for the \emph{generalized covariance matrices}: the form of the generalized covariance matrices of independent variables (Appendix~\ref{app:gen-cums-alpha}), the derivations of the diagonal form of the generalized covariance matrices of discrete ICA (Appendix~\ref{app:derivations-dica-generalized-covariances}), the derivations of the diagonal form of the generalized covariance matrices of the CCA models~\eqref{ncca}--\eqref{mcca} (Appendix~\ref{app:derivations-cca-generalized-covariances}), and approximation of the T-cumulants with the generalized covariance matrix (Appendix~\ref{app:dir-derivative-approx}).
\item
In Appendix~\ref{app:finite-sample-estimators}, we provide expressions for natural \emph{finite sample estimators of the generalized covariance matrices} and the T-cumulant tensors for the considered CCA models.
\item
In Appendix~\ref{app:implementation-details}, we discuss some rather technical \emph{implementation details}: computation of whitening matrices (Appendix~\ref{app:compute-whitening}), 
selection of the projection vectors for the T-cumulants and the processing points for the generalized covariance matrices (Appendix~\ref{app:processing-projection-points}), and the final estimation of the factor loading matrices (Appendix~\ref{app:est-D}).
\item
In Appendix~\ref{app:nojd}, we describe the \emph{non-orthogonal joint diagonalization algorithms} used in this paper.
\item In Appendix~\ref{app:supplementary-experiments}, we present \emph{some supplementary experiments}: a continuous analog of the synthetic experiment from Section~\ref{sec:experiments} (Appendix~\ref{sec:exp-continuous}), an experiment to analyze the sensitivity of the DCCA algorithm with the generalized S-covariance matrices to the choice of the processing points (Appendix~\ref{app:gdcca-vs-delta}), and a detailed description of the experiment with the real data from Section~\ref{sec:experiments} (Appendices~\ref{app:translation} and~\ref{app:translation-preprocess}).
\end{mi}


\subsection{Notation}
\label{app:notation}

\subsubsection{Notation summary}
\label{app:notation-summary}
The vector $\alpha\in\R^K$ refers to the latent sources. Unless otherwise specified, the components $\alpha_1,\dots,\alpha_K$ of the vector $\alpha$ are mutually independent. For a linear single-view model, $x=D\alpha$, the vector $x\in\R^M$ denotes the observation vector (sensors or documents), where $M$ is, respectively, the number of sensors or the vocabulary size. For the two-view model, $x$, $M$, and $D$ take the indices $1$ and $2$.

\subsubsection{Naming convention}
\label{app:naming_convention}
A number of models have the linear form $x=D\alpha$.
Depending on the context, the matrix $D$ is called differently:   topic matrix%
\footnote{ Note that \citet{PodEtAl2015} show that DICA is closely connected (and under some conditions is equivalent) to latent Dirichlet allocation \citep{BleEtAl2003}. 
} 
in the topic learning context,
factor loading or projection matrix in the FA and/or PPCA context,  mixing matrix in the ICA context, or  dictionary in the dictionary learning context. 

Our linear multi-view models, $x_1 = D_1\alpha$ and $x_2 = D_2\alpha$, are closely related the linear models mentioned above.
For example, due to the close relation of DCCA and DICA, the former is closely related to the multi-view topic models \citepsup[see, e.g.,][]{BleJor2003}.
In this paper, we refer to $D_1$ and $D_2$ as the  \emph{factor loading matrices}, although depending on contex any other name can be used.




\subsection{Identifiability}
\label{app:identifiability}
In this section, we prove that the factor loading matrices $D_1$ and $D_2$ of the non-Gaussian CCA~\eqref{ncca}, discrete CCA~\eqref{dcca}, and mixed CCA~\eqref{mcca} models are identifiable up to permutation and scaling if at most one source $\alpha_k$ is Gaussian.
We provide a complete proof for the non-Gaussian CCA case and show that the other two cases can be proved by analogy.

\subsubsection{Identifiability of non-Gaussian CCA~\eqref{ncca}}
The proof uses the notion of the second characteristic function (SCF) of a random variable $x\in\R^M$:
$$
\phi_x(t) = \log \ebb ( e^{i t^{\top} x} ),
$$
for all $t\in\R^M$.
The SCF completely defines the probability distribution of $x$ \citepsup[see, e.g.,][]{JacPro2004}.
Important difference between the SCF and the cumulant generating function~\eqref{cgf} is that the former always exists.

The following property of the SCF is of central importance for the proof: if two random variables, $z_1$ and $z_2$, are independent, then $\phi_{A_1z_1+A_2z_2}(t) = \phi_{z_1}(A_1^{\top}t) + \phi_{z_2}(A_2^{\top}t)$, where $A_1$ and $A_2$ are any  matrices of compatible sizes.


We can now use our CCA model to derive an expression of $\phi_x(t)$.
Indeed, defining a vector $x$ by stacking the vectors $x_1$ and $x_2$, the SCF of $x$ for any $t=[t_1;\;t_2]$, takes the form
$$
\begin{aligned}
\phi_x&(t) = \log \ebb ( e^{it_1^{\top} x_1 + it_2^{\top} x_2} ) \\
&\stackrel{(a)}{=} \log \ebb ( e^{ i\alpha^{\top}(D_1^{\top} t_1 + D_2^{\top} t_2) + i \varepsilon_1^{\top} t_1 + i \varepsilon_2^{\top} t_2 } ) \\
&\stackrel{(b)}{=} \log \ebb (e^{ i \alpha^{\top} (D_1^{\top} t_1 + D_2^{\top} t_2) })  \\
& \hspace*{2cm} + \log \ebb (e^{i\varepsilon_1^{\top} t_1}) + \log \ebb (e^{i\varepsilon_2^{\top} t_2}) \\
&= \phi_{\alpha} (D_1^{\top}t_1 + D_2^{\top} t_2) + \phi_{\varepsilon_1}(t_1) + \phi_{\varepsilon_2}(t_2),
\end{aligned}
$$
where in $(a)$ we substituted the definition~\eqref{ncca} of~$x_1$ and~$x_2$ and in $(b)$ we used the independence $\alpha \ci \varepsilon_1 \ci \varepsilon_2$. Therefore, the blockwise mixed derivatives of $\phi_x$ are equal to
\begin{equation}\label{temp:hessianSCF}
 \partial_1 \partial_2 \phi_{x}(t) = D_1 \phi''_{\alpha} (D_1^{\top} t_1 + D_2^{\top} t_2) D_2^{\top},
\end{equation}
where $\partial_1 \partial_2 \phi_{x}(t) := \nabla_{t_1}\nabla_{t_2} \phi_{x}(h(t_1,t_2))\in\R^{M_1\times M_2}$ and $\phi''_{\alpha}(u) :=\nabla_u^2\phi_{\alpha}(u)$, does not depend on the noise vectors $\varepsilon_1$ and $\varepsilon_2$. 

For simplicity, we first prove the identifiability result when all components of the common sources are non-Gaussian.
The high level idea of the proof is as follows. We assume two different representations of $x_1$ and $x_2$ and using~\eqref{temp:hessianSCF} and the independence of the components of $\alpha$ and the noises, we first show that the two potential dictionaries are related by an orthogonal matrix (and not any invertible matrix), and then show that this implies that the two potential sets of independent components are (orthogonal) linear combinations of each other, which, for non-Gaussian components which are not reduced to point masses, imposes that this orthogonal transformation is the combination of a permutation matrix and marginal scaling---a standard result from the ICA literature \citep[Theorem 11]{Com1994}.

Let us then assume that two equivalent representations of non-Gaussian CCA exist:
\begin{equation}\label{temp:tworepres}
\begin{aligned}
x_1 &= D_1 \alpha + \varepsilon_1 = E_1 \beta + \eta_1,\\
x_2 &= D_2 \alpha + \varepsilon_2 = E_2 \beta + \eta_2,
\end{aligned}
\end{equation}
where the other sources $\beta = (\beta_1, \ldots, \beta_K)$ are also assumed mutually independent and non-degenerate.
As a standard practice in the ICA literature and without loss of generality as the sources have non-degenerate components, one can assume that the sources have unit variances, i.e. $\cov(\alpha,\alpha)=I$ and $\cov(\beta, \beta)=I$, by respectively rescaling the columns of the factor loading matrices. Under this assumption, the two expressions of the cross-covariance matrix are
\begin{equation} \label{eq:D1D2}
\cov(x_1,x_2) = D_1D_2^{\top} = E_1 E_2^{\top},
\end{equation}
which, given that $D_1$, $D_2$ have full rank, 
implies that\footnote{The fact that $D_1$, $D_2$ have full rank and that $E_1$, $E_2$ have $K$ columns, combined with $\eqref{eq:D1D2}$, implies that $E_1$, $E_2$ have also full rank.}
\begin{equation}\label{temp:E1E2}
E_1 = D_1 Q, \quad E_2 = D_2 Q^{-\top},
\end{equation}
where $Q\in\R^{K\times K}$ is some invertible matrix. Substituting the representations~\eqref{temp:tworepres} into the blockwise mixed derivatives of the SCF~\eqref{temp:hessianSCF} and using the expressions~\eqref{temp:E1E2} give
$$
\begin{aligned}
D_1  \phi''_{\alpha} & (D_1^{\top}t_1 + D_2^{\top}t_2) D_2^{\top} \\
&= D_1 Q \phi''_{\beta} (Q^{\top} D_1^{\top} t_1 + Q^{-1}D_2^{\top}t_2) Q^{-1}D_2^{\top}, 
\end{aligned}
$$
for all $t_1\in\R^{M_1}$ and $t_2\in\R^{M_2}$. Since the matrices $D_1$ and $D_2$ have full rank, this can be rewritten as
$$
\begin{aligned}
 \phi''_{\alpha} & (D_1^{\top}t_1 + D_2^{\top}t_2) \\
&= Q \phi''_{\beta} (Q^{\top} D_1^{\top} t_1 + Q^{-1}D_2^{\top}t_2) Q^{-1},
\end{aligned}
$$
which holds for all $t_1\in\R^{M_1}$ and $t_2\in\R^{M_2}$. Moreover, still since $D_1$ and $D_2$ have full rank, we have, for any $u_1, u_2 \in \R^K$ the existence of   $t_1\in\R^{M_1}$ and $t_2\in\R^{M_2}$, such that $u_1 = D_1^\top t_1 $ and $u_2 = D_2^\top t_2$, that is,
\begin{equation}\label{temp:hessians-realtion}
 \phi''_{\alpha} (u_1 + u_2) = Q \phi''_{\beta} (Q^{\top} u_1 + Q^{-1}u_2) Q^{-1},
\end{equation}
for all $u_1,u_2\in\R^K$.



We will now prove two facts:
\begin{enumerate}
\item[(F1)] For any vector $v \in \R^K$, then $\phi''_{\beta} ((Q^{\top} Q -I) v) = - I$, which will imply that $QQ^{\top} = I$ because of the non-Gaussian assumptions.
\item[(F2)] If $QQ^{\top} = I$, then $\phi''_{\alpha}(u) = \phi''_{Q\beta}(u)$ for any $u\in\R^{K}$, which will imply that $Q$ is the composition of a permutation and a scaling. This will end the proof.
\end{enumerate}

\emph{Proof of fact (F1).} By letting $u_1 = Qv$ and $u_2 = - Qv$, we get:
\begin{equation}\label{temp:1}
\phi''_{\alpha} (0) = Q \phi''_{\beta} ( (Q^{\top}Q-I)v) Q^{-1},
\end{equation}
 Since%
\footnote{
Note that $\nabla_u^2\phi_{\alpha}(u) = - \frac{\ebb ({ \alpha \alpha^{\top} e^{iu^{\top}\alpha}})}{\ebb (e^{i u^{\top} \alpha})} + \ecal_{\alpha}(u)\ecal_{\alpha}(u)^{\top}$, where $\ecal_{\alpha}(u)=\frac{\ebb ({ \alpha e^{iu^{\top}\alpha}})}{\ebb (e^{i u^{\top} \alpha})}$.
}
$\phi''_{\alpha}(0)=-\cov(\alpha) = -I$, one gets
$$
\phi''_{\beta} ((Q^{\top}  Q- I)v) = -I,
$$
for any $v\in\R^K$.

Using the property that $\phi''_{A^\top \beta}(v) = A^\top \phi''_\beta(A v) A$ for any matrix $A$, and in particular with $A = Q^{\top} Q - I$, we have that $\phi''_{A^\top \beta}(v) = -A^\top A$, i.e. is constant.


If the second derivative of a function is constant, the function is quadratic. Therefore, $\phi_{A^\top \beta}(\cdot)$ is a quadratic function. Since the SCF completely defines the distribution of its variable (see,e.g., \citetsup{JacPro2004}), $A^\top \beta$ must be Gaussian (the SCF of a Gaussian random variable is a quadratic function). Given Lemma 9 from \citet{Com1994} (i.e., Cramer's lemma: a linear combination of non-Gaussian random variables cannot be Gaussian unless the coefficients are all zero), this implies that $A=0$, and hence $Q^{\top} Q = I$, i.e., $Q$ is an orthogonal matrix.


\emph{Proof of fact (F2)}. Plugging $Q^{\top}=Q^{-1}$ into~\eqref{temp:hessians-realtion}, with $u_1=0$ and $u_2=u$, gives
\begin{equation}\label{temp:2}
\phi''_{\alpha}(u) = Q \phi''_{\beta}(Q^{\top} u) Q^{\top} = \phi''_{Q\beta}(u),
\end{equation}
for any $u\in\R^{K}$.
By integrating both sides of~\eqref{temp:2} and using $\phi_\alpha(0) = \phi_{Q\beta}(0) = 0$, we get that $\phi_\alpha(u) = \phi_{Q\beta}(u) + i \gamma^\top u$ for all $u\in\R^{K}$ for some constant vector $\gamma$. 
Using again that the SCF completely defines the distribution, it follows that $\alpha-\gamma$ and $Q\beta$ have the same distribution. Since both $\alpha$ and $\beta$ have independent components, this is only possible when $Q=\Lambda P$, where $P$ is a permutation matrix and $\Lambda$  is some diagonal matrix \citep[Theorem 11]{Com1994}.

\subsubsection{Case of a single Gaussian source}
Without loss of generality, we assume that the potential Gaussian source is the first one for $\alpha$ and $\beta$.
The first change is in the proof of fact (F1). We use the same argument up to the point where we conclude that $A^\top \beta$ is a Gaussian vector. As only $\beta_1$ can be Gaussian, Cramer's lemma implies that only the first row of $A$ can have non-zero components, that is $A = Q^\top Q  - I = e_1 f^\top$, where $e_1$ is the first basis vector and $f$ any vector. Since $Q^\top Q$ is symmetric, we must have 
$$
Q^\top Q  = I + a e_1 e_1^{\top},
$$
where  $a$ is a constant scalar different than $-1$ as $Q^\top Q $ is invertible. This implies that
$Q^\top Q$ is an invertible diagonal matrix $\Lambda$, and hence
$Q \Lambda^{-1/2}$ is an orthogonal matrix, which in turn implies that $Q^{-1} = \Lambda^{-1} Q^\top$.

Plugging this into~\eqref{temp:hessians-realtion} gives, for any $u_1$ and $u_2$:
$$
 \phi''_{\alpha} (u_1 + u_2) = Q \phi''_{\beta} (Q^{\top} u_1 + \Lambda^{-1} Q^{\top}u_2) \Lambda^{-1} Q^{\top}.
$$
Given that diagonal matrices commute and that $\phi''_{\beta}$ is diagonal for independent sources (see Appendix~\ref{app:gen-cums-alpha}), this leads to
$$
 \phi''_{\alpha} (u_1 + u_2) = Q \Lambda^{-1/2} \phi''_{\beta} (Q^{\top} u_1 + \Lambda^{-1} Q^{\top}u_2) \Lambda^{-1/2} Q^{\top}.
$$
For any given $v \in \R^K$, we are looking for $u_1$ and $u_2$ such that 
$Q^{\top} u_1 + \Lambda^{-1} Q^{\top}u_2 = \Lambda^{-1/2} Q^\top  v$ and $u_1+u_2=v$, which is always possible by setting
$Q^\top u_2 = (\Lambda^{-1/2} + I)^{-1} Q^\top v$ and $Q^\top u_1 = Q^\top v - Q^\top u_2$ by using the special structure of $\Lambda$. Thus, for any $v$,
$$
 \phi''_{\alpha} (v) = Q \Lambda^{-1/2} \phi''_{\beta} (\Lambda^{-1/2} Q^\top  v) \Lambda^{-1/2} Q^{\top}
 =  \phi''_{Q \Lambda^{-1/2} \beta} ( v).
$$
Integrating as previously, this implies that the characteristic function of $\alpha$ and $Q \Lambda^{-1/2} \beta $ differ only by a linear function $i \gamma^\top v$, and thus, that $\alpha - \gamma$ and $Q \Lambda^{-1/2} \beta$ have the same distribution. This in turn, from \citet[Theorem 11]{Com1994}, implies that $Q \Lambda^{-1/2} $ is a product of a scaling and a permutation, which ends the proof.

\subsubsection{Identifiability of discrete CCA~\eqref{dcca} and mixed CCA~\eqref{mcca}}
Given the discrete CCA model, the SCF $\phi_x(t)$ takes the form
$$
\begin{aligned}
\phi_x(t) &= \phi_{\alpha} (D_1^{\top}(e^{it_1}-\one)+D_2^{\top}(e^{it_2}-\one))\\
&+ \phi_{\varepsilon_1}(e^{it_1}-\one) + \phi_{\varepsilon_2}(e^{it_2}-\one),
\end{aligned}
$$
where $e^{it_j}$, for $j=1,2$,  denotes a vector with the $m$-th element equal to $e^{i[t_j]_m}$, and we used the arguments analogous with the non-Gaussian case. The rest of the proof extends with a correction that sometimes one has to replace $D_j$ with $\diag[e^{it_j}]D_j$ and that $u_j =D_j^{\top}(e^{it_j}-\one)$ for $j=1,2$. For the mixed CCA case, only the part related to $x_2$ and $D_2$ changes in the same way as for the discrete CCA case.






















\subsection{The generalized expectation and covariance matrix}
\label{app:generalized-E-cov}


\subsubsection{The generalized expectation and covariance matrix of the sources}
\label{app:gen-cums-alpha}
Note that some properties of the generalized expectation and covariance matrix, defined in~\eqref{gen-expectation} and~\eqref{gen-covariance}, and their natural finite sample estimators are analyzed by \citet{SlaYer2012b}. Note also that we find the name ``generalized covariance matrix'' to be more meaningful than ``charrelation'' matrix as was proposed by previous authors~\citep[see, e.g.][]{SlaYer2012,SlaYer2012b}.

The sources $\alpha=(\alpha_1,\dots,\alpha_K)$ are mutually independent. Therefore, for some $h\in\R^K$, their CGF~\eqref{cgf} $K_{\alpha}(h) = \log\ebb({ e^{\alpha^{\top} h} })$ takes the form
$$
K_{\alpha}(h) = \sumk \log\sbra{\ebb ( e^{\alpha_k h_k} ) }.
$$ 
Therefore, the $k$-th element of the generalized expectation~\eqref{gen-expectation} of $\alpha$ is (separable in $\alpha_k$)
\begin{equation}\label{gen-expectation-alpha}
\sbra{\ecal_{\alpha}(h)}_k = \frac{\ebb ( \alpha_k e^{\alpha_k h_k} )}{\ebb ( e^{\alpha_k h_k} ) }
\end{equation}
and the generalized covariance~\eqref{gen-covariance} of $\alpha$ is diagonal due to the separability and its $k$-th diagonal element is
\begin{equation}\label{gen-covariance-alpha}
\sbra{\ccal_{\alpha}(h)}_{kk} = \frac{ \ebb (\alpha^2_k e^{\alpha_k h_k}) }{\ebb ( e^{\alpha_k h_k} ) } - \sbra{ \ecal_{\alpha} (h)}_k^2.
\end{equation}

\subsubsection{Some expectations of a Poisson random variable}
\label{app:epxectations-of-poisson}
 

Let $x\in\R^M$ be a multivariate Poisson random variable with mean $y\in\R_+^{M}$. Then, for some $t\in\R^M$,
$$
\begin{aligned}
\ebb ({ e^{t^{\top}x } }) &= e^{ y^{\top}(e^t - 1) }, \\
\ebb ({ x_m e^{t^{\top} x} }) &= y_m e^{t_m} e^{ y^{\top}(e^t - 1) }, \\
\ebb ({ x_m^2 e^{t^{\top} x } }) &= \sbra{y_m e^{t_m}+1}y_m e^{t_m} e^{ y^{\top}(e^t - 1) }, \\
\ebb ({ x_{m}x_{m'} e^{t^{\top} x} }) &= y_{m} e^{t_{m}} y_{m'} e^{t_{m'}} e^{ y^{\top}(e^t - 1) }, \quad m\ne m',
\end{aligned}
$$
where $e^t$ denotes an $M$-vector with the $m$-th element equal to $e^{t_m}$.


\subsubsection{The generalized expectation and covariance matrix of discrete ICA}
\label{app:derivations-dica-generalized-covariances}
In this section, we use the expectations of a Poisson random variable presented in Appendix~\ref{app:epxectations-of-poisson}.
 
Given the discrete ICA model~\eqref{dica}, the generalized expectation~\eqref{gen-expectation} of $x\in\R^M$ takes the form
$$
\begin{aligned}
\ecal_x(t) &= \frac{ \ebb (x e^{t^{\top} x}) }{ \ebb (e^{t^{\top} x}) } 
 = \frac{ \ebb \sbra{ \ebb ( x e^{t^{\top} x} | \alpha )} }{ \ebb \sbra{ \ebb (e^{t^{\top} x} | \alpha )} } \\
& = \diag[e^t] D \frac{ \ebb (\alpha e^{\alpha^{\top} h(t) }) }{ \ebb( e^{\alpha^{\top} h(t) }) } \\
& = \diag[e^t] D \ecal_{\alpha} (h(t)),
\end{aligned}
$$
where $t\in\R^M$ is a parameter, $h(t)=D^{\top} (e^t - 1)$, and~$e^t$ denotes an~$M$-vector with the~$m$-th element equal to~$e^{t_m}$. 
Note that in the last equation we used the definition~\eqref{gen-expectation} of the generalized expectation $\ecal_{\alpha} (\cdot)$.

Further, the generalized covariance~\eqref{gen-covariance} of $x$ takes the form
$$
\begin{aligned}
\ccal_x(t) & = \frac{\ebb (xx^{\top} e^{t^{\top}x}) }{\ebb ( e^{t^{\top}x} ) } - \ecal_x(t) \ecal_x(t)^{\top} \\
& = \frac{\ebb \sbra{ \ebb (xx^{\top} e^{t^{\top}x} | \alpha)} }{\ebb\sbra{ \ebb ( e^{t^{\top}x} | \alpha) }  }- \ecal_x(t) \ecal_x(t)^{\top}.
\end{aligned}
$$
Plugging into this expression the expression for~$\ecal_x(t)$ and 
$$
\begin{aligned}
\ebb (xx^{\top} e^{t^{\top}x} | \alpha) &= \diag[e^t]D\ebb ({\alpha\alpha^{\top}e^{\alpha^{\top}h(t)}}) D^{\top} \diag[e^t] \\
&+ \diag[e^t]\diag\sbra{D\ebb ({\alpha e^{\alpha^{\top}h(t)}}) }
\end{aligned}
$$
we get
$$
\ccal_x(t)  = \diag[\ecal_x(t)] + \diag[e^t]{D} \ccal_{\alpha} (h(t)) {D}^{\top}\diag[e^t],
$$
where we used the definition~\eqref{gen-covariance} of the generalized covariance of $\alpha$.

\subsubsection{The generalized CCA S-covariance matrix}
\label{app:derivations-cca-generalized-covariances}
In this section we sketch the derivation of the diagonal form~\eqref{diagGenSmcca} of the generalized S-covariance matrix of mixed CCA~\eqref{mcca}. Expressions~\eqref{diagGenSdcca} and~\eqref{diagGenSncca} can be obtained in a similar way.

Denoting $x=[x_1;\;x_2]$ and $t=[t_1;\;t_2]$ (i.e. stacking the vectors as in~\eqref{stacking}), the CGF~\eqref{cgf} of mixed CCA~\eqref{mcca} can be written as
$$
\begin{aligned}
&K_x(t)  = \log \ebb ({ e^{t_1^{\top} x_1 + t_2^{\top} x_2 } }) \\
&= \log \ebb\sbra{ \ebb({e^{t_1^{\top} x_1 + t_2^{\top} x_2} |\, \alpha, \varepsilon_1, \varepsilon_2}) }\\
& \stackrel{(a)}{=} \log \ebb\sbra{ \ebb({e^{t_1^{\top}x_1} |\, \alpha, \varepsilon_1}) \ebb({e^{t_2^{\top}x_2}|\,\alpha,\varepsilon_2}) } \\
& \stackrel{(b)}{=} \log \ebb\Rra{ e^{t_1^{\top}(D_1\alpha + \varepsilon_1)}e^{(D_2\alpha + \varepsilon_2)^{\top}(e^{t_2}-1)} } \\
& \stackrel{(c)}{=} \log\ebb\Rra{ e^{\alpha^{\top} h(t)} }  
 + \log \ebb \Rra{ e^{\varepsilon_2^{\top}(e^{t_2}-1)} } + \log\ebb ({ e^{t_1^{\top} \varepsilon_1 } }),
\end{aligned}
$$
where $h(t)=({D_1^{\top}t_1 + D_2^{\top}(e^{t_2} - 1})$, in $(a)$ we used the conditional independence of $x_1$ and $x_2$, in $(b)$ we used the first expression from Appendix~\ref{app:epxectations-of-poisson}, and in $(c)$ we used the independence assumption~\eqref{indep}.

The generalized CCA S-covariance matrix is defined as 
$$
S_{12}(t) := \nabla_{t_2} \nabla_{t_1} K_x(t).
$$ 
Its gradient with respect to $t_1$ is
$$
\begin{aligned}
\nabla_{t_1} K_x(t) & = \frac{ D_1\ebb ({\alpha e^{\alpha^{\top} h(t) } }) }{ \ebb ({ e^{\alpha^{\top} h(t) } }) } 
+ \frac{ \ebb ({ \varepsilon_1 e^{t_1^{\top} \varepsilon_1 } }) }{ \ebb ({ e^{t_1^{\top} \varepsilon_1 } }) },
\end{aligned}
$$
where the last term does not depend on $t_2$. Computing the gradient of this expression with respect to $t_2$ gives
$$
\begin{aligned}
S_{12}(t) = D_1 \ccal_{\alpha} ({ h(t) }) \Rra{ \diag [{ e^{t_2} }] D_2 }^{\top},
\end{aligned}
$$
where we substituted expression~\eqref{gen-covariance-alpha} for the generalized covariance of the independent sources.

\subsubsection{Approximation of the T-cumulants with the generalized covariance matrix}
\label{app:dir-derivative-approx}

Let $f_{mm'}(t) = [\ccal_x(t)]_{mm'}$ be a function $\R\to \R^M$ corresponding to the $(m,m')$-th element of the generalized covariance matrix. Then the following holds for its directional derivative at $t_0$ along the direction $t$:
$$
\inner{\nabla f_{mm'}(t_0),t} = \lim_{\delta\to0}\frac{ f_{mm'}(t_0+\delta t) - f_{mm'}(t_0)}{\delta},
$$
where $\inner{\cdot,\cdot}$ stands for the inner product. Therefore, when using the fact that $\nabla f(t_0) = \nabla \ccal_x(t)$ is the generalized cumulant of $x$ at $t_0$ and the definition of a projection of a tensor onto a vector~\eqref{projection}, one obtains for $t_0=\zero$ the approximation of the cumulant $\cum(x)$ with the generalized covariance matrix $\ccal_x(t)$.

Let us define $v_1 = W_1^{\top} u_1$ and $v_1 = W_2^{\top} u_2$ for some $u_1,\,u_2 \in\R^K$. Then, approximations for the T-cumulants~\eqref{Tdcca} of discrete CCA take the following form:
$W_1T_{121}(v_1)W_2$ is approximated by the generalized S-covariances~\eqref{GenSdcca} $S_{12}(t)$ via the following expression
$$
\begin{aligned}
W_1T_{121}(v_1)W_2 & \approx \frac{ W_1 S_{12} (\delta t_1) W_2^{\top} - W_1 S_{12}(\zero) W_2^{\top} }{\delta} \\
& - W_1 \diag(v_1) S_{12} W_2^{\top},
\end{aligned}
$$
where $t_1 = \begin{pmatrix} v_1 \\ 0 \end{pmatrix}$ and $W_1T_{122}(v_2)W_2$ is approximated by the generalized S-covariances $S_{12}(t)$ via
$$
\begin{aligned}
W_1T_{122}(v_2)W_2 & \approx \frac{ W_1 S_{12} (\delta t_2) W_2^{\top} - W_1 S_{12}(\zero) W_2^{\top} }{\delta} 
\\ & - W_1  S_{12} \diag(v_2) W_2^{\top},
\end{aligned}
$$
where $t_2 = \begin{pmatrix} 0 \\ v_2 \end{pmatrix}$ and $\delta$ are chosen to be small.
















\subsection{Finite sample estimators}
\label{app:finite-sample-estimators}
\subsubsection{Finite sample estimators of the generalized expectation and covariance matrix}
\label{app:finite-sample-generalized-covariances}

Following \citet{Yer2000,SlaYer2012b}, we use the most direct way of defining the finite sample estimators of the generalized expectation~\eqref{gen-expectation}  and covariance matrix~\eqref{gen-covariance}.

Given a finite sample $X=\cbra{x_1,x_2, \dots,x_N}$, an estimator of the generalized expectation is
$$
\wh{\ecal}_x (t) = \frac{ \sumn x_n w_n }{\sumn w_n }
$$
where weights $w_n = e^{t^{\top} x_n}$
and an estimator of the generalized covariance is
$$
\wh{\ccal}_x (t) = \frac{ \sumn x_n x_n^{\top} w_n }{\sumn w_n } - \wh{\ecal}_x(t) \wh{\ecal}_x(t)^{\top}.
$$
Similarly, an estimator of the generalized S-covariance matrix is then
$$
\begin{aligned}
\wh{\ccal}_{x_1,x_2} (t) &= \frac{ \sumn x_{1n} x_{2n}^{\top} w_n }{\sumn w_n }
 - \frac{ \sumn x_{1n} w_n }{ \sumn w_n } \frac{ \sumn x_{2n}^{\top} w_n }{ \sumn w_n },
\end{aligned}
$$
where $x=[x_1;\;x_2]$ and $t=[t_1;\;t_2]$ for some $t_1\in\R^{M_1}$ and $t_2\in\R^{M_2}$.

Some properties of these estimators are analyzed by \citet{SlaYer2012b}.

\subsubsection{Finite sample estimators of the DCCA cumulants}
\label{app:finite-sample-cumulants}
In this section, we sketch the derivation of unbiased finite sample estimators for the CCA cumulants $S_{12}$, $T_{121}$, and $T_{122}$. Since the derivation is nearly identical to the derivation of the estimators for the DICA cumulants (see Appendix F.2 of \citet{PodEtAl2015}), all details are omitted.


Given a finite sample $X_1=\cbra{ x_{11}, x_{12},\dots,x_{1N} }$ and $X_2=\cbra{x_{21}, x_{22},\dots,x_{2N}}$, the finite sample estimator of the discrete CCA S-covariance~\eqref{Sdcca}, i.e., $S_{12} := \cum(x_1,x_2)$, takes the form 
\begin{equation}
\label{temp:sample-estimator-s12}
\wh{S}_{12}  = \eta_1 \sbra{ X_1X_2^{\top} - N \webb(x_1) \webb(x_2)^{\top} },
\end{equation}
where $\webb(x_1)=N^{-1}\sumn x_{1n}$, $\webb(x_2) = N^{-1}\sumn x_{2n}$, and $\eta_1 = 1/(N-1)$.

Substitution of the finite sample estimators of the 2nd and 3rd cumulants (see, e.g., Appendix C.4 of \citet{PodEtAl2015}) into the definition of the DCCA T-cumulants~\eqref{Tdcca} leads to the following expressions
\begin{align*}
\wh{W}_1&\wh{T}_{12j}(v_j)\wh{W}_2^{\top} = 
\eta_2 [(\wh{W}_1 X_1) \diag( X_j^{\top} v_j )]\otimes (\wh{W}_2 X_2) \\
&+ \eta_2 \innerp{v_j,\webb(x_j)}  2N [\wh{W}_1\webb(x_1)] \otimes [\wh{W}_2 \webb(x_2)] \\
&- \eta_2 \innerp{v_j,\webb(x_j)} (\wh{W}_1X_1) \otimes (\wh{W}_2 X_2)  \\
&-\eta_2  [ (\wh{W}_1 X_1) (X_j^{\top} v_j) ] \otimes [\wh{W}_2\webb(x_2)] \\
&- \eta_2 [\wh{W}_1\webb(x_1)] \otimes [ (\wh{W}_2 X_2)(X_j^{\top} v_j) ]
 \\
& - \eta_1 (\wh{W}_1^{(j)} X_1) \otimes (\wh{W}_2^{(j)} X_2) \\
& + \eta_1 N [\wh{W}_1^{(j)} \webb(x_1)] \otimes [\wh{W}_2^{(j)} \webb(x_2)],
\end{align*}
where $\eta_2 = N / ( (N-1)(N-2) )$ and $\wh{W}_1^{(1)} = \wh{W}_1\diag(v_1)$, $\wh{W}_2^{(1)}=\wh{W}_2$, $\wh{W}_1^{(2)}=\wh{W}_1$, and $\wh{W}_2^{(2)}=\wh{W}_2\diag(v_2)$.

In the expressions above, $\wh{W}_1$ and $\wh{W}_2$ denote whitening matrices of $\wh{S}_{12}$, i.e. such that $\wh{W}_1 \wh{S}_{12} \wh{W}_2^{\top} = I$.




\subsection{Implementation details}
\label{app:implementation-details}

\subsubsection{Construction of S- and T-cumulants}
\label{app:S-and-T-cumulants}
By analogy with \citet{PodEtAl2015}, the target matrices for joint diagonalization can be constructed from S- and T-cumulants.

When dealing with the S- and T-cumulants, the target matrices are obtained via tensor projections. We define a projection $\tcal(v)\in\R^{M_1\times M_2}$ of a third-order tensor $\tcal\in\R^{M_1\times M_2\times M_3}$ onto a vector $v\in\R^{M_3}$ as
\begin{equation}\label{projection}
[\tcal(v)]_{m_1m_2} := \sum_{m_3=1}^{M_3} [\tcal]_{m_1m_2m_3} v_{m_3}.
\end{equation}
Note that the projection $\tcal(v)$ is a matrix.
Therefore, given $2P$ vectors $\cbra{v_{11},v_{21}, v_{12}, v_{22}, \dots,v_{1P},v_{2P}}$, one can construct $2P + 1$ matrices 
\begin{equation}\label{jdMatricesS&T}
\{S_{12}, \;\; T_{121}(v_{1p}), \;\; T_{122}(v_{2p}), \;\; \mbox{ for } p=1,\dots, P\},
\end{equation}
which have the diagonal form~\eqref{diagSdcca} and~\eqref{diagTdcca}. 
Importantly, the tensors are never constructed (see \citet{AnaEtAl2012,AnaEtAl2014,PodEtAl2015} and Appendix~\ref{app:finite-sample-cumulants}).

\subsubsection{Computation of whitening matrices}
\label{app:compute-whitening}
One can compute such whitening matrices~\eqref{whitening} via the singular value decomposition (SVD) of $S_{12}$. Let $S_{12} = U \Sigma V^{\top}$ be the SVD of $S_{12}$, then one can define $W_1 = U_{1:K} \Lambda$ and $W_2 = V_{1:K} \Lambda$, where $U_{1:K}$ and $V_{1:K}$ are the first $K$ left- and right-singular vectors and $\Lambda=\diag( \sigma_1^{-1/2},\dots,\sigma_K^{-1/2})$ and $\sigma_1,\dots,\sigma_K$ are the $K$ largest singular values. 

Although SVD is computed only once, the size of the matrix $S_{12}$ can be significant even for storage. To avoid construction of this large matrix and speed up SVD, one can use randomized SVD techniques \citep{HalEtAl2011}. Indeed, since the sample estimator $\wh{S}_{12}$ has the form~\eqref{temp:sample-estimator-s12}, one can reduce this matrix by sampling two Gaussian random matrices $\Omega_1 \in\R^{\tilde{K} \times M_1}$ and $\Omega_2 \in\R^{\tilde{K} \times M_2}$, where $\tilde{K}$ is slightly larger than $K$. Now, if $U$ and $V$ are the $K$ largest singular vectors of the reduced matrix $\Omega_1 \wh{S}_{12} \Omega_2$, then $\Omega_1^{\dagger} U$ and $\Omega_2^{\dagger} V$ are approximately (and up to permutation and scaling of the columns) the $K$ largest singular vectors of $\wh{S}_{12}$.

\subsubsection{Applying whitening transform to DCCA T-cumulants}
\label{app:whiten-t-cumulants}
Transformation of the T-cumulants~\eqref{jdMatricesS&T} with whitening matrices $W_1$ and $W_2$ gives new tensors $\wh{T}_{12j} \in\R^{K\times K\times K}$:
\begin{equation}\label{rrrr}
\wh{T}_{12j} := T_{12j} \times_1 W_1^{\top} \times_2 W_2^{\top} \times_3 W_j^{\top},
\end{equation}
where $j=1,2$. Combining this transformation with the projection~\eqref{projection}, one obtains $2P+1$ matrices
\begin{equation}\label{jdmatricesS&Twhitened}
W_1S_{12}W_2^{\top},\; W_1 T_{12j}(W^{\top}_j u_{jp}) W_2^{\top},
\end{equation}
where $p=1,\dots,P$ and $j=1,2$ and we used $v_{jp} = W_j^{\top} u_{jp}$ to take into account whitening along the third direction. By choosing $u_{jp}\in\R^K$ to be the canonical vectors of the $R^K$, the number of tensor projections is reduced from $M=M_1+M_2$ to $2K$.

\subsubsection{Choice of projection vectors or processing points}
\label{app:processing-projection-points}


For the T-cumulants~\eqref{jdMatricesS&T}, we choose the $K$ projection vectors as $v_{1p}=W_1^{\top}e_p$ and $v_{2p}=W_2^{\top}e_p$, where $e_p$ is one of the columns of the $K$-identity matrix (i.e., a canonical vector). For the generalized S-covariances~\eqref{jdmatricesGenS}, we choose the processing points as $t_{1p} = \delta_1 v_{1p}$ and $t_{2p} = \delta_2 v_{2p}$, where $\delta_j$, for $j=1,2$ are set to a small value such as $0.1$ divided by $\sum_m\ebb(|x_{jm}|)/M_j$, for $j=1,2$. 


When projecting a tensor $T_{12j}$ onto a vector, part of the information contained in this tensor gets lost. To preserve all information, one could project a tensor $T_{12j}$ onto the canonical basis of $\R^{M_j}$ to obtain $M_j$ matrices. However, this would be an expensive operation in terms of both memory and computational time. In practice, we use the fact, that the tensor $T_{12j}$, for $J=1,2$, is transformed with whitening matrices~\eqref{rrrr}. Hence, the projection vector has to include multiplication by the whitening matrices. Since they reduce the dimension to $K$, choosing the canonical basis in $\R^{K}$ becomes sufficient. Hence, the choice $v_{1p} = W_1^{\top} e_p$ and $v_{2p} = W_2^{\top} e_p$, where $e_p$ is one of the columns of the $K$-identity matrix.

Importantly, in practice, the tensors are never constructed (see Appendix~\ref{app:finite-sample-cumulants}). 

The choice of the processing points of the generalized covariance matrices has to be done carefully. Indeed, if the values of $t_1$ or $t_2$ are too large, the exponents blow up. Hence, it is reasonable to maintain the values of the processing points very small. Therefore, for $j=1,2$, we set $t_{jp} = \delta_j v_{jp}$ where $\delta_j$ is proportional to a parameter $\delta$ which is set to a small value ($\delta = 0.1$ by default), and the scale is determined by the inverse of the empirical average of the component of $x_j$, i.e.: 
\begin{equation} \label{eq:delta_j}
\delta_j := \delta \frac{ N M_j }{\sum_{n=1}^N \sum_{m=1}^{M_j} [ |X_j| ]_{mn}},
\end{equation}
for $j=1,2$. See Appendix~\ref{app:gdcca-vs-delta} for an experimental comparison of different values of $\delta$ (the default value used in other experiments is $\delta=0.1$).




\subsubsection{Finalizing estimation of $D_1$ and $D_2$}
\label{app:est-D}

The non-orthogonal joint diagonalization algorithm outputs an invertible matrix $Q$. If the estimated factor loading matrices are not supposed to be non-negative (continuous case of NCCA~\eqref{ncca}), then
\begin{equation}\label{temp:finalizing-estimation}
\begin{aligned}
D_1 &= W_1^{\dagger} Q, \\
D_2 &= W_2^{\dagger} Q^{-1},
\end{aligned}
\end{equation}
where $\dagger$ stands for the pseudo-inverse.
For the spectral algorithm, where $Q$ are eigenvectors of a non-symmetric matrix and are not guaranteed to be real, only real parts are kept after evaluating matrices $D_1$ and $D_2$ in accordance with~\eqref{temp:finalizing-estimation}.

If the matrices $D_1$ and/or $D_2$ have to be non-negative (the discrete case of DCCA~\eqref{dcca} and MCCA~\eqref{mcca}), they have to be further mapped. For that, we select the sign of each column such that the vector (column) has less negative than positive components, which is measured by the sum of squares of the components of each sign, (this is necessary since the scaling unidentifiability includes the scaling by $-1$) and then truncate all negative values at 0. 

In practice, due to the scaling unidentifiability, each column of the obtained matrices $D_1$ and $D_2$ can be further normalized to have the unit $\ell_1$-norm. This is applicable in all cases (D/M/NCCA).




\subsection{Jacobi-like joint diagonalization of non-symmetric matrices}
\label{app:nojd}

Given $N$ non-defective (a.k.a.~diagonalizable) not necessary \emph{normal}\footnote{A real matrix $A$ is normal if $A^{\top}A=AA^{\top}$.} matrices 
$$
\acal = \cbra{A_1,\, A_2,\, \dots,\, A_N},
$$ 
where each matrix $A_n\in\R^{M\times M}$, find such matrix $Q\in\R^{M\times M}$ that matrices 
$$
Q^{-1}\acal Q = \cbra{Q^{-1}A_1Q, \, Q^{-1}A_2Q,\, \dots,\,Q^{-1}A_NQ}
$$ 
are (jointly) as diagonal as possible. We refer to this problem as a non-orthogonal joint diagonalization (NOJD) problem.\footnote{An orthogonal joint diagonalization problem corresponds to the case where the matrices $A_1,\,A_2,\,\dots,\,A_N$ are normal and, hence, diagonalizable by an orthogonal matrix $Q$.} 



\begin{algorithm}[!ht]
   \caption{Non-orthogonal joint diagonalization (NOJD)}
   \label{alg:nojd}
\begin{algorithmic}[1]
	\STATE Initialize: $\acal^{(0)}\leftarrow \acal$ and $Q^{(0)} \leftarrow I_M$ and iterations $\ell=0$
	\FOR {sweeps $k=1,2,\dots$}
		\FOR {$p=1,\,\dots,\,M-1$}
			\FOR {$q=p+1,\,\dots,\,M$}
			    \STATE Increase $\ell= \ell + 1$
				\STATE Find the (approx.) shear parameter $y^*$ defined in~\eqref{shear-opt} 
				\STATE Find the Jacobi angle $\theta^*$ defined in~\eqref{unitary-opt} 
				\STATE Update $Q^{(\ell)} \leftarrow Q^{(\ell-1)}S^{(\ell)}_{*}U_*^{(\ell)}$
				\STATE Update $\acal^{(\ell)} \leftarrow U^{(\ell)\top}_{*}S^{(\ell)-1}_{*} \acal^{(\ell-1)} S^{(\ell)}_{*}U^{(\ell)}_{*}$ \label{line:Aupdate}
			\ENDFOR
		\ENDFOR
	\ENDFOR
	\STATE Output: $Q^{(\ell)}$
\end{algorithmic}
\end{algorithm}


\textbf{Algorithm}. Non-orthogonal {J}acobi-like joint diagonalization algorithms have the high level structure which is outlined in Alg.~\ref{alg:nojd}.


The algorithm iteratively constructs the sequence of matrices $\acal^{(\ell)}=\cbra{A_1^{(\ell)},\,A_2^{(\ell)},\,\dots,\,A_N^{(\ell)}}$, which is initialized with $\acal^{(0)}=\acal$. Each such iteration $\ell$ corresponds to a single update (Line~\eqref{line:Aupdate} of Alg.~\eqref{alg:nojd}) of the matrices with the optimal shear $S^{(\ell)}_{*}$ and unitary $U^{(\ell)}_{*}$ transforms:
$$
A_n^{(\ell)} = U^{(\ell)\top}_{*} S^{(\ell)-1}_{*} A_n^{(\ell-1)} S^{(\ell)}_{*}U^{(\ell)}_{*},
$$
where $S^{(\ell)}_{*}=S^{(\ell)}(y^*)$ and $U^{(\ell)}_{*}=U^{(\ell)}(\theta^*)$ for the chosen in accordance with some rules (see below) optimal shear parameter $y^*$ and optimal Jacobi (=Givens) angle $\theta^*$.

For the theoretical analysis purposes, the two transforms are considered separately:
\begin{equation}\label{Aprimes}
\begin{aligned}
A'^{(\ell)}_n &= S^{(\ell)-1}(y) A^{(\ell-1)}_n S^{(\ell)}(y), \\
A^{(\ell)}_n & = A''^{(\ell)}_n = U^{(\ell)\top}(\theta) A'^{(\ell)}_n U^{(\ell)}(\theta).
\end{aligned}
\end{equation}

Each such iteration $\ell$ is a combination of the iteration $k$ and the pivots $p$ and $q$ (see Alg.~\ref{alg:nojd}). The iteration $k$ is referred to as a \emph{sweep}. Within each sweep $k$, $M(M-1)/2$ pivots $p<q$ are chosen in accordance with the lexicographical rule. The rule for the choice of pivots can affect convergence as was analyzed for the single matrix case \citep[see, e.g.,][]{Ruh1968,Ebe1962}, where more sophisticated rules were proposed for the algorithm to have a quadratic convergence phase. However, up to our best knowledge, no such analysis was done for the several matrices case. We assume the simple lexicographical rule all over the paper.

The \emph{shear transform} is defined by the hyperbolic rotation matrix $S^{(\ell)} = S^{(\ell)}(y)$ which is equal to the identity matrix except for the following entries
\begin{equation}\label{shear}
\begin{pmatrix}
S^{(\ell)}_{pp} & S^{(\ell)}_{pq} \\
S^{(\ell)}_{qp} & S^{(\ell)}_{qp} 
\end{pmatrix}
=
\begin{pmatrix}
\cosh y & \sinh y \\
\sinh y & \cosh y
\end{pmatrix},
\end{equation}
where the \emph{shear parameter} $y\in\R$.
The \emph{unitary transform} is defined by the Jacobi (=Givens) rotation matrix $U^{(\ell)}= U^{(\ell)}(\theta)$ which is equal to the identity matrix except for the following entries
\begin{equation}\label{unitary}
\begin{pmatrix}
U^{(\ell)}_{pp} & U^{(\ell)}_{p	q} \\
U^{(\ell)}_{qp} & U^{(\ell)}_{qp} 
\end{pmatrix}
=
\begin{pmatrix}
\cos \theta & \sin \theta \\
-\sin \theta  & \cos \theta
\end{pmatrix},
\end{equation}
where the \emph{Jacobi (=Givens)} angle $\theta\in\sbra{-\frac{\pi}{4}, \frac{\pi}{4}}$.

The following two objective functions are of the central importance for this type of algorithms: (a) the sum of squares of all the off-diagonal elements of the matrices\footnote{In the JUST algorithm \citep{IfeEtAl2009}, this objective function is also considered for the (shear transformed) matrix $\acal'^{(\ell)}$.} $\acal''^{(\ell)}$ which are the transformed with the unitary transform $U^{(\ell)}$  matrices $\acal'^{(\ell)}$:
\begin{equation}
\label{obj-off}
\off\rbra{\acal''^{(\ell)}} = \sum_{n=1}^N \off \rbra{ U^{(\ell)\top}  A'^{(\ell)}_n U^{(\ell)} }
\end{equation}
and (b) the sum of the squared Frobenius norms of the matrices $\acal'^{(\ell)}$ which are the transformed with the share transform $S^{(\ell)}$ matrices $\acal^{(\ell-1)}$:
\begin{equation}\label{obj-fro}
\norm{\acal'^{(\ell)}}_F^2 = \sum_{n=1}^N \norm{ S^{(\ell)-1}A_n^{(\ell-1)}S^{(\ell)}}_F^2.
\end{equation}
We refer to~\eqref{obj-off} as the \emph{diagonality measure} and to~\eqref{obj-fro} as the \emph{normality measure}.

All the considered algorithms find the optimal Jacobi angle $\theta^*$ as the minimizer of the diagonality measure of the (unitary transformed) matrices $\acal''^{(\ell)}$~\eqref{Aprimes}:
\begin{equation}\label{unitary-opt}
\theta^* = \argmin_{\theta\in[-\frac{\pi}{4}, \frac{ \pi}{4}]} \off \rbra{ \acal''^{(\ell)} },
\end{equation}
which admits a unique closed form solution \citep{CarSou1996}.
The optimal shear parameter $y^*$ is found%
\footnote{The JUST algorithm is an exception here, since it minimizes the diagonality measure $\off[{ \acal'^{(\ell)}}]$ of the (shear transformed) matrices $\acal'^{(\ell)}$ with respect to $y$.
}
as a minimizer of the normality measure of the (shear transformed) matrices $\acal'^{(\ell)}$~\eqref{Aprimes}:
\begin{equation}\label{shear-opt}
y^* = \argmin_{y\in\R} \norm{\acal'^{(\ell)}}_F^2.
\end{equation}
All the considered algorithms \citep{FuGao2006,IfeEtAl2009,LucAlb2010} solve this step only approximately. In particular, the sh-rt algorithm \citep{FuGao2006} approximates the equation for finding the nulls of the gradient of the objective; the JUST algorithm \citep{IfeEtAl2009} replaces the normality measure with the diagonality measure and provides a closed form solution for the resulting problem; and the JDTM algorithm \citep{LucAlb2010} replaces the normality measure with the sum of only two squared elements $A'_{n,pq}$ and $A'_{n,qp}$ and provides a closed form solution for the resulting problem.

The three NOJD algorithms can have slightly different convergence properties, however, for the purposes of this paper their performance can hardly be distinguished. That is, the difference in the performance of the algorithms in terms of the $\ell_1$-error of the factor loading matrices is hardly noticeable. For the experiments, we use the JDTM algorithm, the other two algorithms could be equally used. To the best of our knowledge, no theoretical analysis of the NOJD algorithms is available, except for the single matrix case when they boil down to the (non-symmetric) eigenproblem \citep{Ebe1962,Ruh1968}.

The following intuitively explains why the normality measure, i.e. the sum of the squared Frobenius norms, has to be minimized at the shear transform. As \citep{Ruh1968} mention, for every matrix $A$ and non-singular $Q$:
$$
\inf_Q \norm{Q^{-1}AQ}_F^2 = \norm{\Lambda}_F^2,
$$
where $\Lambda$ is the diagonal matrix containing the eigenvalues of $A$. Therefore, a diagonalized version of the matrix $A$ must have the smallest Frobenius norm. Since the unitary transform does not change the Frobenius norm, it can only be minimized with the shear transform. Further, if a matrix is normal, i.e. $A^{\top}A=AA^{\top}$ with a symmetric matrix as a particular case, the upper triangular matrix in its Schur decomposition is zero \citepsup[Chapter 7]{GolVan1996} and then the Schur vectors correspond to the (orthogonal in this case) eigenvectors of this matrix. Therefore, a normal non-defective matrix can be diagonalized by an orthogonal matrix, which preserves the Frobenius norm. Hence, the shear transform by minimizing the normality measure decreases the deviation from normality and then the unitary transform by minimizing the diagonality measure decreases the deviation from diagonality.
















\subsection{Supplementary experiments}
\label{app:supplementary-experiments}

\begin{figure*}[!t]
\centering 
\begin{tabular}{ccc}
\includegraphics[width=.3\textwidth,clip=true, trim=5 5 5 5]{cca_exp_15Feb_K1_1_K2_1_K_1.pdf} &
\includegraphics[width=.3\textwidth,clip=true, trim=5 5 5 5]{cca_exp_15Feb_K1_10_K2_10_K_10.pdf} &
\includegraphics[width=.3\textwidth,clip=true, trim=5 5 5 5]{gdcca_vs_delta_22feb.pdf}
\end{tabular}
\vspace{-1em}
\caption{\textbf{(left and middle)} The continuous synthetic data experiment from Appendix~\ref{sec:exp-continuous}  with $M_1=M_2=20$, $c=c_1=c_2=0.1$ and $L_n=L_s=1000$. The number of factors: \textbf{(left)} $K_1=K_2=K=1$  and \textbf{(middle)} $K_1=K_2=K=10$. \textbf{(right)}: An experimental analysis of the performance of DCCAg with generalized covariance matrices using different parameters $\delta_j$ for the processing points. The numbers in the legend correspond to the values of $\delta$ defining $\delta_j$ via~\eqref{eq:delta_j} in Appendix~\ref{app:processing-projection-points}. The default value (def) is $\delta = 0.1$.
The data is the discrete synthetic data as described in Section~\ref{sec:experiments} with the parameters set as in Fig.~\ref{fig:dica} (right).
\vspace{-1.5em}
}
\label{fig:cca-exp}
\end{figure*}
\setlength{\textfloatsep}{11pt}

The code for reproducing the experiments described in Section~\ref{sec:experiments} as well as in this appendix is available at \href{https://github.com/anastasia-podosinnikova/cca}{https://github.com/anastasia-podosinnikova/cca}.

\subsubsection{Continuous synthetic data}
\label{sec:exp-continuous}
This experiment is essentially a continuous analogue to the synthetic experiment with the discrete data from Section~\ref{sec:experiments}.

\ppp{Synthetic data.} We sample synthetic data from the linear non-Gaussian CCA (NCCA) model~\eqref{linear-cca} with each view $x_j = D_j \alpha + F_j \beta_j$. The (non-Gaussian) sources are $\alpha \sim z_{\alpha} \gam(c,b)$, where $z_{\alpha}$ is a Rademacher random variable (i.e., takes the values $-1$ or $1$ with the equal probabilities). The noise sources are $\beta_j \sim z_{\beta_j} \gam(c_j,b_j)$, for $j=1,2$,  where again $z_{\beta_j}$ is a Rademacher random variable. Parameters of the gamma distribution are initialized by analogy with the discrete case (see Section~\ref{sec:experiments}). The elements of the matrices $D_j$ and $F_j$, for $j=1,2$, are sampled i.i.d. for the uniform distribution in $[-1, \, 1]$. Each column of $D_j$ and $F_j$, for $j=1,2$, is normalized to have the unit $\ell_1$-norm.

\ppp{Algorithms.} We compare gNCCA (the implementation of NCCA with the generalized S-covariance matrices with the default values of the parameters $\delta_1$ and $\delta_2$ as described in Appendix~\ref{app:processing-projection-points}) the spectral algorithm for NCCA (also with the generalized S-covariance matrices) to the JADE algorithm 
\citep[the code is available at 
\href{http://perso.telecom-paristech.fr/~cardoso/Algo/Jade/jadeR.m}{http://perso.telecom-paristech.fr/~cardoso/Algo/Jade/jadeR.m;}][]{CarSou1993}
for independent component analysis (ICA) and to classical CCA.

\ppp{Synthetic experiment.} In Fig.~\ref{fig:cca-exp} (left and middle), the results of the experiment for the different number of topics are presented. The error of the classical CCA is high due to the mentioned unidentifiability issues.




\subsubsection{Sensitivity of the generalized covariance matrices to the choice of the processing points}
\label{app:gdcca-vs-delta}
In this section, we experimentally analyze the performance of the DCCAg algorithm based on the generalized S-covariance matrices vs. the parameters $\delta_1$ and $\delta_2$. We use the experimental setup of the synthetic discrete data from Section~\ref{sec:experiments} with $K_1=K_2=K=10$. The results are presented in Fig.~\ref{fig:cca-exp} (right).





\setlength{\textfloatsep}{9pt}
\begin{table*} 
\centering 
\begin{tabular}{cc|cc|cc|cc} 
              farmers &         agriculteurs &              division &                   no &                  nato &                 otan &                   tax &              imp\^ots  \\ 
           agriculture &            programme &             negatived &                 vote &                kosovo &               kosovo &                budget &               budget  \\ 
               program &             agricole &                paired &             rejet\'ee &                forces &           militaires &               billion &              enfants  \\ 
                  farm &                 pays &               declare &                 voix &              military &               guerre &              families &            \'economie  \\ 
               country &            important &                  yeas &                 mise &                   war &        international &                income &              ann\'ees  \\ 
               support &            probl\`eme &               divided &             pairs &                troops &                 pays &               country &              dollars  \\ 
              industry &                 aide &                  nays &                porte &               country &           r\'efugi\'es &                  debt &                 pays  \\ 
                 trade &          agriculture &                  vote &               contre &                 world &            situation &              students &             finances  \\ 
             provinces &              ann\'ees &                 order &         d\'eclaration &              national &                 paix &              children &             familles  \\ 
                  work &              secteur &                deputy &           suppl\'eant &                 peace &          yougoslavie &                 money &               fiscal  \\ 
               problem &            provinces &             thibeault &                 vice &         international &            milosevic &               finance &            milliards  \\ 
                 issue &                 gens &            mcclelland &           lethbridge &              conflict &               forces &             education &            lib\'eraux  \\ 
                    us &            \'economie &                    ms &              poisson &             milosevic &               serbes &               liberal &               jeunes  \\ 
                   tax &            industrie &               oversee &                  mme &                debate &         intervention &                  fund &                 gens  \\ 
                 world &              dollars &                  rise &              plantes &               support &              troupes &                  care &            important  \\ 
                  help &               mesure &                  past &               harvey &                action &          humanitaire &               poverty &               revenu  \\ 
               federal &                 faut &                  army &             perdront &              refugees &              nations &                  jobs &               mesure  \\ 
             producers &            situation &              peterson &             sciences &                ground &              conflit &              benefits &               argent  \\ 
              national &          r\'eformiste &                  heed &             libert\'e &                happen &             ethnique &                 child &               sant\'e  \\ 
              business &               accord &                 moral &              pri\`ere &                 issue &                monde &                   pay &                payer  \\ 
 \end{tabular}
 \vspace{-.5em}
\caption{ The real data (translation) experiment. Topics 1 to 4. \vspace{-1.5em}} 
\label{tab:1}
\end{table*}





\subsubsection{Real data experiment -- translation topics}
\label{app:translation}  

For the real data experiment, we estimate the factor loading matrices (topics, in the following) $D_1$ and $D_2$ of aligned proceedings of the 36-th Canadian Parliament in English and French languages (can be found at \href{http://www.isi.edu/natural-language/download/hansard/}{http://www.isi.edu/natural-language/download/hansard/}). 

Although going into details of natural language processing (NLP) related problems is not the goal of this paper, we do minor pre-processing (see Appendix~\ref{app:translation-preprocess}) of this text data to improve the presentation of the estimated bilingual topics $D_1$ and $D_2$.

The 20 topics obtained with DCCA are presented in Tables~\ref{tab:1}--\ref{tab:5}. For each topic, we display the 20 most frequent words (ordered from top to bottom in the decreasing order). Most of the topic have quite clear interpretation. Moreover, we can often observe the pairs of words which are each others translations in the topics. Take, e.g., 
\begin{mi}
\item 
the topic 10: the phrase ``pension plan'' can be translated as ``r\'egime de retraite'', the word ``benefits'' as ``prestations'', and abbreviations ``CPP'' and ``RPC'' stand for ``Canada Pension Plan'' and ``R\'egime de pensions du Canada'', respectively;
\item 
the topic 3: ``OTAN'' is the French abbreviation for ``NATO'', the word ``war'' is translated as ``guerre'', and the word ``peace'' as ``paix'';
\item
the topic 9: ``Nisga'' is the name of an Indigenous (or ``aboriginal'') people in British Columbia, the word ``aboriginal'' translates to French as ``autochtontes'', and, e.g., the word ``right'' can be translated as ``droit''.
\end{mi}
Note also that, e.g., in topic 10, although the French words ``ans'' and ``ann\'ees'' are present in the French topic, their English translation ``year'' is not, since it was removed as one of the 15 most frequent words in English (see Appendix~\ref{app:translation-preprocess}).





\subsubsection{Data preprocessing} 
\label{app:translation-preprocess}
For the experiment, we use House Debate Training Set of the Hansard collection, which can be found at \href{http://www.isi.edu/natural-language/download/hansard/}{http://www.isi.edu/natural-language/download/hansard/}.

%
%

To pre-process this text data, we perform case conversion, stemming, and removal of some stop words. For stemming, the SnowballStemmer of the NLTK toolbox by \citetsup{NLTK} was used for both English and French languages. Although this stemmer has particular problems (such as mapping several different forms of a word to a single stem in one language but not in the other), they are left beyond our consideration. Moreover, in addition to the standard stop words of the NLTK toolbox, we also removed the following words that we consider to be stop words for our task\footnote{This list of words was obtained by looking at words that appear in the top-20 words of a large number of topics in a first experiment. Removing these words did not change much the content of the topics, but made them much more interpretable.} (and their possible forms):
\begin{mi}
\item from English: ask, become, believe, can, could, come, cost, cut, do, done, follow, get, give, go, know, let, like, listen, live, look, lost, make, may, met, move, must, need, put, say, see, show, take, think, talk, use, want, will, also, another, back, day, certain, certainly, even, final, finally, first, future, general, good, high, just, last, long, major, many, new, next, now, one, point, since, thing, time, today, way, well, without;
\item from French (translations in brackets): demander (ask), doit (must), devenir (become), dit (speak, talk), devoir (have to), donner (give), ila (he has), met (put), parler (speak, talk), penser (think), pourrait (could), pouvoir (can), prendre (take), savoir (know), aller (go), voir (see), vouloir (want), actuellement, apr\`es (after), aujourd'hui (today), autres (other), bien (good), beaucoup (a lot), besoin (need), cas (case), cause, {c}ela (it), certain, chose (thing), d\'ej\`a (already), dernier (last), \'egal (equal), entre (between), fa{\c{c}}on (way), grand (big), jour (day), lorsque (when), neuf (new), pass\'e (past), plus, point, pr\'esent, pr\^ets (ready), prochain (next), quelque (some), suivant (next), unique.
\end{mi}



After stemming and removing stop words, several files had different number of documents in each language and had to be removed too. The numbers of these files are: 16, 36, 49 55, 88, 103, 110, 114, 123, 155, 159, 204, 229, 240, 2-17, 2-35. 

We also removed the 15 most frequent words from each language. These include:
\begin{mi}
\item in English: Mr, govern, member, speaker, minist(er), Hon, Canadian, Canada, bill, hous(e), peopl(e), year, act, motion, question;
\item in French: gouvern(er), pr\'esident, loi, d\'eput(\'e), ministr(e), canadien, Canada, projet, Monsieur, question, part(y), chambr(e), premi(er), motion, Hon.
\end{mi}
Removing these words is not necessary, but improves the presentation of the learned topics significantly. Indeed, the most frequent words tend to appear in nearly every topic (often in pairs in both languages as translations of each other, e.g., ``member'' and ``d\'eput\'e'' or ``Canada'' in both languages, which confirms one more time the correctness of our algorithm).

%

Finally, we select $M_1=M_2=5,000$ words for each language to form matrices $X_1$ and $X_2$ each containing $N=11,969$ documents in columns. As stemming removes the words endings, we map the stemmed words to the respective most frequent original words when showing off the topics in Tables~\ref{tab:1}-\ref{tab:5}.



\subsubsection{Running time} 
\label{app:running-time}
For the real experiment, the runtime of DCCA algorithm is 24 seconds including 22 seconds for SVD at the whitening step. In general, the computational complexity of the D/N/MCCA algorithms is bounded by the time of SVD plus $O(RNK) + O(NK^2)$, where $R$ is the largest number of non-zero components in the stacked vector $x=[x_1;\;x_2]$, plus the time of NOJD for $P$ target matrices of size $K$-by-$K$. In practice, DCCAg is faster than DCCA.







\bibliographysup{litsup}
\bibliographystylesup{icml2016}









\newpage
\cleardoublepage
\setlength{\textfloatsep}{9pt}
\begin{table*}[h!] 
\centering
\begin{tabular}{cc|cc|cc|cc} 
                 work &              travail &               justice &               jeunes &              business &          entreprises &                 board &           commission  \\ 
               workers &        n\'egociations &                 young &              justice &                 small &              petites &                 wheat &                 bl\'e  \\ 
                strike &         travailleurs &                 crime &             victimes &                 loans &            programme &               farmers &         agriculteurs  \\ 
           legislation &               gr\`eve &             offenders &             syst\'eme &               program &              banques &                 grain &       administration  \\ 
                 union &               emploi &               victims &                crime &                  bank &             finances &             producers &          producteurs  \\ 
             agreement &                droit &                system &               mesure &                 money &            important &             amendment &                grain  \\ 
                labour &             syndicat &           legislation &             criminel &               finance &            \'economie &                market &              conseil  \\ 
                 right &             services &              sentence &        contrevenants &                access &              secteur &             directors &                ouest  \\ 
              services &               accord &                 youth &                peine &                  jobs &               argent &               western &           amendement  \\ 
          negotiations &                 voix &              criminal &                  ans &               economy &              emplois &              election &              comit\'e  \\ 
              chairman &              adopter &                 court &                 juge &              industry &            assurance &               support &          r\'eformiste  \\ 
                public &           r\'eglement &                 issue &              enfants &             financial &          financi\'ere &                 party &               propos  \\ 
                 party &              article &                   law &            important &               billion &              appuyer &                  farm &            important  \\ 
             employees &               retour &             community &                 gens &               support &               cr\'eer &           agriculture &               compte  \\ 
            collective &                 gens &                 right &            tribunaux &                  ovid &                 choc &                clause &                 prix  \\ 
                agreed &              conseil &                reform &                droit &                merger &               acc\'es &                ottawa &                   no  \\ 
                 board &       collectivit\'es &               country &            probl\`eme &           information &            milliards &                    us &         dispositions  \\ 
           arbitration &               postes &               problem &          r\'eformiste &                  size &               propos &                  vote &          information  \\ 
                 grain &                grain &                person &              trait\'e &                 korea &                  pme &                   cwb &               mesure  \\ 
                 order &              tr\'esor &               support &                 faut &             companies &              obtenir &                states &             produits  \\ 
 \end{tabular}
\caption{ The real data (translation) experiment. Topics 5 to 8. } 
\label{tab:2}
\end{table*}
\setlength{\textfloatsep}{9pt}
\begin{table*}[h!] 
\centering 
\begin{tabular}{cc|cc|cc|cc} 
                nisga &                nisga &               pension &              r\'egime &          newfoundland &                terre &                health &               sant\'e \\ 
                treaty &          autochtones &                  plan &             pensions &             amendment &                droit &              research &            recherche  \\ 
            aboriginal &              trait\'e &                  fund &          cotisations &                school &        modifications &                  care &            f\'ed\'eral  \\ 
             agreement &               accord &              benefits &          prestations &             education &            provinces &               federal &            provinces  \\ 
                 right &                droit &                public &             retraite &                 right &               \'ecole &             provinces &                soins  \\ 
                  land &              nations &            investment &               emploi &          constitution &              comit\'e &                budget &               budget  \\ 
               reserve &          britannique &                 money &            assurance &             provinces &           \'education &               billion &              dollars  \\ 
              national &            indiennes &          contribution &       investissement &             committee &         enseignement &                social &             syst\'eme  \\ 
               british &                terre &                   cpp &                fonds &                system &             syst\'eme &                 money &             finances  \\ 
              columbia &             colombie &            retirement &              ann\'ees &                reform &              enfants &                   tax &            transfert  \\ 
                indian &            r\'eserves &                   pay &                  ans &              minority &                 vote &                system &            milliards  \\ 
                 court &                  non &               billion &               argent &        denominational &           amendement &            provincial &              domaine  \\ 
                 party &             affaires &                change &            important &            referendum &         constitution &                  fund &              sociale  \\ 
                   law &        n\'egociations &               liberal &       administration &              children &            religieux &               country &              ann\'ees  \\ 
                native &                bande &           legislation &              dollars &                quebec &         r\'ef\'erendum &                quebec &              maladie  \\ 
                   non &          r\'eformiste &                 board &               propos &               parents &              article &              transfer &            important  \\ 
          constitution &         constitution &            employment &            milliards &              students &          r\'eformiste &                  debt &            programme  \\ 
           development &          application &                   tax &                 gens &                change &              qu\'ebec &               liberal &            lib\'eraux  \\ 
                reform &                 user &                  rate &                 taux &                 party &    constitutionnelle &              services &        environnement  \\ 
           legislation &              gestion &             amendment &                  rpc &              labrador &     confessionnelles &                 issue &            assurance  \\ 
 \end{tabular}
\caption{ The real data (translation) experiment. Topics 9 to 12. } 
\label{tab:3}
\end{table*}
\setlength{\textfloatsep}{9pt}
\begin{table*}[h!] 
\centering 
\begin{tabular}{cc|cc|cc|cc} 
                party &                 pays &                   tax &               agence &                quebec &              qu\'ebec &                 court &              p\^eches  \\ 
               country &            politique &             provinces &            provinces &               federal &          qu\'eb\'ecois &                 right &                droit  \\ 
                 issue &            important &                agency &               revenu &           information &            f\'ed\'eral &             fisheries &                 juge  \\ 
                    us &              comit\'e &               federal &              imp\^ots &             provinces &            provinces &              decision &                cours  \\ 
                debate &            lib\'eraux &               revenue &               fiscal &            protection &           protection &                  fish &                 gens  \\ 
               liberal &          r\'eformiste &             taxpayers &            f\'ed\'eral &                 right &       renseignements &                 issue &            d\'ecision  \\ 
             committee &                 gens &          equalization &        contribuables &           legislation &                droit &                   law &            important  \\ 
                  work &               d\'ebat &                system &                payer &            provincial &            personnel &                  work &                 pays  \\ 
                 order &               accord &              services &                 taxe &                person &               priv\'e &                    us &              trait\'e  \\ 
               support &        d\'emocratique &        accountability &        p\'er\'equation &                   law &            prot\'eger &                 party &         conservateur  \\ 
                reform &          qu\'eb\'ecois &             amendment &               argent &          constitution &        \'electronique &                debate &              r\'egion  \\ 
              election &           r\'eglement &               billion &             services &               privacy &              article &               justice &            probl\`eme  \\ 
                 world &               propos &                 money &             fonction &               country &             commerce &               problem &             supr\'eme  \\ 
                quebec &            coll\'egue &                 party &             modifier &            electronic &          provinciaux &             community &            tribunaux  \\ 
              standing &        parlementaire &            provincial &              article &                 court &                 bloc &               supreme &                 faut  \\ 
              national &              appuyer &                public &           minist\'ere &                  bloc &                  vie &               country &            situation  \\ 
              interest &           opposition &              business &       administration &              students &          application &                  area &             victimes  \\ 
             important &           \'elections &                reform &         d\'eclaration &               section &             citoyens &                  case &              appuyer  \\ 
                 right &                 bloc &                office &                  tps &                 clear &                  non &                 order &               mesure  \\ 
                public &            industrie &               support &          provinciaux &                states &            nationale &            parliament &               trouve  \\ 
 \end{tabular}
\caption{The real data (translation) experiment.  Topics 13 to 16. } 
\label{tab:4}
\end{table*}
\setlength{\textfloatsep}{9pt}
\begin{table*}[t]
\centering  
\begin{tabular}{cc|cc|cc|cc} 
          legislation &            important &              national &            important &                  vote &                 voix &                 water &                  eau  \\ 
                 issue &        environnement &                  area &                 gens &                  yeas &                   no &                 trade &           ressources  \\ 
             amendment &               mesure &                 parks &        environnement &              division &              adopter &             resources &               accord  \\ 
             committee &              enfants &                  work &                parcs &                  nays &                 vote &               country &        environnement  \\ 
               support &              comit\'e &               country &                 pays &                agreed &                  non &             agreement &            important  \\ 
            protection &               propos &                    us &               marine &                deputy &               contre &             provinces &            industrie  \\ 
           information &                 pays &           development &               mesure &                paired &            d\'epenses &              industry &          am\'ericains  \\ 
              industry &              appuyer &               support &               propos &           responsible &               accord &            protection &                 pays  \\ 
             concerned &           protection &             community &            f\'ed\'eral &              treasury &              conseil &                export &            provinces  \\ 
                 right &              article &               federal &               jeunes &               divided &               budget &         environmental &         exportations  \\ 
             important &                droit &                 issue &              appuyer &                 order &              cr\'edit &                    us &             \'echange  \\ 
                change &               accord &           legislation &              ann\'ees &                fiscal &              tr\'esor &            freshwater &         conservateur  \\ 
                 world &                 gens &                  help &            assurance &                amount &                  oui &               federal &      responsabilit\'e  \\ 
                   law &           amendement &               liberal &              gestion &               pleased &                 mise &                 world &                effet  \\ 
              families &              adopter &                 world &         conservateur &                budget &               propos &                 issue &            quantit\'e \\ 
                  work &            industrie &           responsible &               accord &                    ms &                porte &           legislation &              trait\'e  \\ 
              children &                  non &             concerned &              r\'egion &        infrastructure &                  lib &           environment &             commerce  \\ 
                 order &            soci\'et\'e &             committee &            probl\`eme &                 board &             pairs &           responsible &                 unis  \\ 
              national &                porte &               problem &            nationale &               consent &            veuillent &           development &            \'economie  \\ 
                states &                   no &             important &              qu\'ebec &             estimates &                 vice &               culture &                alena  \\ 
 \end{tabular}
\caption{The real data (translation) experiment.  Topics 17 to 20. } 
\label{tab:5}
\end{table*}












\end{document} 


%
%
%
%
%
%
%
%
%
%
\documentclass{article}



\usepackage{graphicx}
\PassOptionsToPackage{round}{natbib} % like JMLR, ICML, etc. -- round is prettier! ;)
\usepackage[final]{nips_2017_forarxiv}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algpseudocode}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{caption}
\captionsetup[table]{skip=5pt}
\newcommand{\note}[1]{{\textbf{\color{red}#1}}}
\newcommand{\SEARNN}{\textsc{SeaRnn}}
\newcommand{\SEARN}{\textsc{Searn}}
\newcommand{\REINFORCE}{\textsc{Reinforce}}
\newcommand{\ACTORCRITIC}{\textsc{Actor-Critic}}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}


\hypersetup{
	plainpages=false,
	colorlinks=true,              % Use link colors
	linkcolor=blue,               % Color for normal internal links
	anchorcolor=blue,             % Color for anchor text
	citecolor=blue,               % Color for bibliographical citations in text
	filecolor=blue,               % Color for URLs which open local files
	pagecolor=blue,               % Color for links to other pages
	urlcolor=blue,                % Color for linked URLs
	pdfview=FitH,                 % Fit the width of the document
	pdfstartview=FitH,            % Fit the width of the document
	pdfpagelayout=SinglePage      % Page-down goes to next page
}



\title{Seq2L2S}




\author{
	R\'emi Leblond\thanks{Equal contribution.} \qquad Jean-Baptiste Alayrac\footnotemark[1] \qquad Anton Osokin \\
	Département d’informatique de l’ENS \\ 
	École normale supérieure, CNRS, INRIA, PSL Research University\\
	\And
	Simon Lacoste-Julien \\	
	Department of CS \& OR\\
	Université de Montr\'eal
}


\title{\textsc{SeaRnn}: Training RNNs with global-local losses}


\begin{document}
	\maketitle

	\begin{abstract}
		We propose \SEARNN, a novel training algorithm for recurrent neural networks (RNNs) inspired by the ``learning to search'' (L2S) approach to structured prediction.
		RNNs have been widely successful in structured prediction applications such as machine translation or parsing, and are commonly trained using maximum likelihood estimation (MLE).
		Unfortunately, this training loss is not always an appropriate surrogate for the test error: by only maximizing the ground truth probability, it fails to exploit the wealth of information offered by structured losses.
		Further, it introduces discrepancies between training and predicting (such as exposure bias) that may hurt test performance.
		Instead, \SEARNN\ leverages test-alike search space exploration to introduce global-local losses that are closer to the test error.
		We demonstrate improved performance over MLE on three different tasks: OCR, spelling correction and text chunking.
		Finally, we propose a subsampling strategy to enable \SEARNN\ to scale to large vocabulary sizes.
		\end{abstract}

	\section{Introduction}
	Recurrent neural networks (RNNs) have been recently quite successful in structured prediction applications such as machine translation~\citep{Sutskever2014}, parsing~\citep{Ballesteros2016b} or caption generation~\citep{Vinyals2015}.
	These models use the same repeated \emph{cell} (or unit) to output a sequence of tokens one by one.
	As each prediction takes into account all previous predictions, this cell learns to output the next token conditioned on the previous ones.
	The standard training loss for RNNs is derived from maximum likelihood estimation (MLE): we consider that the cell outputs a probability distribution at each step in the sequence, and we seek to maximize the probability of the ground truth.

	Unfortunately, this training loss is not a particularly close surrogate to the various test errors we want to minimize.
	A striking example of discrepancy is that the MLE loss is close to 0/1: it makes no distinction between candidates that are close or far away from the ground truth (with respect to the structured test error), thus failing to exploit valuable information.
	Another example of train/test discrepancy is called \textit{exposure} or \textit{exploration bias}~\citep{Ranzato2016b}: in traditional MLE training the cell learns the conditional probability of the next token, based on the previous ground truth tokens -- this is often referred to as \textit{teacher forcing}.
	However, at test time the model does not have access to the ground truth, and thus feeds its own previous predictions to its next cell for prediction instead.

	Improving RNN training thus appears as a relevant endeavor, which has received much attention recently.
	In particular, ideas coming from reinforcement learning (RL), such as the \textsc{Reinforce} and \textsc{Actor-Critic} algorithms~\citep{Ranzato2016b, Bahdanau2016}, have been used to derive training losses that are more closely related to the test error that we actually want to minimize.

	In order to address the issues of MLE training, we propose instead to use ideas from the structured prediction field, in particular from the ``learning to search'' (L2S) approach introduced by~\citet{Daume2009b} and later refined by~\citet{Ross2014c} and~\citet{Chang2015} among others.

	\paragraph{Contributions.}
	In Section~\ref{sec:RNN}, we review the limitations of MLE training for RNNs in details.
	We also clarify some related claims made in the recent literature.
	In Section~\ref{sec:L2S}, we make explicit the strong links between RNNs and the L2S approach.
	In Section~\ref{sec:TL}, we present \SEARNN, a novel training algorithm for RNNs, using ideas from L2S to derive a  \emph{global-local} loss that is much closer to the test error than MLE.
	We demonstrate that this novel approach leads to significant improvements on three difficult structured prediction tasks, including a spelling correction problem recently introduced in~\citet{Bahdanau2016}.
	As this algorithm is quite costly, in Section~\ref{sec:scaling} we investigate scaling solutions.
	We propose a subsampling strategy that allows us to considerably reduce training times while maintaining improved performance compared to MLE.
	Finally, in Section~\ref{sec:related} we contrast our novel approach to the related RL-inspired methods.

	\vspace{-1mm}
	\section{Traditional RNN training and its limitations}\label{sec:RNN}
	\vspace{-1.5mm}
	RNNs are a large family of neural network models aimed at representing sequential data.
	To do so, they produce a sequence of states $(h_1, ..., h_T)$ by recursively applying the same transformation (or \emph{cell}) $f$ on the sequential data: $h_t = f(h_{t-1}, y_{t-1}, x)$, with $h_0$ an initial state and $x$ an optional input.

	Many possible design choices fit this framework.
	We focus on a subset typically used for structured prediction, where we want to model the \emph{joint probability} of a target sequence $(y_1, \ldots, y_{T_x}) \in \mathcal{A}^{T_x}$ given an input $x$ (e.g. the \emph{decoder} RNN in the encoder-decoder architecture~\citep{Sutskever2014, Cho2014}).
	Here $\mathcal{A}$ is the alphabet of output tokens and $T_x$ is the length of the output sequence associated with input $x$ (though $T_x$ may take different values, in the following we drop the dependency in $x$ and use $T$ for simplicity).
	To achieve this modeling, we feed $h_t$ through a projection layer (i.e. a linear classifier) to obtain a vector of scores $s_t$ over all possible tokens $a \in \mathcal{A}$, and normalize these with a softmax layer (an exponential normalizer) to obtain a distribution $o_t$ over tokens:
	\begin{equation}
	h_t = f(h_{t-1}, y_{t-1}, x) \,; \qquad s_t=\textnormal{proj}(h_t)\,; \quad o_t = \textnormal{softmax}(s_t)  \qquad \forall \, 1\leq t \leq T \,.
	\end{equation}
	The vector $o_t$ is interpreted as the predictive conditional distribution for the $t^{\text{th}}$ token given by the RNN model, i.e. $p(a | y_1, \ldots, y_{t-1},x) := o_t(a)$ for $a \in \mathcal{A}$.
	Multiplying the values $o_t(y_t)$ together thus yields the joint probability of the sequence $y$ defined by the RNN (thanks to the chain rule):
	\begin{equation}
	p(y_1, ..., y_T | x) = p(y_1 | x) p(y_2 | y_1, x)\,... \,p(y_T | y_1, ..., y_{T-1}, x) := \Pi_{t=1}^T o_t(y_t) \, .
	\end{equation}
	As pointed in~\citet{Goodfellow2016}, the underlying structure of these RNNs as graphical models is thus a complete graph, and there is no conditional independence assumption to simplify the difficult prediction task of computing $\argmax_{y \in \mathcal{Y}} p(y|x)$. In practice, one typically uses either beam search to approximate this decoding, or a sequence of \emph{greedy} predictions $\hat{y}_t := \arg\max_{a \in \mathcal{A}} p(a|\hat{y}_1, \ldots, \hat{y}_{t-1}, x)$.

	If we use the ``teacher forcing'' regimen, where the inputs to the RNN cell are the ground truth tokens (as opposed to its own greedy predictions), we obtain the probability of each ground truth sequence according to the RNN model.
	We can then use MLE to derive a loss to train the RNN.
	One should note here that despite the fact that the individual output probabilities are at the token level, the MLE loss involves the joint probability (computed via the chain rule) and is thus at the \textit{sequence level}.

	\vspace{-1.5mm}
	\paragraph{The limitations of MLE training.}
	While this maximum likelihood style of training has been very successful in various applications, it suffers from several known issues, especially for structured prediction problems.
	The first one is called \textit{exposure} or \textit{exploration bias}~\citep{Ranzato2016b}.
	During training (with teacher forcing), the model learns the probabilities of the next tokens conditioned on the ground truth.
	But at test time, the model does not have access to the ground truth and outputs probabilities are conditioned on its own previous predictions instead.
	Therefore if the predictions differ from the ground truth, the model has to continue based on an exploration path it has not seen during training, which means that it is less likely to make accurate predictions.
	This can lead to a compounding of errors, where mistakes in prediction accumulate and prevent good performance.

	The second major issue is the discrepancy between the training loss and the various test errors associated with the tasks for which RNNs are used (e.g. edit distance, F1 score...).
	Of course, a single surrogate is not likely be a good approximation for all these errors.
	One salient illustration is that MLE ignores the information contained in structured losses.
	It is only focusing on maximizing the probability of the ground truth.
	This means that it does not distinguish between a prediction that is very close to the ground truth and one that is very far away.
	Thus, most of the information given by a structured loss is not leveraged with this approach.

	\vspace{-2mm}
	\paragraph{Local vs. sequence-level.}
	Some recent papers~\citep{Ranzato2016b, Wiseman2016b} also point out the fact that since RNNs output next token predictions, their loss is local instead of sequence-level, contrary to the error we typically want to minimize.
	This claim seems to contradict the standard RNN analysis, which postulates that the underlying graphical model is the complete graph: that is, the RNN outputs the probability of the next tokens conditioned on all the previous predictions.
	Thanks to the chain rule, one recovers the probability of the whole sequence.
	Thus the maximum likelihood training loss is indeed a \textit{sequence level} loss, even though we can decompose it in a product of local losses at each cell.
	However, if we assume that the RNN outputs are only conditioned on the last few predictions (instead of all previous ones), then we can indeed consider the MLE loss as local.
	In this setting the underlying graphical model obeys Markovian constraints (as in maximum entropy Markov models (MEMMs)) rather than the complete graph; this corresponds to the assumption that the information from the previous inputs is imperfectly carried through the network to the cell, preventing the model from accurately representing long-term dependencies.


	Given all these limitations, exploring novel ways of training RNNs appears to be a worthy endeavor, and this field has attracted a lot of interest in the past few years.
	Contrary to many papers which try to adapt ideas coming from the reinforcement learning literature, we focus in this paper on the links we can draw with structured prediction, and in particular with the L2S approach.

	\vspace{-1mm}
	\section{Links between RNNs and learning to search}\label{sec:L2S}
	\vspace{-1mm}
	The L2S approach to structured prediction was first introduced by~\citet{Daume2009b}.
	The main idea behind it is a \textit{learning reduction}~\citep{beygelzimer2016learning}: transforming a complex learning problem (structured prediction) into a simpler one that we know how to solve (multiclass classification).
	To achieve this,~\citet{Daume2009b} propose in their \SEARN\ algorithm to train a shared local classifier to predict each token\footnote{Note that the vocabulary used in this literature is slightly different from that of RNNs: tokens are rather referenced as actions, predictions as decisions and models as policies. } \emph{sequentially} (conditioned on all inputs and all past decisions), thus searching greedily step by step in the big combinatorial space of structured outputs.
	The idea that tokens can be predicted one at a time, conditioned on their predecessors, is central to this approach.

	The training procedure is iterative: at the beginning of each round, one uses the current model or policy to build an intermediate dataset to train the shared classifier on.
	The specificity of this new dataset is that each sample is accompanied by a cost vector containing one entry per token in the output vocabulary $\mathcal{A}$.
	To obtain these cost vectors, one starts by applying a \textit{roll-in} strategy to predict all the tokens up to $T$, thus building one trajectory (or exploration path) per sample in the search space.
	Then, at each time step, one picks arbitrarily each possible token (diverging from the roll-in trajectory) and then continues predicting to finish the modified trajectory using a \textit{roll-out} strategy.
	One then computes the cost of all the obtained sequences, and ends up with $T$ vectors (one per time step) of size $|\mathcal{A}|$ (the number of possible tokens) for every sample.
	Figure~\ref{fig:mainpaper} describes the same process for our \SEARNN\ algorithm (although it uses a different classifier).	

	One then extracts features from the ``state'' at each time step $t$ (which encompasses the full input and the previous tokens predicted up to $t$ during the roll-in).
	Combining the cost vectors to these features yields the new intermediary dataset.
	The original problem is thus reduced to multi-class \emph{cost-sensitive} classification, which can be further reduced to binary classification.
	Once the shared classifier has been fully trained on this new dataset, the policy is updated for the next round.
	Theoretical guarantees for various policy updating rules are provided by e.g.~\citet{Daume2009b} and~\citet{Chang2015}. 
	

	\vspace{-1mm}
	\paragraph{Roll-in and roll-out strategies.}
	The strategies used to create the intermediate datasets fulfill different roles.
	The \textit{roll-in} policy controls what part of the search space the algorithm explores, while the \textit{roll-out} policy determines how the cost of each token is computed.
	The main possibilities for both roll-in and roll-out are explored by~\citet{Chang2015}.
	The \textit{reference} policy tries to pick the optimal token based on the ground truth.
	During the roll-in, it corresponds to picking the ground truth.
	For the roll-out phase, while it is easy to compute an optimal policy in some cases (e.g. for the Hamming loss where simply copying the ground truth is also optimal), it is often intractable (e.g. for BLEU score).
	One then uses a heuristic (in our experiments the reference policy is always to copy the ground truth for both roll-in and roll-out).
	The \emph{learned policy} simply uses the current model instead, and the \textit{mixed} policy stochastically combines both.
	According to~\citet{Chang2015}, the best combination when the reference policy is poor is to use a learned roll-in and a mixed roll-out.

	\vspace{-3mm}
	\paragraph{Links to RNNs.}	
	One can identify the following interesting similarities between a greedy approach to RNNs and L2S.
	Both models handle sequence labeling problems by outputting tokens recursively, conditioned on past decisions.
	Further, the RNN ``cell'' is shared at each time step and can thus also be seen as a shared local classifier that is used to make structured predictions, as in the L2S framework.

	Despite this connection, many differences remain.
	For example, while there is a clear equivalent to the roll-in strategy in RNNs, i.e. the decision to train with or without teacher forcing (conditioning the outputs on the ground truth or instead on the previous predictions of the model), there are no roll-outs involved in standard RNN training.
	We thus consider next whether ideas coming from L2S could mitigate the limitations of MLE training for RNNs.
	In particular, one key property of L2S worth porting over to RNN training is that the former fully leverages structured losses information, contrarily to MLE as previously noted.


	\vspace{-1mm}
	\section{Improving RNN training with L2S}\label{sec:TL}
	\vspace{-1mm}
\begin{figure}[t!]
	\centering
	\includegraphics[width=0.9\linewidth]{figs/drawing.pdf}
	\caption{
		\footnotesize
		Illustration of the roll-in/roll-out mechanism used in \SEARNN.
		The goal is to obtain a vector of costs for each cell of the RNN in order to define a \emph{cost sensitive loss} to train the network.
		These vectors have one entry per possible token.
		Here, we show how to obtain the vector of costs for the red cell.
		First, we use a \emph{roll-in} policy to predict until the cell of interest.
		We highlight here the \emph{learned} strategy where the network passes its own prediction to the next cell.
		Second, we proceed to the \emph{roll-out} phase.
		We feed every possible token (illustrated by the red letters) to the next cell and let the model predict the full sequence.
		For each token $a$, we obtain a predicted sequence $\hat{y}_a$.
		Comparing it to the ground truth sequence $y$ yields the associated cost $c(a)$.
	}
	\vspace{-4mm} %
	\label{fig:mainpaper}
\end{figure}

	Since we are interested in leveraging structured loss information, we can try to obtain it in the same fashion as L2S.
	The main tool that L2S uses in order to construct a cost-sensitive dataset is the roll-out policy.
	In many classical structured prediction use cases, one does not need to follow through with a policy because the ``cost-to-go'' that the roll-out yields is either free or easily computable from the ground truth.
	We are however also interested in cases where this information is unavailable, and roll-outs are needed to approximate it (e.g. for machine translation).
	This leads to several questions: How can we integrate roll-outs in a RNN model? How do we use this additional information, i.e. what loss do we use to train the model on? How do we make it computationally tractable?

	\vspace{-1mm}
	\paragraph{The \SEARNN\ Algorithm.}
    The basic idea of the \SEARNN\ algorithm is quite simple: we borrow from L2S the idea of using a \emph{global} loss for each \emph{local} cell of the RNN.
	As in L2S, we first compute a \emph{roll-in} trajectory, following a specific roll-in strategy.
	Then, at each step $t$ of this trajectory, we compute the costs $c_t(a)$ associated with each possible token $a$.
	To do so we pick $a$ at this step and then follow a \emph{roll-out} strategy to finish the output sequence $\hat{y}_a$.
	We then compare $\hat{y}_a$ with the ground truth using the test error itself, rather than a surrogate.
	By repeating this for the $T$ steps we obtain $T$ cost vectors.
	We use this information to derive one \emph{cost-sensitive} training loss for each cell, which allows us to compute an update for the parameters of the model.
	The full process for one cell is illustrated in Figure~\ref{fig:mainpaper}.
	Our losses are \emph{global-local}, in the sense that they appear at the local level but all contain sequence-level information.
	Our final loss is the sum over the $T$ local losses.
	We provide the pseudo-code for \SEARNN\ in Appendix~\ref{app:alg}.

	\vspace{-1mm}
	\paragraph{Choosing a multi-class classifier.}
	\SEARNN\ appears quite similar to L2S, but there are a few key differences that merit more explanation.
	As the RNN cell can serve as a multi-class classifier, in \SEARNN\ we could pick the cell as a (shallow) shared classifier.
	Instead, we pick the RNN itself, thus getting a (deep) shared classifier that also learns the features.
	The difference between the two options is more thoroughly detailed in Appendix~\ref{apx:design}.
	Arbitrarily picking a token $a$ during the roll-out phase can then be done by emulating the teacher forcing technique: if predicted tokens are fed back to the model (say if the roll-out strategy requires it), we use $a$ for the next cell (instead of the prediction the cell would have output).
	We also use $a$ in the output sequence before computing the cost.

	\vspace{-0.5mm}
	\paragraph{Choosing a cost-sensitive loss.}
	We now also explain our choice for the training loss function derived from the cost vectors.
	One popular possibility from L2S is go the full reduction route down to binary classification.
	However, this technique involves creating multiple new datasets (which is hard to implement as part of a neural network), as well as training $|\mathcal{A}|^2$ binary classifiers.
	We rather simply work with the multi-class classifier encoded by the RNN cell with training losses defined next.

	One central idea in L2S is to learn the target tokens the model should aim for.
	This can be more meaningful than blindly imposing the ground truth as target, in particular when the model has deviated from the ground truth trajectory.
	In the specific context of RNN training, we call this approach \emph{target learning}.
	It is related to the \emph{dynamic oracle} concept introduced by~\citet{Golberg2012}.
	We define three losses that follow this principle.
	In the following, each loss is defined at the cell level.
	The global loss is the sum of all $T$ losses.
	$s_t(a)$ refers to the score output by cell $t$ for token $a$.

	\paragraph{Log-loss (LL).}
	Our first loss is a simple log-loss with the minimal cost token as target:
	\begin{equation}
	\mathcal{L}_t(s_t; c_t) = -\log\left(\frac{e^{s_t(a^\star)}} {\textstyle\sum_{i=1}^A e^{s_t(i)}}\right)  \; \text{where} \; a^\star=\argmin_{a \in \mathcal{A}} c(a) \, .
	\end{equation}
	It is structurally similar to MLE, which is a significant advantage from an optimization perspective: as RNNs have mostly been trained using MLE, this allows us to leverage decades of previous work.
	Note that when the reference policy is to always copy the ground truth (which is sometimes optimal, e.g. when the test error is the Hamming loss), $a^\star$ is always the ground truth token.
	LL with reference roll-in and roll-out is in this case \emph{equivalent} to MLE.

	\paragraph{Log-loss with cost-augmented softmax (LLCAS).}
	The log-loss approach appears to be relatively wasteful with the structured information we have access to since we are only exploiting the minimal cost value.
	A slight modification allows us to exploit this information more meaningfully: we add information about the full costs in the exponential, following e.g.~\citet{pletscher10}.
	\begin{equation}
	\mathcal{L}_t(s_t; c_t) = -\log\left(\frac{e^{s_t(a^\star)+c_t(a^\star)}} {\textstyle\sum_{i=1}^A e^{s_t(i)+c_t(i)}}\right)  \; \text{where} \; a^\star=\argmin_{a \in \mathcal{A}} c(a) \, .
	\end{equation}
	The associated gradient update discriminates between tokens based on their costs.
	It leverages the structured loss information more directly and thus mitigates the 0/1 nature of MLE better.

	\paragraph{Structured hinge loss (SHL).} The LLCAS can be seen as a smooth version of the (cost-sensitive) structured hinge loss used for structured SVMs~\citep{tso05svmstruct}, that we also consider:
	\begin{equation}
	\mathcal{L}_t(s_t; c_t) = \max_{a \in \mathcal{A}} \big(s_t(a) + c_t(a)\big)  - s_t(a^\star) \; \text{where} \; a^\star=\argmin_{a \in \mathcal{A}} c(a) \, .
	\end{equation}


	\paragraph{Optimization.}
	Note that we do not need the test error to be differentiable, as our costs $c_t(a)$ are fixed when we minimize our training loss.
	This corresponds to defining a different loss at each round, which is the way it is done in L2S.
	In this case our gradient is unbiased.
	However, if instead we consider that we define a single loss for the whole procedure, then the costs depend on the parameters of the model and we effectively compute an approximation of the gradient.
	Whether it is possible not to fix the costs and to backpropagate through the roll-in and roll-out remains an open problem.

	Another difference between \SEARN\ and RNNs is that RNNs are typically trained using stochastic gradient descent, whereas \SEARN\ is a batch method.
	In order to facilitate training, we decide to go for the stochastic optimization route, by selecting a random mini-batch of samples at each round (as proposed in~\citet{Chang2015}).
	We also choose to do a single gradient step on the parameters with the associated loss (contrary to \SEARN\ where the reduced classifier is fully trained at each round).

	\vspace{-1mm}
	\paragraph{Expected benefits.}
	\SEARNN\ can improve performance because of a few key properties.
	First, our losses leverage the test error, leading to potentially much better surrogates than MLE.

	Second, all of our training losses (even plain LL) leverage the structured information that is contained in the computed costs.
	This is much more satisfactory than MLE which does not exploit this information and ignores nuances between good and bad candidate predictions.
	Indeed, our hypothesis is that the more complex the error is, the more \SEARNN\ can improve performance.

	Third, the exploration bias we find in teacher forcing can be mitigated by using a ``learned'' roll-in strategy, which can be the best roll-in strategy for L2S applications according to~\citet{Chang2015}.

	Fourth, the loss at each cell is \textit{global}, in the sense that the computed costs contain information about full sequences.
	This may help with the classical vanishing gradients problem that is prevalent in RNN training and motivated the introduction of specialized cells such as LSTMs~\citep{Hochreiter1997} or GRUs~\citep{Cho2014}.
	It may also alleviate the label bias issue that might appear if the information is not flowing perfectly through the RNN, as pointed out in Section~\ref{sec:RNN}.

\begin{table}[]
	\centering

\resizebox{\textwidth}{!}{
\begin{tabular}{@{}clccc@{}rl@{\hspace{-0.09cm}}rrrrrr@{}}
	\toprule
	\multicolumn{2}{c}{\multirow{3}{*}{Dataset}} &   \multirow{3}{*}{$A$} & \multirow{3}{*}{$T$} & \multirow{3}{*}{Cost}     & \multirow{3}{*}{\textbf{MLE}}      & & \multicolumn{3}{c}{\textbf{LL}}              & \multicolumn{3}{c}{\textbf{LLCAS}}            \\
	\cmidrule(lr){8-10}
	\cmidrule(l){11-13}
	\multicolumn{4}{c}{}                         &                            &  & \textit{roll-in} & learned & reference   & \multicolumn{1}{r}{learned} & learned & reference   & learned   \\
	\multicolumn{4}{c}{}                         &                            &  & \textit{roll-out}   & \phantom{lel}mixed   & \multicolumn{1}{r}{learned} & learned   & mixed   & learned & learned \\ \midrule
	\multicolumn{2}{c}{OCR}                      & 26           & 15 &    Hamming     & $2.8$ &                               & $1.9$ & $2.5$ & $\textbf{1.8}$                      & $1.9$ & $2.4$ & $1.9$                       \\
	\multicolumn{2}{c}{CoNLL}                      &     22 & 70  & norm. Hamming              & $4.2$ &  & $\textbf{3.7}$ & ${\color{gray}6.1}$ & ${\color{gray}5.6}$                      & ${\color{gray}5.8}$ & ${\color{gray}5.3}$ & ${\color{gray}5.1}$                       \\
	\multirow{2}{*}{Spelling}        & 0.3       &   \multirow{2}{*}{43} & \multirow{2}{*}{10} & \multirow{2}{*}{edit}   & $19.6$ & & $17.8$                                & $19.5$ & $17.9$ & $\textbf{17.7}$                      & $19.6$ & $\textbf{17.7}$   \\
	& 0.5       &        & &                    & $43.0$          &                      & $37.3$ & $43.3$ & $37.5$                      & $\textbf{37.1}$ & $43.3$ & $38.2$                         \\
	\bottomrule

\end{tabular}
}
	\caption{\footnotesize Comparison of the \SEARNN\ algorithm with MLE for different cost sensitive losses and different roll-in/roll-out strategies.
		We provide the number of actions $A$ and the maximum sequence length $T$.
		Note that we use 0.5 as the mixing probability when we use the mixed roll-out strategy.
		Greyed figures indicate unstable runs where we report the test error of the model with minimum validation error.}
	\label{tab:exp_1}
	\vspace*{-0.6cm}
\end{table}

	\vspace{-1mm}
	\paragraph{Experiments.}
	In order to validate these theoretical benefits, we ran \SEARNN\ on three datasets and compared its performance against that of MLE.
	For fair comparison, we use the same optimization routine for all methods. 
	We pick the one that performs best for the MLE baseline. 

	The first dataset is the optical character recognition (OCR) dataset introduced in~\citet{taskar03OCR}.
	The task is to output English words given an input sequence of handwritten characters.
	We use an encoder-decoder model with GRU cells~\citep{Cho2014} of size 128.
	For all runs, we use SGD with constant step-size 0.5 and batch size of 64.
	The cost used in the \SEARNN\ algorithm is the hamming error.
	Performance are reported on the test set with the total hamming error, normalized by the total number of characters.

	The second experiment is run on the text-chunking CoNLL~\citep{Sang2000} dataset (see Appendix~\ref{app:conll} for details).
	We use an encoder-decoder model with 2 layers GRU cells of size 172.
	Our cost here is the sentence-level normalized Hamming error.
    On this dataset, the Adadelta optimizer with learning rate 1 worked best.

	The third dataset is the Spelling dataset introduced in~\citet{Bahdanau2016}.
	The task is to recover correct text from a corrupted version.
    This dataset is synthetically generated from a text corpus (One Billion Word dataset): for each character, we decide with some fixed probability whether or not to replace it with a random one.
	The total number of tokens $A$ is 43 (alphabet size plus a few special characters) and the maximum sequence length $T$ is 10 (sentences from the corpus are clipped).
	We provide results for two sub-datasets generated with the following replacement probabilities:  0.3 and 0.5.
	For this task, we follow~\citet{Bahdanau2016} and use the edit distance as our cost.
	It is defined as the edit distance between predicted sequence and the ground truth sequence over the ground truth length.
	We use an encoder-decoder model with GRU cells of size 100 with the attention mechanism described in~\citep{Bahdanau2016}.
	For all runs, we use the Adam optimizer with learning rate 0.001 and batch size of 128.
	Results are given in Table~\ref{tab:exp_1}.

	\vspace{-1mm}
	\paragraph{Key takeaways.}
	First, \SEARNN\ outperforms MLE by a significant margin on the three different tasks and datasets, which confirms our intuition that taking structured information into account enables better performance.
	Second, we observed that the SHL loss was not improving results in general, while LL and LLCAS  -- which are structurally close to MLE -- achieve better performance.
	This might be explained by the fact that RNN architectures and optimization techniques have been evolving for decades with MLE training in mind.
	Third, the best roll-in/out strategy appears to be combining a learned roll-in and a mixed roll-out, which is consistent with the claims from~\citet{Chang2015}.
	Fourth, the spelling task shows us that the harder the task is (hence the less a simplistic roll-out strategy -- akin to MLE -- is efficient), the stronger the improvements \SEARNN\ makes over MLE.
	One should note that we get improvements even in the case where simply outputting the ground truth is the optimal policy, regardless of the current trajectory.

	\vspace{-0.5mm}
	\section{Scaling up \SEARNN}\label{sec:scaling}
	\vspace{-1mm}
	While \SEARNN\ does provide significant improvements on the two tasks we have tested it on, it comes with a rather heavy price, since a large number of roll-outs (i.e. forward passes) have to be run in order to compute the costs.
	This number, $|\mathcal{A}|T$, is proportional both to the length of the sequences, and to the number of possible tokens.
	\SEARNN\ is therefore not directly applicable to tasks with large output sequences or vocabulary size (such as machine translation) where computing so many forward passes becomes a computational bottleneck.
	Even though forward passes can be parallelized more heavily than backward ones (because they do not require maintaining activations in memory), their asymptotic cost remains in $\mathcal{O}(d T)$, where $d$ is the number of parameters of the model.

	There are a number of ways to mitigate this issue.
	In this paper, we focus on subsampling both the cells and the tokens when computing the costs.
	That is, instead of computing a cost vector for each cell, we only compute them for a subsample of all cells.
	Similarly, we also compute these costs only for a small portion of all possible tokens.
	The speedups we can expect from this strategy are large, since the total number of roll-outs is proportional to both the quantities we are decreasing.

	\vspace{-1mm}
	\paragraph{Sampling strategies.}
	First, we need to decide how we select the steps and tokens that we sample.
	We have chosen to sample steps uniformly when we do not take all of them.
	On the other hand, we have explored several different possibilities for token sampling.
	The first is indeed the uniform sampling strategy.
	We also tested sampling according to pre-computed, corpus-wide statistics.
	Finally, we tried 3 samplings using the current state of our model: stochastic current policy sampling (where we use the current state of the stochastic policy to pick at random), a biased version of current policy sampling where we boost the scores of the low-probability tokens, and finally a \emph{top-k} strategy where we take the top k tokens according to the current policy.
	Note that in all strategies we always sample the ground truth action to make sure that our performance is at least as good as MLE.

	\vspace{-1mm}
	\paragraph{Adapting our losses to sampling.}
	Several losses require computing the costs of all possible tokens at a given step, including LL.
	One could still use LL by simply making the assumption that the token with minimum cost is always sampled.
	However this is a rather strong assumption and it means pushing down the scores of tokens that were not even sampled and hence could not compete with the others.
	To alleviate this issue, we replace the full softmax by a layer applied only on the tokens that were sampled~\citep{Jean2015}.
	While the target can still only be in the sampled tokens, the unsampled tokens are left alone by the gradient update, at least for the first order dependency.
	This trick is even more needed for LLCAS, which otherwise requires a ``reference'' score for unsampled tokens, adding a difficult to tune hyperparameter.
	We refer to these new losses as sLL and sLLCAS.

	\vspace{-1mm}
	\paragraph{Experiments.}
	The main goal of these experiments is to assess whether or not combining subsampling with the \SEARNN\ algorithm is a viable strategy.
	To do so we ran the method on two datasets that we used in the previous section.
	We decided to only focus on subsampling tokens as the vocabulary size is usually the blocking factor rather than the sequence length.
	Thus in the following we always sample all cells.
	We evaluate different sampling strategies and training losses.
	For all experiments, we use the learned strategy for roll-in and the mixed one for roll-out and we sample 5 tokens per cell.
	Finally, we use the same optimization techniques than in the previous experiment.

	\begin{table}[]
		\centering
		\resizebox{\textwidth}{!}{
		\begin{tabular}{@{}clcccllccllccccllc@{}}
			\toprule
			\multicolumn{2}{c}{\multirow{2}{*}{Dataset}} & \multirow{2}{*}{\textbf{MLE}}                                                                            & \multicolumn{5}{c}{\textbf{LL}}                                                                   & \multicolumn{5}{c}{\textbf{sLL}}                                                                   & \multicolumn{5}{c}{\textbf{sLLCAS}}                                           \\
			\cmidrule(r){4-8}
			\cmidrule(lr){9-13}
	        \cmidrule(l){14-18}
			\multicolumn{2}{c}{}                         &  & uni.   & stat.  & \multicolumn{1}{c}{pol.} & \multicolumn{1}{c}{bias.} & \multicolumn{1}{c}{top-k} & uni.   & \multicolumn{1}{c}{stat.} & \multicolumn{1}{c}{pol.} & bias.   & \multicolumn{1}{c}{top-k} & uni.   & stat.  & \multicolumn{1}{c}{pol.} & \multicolumn{1}{c}{bias.} & top-k  \\ \midrule
			\multicolumn{2}{c}{OCR}                      & $2.84$    & $1.94$                   & $\textbf{1.50}$                   & $1.96$                     & $1.84$ & $2.13$                    & $1.82$                   & $1.91$ & $1.86$                     & $2.25$ & $2.69$ & $2.03$                   & $2.33$                   & $\textbf{1.50}$  & $2.37$ & $1.94$ \\
			\multirow{2}{*}{Spelling}        & 0.3       & $19.6$
			&$17.7$
			&$17.7$
			&$17.7$
			&$17.7$
			&$17.8$
			&$18.8$
			&$20.5$
			&$17.5$			
			&$17.7$
			&$18.4$
			&$18.8$
			&$20.2$
			&$17.7$
			&$17.7$
			&$18.2$

			\\


			& 0.5       & $43.0$

			&$37.0$
			&$36.7$
			&$37.1$
			&$36.6$
			&$36.6$
			&$37.4$
			&$41.4$
			&$37.7$
			&$36.9$
			&$41.7$
			&$37.6$
			&$41.7$
			&$37.0$
			&$37.8$
			&$40.5$


			\\ \bottomrule
		\end{tabular}
	}
		\caption{\footnotesize Comparison of the \SEARNN\ algorithm with MLE for different datasets using the sampling approach.
			}
		\label{tab:exp_2}
		\vspace*{-0.6cm}
	\end{table}

	\vspace{-1mm}
	\paragraph{Key takeaways.}
	Results are given in Table~\ref{tab:exp_2}.
	The analysis of this experiment yields interesting observations.
	First, and perhaps most importantly, subsampling appears to be a viable strategy to obtain a large part of the improvements of \SEARNN\ while keeping computational costs under control.
	Indeed, we recover a substantial part of the improvements of the full method while only sampling a fraction of all possible tokens.
	Second, it is difficult to decide on a best strategy for token sampling.
	Consequently, a mixture of several might be the best option.
	Third, it seems also difficult to distinguish a best performing loss.
	Experimentations with larger vocabulary size might be needed to better differentiate the different tokens sampling strategies and losses.
	Finally, this sampling technique yields a 5x speedup, therefore validating our scaling approach.

 	\vspace{-1mm}
	\section{Discussion}\label{sec:related}
	\vspace{-1mm}
	\paragraph{RL-inspired approaches.}
	In structured prediction tasks, we have access to ground truth trajectories, i.e. a lot more information than in traditional RL.
	One major direction of research has been to adapt RL techniques to leverage this additional information.
	The main idea is to try to optimize the expectation of the test error directly (under the stochastic policy parameterized by the RNN):
	\begin{equation}
	\mathcal{L}(\theta) = - \sum_{i=1}^N \mathbb{E}_{(y_1^i, .., y_T^i)  \sim \pi(\theta)} r(y_1^i, .., y_T^i) \, .
	\end{equation}
	Since we are taking an expectation over all possible structured outputs, the only term that depends on the parameters is the probability term (the tokens in the error term are fixed).
	This allows this loss function to support non-differentiable test errors, which is a key advantage.
	Of course, actually computing the expectation over an exponential number of possibilities is computationally intractable.

	To circumvent this issue,~\citet{Shen2016b} subsample trajectories according to the learned policy, while~\citet{Ranzato2016b, Rennie2016} use the \REINFORCE\ algorithm, which essentially approximates the expectation with a single trajectory sample.
	~\citet{Bahdanau2016} adapt the \ACTORCRITIC\ algorithm, where a second \textit{critic} network is trained to approximate the expectation.

	While all these approaches report significant improvement on various tasks, one trait they share is that they only work when initialized from a good pre-trained model.
	This phenomenon is often explained by the sparsity of the information contained in ``sequence-level'' losses.
	Indeed, in the case of \REINFORCE, no distinction is made between the tokens that form a sequence: depending on whether the sampled trajectory is above a global baseline, all tokens are pushed up or down by the gradient update.
	This means good tokens are sometimes penalized and bad tokens rewarded.

	In contrast, \SEARNN\ uses ``global-local'' losses, with a local loss attached to each step, which contains global information since the costs are computed on full sequences.
	To do so, we have to ``sample'' more trajectories through our roll-in/roll-outs.
	As a result, \SEARNN\ does not require warm-starting to achieve good experimental performance.
	This distinction is quite relevant, because warm-starting means initializing in a specific region of parameter space which may be hard to escape.
	Exploration is less constrained when starting from scratch, leading to potentially larger gains over MLE.

	Reinforcement-based models also often require optimizing additional models (\REINFORCE\ needs to learn baselines and \ACTORCRITIC\ the critic model), which can introduce more complexity (e.g. target networks). \SEARNN\ does not.
	This too may contribute to the warm start difference.

	Finally, while minimizing the expected reward allows the RL approaches to use gradient descent even though the test error might not be differentiable, it introduces another discrepancy between training and testing.
	Indeed, at test time, one does not decode by sampling from the stochastic policy.
	Instead, one selects the best performing sequence (according to a search algorithm, such as greedy or beam search).
	\SEARNN\ avoids this averse effect by computing costs using the same decoding technique as the one used at test time, so that its loss can be even closer to the test loss.
	The price we pay here is that we approximate the gradient by fixing the costs, although they are dependent on the parameters.

	\vspace{-1mm}
	\paragraph{L2S-inspired approaches.}
	Several other papers have tried using L2S-like ideas for better RNN training.
	~\citet{Wiseman2016b} propose to include the beam search procedure in a sequence-level loss, following the ``Learning A Search Optimization'' approach of~\citet{Daume2005}.
	Here again the sequence-level loss information is too sparse and warm starting has to be used.
	\citet{Ballesteros2016b} use a loss that is similar to LL for parsing.
	However, their approach is limited to a specific task where cost-to-go are essentially free, whereas \SEARNN\ can be used on any task.
	This limit also affects~\citet{Sun2017}, in which new gradient procedures are introduced to incorporate neural classifiers in the \textsc{AggreVaTe}~\citep{Ross2014c} variant of L2S.

	\vspace{-1mm}
	\paragraph{Conclusion and future work.}
	We have described \SEARNN, a novel algorithm that uses core ideas from the learning to search framework in order to alleviate the known limitations of MLE training for RNNs.
    By leveraging structured cost information obtained through strategic exploration, we define global-local losses.
    These losses give a \emph{global} feedback related to the structured task at hand, distributed \emph{locally} within the cells of the RNN.
	This alternative procedure allows us to train RNNs from scratch and to outperform MLE on three challenging structured prediction tasks.
	Finally we have proposed promising scaling techniques that open up the possibility of applying \SEARNN\ on structured tasks for which the output vocabulary is very large, such as neural machine translation.

	
	\clearpage
	\subsubsection*{Acknowledgments}
	This work was partially supported by a Google Research Award and ERC grant Activia (no. 307574).\\

	{\small
	\bibliography{paper}
	\bibliographystyle{abbrvnat}
}
	\clearpage
	\appendix

	\section{\SEARNN\ algorithm.}
\label{app:alg}
\definecolor{comment}{RGB}{86, 115, 154}
\begin{algorithm}[]
	\caption{\SEARNN\ algorithm (for a simple encoder-decoder network)}
	\label{jcrAlgo}
	\begin{algorithmic}
		\State Initiate the weights $\omega$ of the RNN network.
		\For{ $i$ in 1 to $N$}
		\State Sample $B$ ground truth input/output structured pairs  $\{(x^1, y^1), \cdots,(x^B,y^B)\}$
		\State {\color{comment} \footnotesize \# Perform the roll-in/roll-outs to get the costs. Can be heavily parallelized}
		\For{ $b$ in 1 to $B$}
		\State Compute input features $\phi(x^b)$
		\For{ $t$ in 1 to $T$}
        \State {\color{comment} \footnotesize \# The following roll-in is actually run only once}
		\State Run the RNN until the $t^{\text{th}}$ cell with $\phi(x^b)$  as initial state by following the roll-in strategy
		\State Collect the state in order to perform roll-outs
		\State {\color{comment} \footnotesize \# Roll-outs for all actions in order to collect the cost vector}
		\For{ $a$ in 1 to $A$}
		\State Run the RNN from the $t^{\text{th}}$ cell to the end by enforcing action $a$ at cell $t$
		\State Decode to obtain the output sequence $\hat{y}^b_t(a)$
		\State Collect the cost $c_t^b(a)$ by comparing $\hat{y}^b_t(a)$ and $y^b$

		\EndFor
		\EndFor
		\EndFor

		\State Obtain the losses for each cell with the collected costs
		\State Update the parameters of the network $\omega$ by doing a single gradient step
		\EndFor
	\end{algorithmic}
\end{algorithm}


	\section{Details on CoNLL experiments}

	\label{app:conll}
	The CoNLL-2000~\citep{Sang2000} dataset\footnote{\url{http://www.clips.uantwerpen.be/conll2000/chunking/}} poses the task of text chunking, consisting in splitting an input sentence into syntactically related non-overlapping consecutive groups of words, called phrases or chunks.
	The input sequence~$x$ consists in tuples of words and the corresponding part-of-speech (POS) tags.
	The size of input vocabulary is 17,464 for words and 44 for POS tags.
	The output sequence~$y$ has to be of the same length and each token can take one of the 23 values representing chunks in a so-called BIO encoding.
	The training and test sets consist of 8,936 and 2,012 items, respectively.

	The major difference from OCR consists in the fact that there are two input tokens at each time step.
	We use 50 dimensional embeddings for the words and 44 dimensional embeddings for POS tags.
	Since many words appear in the dataset very few times, we initialize the embeddings from
	SENNA v3.0 embeddings\footnote{\url{http://ronan.collobert.com/senna/}} of~\citet{collobert2011natural}.
	To construct the final dictionary we lower case all the words and take the ones that have SENNA embeddings.
	We also separately add the words that appear in the dataset more than 10 times.
	Finally, we merge all the tokens representing numbers together and assign them to the special <NUM> token.
	The final dictionary is of size 15,222.
	All the out-of-dictionary words are assigned to the special <UNK> token.

	For both the encoder and decoder, we use two layers of GRU cells each with memory dimension of 172.
	In addition, the encoder is bidirectional.
	To improve the decoder, we use the attention mechanism as proposed by~\citet{Bahdanau2015,Bahdanau2016}, the input feeding mechanism as described by~\citet[Section~3.3]{Luong15}, and we add dropout regularization~\citep{srivastava2014dropout} of strength 0.3 between the two layers and on the output of the attention mechanism.

	\section{Design decisions}\label{apx:design}
	\paragraph{Choosing a classifier: to backpropagate or not to backpropagate?}
	In standard L2S, the classifier and the feature extractor are clearly delineated. 
	The latter is a fixed hand-crafted transformation applied on the input and the partial sequence that has already been predicted.
	One then has to pick a classifier and its convergence properties carry over to the initial problem.
	
	In \SEARNN, we choose the RNN itself as our classifier.
	The fixed feature extractor is reduced to the bare minimum (e.g. one-hot encoding) and the classifier performs feature learning afterwards.
	In this setting, the intermediate dataset is the initial state and all previous decisions $(x, y_{1:t-1})$ combined with the cost vector.\footnote{In the encoder-decoder architecture, the decoder RNN does not receive $x$ directly, but rather $\phi(x)$, the features extracted from the input by the encoder RNN. In this case, our \SEARNN\ classifier includes both the encoder and the decoder RNNs.}
	
	An alternative way to look at RNNs, is to consider the RNN cell as a shared classifier in its own right, and the beginning of the RNN (including the previous cells) as a feature extractor.
	One could then pick the RNN cell (instead of the full RNN) as the \SEARNN\ classifier, in which case the intermediate dataset would be $(h_{t-1}, y_{t-1})$\footnote{One could also add $\psi(x)$, features learned from the input through e.g. an attention mechanism.} (the state at the previous step, combined with the previous decision) plus the cost vector.
	
	While this last perspective -- seeing the RNN cell as the shared classifier instead of the full RNN -- is perhaps more intuitive, it actually fits the L2S framework less well.
	Indeed, there is no clear delineation between classifier and feature extractor as these functions are carried out by different instances of the same RNN cell (and as such share weights).
	This means that the feature extraction in this case is learned instead of being fixed.
	
	This choice of classifier has a direct consequence on the optimization routine.
	In case we pick the RNN itself, then each loss gradient has to be fully backpropagated through the network.
	On the other hand, if the classifier is the cell itself, then one should not backpropagate the gradient updates.	

	\paragraph{Reference policy.}
	The reference policy defined by~\citet{Daume2009b} picks the action which ``minimizes the (corresponding) cost, assuming all future decisions are made optimally'', i.e. $\argmin_{y_t} \min_{y_{t+1:T}} l(y_{1:T}, y)$.

	For the roll-in phase, this policy corresponds to always picking the ground truth, since it leads to predicting the full ground truth sequence and hence the best possible loss.

	For the roll-out phase, computing this policy explicitly is easy in a few select cases.
	However, in the general case it is not tractable.
	One then has to turn to heuristics, whose performance can be relatively poor.
	While~\citet{Chang2015} tell us that overcoming a bad reference policy can be done through a careful choice of roll-in/roll-out policies, the fact remains that the better the reference policy is, the better performance will be.
	Choosing this heuristic well is then quite important.

	The most basic heuristic is of to simply use the ground truth.
	Of course, one can readily see that it is not always optimal.
	For example, when the model skips a token and outputs the next one, $a$, instead, it may be more beneficial to also skip $a$ in the roll-out phase rather than to repeat it.

	Although we chose this basic heuristic in this paper, we believe that using tailored alternatives can yield better results for tasks where it is suboptimal, such as machine translation.



\end{document}
\documentclass[10pt,twocolumn,letterpaper]{article}
\pdfoutput=1

\usepackage{cvpr}
%

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{soul}

%
\usepackage{url}
\usepackage{amsfonts,bm}
\usepackage{epstopdf}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{array}
\usepackage{booktabs,siunitx}
\usepackage{footmisc}

\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks]{hyperref}
%
\usepackage{multibib} %
\newcites{sup}{Supplementary References}

%

\newcommand{\Ctext}{C_o} %

\usepackage{algorithmic,algorithm}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

%
\newcommand{\new}[1]{\textcolor{red}{#1}}
\newcommand{\newer}[1]{\textcolor{cyan}{#1}}
%
%
%
%


\newcommand{\jb}[1]{\textcolor{red}{JB: #1}}

%
\hypersetup{
  plainpages=false,
  pdfpagemode=UseNone, %
  %
  colorlinks=true,   %
  linkcolor=blue,    %
  anchorcolor=blue,  %
  citecolor=blue,    %
  filecolor=blue,    %
  pagecolor=blue,    %
  urlcolor=blue,     %
  pdfview=FitH,               %
  pdfstartview=FitH,          %
  pdfpagelayout=SinglePage    %
}
\newcommand{\SLJ}[1]{\textcolor{red}{SLJ: #1}}

\renewcommand{\baselinestretch}{0.96}

\cvprfinalcopy %

\def\cvprPaperID{1383} %
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

%
\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

%
\title{Unsupervised Learning from Narrated Instruction Videos}

\author{
Jean-Baptiste Alayrac\thanks{WILLOW project-team, D\'{e}partement d'Informatique de l'Ecole Normale Sup\'{e}rieure, ENS/INRIA/CNRS UMR 8548, Paris, France.} \ \thanks{SIERRA project-team, D\'epartement d'Informatique de l'Ecole Normale Sup\'{e}rieure, ENS/INRIA/CNRS UMR 8548, Paris, France.}
\and
Piotr Bojanowski\footnotemark[1]
\and
Nishant Agrawal \footnotemark[1] \ \thanks{IIIT Hyderabad} 
\and
Josef Sivic\footnotemark[1]
\and
Ivan Laptev\footnotemark[1] 
\and
Simon Lacoste-Julien\footnotemark[2] 
}

\maketitle

%
\begin{abstract}
   We address the problem of automatically learning the main steps to complete a certain task, such as changing a car tire, 
from a set of narrated instruction videos. 
%
The contributions of this paper are three-fold.
First, we develop a new unsupervised learning approach that takes advantage of the complementary nature of the input video and the associated narration.   
The method solves two clustering problems, one in text and one in video, applied one after each other and linked by joint constraints to obtain a single coherent sequence of steps in both modalities.  
Second, we collect and annotate a new challenging dataset of real-world instruction videos from the Internet. The dataset contains about 800,000 frames for five different tasks\footnote{How to : change a car tire, perform CardioPulmonary resuscitation (CPR), jump a car, repot a plant and make coffee} that include complex interactions between people and objects, and are captured in a variety of indoor and outdoor settings.	
Third, we experimentally demonstrate that the proposed method can automatically discover, in an \emph{unsupervised manner}, the main steps to achieve the task and locate the steps in the input videos. 
%
%
%
%
%
\end{abstract}

%
%
\section{Introduction}
Millions of people watch narrated instruction videos\footnote{Some instruction videos on YouTube have tens of millions of views, 
e.g.~\url{www.youtube.com/watch?v=J4-GRH2nDvw}.} to learn new tasks such as assembling IKEA furniture or changing a flat car tire. 
Many of such tasks have large amounts of videos available on-line. For example, querying for ``how to change a tire'' results in more than 300,000 hits on YouTube.
Most of these videos, however, are made with the intention to teach other people to perform the task and do not provide direct supervisory signal for automatic learning algorithms. Developing unsupervised methods that could learn tasks from myriads of instruction videos on the Internet is therefore a key challenge.
%
Such automatic cognitive ability would enable constructing virtual assistants and smart robots that learn new skills from the Internet to, for example, help people achieve new tasks in unfamiliar situations. 

%
%
%
%
%




%

In this work, we consider instruction videos and develop a method that learns a sequence of steps, as well as their textual and visual representations, required to achieve a certain task. 
For example, given a set of narrated instruction videos demonstrating how to change a car tire, our method automatically discovers consecutive steps for this task such as {\em loosen the nuts of the wheel}, {\em jack up the car}, {\em remove the spare tire} and so on as illustrated in Figure~\ref{fig:mainpaper}. 
In addition, the method learns the visual and linguistic variability of these steps from natural videos.
%

 \begin{figure*}[t]
       \centering
       \includegraphics[width=\linewidth]{figs/teaserfigv2.pdf}
     \caption{\small Given a set of narrated instruction videos demonstrating a particular task, we wish to automatically discover the main steps to achieve the task and associate each step with its corresponding narration and appearance in each video. Here frames from two videos demonstrating changing the car tire are shown, together with excerpts of the corresponding narrations.  Note the large variations in both the narration and appearance of the different steps highlighted by the same colors in both videos (here only three steps are shown).}
     \vspace{-4mm}
     %
     \label{fig:mainpaper}
 \end{figure*}


Discovering key steps from instruction videos is a highly challenging task.
%
%
%
First, linguistic expressions for the same step can have high variability across videos, for example: ``...Loosen up the wheel nut just a little before you start jacking the car...'' and ``...Start to loosen the lug nuts just enough to make them easy to turn by hand...''.
Second, the visual appearance of each step varies greatly between videos as the people and objects are different, the action is captured from a different viewpoint, and the way people perform actions also vary.
Finally, there is also a variability of the overall structure of the sequence of steps achieving the task. For example, 
some videos may omit some steps or change slightly their order.

To address these challenges, in this paper we develop an unsupervised learning approach that takes advantage of the complementarity of the 
visual signal in the video and the corresponding natural language narration to resolve their ambiguities. 
%
We assume that the same ordered sequence of steps (also called script in the NLP literature~\cite{Regneri10learning}) is common to all input videos of the same task, but the actual sequence and the individual steps are unknown and are learnt directly from data. 
This is in contrast to other existing methods for modeling instruction videos~\cite{Malmaud15what} that assume a script (recipe) is known and fixed in advance. 
We address the problem by first performing temporal clustering of text followed by clustering in video, where the two clustering tasks are linked by joint constraints. 
The complementary nature of the two clustering problems helps to resolve ambiguities in the two individual modalities. For example, two video segments with very different appearance but depicting the same step can be grouped together  because they are narrated in a similar language.
Conversely, two video segments described with very different expressions, for example, ``jack up the car'' and ``raise the vehicle'' can be identified as belonging to the same instruction step because they have similar visual appearance.
The output of our method is the script listing the discovered steps of the task as well as the temporal location of each step in the input videos. 
We validate our method on a new dataset of instruction videos composed of five different tasks with a total of 150 videos and about 800,000 frames.

%
%
%
%
%
%
%

%

\section{Related work}

This work relates to unsupervised and weakly-supervised learning methods in computer vision and natural language processing.
Particularly related to ours is the work on learning script-like knowledge from natural language descriptions~\cite{Chambers08,Frermann14,Regneri10learning}. These methods aim to discover typical events (steps) and their order for particular scenarios (tasks)\footnote{We here assign the same meaning to terms ``event'' and ``step'' as well as to terms ``script'' and ``task''.} such as ``cooking scrambled egg'', ``taking a bus'' or ``making coffee''. While~\cite{Chambers08} uses large-scale news copora,~\cite{Regneri10learning} argues that many events are implicit and are not described in such general-purpose text data. Instead,~\cite{Frermann14,Regneri10learning} use event sequence descriptions collected for particular scenarios. Differently to this work, we learn sequences of events from narrated instruction videos on the Internet. Such data contains detailed event descriptions but is not structured and contains more noise compared to the input of~\cite{Frermann14,Regneri10learning}. 
%
%
%

Interpretation of narrated instruction videos has been recently addressed in~\cite{Malmaud15what}. While this work analyses cooking videos at a great scale, it relies on readily-available recipes which may not be available for more general scenarios. Differently from~\cite{Malmaud15what}, we here aim to learn the steps of instruction videos using a discriminative clustering approach.
A similar task to ours is addressed in~\cite{Naim15discriminative} using latent variable structured perceptron algorithm to align nouns in instruction sentences with objects touched by hands in instruction videos.
%
However, similarly to~\cite{Malmaud15what}, \cite{Naim15discriminative} uses laboratory experimental protocols as textual input, whereas here we consider a weaker signal in the form of the real transcribed narration of the video.


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

In computer vision, unsupervised action recognition has been explored in simple videos~\cite{Niebles08}. 
More recently, weakly supervised learning of actions in video using video scripts or event order has been addressed in~\cite{Bojanowski13finding,Bojanowski14weakly,Bojanowski15weakly,Duchenne2009automatic,Laptev08a}. 
Particularly related to ours is the work~\cite{Bojanowski14weakly} which explores the known order of events to localize and learn actions in training data. 
While~\cite{Bojanowski14weakly} uses manually annotated sequences of events, we here discover the sequences of main events by clustering  transcribed narrations of the videos. Related is also the work of
\cite{Bojanowski15weakly} that aligns natural text descriptions to video but in contrast to our approach does not discover automatically the common sequence of main steps. 
Methods in~\cite{Niebles10a,Raptis13} learn in an unsupervised manner the temporal structure of actions from video but do not discover textual expressions for actions as we do in this work.
The recent concurrent work~\cite{Sener15unsupervised} is addressing, independently of our work, a similar problem but with a different approach based on a probabilistic generative model and considering a different set of tasks mainly focussed on cooking activities.
%
%

%

Our work is also related to video summarization and in particular to the recent work on category-specific video summarization \cite{Potapov14category,Sun14ranking}. While summarization is a subjective task, we here aim to extract the key steps required to achieve a concrete task that  consistently appear in the same sequence in the input set of videos. In addition, unlike video summarization~\cite{Potapov14category,Sun14ranking} we jointly exploit visual and linguistic modalities in our approach.

%
%
%

%
%
%
%
%

%


\vspace{3mm}
%
\section{New dataset of instruction videos}
\label{sec:dataset}
%
%
We have collected a dataset of narrated instruction videos for five tasks: \textit{Making a coffee}, \textit{Changing car tire}, \textit{Performing cardiopulmonary resuscitation (CPR)}, \textit{Jumping a car} and \textit{Repotting a plant}. 
The videos were obtained by searching YouTube with relevant keywords. 
The five tasks were chosen so that they have a large number of available videos with English transcripts while trying to cover a wide range of activities that include complex interactions of people with objects and other people.
For each task, we took the top 30 videos with English ASR returned by YouTube.
We also quickly verified that each video contains a person actually performing the task (as opposed to just talking about it).
The result is a total of 150 videos, 30 videos for each task.
%
The average length of our videos is about 4,000 frames (or 2 minutes) and the entire dataset contains about 800,000 frames.

%
%
The selected videos have English transcripts obtained from YouTube's automatic speech recognition (ASR) system.
To remove the dependence of results on errors of the particular ASR method, we have manually corrected misspellings and punctuations in the output transcriptions.
We believe this step will soon become obsolete given rapid improvements of ASR methods. 
As we do not modify the content of the spoken language in videos, the transcribed verbal instructions still represent an extremely challenging example of natural language with large variability in the used expressions and terminology.
%
Each word of the transcript is associated with a time interval in the video (usually less than 5 seconds) obtained from the closed caption timings.

%
%
%
For the purpose of evaluation, we have manually annotated the temporal location in each video of the main steps necessary to achieve the given task.
For all tasks, we have defined the ordered sequence of ground truth steps before running our algorithm.
The choice of steps was made by an agreement of 2-3 annotators who have watched the input videos and verified the steps on instruction video websites such as \url{http://www.howdini.com}.
 While some steps can be occasionally left out in some videos or the ordering slightly modified, overall we have observed a good consistency 
 in the given sequence of instructions among the input videos.
We measured that only 6\% of the step annotations did not fit the global order, while a step was missing from the video 27\% of the time.\footnote{We describe these measurements in more details in the supplementary material given in Appendix~\ref{subsec:score_dataset}.}
%
%
We hypothesize that this could be attributed to the fact that all videos are made with the same goal of giving other humans clear, concise and comprehensible verbal and visual instructions on how to achieve the given task.  
Given the list of steps for each task, we have manually annotated each time interval in each input video to one of the ground truth steps (or no step).
%
%
%
%
%
%
%
%
%
%
The actions of the individual steps are typically separated by hundreds of frames where the narrator transitions between the steps or explains verbally what is going to happen.  
Furthermore, some steps could be missing in some videos, or could be present but not described in the narration.  Finally, the temporal alignment between the narration and the actual actions in video is only coarse as the action is often described before it is performed. 

%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%

\begin{figure*}[t]
\includegraphics[width=1.02\linewidth]{figs/reduced_fig2update.pdf}
   \caption{\small {\bf Clustering transcribed verbal instructions.} {\bf Left:} The input raw text for each video is converted into a sequence of direct object relations. Here, an illustration of four sequences from four different videos is shown.  {\bf Middle:} Multiple sequence alignment is used to align all sequences together. Note that different direct object relations are aligned together as long as they have the same sense, e.g. ``loosen nut" and ``undo bolt".  {\bf Right:} The main instruction steps are extracted as the $K=3$ most common steps in all the sequences. \vspace{-.3cm}
   }
   \label{tab:MSAillustration}
   \vspace*{-0.1cm}
\end{figure*}

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\section{Modelling narrated instruction videos}
\label{subsec:model_var}

We are given a set of $N$ instruction videos all depicting the same task (such as ``changing a tire'').
The $n$-th input video is composed of a video stream of $T_n$ segments of frames $(x^n_t)_{t=1}^{T_n}$ and an audio stream containing a detailed verbal description of the depicted task. 
We suppose that the audio description was transcribed to raw text and then processed to a sequence of $S_n$ text tokens $(d^n_s)_{s=1}^{S_n}$. 
Given this data, we want to automatically recover the sequence of $K$ main steps that compose the given task and locate each step within each input video and text transcription. 

%
 We formulate the problem as two clustering tasks, one in text and one in video, applied one after each other and linked by joint constraints linking the two modalities.
  This two-stage approach is based on the intuition that the variation in natural language describing each task is easier to capture than the visual variability of the input videos. 
 In the first stage, we cluster the text transcripts into a sequence of $K$ main steps to complete the given task.  Empirically, we have found (see results in Sec.~\ref{sec:step_discovery}) that it is possible to discover the sequence of the $K$ main steps for each task with high precision. However, the text itself gives only a poor localization of each step in each video. 
 Therefore, in the second stage we accurately localize each step in each video by clustering the input videos using the sequence of $K$ steps extracted from text as constraints on the video clustering. 
 %
%
%
%
%
%
To achieve this, we use two types of constraints between video and text.  First, we assume that both the video and the text narration follow the same sequence of steps. This results in a global ordering constraint on the recovered clustering. Second, we assume that people perform the action approximately at the same time that they talk about it. This constraint temporally links the recovered clusters in text and video. %
%
%
The important outcome of the video clustering stage is that the $K$ extracted steps get propagated by visual similarity to videos where the text descriptions are missing or ambiguous. 

We first describe the text clustering in Sec.~\ref{subsec:model_text} and then introduce the video clustering with constraints in Sec.~\ref{sec:model-video}.
%
%
%
%

%
%
%


%
%
%



%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\subsection{Clustering transcribed verbal instructions}
\label{subsec:model_text}
\label{sec:text}


The goal here is to cluster the transcribed verbal descriptions of each video into a sequence of \emph{main steps} necessary to achieve the task.  
This stage is important as the resulting clusters will be used as constraints for jointly learning and localizing the main steps in video. 
We assume that the important steps are common to many of the transcripts and that the sequence of steps is (roughly) preserved in all transcripts.  
Hence, following~\cite{Regneri10learning}, we formulate the problem of clustering the input transcripts as a multiple sequence alignment problem.
However, in contrast to~\cite{Regneri10learning} who cluster manually provided descriptions of each step, we wish to cluster transcribed verbal instructions.
Hence our main challenge is to deal with the variability in spoken natural language.  To overcome this challenge, we take advantage of the fact that completing a certain task usually involves interactions with objects or people and hence we can extract a more structured representation from the input text stream.      

More specifically, we represent the textual data as a sequence of {\em direct object relations}.
A direct object relation~$d$  is a pair composed of a verb and its direct object complement, such as ``remove tire".
Such a direct object relation can be extracted from the dependency parser of the input transcribed narration~\cite{Marneffe06generating}.
%
We denote the set of all different direct object relations extracted from all narrations as $\mathcal{D}$,
with cardinality~$D$. 
%
%
For the $n$-th video, we thus represent the text signal as a sequence of direct object relation tokens: $d^n = (d_1^n, \dots, d_{S_n}^n)$, where the length $S_n$ of the sequence varies from one video clip to another. 
This step is key to the success of our method as it allows us to convert the problem of clustering raw transcribed text into an easier problem of clustering sequences of direct object relations.    
%
%
The goal is now to extract from the narrations the most common sequence of  $K$ main steps to achieve the given task. 
%
To achieve this, we first find a globally consistent alignment of the direct object relations that compose all text sequences by solving a multiple sequence alignment problem.
Second, we pick from this alignment the $K$ most globally consistent clusters across videos. %
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\textbf{Multiple sequence alignment model.} We formulate the first stage of finding the common alignment between the input sequences of direct object relations as a multiple sequence alignment problem with the \emph{sum-of-pairs score}~\cite{wang1994msaNPhard}. 
In details, a global alignment can be defined by re-mapping each input sequence $d^n$ of tokens to a global common template of $L$ slots, for $L$ large enough. We let $(\phi(d^n))_{1\leq l \leq L}$ represent
the (increasing) re-mapping for sequence $d^n$ at the new locations indexed by $l$:  $\phi(d^n)_l$ represents the direct object relation put at location $l$, with $\phi(d^n)_l = \varnothing$ if a slot is left empty (denoting the insertion of a gap in the original sequence of tokens). See the middle of Figure~\ref{tab:MSAillustration} for an example of re-mapping.
The goal is then to find a global alignment that minimizes the following sum-of-pairs cost function:
\vspace{-3mm}
\begin{equation}
\sum_{(n,m)} \sum_{l=1}^L c(\phi(d^n)_l,\phi(d^{m})_{l}),
\label{eq:msa_cost}
\end{equation}
where $c(d_1,d_2)$ denotes the cost of aligning the direct object relations $d_1$ and $d_2$ at the same common slot~$l$ in the global template. The above cost thus denotes the sum of all pairwise alignments of the individual sequences (the outer sum), where the quality of each alignment is measured by summing the cost $c$ of matches of individual direct object relations mapped into the common template sequence. We use a negative cost when $d_1$ and $d_2$ 
%
%
%
%
%
%
are similar according to the distance in the WordNet tree~\cite{Fellbaum98Wordnet,Miller95Wordnet} of their verb and direct object constituents, and positive if they are dissimilar (details are given in 
Sec.~\ref{sec:experiments}).
As the verbal narrations can talk about many other things than the main steps of a task,
we set $c(d,d')=0$ if either $d$ or $d'$ is $\varnothing$.
An illustration of clustering the transcribed verbal instructions into a sequence of $K$ steps is shown in Figure~\ref{tab:MSAillustration}.
%
%


\textbf{Optimization using Frank-Wolfe.} 
Optimizing the cost~\eqref{eq:msa_cost} is NP-hard~\cite{wang1994msaNPhard} 
because of the combinatorial nature of the problem. The standard
solution from computational biology is to apply a heuristic algorithm that proceeds by incremental pairwise alignment using dynamic programming~\cite{Lee01poa}.
In contrast, we show in Appendix~\ref{subsec:details_msa} that the multiple sequence alignment problem given by~\eqref{eq:msa_cost} can be reformulated as an integer quadratic program with combinatorial constraints, for which the Frank-Wolfe optimization algorithm has been used recently with increasing success~\cite{Bojanowski14weakly,Jaggi2013,Joulin14efficient,Lacoste15GlobalLinearFW}. 
Interestingly, we have observed empirically (see Appendix~\ref{subsec:comparison_msa}) that
the Frank-Wolfe algorithm was giving better solutions (in terms of objective~\eqref{eq:msa_cost}) than the state-of-the-art heuristic procedures for this task~\cite{Higgins88clustal,Lee01poa}. Our Frank-Wolfe based solvers also offer us greater flexibility in defining the alignment cost and scale better with the length of input sequences and the vocabulary of direct object relations. 
%

\textbf{Extracting the main steps.}
After a global alignment is obtained, we sort the global template $l$
by the number of direct object relations aligned to each slot. Given
$K$ as input, the top $K$ slots give the main instruction steps for
the task, unless there are multiple steps with the same support, which
go beyond $K$. In this case, we pick the next smaller number below $K$
which excludes these ties, allowing the choice of an \emph{adaptive}
number of main instruction steps when there is not enough saliency for
the last steps.  This strategy essentially selects $k\leq K$ salient
steps, while refusing to make a choice among steps with equal support
that would increase the total number of steps beyond $K$.  As we will
see in our results in Sec.~\ref{subsec:exp0}, our algorithm
sometimes returns a much smaller number than $K$ for the main
instruction steps, giving more robustness to the exact choice of
parameter $K$.

\textbf{Encoding of the output.}
We post-process the output of multiple sequence alignment into an assignment matrix $R_n \in \{0, 1\}^{S_n \times K}$ for each input video $n$, 
where $(R_n)_{sk} = 1$ means that the direct object token $d^n_s$ has been assigned to step $k$. 
If a direct object has not been assigned to any step, the corresponding row of the matrix $R_n$ will be zero.

\subsection{Discriminative clustering of videos under text constraints}
\label{sec:model-video}

Given the output of the text clustering that identified the important $K$ steps forming a task,
we now want to find their temporal location in the video signal. We formalize this problem
as looking for an assignment matrix $Z_n\in \{0, 1\}^{T_n \times K}$ for each input video $n$,
where $(Z_n)_{tk} = 1$ indicates the visual presence of step $k$ at time interval~$t$ in video~$n$, and $T_n$ is the length of video~$n$.
Similarly to $R_n$, we allow the possibility that a whole row of $Z_n$ is zero, indicating that no step 
is visually present for the corresponding time interval.

We propose to tackle this problem using a discriminative clustering approach with global
ordering constraints, as was successfully used in the past
for the temporal localization of actions in videos~\cite{Bojanowski14weakly}, 
but with additional \emph{weak temporal constraints}.
In contrast to~\cite{Bojanowski14weakly} where the order of actions was manually
given for each video, our multiple sequence alignment approach automatically
discovers the main steps. More importantly, we also use the \emph{text caption timing}
to provide a fine-grained weak temporal supervision for the visual 
appearance of steps, which is described next.

\setlength{\tabcolsep}{2pt}
\begin{table*}[t]\centering
\resizebox{\textwidth}{!}{
    \footnotesize
    \begin{tabular}{lr >{\centering\hspace{0.5pt}}m{0cm} lr >{\centering\hspace{0.5pt}}m{0cm} lr >{\centering\hspace{0.5pt}}m{0cm} lr >{\centering\hspace{0.5pt}}m{0cm} lr}
    \toprule
         \multicolumn{2}{c}{Changing a tire}  & \phantom{abc} & \multicolumn{2}{c}{Performing CPR} & \phantom{abc} & \multicolumn{2}{c}{Repot a plant} & \phantom{abc} & \multicolumn{2}{c}{Make coffee} & \phantom{abc} & \multicolumn{2}{c}{Jump car} \\
    
\cmidrule{1-2} \cmidrule{4-5} \cmidrule{7-8} \cmidrule{10-11} \cmidrule{13-14} 
GT (\textbf{11}) & $K\leq 10$ && GT (\textbf{7}) & $K\leq 10$  && GT (\textbf{7}) & $K\leq 10$  && GT (\textbf{10)} & $K\leq 10$ && GT (\textbf{12}) & $K\leq 10$  \\
\cmidrule{1-2} \cmidrule{4-5} \cmidrule{7-8} \cmidrule{10-11} \cmidrule{13-14} 
\textit{get tools out}  & \textbf{get tire}     &&  \textit{open airway}   & \textbf{open airway}      &&  \textit{take plant}   & \textbf{remove plant}    && \textit{add coffee}    & \textbf{put coffee}    && \textit{connect red A} & \textbf{connect cable}    \\ 

     
\textit{start loose}  & \textbf{loosen nut}     &&  \textit{check pulse}   & \textbf{put hand}     &&  \textit{put soil}  & \textbf{use soil}    &&   & \textbf{fill chamber}      &&   & \textbf{charge battery}    \\  


\textit{}          & \textbf{put jack}     &&        & \textbf{tilt head}     &&  \textit{loosen roots}    & \textbf{loosen soil}   && \textit{fill water}     & \textbf{fill water}   && \textit{connect red B}  & \textbf{connect end}    \\  


\textit{jack car}  & \textbf{jack car}     &&        & \textbf{lift chin}     &&  \textit{place plant}    &\textbf{place plant}   && \textit{screw filter}  & \textbf{put filter}   && \textit{start car A}  & \textbf{start car}    \\  


\textit{unscrew wheel}  & \textbf{remove nut}     &&  \textit{give breath}   & \textbf{give breath}     &&  \textit{add top}   & \textbf{add soil}    && \textit{}     & \textbf{see steam } &&  \textit{remove cable A}  &\textbf{remove cable}    \\  


\textit{remove wheel}  & \textbf{take wheel}     &&  \textit{do compressions}   & \textbf{do compression}    &&  \textit{water plant}   & \textbf{water plant}    && \textit{put stove}     & \textbf{take minutes }                  &&  \textit{remove cable B}  &\textbf{disconnect cable}   \\  


\textit{put wheel}  & \textbf{take tire}     &&  \textit{}   & \textbf{open airway}      &&      &    && \textit{}    & \textbf{make coffee}   &&    &  \\  


\textit{screw wheel}  & \textbf{put nut}     &&  \textit{}   & \textbf{start compression}     &&      &     &&   \textit{see coffee}  &  \textbf{see coffee}   &&    &     \\  


\textit{lower car}  & \textbf{lower jack}     &&  \textit{}   & \textbf{do compression}     &&      &    &&    \textit{pour coffee}  &  \textbf{make cup}    &&    &     \\  


\textit{tight wheel}  & \textbf{tighten nut}     &&  \textit{}   & \textbf{give breath}      &&      &     &&      &    &&   &     \\  
\midrule
Precision  & 0.9    &&  Precision &   0.4   &&  Precision &   1 && Precision &   0.67  &&  Precision &   0.83   \\
Recall     & 0.9    &&  Recall    &   0.57  &&  Recall &   0.86  && Recall &   0.6 && Recall &   0.42   \\

        \bottomrule
    \end{tabular}
    
}

    \vspace{-2mm}

    \caption{\small 
        Automatically recovered sequences of steps for the five tasks.
        Each recovered step is represented by one of the aligned direct object relations  (shown in bold). 
        Note that most of the recovered steps correspond well to the ground truth steps (shown in italic).
        The results are shown for the maximum number of discovered steps $K$ set to $10$. Note how our method automatically selects less than 10 steps in some cases. These are the automatically chosen $k\leq K$ steps that are the most salient in the aligned narrations as described in Sec.~\ref{subsec:model_text}.  For {\em CPR}, our method recovers fine-grained steps e.g.~{\em tilt head}, {\em lift chin}, which are not included in the main ground truth steps, but nevertheless could be helpful in some situations, as well as repetitions that were not annotated but were indeed present.
%
%
    }
    \label{exp:MSARes}

    \vspace{-4mm}

\end{table*}

\setlength{\tabcolsep}{6pt}

%
\textbf{Temporal weak supervision from text.} 
From the output of the multiple sequence alignment (encoded in the matrix $R_n \in \{0,1\}^{S_n \times K}$),
each direct object token $d_s^n$ has been assigned to one of the possible $K$ steps,
or to no step at all. 
We use the tokens that have been assigned to a step as 
a constraint on the visual appearance of the same step in the video (using
the assumption that people do what they say approximately when they say it).
We encode the closed caption timing alignment by a binary matrix 
$A_n \in \{0,1\}^{S_n \times T_n}$ for each video, 
where $(A_n)_{st}$ is $1$ if the $s$-th direct object is mentioned in a closed caption 
that overlaps with the time interval $t$ in video.
Note that this alignment is only approximate as people 
usually do not perform the action exactly at the same time that they talk about it, 
but instead with a varying delay.
Second, the alignment is noisy as people typically perform the action only 
once, but often talk about it multiple times (e.g. in a summary at the beginning of the video).
We address these issues by the following two \emph{weak supervision} constraints.
First, we consider a larger set of possible time intervals $[t-\Delta_b, t+\Delta_a]$ in the matrix $A$ rather than the exact time interval $t$ given by the timing of the closed caption.
$\Delta_b$ and $\Delta_a$ are global parameters fixed either qualitatively, or by cross-validation
if labeled data is provided. 
Second, we put as a constraint that the action happens at least once in the set of all possible video time intervals where the action is mentioned in the transcript (rather than every time it is mentioned).
These constraints can be encoded as the following linear inequality constraint on $Z_n$: $A_n Z_n \geq R_n$
(see Appendix~\ref{subsec:fw_dp} for the detailed derivation). 

%
\textbf{Ordering constraint.} In addition, we also enforce that the temporal order of the steps appearing
visually is consistent with the discovered script from the text, encoding
our assumption that there is a common ordered script for the task across videos.
We encode these sequence constraints on $Z_n$ in a similar manner to~\cite{Bojanowski15weakly},
which was shown to work better than the encoding used in~\cite{Bojanowski14weakly}.
In particular, we only predict the \emph{most salient} time interval in the video that describes a given step.
This means that a particular step is assigned to \emph{exactly one} time interval in each video.
We denote by $\mathcal{Z}_n$ this sequence ordering constraint set.

%
\textbf{Discriminative clustering.}
The main motivation behind discriminative clustering is to find a \emph{clustering} of the data that can be easily recovered by a \emph{linear classifier} through the minimization of an appropriate \emph{cost function} over the assignment matrix $Z_n$.
The approach introduced in \cite{Bach07diffrac} allows to easily add prior information on the expected clustering.
Such priors have been recently introduced in the context of aligning video and text~\cite{Bojanowski14weakly, Bojanowski15weakly} in the form of ordering constraints over the latent label variables.
Here we use a similar approach to cluster the $N$ input video streams $(x_t)$ into a sequence of $K$ steps, as follows.
We represent each time interval by a $d$-dimensional feature vector. 
The feature vectors for the $n$-th video are stacked in a $T_n\times d$ design matrix denoted by $X_n$. 
We denote by $X$ the $T \times d$ matrix obtained by the concatenation of all $X_n$ matrices
(and similarly, by $Z$, $R$ and $A$ the appropriate concatenation of the $Z_n$, $R_n$ and $A_n$ matrices over $n$). 
%
In order to obtain the temporal localization into $K$ steps, we learn a linear classifier represented by a $d \times K$ matrix denoted by $W$.
This model is shared among all videos. 
%
%

The target assignment~$\hat{Z}$ is found by minimizing the clustering cost function $h$ under 
both the consistent script ordering constraints $\mathcal{Z}$ and
our weak supervision constraints:
%
\begin{equation}
\underset{Z}{\text{minimize}} \quad h(Z) \quad  \text{ s.t. }  \underbrace{Z \in \mathcal{Z}}_{\text{ordered script}}, \quad
\underbrace{AZ \geq R}_{\substack{\text{weak textual}\\ \text{constraints}} }.
\label{eq:videocost}
\end{equation}
The clustering cost $h(Z)$ is given as in DIFFRAC~\cite{Bach07diffrac} as:
\begin{align}
%
    h(Z) = \min_{W \in \mathbb{R}^{K \times d}} \ \underbrace{\frac{1}{2T} \|Z - X W\|_F^2}_\text{Discriminative loss on data} + \underbrace{\frac{\lambda}{2} \|W\|_F^2}_\text{Regularizer}. 
    \label{eq:discriminative}
\end{align}
The first term in~(\ref{eq:discriminative}) is the discriminative loss on the data that measures how easy the input data $X$ is separable by the linear classifier $W$ when the target classes are given by the assignments $Z$.  
For the squared loss considered in eq.~(\ref{eq:discriminative}), the optimal weights $W^*$ minimizing~(\ref{eq:discriminative}) can be found in closed form, 
%
which significantly simplifies the computation. 
However, to solve~\eqref{eq:videocost}, we need to optimize over assignment matrices $Z$ that encode sequences of events and incorporate constraints given by clusters obtained from transcribed textual narrations (Sec.~\ref{sec:text}). 
This is again done by using the Frank-Wolfe algorithm, which allows the use of \emph{efficient dynamic programs} 
to handle the combinatorial constraints on $Z$.
More details are given in Appendix~\ref{sec:details_diffrac}.
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%

%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
\begin{figure*}[t!]
    \centering
    \begin{subfigure}[t]{0.19\linewidth}
        \centering
        \includegraphics[width=\linewidth]{rebuttal/figs/f1_changing_tire_withdobj_rebuttal_with_18_singleK_opti_CR_less_test.pdf}
        \caption{Change tire (\textbf{11})}
    \end{subfigure}%
    ~ 
    \begin{subfigure}[t]{0.19\linewidth}
        \centering
        \includegraphics[width=\linewidth]{rebuttal/figs/f1_cpr_withdobj_rebuttal_with_18_singleK_opti_CR_less_test.pdf}
        \caption{Perform CPR (\textbf{7})}
    \end{subfigure}%
	~
	\begin{subfigure}[t]{0.19\linewidth}
        \centering
        \includegraphics[width=\linewidth]{rebuttal/figs/f1_repot_withdobj_rebuttal_with_18_singleK_opti_CR_less_test.pdf}
        \caption{Repot plant (\textbf{7})}
    \end{subfigure}%
    ~
    \begin{subfigure}[t]{0.19\linewidth}
        \centering
        \includegraphics[width=\linewidth]{rebuttal/figs/f1_coffee_withdobj_rebuttal_with_18_singleK_opti_CR_less_test.pdf}
        \caption{Make coffee (\textbf{10})}
    \end{subfigure}%
    ~
    \begin{subfigure}[t]{0.19\linewidth}
        \centering
        \includegraphics[width=\linewidth]{rebuttal/figs/f1_jump_car_withdobj_rebuttal_with_18_singleK_opti_CR_less_test.pdf}
        \caption{ Jump car (\textbf{12})}
    \end{subfigure}%
    \vspace{-1mm}
    \caption{\small Results for temporally localizing recovered steps in the input videos.
    We give in \textbf{bold} the number of ground truth steps.   
%
    }
    \vspace{-3mm}
    \label{tab:exp-localization}
\end{figure*}


\section{Experimental evaluation}
\setlength{\tabcolsep}{6pt}
\label{sec:experiments}
%
In this section, we first describe the details of the text and video features. 
Then we present the results divided into two experiments:
(i) in Sec.~\ref{sec:step_discovery}, we evaluate the quality of steps extracted from video narrations, and
(ii) in Sec.~\ref{sec:localization}, we evaluate the temporal localization of the recovered steps in video using constraints derived from text. 
All the data and code are available at our project webpage~\cite{Alayrac15UnsupervisedWeb}.


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


\textbf{Video and text features.} 
We represent the transcribed narrations as sequences of direct object relations. 
For this purpose, we run a dependency parser~\cite{Marneffe06generating} on each transcript. 
We lemmatize all direct object relations and keep the ones for which the direct object corresponds to nouns.
%
%
%
To represent a video, we use motion descriptors in order to capture actions (loosening, jacking-up, giving compressions) and frame appearance descriptors to capture the depicted objects (tire, jack, car). 
We split each video into 10-frame time intervals and represent each interval by its motion and appearance descriptors aggregated over a longer block of 30 frames. The motion representation is a histogram of local optical flow (HOF) descriptors aggregated into a single bag-of-visual-word vector of 2,000 dimensions~\cite{Wang13action}.  The visual vocabulary is generated by k-means on a separate large set of training descriptors.
To capture the depicted objects in the video, we apply the VGG-verydeep-16 CNN~\cite{Simonyan14c} over each frame in a sliding window manner over multiple scales. This can be done efficiently in a fully convolutional manner. The resulting 512-dimensional feature maps of conv5 responses are then aggregated into a single bag-of-visual-word vector of 1,000 dimensions, which aims to capture the presence/absence of different objects within each video block. A similar representation (aggregated into compact VLAD descriptor) was shown to work well recently for a variety of recognition tasks~\cite{Cimpoi15}. The bag-of-visual-word vectors representing the motion and the appearance are normalized using the Hellinger normalization and then concatenated into a single 3,000 dimensional vector representing each time interval.

%
%
%
%
\textbf{WordNet distance.}
For the multiple sequence alignment presented in Sec.~\ref{subsec:model_text}, we set $c(d_1,d_2)=-1$ if $d_1$ and $d_2$ have both their verbs and direct objects that match exactly in the Wordnet tree (distance equal to 0).
Otherwise we set $c(d_1,d_2)$ to be 100.
This is to ensure a high precision for the resulting alignment. %


%
\subsection{Results of step discovery from text narrations}
\label{sec:step_discovery}
%
\label{subsec:exp0}

Results of discovering the main steps for each task from text narrations are presented in Table~\ref{exp:MSARes}.
We report results of the multiple sequence alignment described in Sec.~\ref{subsec:model_text} when the maximum number of recoverable steps is $K=10$. Additional results for different choices of $K$ are given in the Appendix~\ref{subsec:script_disc}. %
%
%
With increasing $K$, we tend to recover more complete sequences at the cost of occasional repetitions, e.g.~{\em position jack} and {\em jack car} that refer to the same step. 
%
To quantify the performance, we measure precision as the proportion of correctly recovered steps appearing in the correct order. 
We also measure recall as the proportion of the recovered ground truth steps. 
The values of precision and recall are given at the bottom of Table~\ref{exp:MSARes}.
%
%
%


%
\subsection{Results of localizing instruction steps in video}
\label{sec:localization}
%

In the previous section, we have evaluated the quality of the sequences of steps recovered from the transcribed narrations. 
In this section, we evaluate how well we localize the individual instruction steps in the video by running our two-stage approach from Sec.~\ref{subsec:model_var}.


\textbf{Evaluation metric.}
To evaluate the temporal localization, we need to have a one-to-one mapping between the discovered steps in the videos and the ground truth steps.  Following~\cite{Liao05Clustering}, we look for a one-to-one global matching (shared across all videos of a given task) that maximizes the evaluation score for a given method (using the Hungarian algorithm).
Note that this mapping is used only for evaluation, the algorithm does not have access to the ground truth annotations for learning.


The goal is to evaluate whether each ground truth step has been correctly localized in all instruction videos. We thus use the \emph{F1 score} that combines precision and recall into a single score as our evaluation measure. 
%
For a given video and a given recovered step, our video clustering method predicts exactly one video time interval $t$. 
This detection is considered correct if the time interval falls inside any of the corresponding ground truth intervals, and incorrect otherwise (resulting in a false positive for this video). 
We compute the recall across all steps and videos, defined as the ratio of the number of correct predictions over the total number of possible ground truth steps across videos. A recall of 1 indicates that every ground truth step has been correctly detected across all videos. 
%
The recall decreases towards 0 when we miss some ground truth steps (missed detections). 
This happens either because this step was not recovered globally, or because it was detected in the video at an incorrect location. This is because the algorithm predicts exactly one occurrence of each step in each video. %
Similarly, precision measures the proportion of correct predictions among all  $N \! \cdot \!K_{\mathrm{pred}}$  possible predictions, where $N$ is the number of videos and $K_{\mathrm{pred}}$
is the number of main steps used by the method. The F1 score is the harmonic mean of precision and recall, giving a score that ranges between 0 and 1, with the perfect score of 1 when all the steps are predicted at their correct locations in all videos.  

\textbf{Hyperparameters.} 
We set the values of parameters $\Delta_b$ and $\Delta_a$ to 0 and 10 seconds. %
The setting is the same for all five tasks.
This models the fact that typically each step is first described verbally and then performed on the camera.
%
We set $\lambda = 1/(N K_{\mathrm{pred}})$ for all methods that use~\eqref{eq:discriminative}.

\textbf{Baselines.} 
We compare results to four baselines. 
To demonstrate the difficulty of our dataset, we first evaluate a ``Uniform" baseline, which simply distributes instructions steps uniformly over the entire instruction video.
The second baseline ``Video only"~\cite{Bojanowski14weakly} does not use the narration and performs only discriminative clustering on visual features with a global order constraint.\footnote{We use here the improved model from~\cite{Bojanowski15weakly} which does not require a ``background class'' and yields a stronger baseline equivalent to our model~\eqref{eq:videocost} \emph{without} the weak textual constraints.}
%
The third baseline ``Video + BOW dobj'' basically adds text-based features to the ``Video only'' baseline (by concatenating the text and video features in the discriminative clustering approach).
Here the goal is to evaluate the benefits of our two-stage clustering approach, in contrast to this single-stage clustering baseline. 
The text features are bag-of-words histograms over a fixed vocabulary of direct object relations.\footnote{Alternative features of bag-of-words histograms treating separately nouns and verbs also give similar results.}
The fourth baseline is our own implementation of the alignment method of~\cite{Malmaud15what} (without the supervised vision refinement procedure that requires a set of pre-trained visual classifiers that are not available a-priori in our case).
%
%
We use~\cite{Malmaud15what} to re-align the speech transcripts to the sequence of steps discovered by our method of Sec.~\ref{subsec:model_text} (as a proxy for the recipe assumed to be known in~\cite{Malmaud15what}).\footnote{Note that our method finds at the same time the sequence of steps (a recipe in ~\cite{Malmaud15what}) and the alignment of the transcripts.}
To assess the difficulty of the task and dataset, we also compare results with a ``Supervised" approach.
The classifiers $W$ for the visual steps are trained by running the discriminative clustering of Sec.~\ref{sec:model-video} with only ground truth annotations as constraints on the training set.
At test time, these classifiers are used to make predictions under the global ordering constraint on unseen videos.
%
%
We report results using 5-fold cross validation for the supervised approach, with the variation 
across folds giving the error bars. 
For the unsupervised discriminative clustering methods, the error bars represent the variation of performance obtained from different rounded solutions collected during the Frank-Wolfe optimization.
%

%
%
%
%


\begin{figure}
%
\centering\includegraphics[width=0.98\linewidth]{figs/qual_res_v3_v1}
\vspace{-1mm}
\caption{\footnotesize {\bf Examples of three recovered instruction steps for each of the five tasks in our dataset.} %
For each step, we first show clustered direct object relations, followed by representative example frames localizing the step in the videos. Correct localizations are shown in green. Some steps are incorrectly localized in some videos (red), but often look visually very similar.  
%
{\bf See Appendix~\ref{subsec:action_loc} for additional results.}
}
\label{fig:qualitative_results} 
\end{figure}


\textbf{Results.} 
%
Results for localizing the discovered instruction steps are shown
in Figure~\ref{tab:exp-localization}. 
In order to perform a fair comparison to the baseline methods that require a known number of steps $K$, we report results for a range of $K$ values. Note that in our case the actual number of automatically recovered steps can be (and often is) smaller than $K$.
%
%
For {\em Change tire} and {\em Perform CPR}, our method consistently outperforms all baselines for all values of $K$ demonstrating the benefits of our approach. 
For {\em Repot}, our method is comparable to text-based baselines, underlying the importance of the text signal for this problem.
For {\em Jump car}, our method delivers the best result (for $K=15$) but struggles for lower values of $K$, which we found was due to visually similar repeating steps (e.g. start car A and start car B) which are mixed-up for lower values of $K$.
For the {\em Make coffee} task, the video only baseline is comparable to our method, which by inspecting the output could be attributed to large variability of narrations for this task. 
Qualitative results of the recovered steps are illustrated in Figure~\ref{fig:qualitative_results}.

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


\section{Conclusion and future work}
We have described a method to automatically discover the main steps of a task from a set of narrated instruction videos in an unsupervised manner.
%
The proposed approach has been tested on a new annotated dataset of challenging real-world instruction videos containing complex person-object interactions in a variety of indoor and outdoor scenes. Our work opens up the possibility for large scale learning from instruction videos on the Internet.
Our model currently assumes the existence of a common script with a fixed ordering of the main steps.
While this assumption is often true, e.g.~one cannot remove the wheel before jacking up the car, or make coffee before filling the water, some tasks can be performed while swapping (or even leaving out) some of the steps. Recovering more complex temporal structures is an interesting direction for future work. 
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


\small{
\paragraph{Acknowledgments}
This research was supported in part by a Google Research Award, and the ERC grants VideoWorld (no. 267907), Activia (no. 307574) and LEAP (no. 336845).
}
%
%
%
%

%

{\small
\bibliographystyle{ieee}
\bibliography{biblio}
}

%
%
%
%
%

%
%
%
%
%
%





%

\clearpage

\appendix

%
\section*{Outline of Supplementary Material}
%
\label{app:intro}
This supplementary material provides additional details for our method and presents a more complete set of results.
%
Section~\ref{app:dataset} gives detailed statistics and an illustration of the newly collected dataset of instruction videos.
Section~\ref{app:text_msa} gives details about our new formulation of the multiple sequence alignment problem (Section~\ref{subsec:model_text} of the main paper) as a quadratic program and presents empirical results showing that our Frank-Wolfe optimization approach obtains solutions with lower objective values than the state-of-the-art heuristic algorithms for multiple sequence alignment. 
Section~\ref{sec:details_diffrac} provides the details for the discriminative clustering of videos with text constraints that was briefly described in Section~\ref{sec:model-video} of the main paper.
Section~\ref{subsec:details_experiments} gives additional details about the experimental protocol used in Section~\ref{sec:localization} in the main paper.
Finally, in Section~\ref{sec:qual_res}, we give a more complete set of qualitative results for both the clustering of transcribed verbal instructions (see~\ref{subsec:script_disc}) and localizing instruction steps in video (see~\ref{subsec:action_loc}).

%
%
%

%
%

%
\section{New challenging dataset of instruction videos}
\label{app:dataset}
%

\subsection{Dataset statistics}
\label{subsec:score_dataset}

In this section, we introduce three different scores which aim to illustrate different properties of our dataset.
The scores characterize (i) the step ordering consistency, (ii) the missing steps and (iii) the possible step repetitions.
 
Let $N$ be the number of videos for a given task and $K$ the number of steps defined in the ground truth.
We assume that the ground truth steps are given in an ordered fashion, meaning the global order is defined as the sequence $\left\lbrace 1,\ldots, K \right\rbrace $. 
For the $n$-th video, we denote by $g_n$ the total number of annotated steps, by $u_n$ the number of unique annotated steps and finally by $l_n$ the length of the longest common subsequence between the annotated sequence of steps and the ground truth sequence $\left\lbrace 1,\ldots, K \right\rbrace$.

\paragraph{Order consistency error.}
The \emph{order error} score $O$ is defined as the proportion of non repeated annotated steps that are not consistent with the global ordering.
In other words, it is defined as the number of steps that do not fit the global ordering defined in the ground truth divided by the total number of unique annotated steps.
More formally, $O$ is defined as follows:
\begin{equation}
O := 1-\frac{\sum_{n=1}^N l_n}{\sum_{n=1}^N u_n}.
\label{order_score}
\end{equation}

\paragraph{Missing steps.} 
We define the \emph{missing steps} score $M$ as the proportion of steps that are visually missing in the videos when compared to the ground truth.
Formally, 
\begin{equation}
M := 1-\frac{\sum_{n=1}^N u_n}{KN}.
\label{missing_score}
\end{equation}

\paragraph{Repeated steps.}
The \emph{repetition score} $R$ is defined as the proportion of steps that are repeated:
\begin{equation}
R := 1-\frac{\sum_{n=1}^N u_n}{\sum_{n=1}^N g_n}.
\label{repet_score}
\end{equation}

\paragraph{Results.}
In Table~\ref{tab:dataset_stat}, we give the previously defined statistics for the five tasks of the instruction videos dataset.
Interestingly, we observed that globally the order is consistent for the five tasks with a total order error of only $6\%$.
Steps are missing in $27\%$ of the cases. 
This illustrates the difficulty of defining the right granularity of the ground truth for this task. 
Indeed, some steps might be optional and thus not visually demonstrated in all videos.
Finally the global repetition score is $14\%$.
Looking more closely, we observe that the \emph{Performing CPR} task is the main contributor to this score.
This is obviously a good example where one needs to repeat several times the same steps (here alternating between compressions and giving breath).
Even if our model is not explicitly handling this case, we observed that our multiple sequence alignment technique for clustering the text inputs discovered these repetitions (see Table~\ref{tab:script_res}).
Finally, these statistics show that the problem introduced in this paper is very challenging and that designing models which are able to capture more complex structure in the organization of the steps is a promising direction for future work.

\begin{table*}[ht]
\centering
\begin{tabular}{@{}lrrrrrr@{}}
\toprule
Task             & \multicolumn{1}{l}{Changing tire} & \multicolumn{1}{l}{Performing CPR} & \multicolumn{1}{l}{Repoting plant} & Making coffee & \multicolumn{1}{c}{Jumping cars} & \multicolumn{1}{c}{\textbf{Average}} \\ \midrule
Order error      & 0.7\%                             & 11\%                               & 6\%                                & 3\%           & 8\%                              & \textbf{6\%}                       \\
Missing steps    & 16\%                              & 32\%                               & 30\%                               & 28\%          & 27\%                             & \textbf{27\%}                      \\
Repetition score & 4\%                               & 50\%                               & 7\%                                & 11\%          & 0.4\%                            & \textbf{14\%}                      \\ \bottomrule
\end{tabular}
\caption{Statistics of the instruction video dataset.}
\label{tab:dataset_stat}
\end{table*}

\subsection{Complete illustration of the dataset}

%
Figure~\ref{fig:datasetApp} illustrates all five tasks in our newly collected dataset.
For each task, we show a subset of 3 events that compose the task.
Each event is represented by several sample frames and extracted verbal narrations.
Note the large variability of verbal expressions and the terminology in the transcribed narrations as well as the large variability of visual appearance  due to viewpoint, used objects, and actions performed in different manner.
At the same time, note the the consistency of the actions between the different videos and the underlying script of each task.
%


%
%
\section{Clustering transcribed verbal instructions}
%
\label{app:text_msa}
In this section, we review in details the way we model the text clustering.
In particular, we give details on how we can reformulate multiple sequence alignment as a quadratic program.
Recall that we are given $N$ narrated instruction videos.
For the $n$-th video, the text signal is represented as a sequence of direct object relation tokens : $d^n=(d_1^n,\dots,d^n_{S_n})$, where the length $S_n$ of the sequences varies from one video clip to another.
The number of possible direct object relations in our dictionary is denoted $D$.
The multiple sequence alignment (MSA) problem was formulated
as mapping each input sequence $d^n$ of tokens to a global common
template of $L$ slots, while minimizing the sum-of-pairs score given in~\eqref{eq:msa_cost}.
For each input sequence $d^n$, we used the notation $(\phi(d^n))_{1\leq l\leq L}$
to denote the re-mapped sequence of tokens into $L$ slots: $\phi(d^n)_l$
represents the direct object relation put at location $l$, with $\phi(d^n)_l = \varnothing$
denoting that a gap was inserted in the original sequence and
the slot $l$ is left empty.
We also have defined a cost $c(d_1, d_2)$ of aligning two direct object 
relations together, with the possibility that $d_1$ or $d_2$
is~$\varnothing$, in which case we defined the cost to be $0$
by default.
In the following, we summarize the cost of aligning non-empty
direct object relations
by the matrix $\Ctext\in\mathbb{R}^{D\times D}$.
$(\Ctext)_{ij}$ is equal to the cost of aligning the $i$-th and the 
$j$-th direct object relation from the dictionary together.


\subsection{Reformulating multiple sequence alignment as a quadratic program}
\label{subsec:details_msa}

We now present our formalization of the search problem as a quadratic program.
To the best of our knowledge this is a new formulation of the multiple sequence alignment (MSA) problem, 
which in our setting (results shown later) consistently obtains better values of the multiple sequence alignment objective than the current
state-of-the-art MSA heuristic algorithms.
 
We encode the identity of a direct object relation with a $D$-dimensional
indicator vector.
The text sequence $n$ can then be represented by an indicator matrix $Y_n \in \{0,1\}^{S_n\times D}$.
The $j$-th row of $Y_n$ indicates which direct object relations is evoked at the $j$-th position.
Similarly, the token re-mapping  $(\phi(d^n))_{1\leq l\leq L}$ can
be represented as a $L \times D$ indicator matrix; where 
each row $l$ encodes which token is appearing in slot $l$ (and
a whole row of zero is used to indicates an empty $\varnothing$ slot).
This re-mapping can be constructed from two 
pieces of information: first, which token index $s$ of the original sequence
is re-mapped to which global template slot $l$; 
we represent this by the decision matrix  $U_n\in\{0,1\}^{S_n\times L}$,
which satisfies very specific constraints (see below).
The second piece of information is the composition 
of the input sequence encoded by $Y_n$.
We thus have $\phi(d^n) = U_n^T Y_n$ (as a $L \times D$ indicator
matrix). Given this encoding, the cost matrix $\Ctext$, and the fact
that the alignment of empty slots has zero cost, we
can then rewrite the MSA problem that minimizes
the sum-of-pairs objective~\eqref{eq:msa_cost}
as follows:
\begin{equation}
\begin{aligned}
& \underset{U_n, n\in\{1,\ldots,N\}}{\text{minimize}}
& & \sum_{(n,m)} \mathrm{Tr}(U_n^TY_n \Ctext Y_m^TU_m) \\
& \text{subject to}
& & U_n \in \mathcal{U}_n, \; n = 1, \ldots, N.
\end{aligned}
\label{eq:msaeq}
\end{equation}
In the above equation, the trace ($\mathrm{Tr}$) is computing the cost of aligning sequence $m$ with sequence $n$
(the inner sum in~\eqref{eq:msa_cost}).
Moreover, $\mathcal{U}_n$ is a constraint set that encodes the fact that $U_n$ has to be a valid (increasing) re-mapping.\footnote{More formally  $\mathcal{U}_n :=\{U\in\{0,1\}^{S_n \times L}$ s.t.  $U\textbf{1}_L=\textbf{1}_{S_n}$ and $\forall l, \left( U_{sl}=1 \Rightarrow ((\forall s'>s,l'\leq l),  U_{s'l'}=0 \right)\}$.}
As before, we can eliminate the video index $n$ by simply stacking the assignment matrices $U_n$ in one matrix $U$ of size $S\times L$.
Similarly, we denote $Y$ the $S\times D$ matrix which is obtained by the concatenation of all the $Y_n$ matrices. 
We can then rewrite the equation~(\ref{eq:msaeq}) as a quadratic program over the (integer) variable $U$:

\begin{equation}
\underset{U}{\text{minimize}} \ \mathrm{Tr}(U^TBU), \ \text{subject to} \ U \in \mathcal{U}.
\label{eq:msaeqcompact}
\end{equation}
In this equation, the $S \times S$ matrix $B$ is deduced from the input sequences and the cost between different direct object relations by computing $B := Y \Ctext Y^T$.
It represents the pairwise cost at the token level, i.e. the cost of aligning token $s$ in one
sequence to token $s'$ in another sequence.
%
%

\subsection{Comparison of methods}
\label{subsec:comparison_msa}

\begin{table*}[!ht] 
    \centering
    \setlength{\tabcolsep}{.6em} 
    \begin{tabular}{lccccc} 
        \toprule
        Task 			&  Changing tire   &  Performing CPR  & Repotting plant &  Making coffee   & Jumping cars\\ 
        \midrule
%
        Poa~\cite{Lee01poa} 			    &   11.30   &   -3.82   &  1.65    &   -2.99   &   4.55	\\ 
        Ours using Frank-Wolfe 					    &  \textbf{-5.18}  &   \textbf{-4.51}   & \textbf{-3.55} &   \textbf{-3.86}   & \textbf{-4.67} 	\\ 
        \bottomrule
    \end{tabular}  
%
    \caption{Comparison of different optimization approaches for solving problem~\eqref{eq:msaeqcompact}. (Objective value, lower is better).}  
    \label{tab:msaObj}
%
\end{table*} 

The problem~(\ref{eq:msaeqcompact}) is NP-hard~\cite{wang1994msaNPhard} in general, as is typical 
for integer quadratic programs.
However, much work has been done in computational biology
to develop efficient heuristics to solve the MSA problem,
as it is an important problem in their field.
We briefly describe below some of the existing heuristics to solve it,
and then present our Frank-Wolfe optimization approach,
which gave surprisingly good empirical results
for our problem.\footnote{We stress here that we do not claim 
that our formulation of the multiple sequence alignment (MSA) problem as a quadratic program outperforms the state-of-the-art 
computational biology heuristics for their MSA problems \emph{arising in biology}.
We report our observations on application of multiple sequence alignment to our application,
which might have a structure for which these heuristics are not as appropriate.}


\paragraph{Standard methods.}
Here, we compare to a standard state-of-the-art method for multiple sequence alignment~\cite{Lee01poa}.
Similarly to~\cite{Higgins88clustal}, they first align two sequences and merge them in a common template.
Then they align a new sequence to the template and then update the template.
They continue like this until no sequence is left.
%
%
Differently from~\cite{Higgins88clustal}, they use a better representation of the template by using partial order graph instead of simple linear representations.
This gives more accuracy for the final alignment.
For the experiments, we use the author's implementation.\footnote{Code available at \url{http://sourceforge.net/projects/poamsa/}.}

\paragraph{Our solution using Frank-Wolfe optimization.}
We first note that problem~\eqref{eq:msaeqcompact} has a very similar 
structure to an optimization problem that we solve using Frank-Wolfe optimization 
for the discriminative clustering of videos; see Equations~\eqref{eq:appAexph} and~\eqref{eq:hFWproblem} below.
For this, we first perform a continuous relaxation of the set of constraints $\mathcal{U}$ by replacing it with its convex hull $\bar{\mathcal{U}}$.
The Frank-Wolfe optimization algorithm~\cite{Jaggi2013} can solve quadratic program over constraint sets
for which we have access to an efficient linear minimization oracle.
In the case of $\mathcal{U}$,
the linear oracle can be solved exactly with a dynamic program very similar to the one described in Section~\ref{subsec:fw_dp}.
We note here that even with the continuous relaxation over $\bar{\mathcal{U}}$, the resulting problem is still non-convex because $B$ is not positive semidefinite -- this is because of the cost function appearing in the MSA problem.
However, the standard convergence proof for Frank-Wolfe can easily be extended to show
that it converges at a rate of $O(1/\sqrt{k})$ to a stationary point on non-convex objectives~\citesup{lacoste16nonconvexFW}.
Once the algorithm has converged to a (local) stationary point,
we need to round the fractional solution to obtain a valid encoding $U$.
We follow here a similar rounding strategy that was originally proposed by~\citesup{Chari15FWtrack}
and then re-used in~\cite{Joulin14efficient}: we pick the last visited
corner (which is necessarily integer) 
which was given as a solution to the linear minimization oracle
(this is called Frank-Wolfe rounding). 

\paragraph{Results.} In Table~\ref{tab:msaObj}, we give the value of the objective~(\ref{eq:msaeqcompact}) for the rounded solutions obtained by the two different optimization approaches (lower is better), for the MSA problem on our
five tasks.
Interestingly, we observe that the Frank-Wolfe algorithm consistently outperforms the state-of-the-art method of~\cite{Lee01poa} in our setting.


\section{Discriminative clustering of videos under text constraints }
\label{sec:details_diffrac}

We give more details here on the discriminative clustering
framework from~\cite{Bojanowski14weakly,Bojanowski15weakly}
(and our modifications to include the text constraints) that we use
to localize the main actions in the video signal.

\subsection{Explicit form of $h(Z)$}
\label{subsec:explicit_h}

We recall that $h(Z)$ is the cost of clustering all the video streams $\{x^n\}, n=1,\ldots, N$, into a sequence of $K$ steps. 
The design matrix $X$  $\in \mathbb{R}^{T \times d}$ contains the feature describing the time intervals in our videos.
The indicator latent variable $Z\in \mathcal{Z} := \{0,1\}^{T\times K}$ encodes the visual presence of a step $k$ at a time interval $t$. 
Recall also that $X$ and $Z$ contains the information about all videos $n \in \{1,\ldots, N\}$.
Finally, $W\in \mathbb{R}^{d\times K}$ represents a linear classifier for our $K$ steps, that is shared among all videos. We now derive the explicit form of $h(Z)$ as in the DIFFRAC approach~\cite{Bach07diffrac}, though yielding a somewhat simpler expression (as in~\cite{Bojanowski15weakly}) due to our use of a (weakly regularized) bias feature in $X$ instead of a separate (unregularized) bias $b$. 
Consider the following joint cost function $f$ on $Z$ and $W$ defined as
\begin{align}
f(Z,W) =  \frac{1}{2T} \|Z - X W\|_F^2+ \frac{\lambda}{2} \|W\|_F^2.
\label{eq:appAf}
\end{align}
The cost function $f$ simply represents the ridge regression objective with output labels $Z$ and input design matrix $X$. We note that $f$ has the nice property of being \emph{jointly} convex in both $Z$ and $W$, implying that its unrestricted minimization with respect to $W$ yields a \emph{convex} function in $Z$. This minimization defines our clustering cost $h(Z)$; rewriting the definition of $h$ with the joint cost $f$ from~\eqref{eq:appAf}, we have:
\begin{align}
%
    h(Z) = \min_{W \in \mathbb{R}^{d \times K}} \ f(Z,W).
\label{eq:appAh}
\end{align}
As $f$ is strongly convex in $W$ (for any $Z$), we can obtain its unique minimizer $W^*(Z)$ as a function of $Z$ by zeroing its gradient and solving for $W$.
For the case of the square loss in equation~\eqref{eq:appAf}, the optimal classifier $W^*(Z)$ can be computed in closed form:
\begin{align}
%
%
W^*(Z) =  (X^TX+T\lambda I_d)^{-1}X^TZ, 
\label{eq:appAWopt}
\end{align}
where $I_d$ is the $d$-dimensional identity matrix.
We obtain the explicit form for $h(Z)$ by substituting the expression~\eqref{eq:appAWopt} for $W^*(Z)$ in equation~\eqref{eq:appAf} and properly simplifying the expression:
\begin{align}
h(Z) = f(Z,W^*) = \frac{1}{2T}\text{Tr}(ZZ^TB),
\label{eq:appAexph}
\end{align}
where $B := I_T-X(X^TX+T\lambda I_d)^{-1}X^T$ is a strictly positive definite matrix (and so $h$ is actually strongly convex). The clustering cost is a quadratic function in $Z$, encoding how the clustering decisions in one interval $t$ interact with the clustering decisions in another interval $t'$. In the next section, we explain how we can optimize the clustering cost $h(Z)$ subject to the constraints from Section~\ref{sec:model-video} using the Frank-Wolfe algorithm.

\subsection{Frank Wolfe algorithm for minimizing $h(Z)$}
\label{subsec:fw_dp}

The localization of steps in the video stream is done by solving the following optimization problem
(repeated from~\eqref{eq:videocost} here for convenience):
\begin{equation}
\underset{Z}{\text{minimize}} \quad h(Z) \quad  \text{ s.t. }  \underbrace{Z \in \mathcal{Z}}_{\text{ordered script}}, \quad
\underbrace{AZ \geq R}_{\substack{\text{weak textual}\\ \text{constraints}} }.
\label{eq:hFWproblem}
\end{equation}
where $Z$ is the latent assignment matrix of video time intervals to $K$ clusters and $R$ is the matrix of assignments of direct object relations in text to $K$ clusters.
%
Note that $R$ is obtained from the text clustering using multiple sequence alignment as described in Section~\ref{sec:text} and~\ref{subsec:details_msa}, and is fixed before optimizing over $Z$.
$R$ is a $S \times K$ matrix obtaining by picking the $K$ main columns of the $U$ matrix
defined in Section~\ref{subsec:details_msa}. This selection step
was described in the ``extracting the main steps'' paragraph in Section~\ref{sec:text}.

%
The constraint set encodes several concepts.
First, it imposes the temporal consistency between the text stream and the video stream. 
We recall that this constraint was written as $AZ \geq R$,\footnote{
When $R_{sk} = 0$, then this constraint does not do anything. When $R_{sk} = 1$ (i.e. the
text token $s$ was assigned to the main action $k$), then the constraint enforces
that $\sum_{t \in A_{s \cdot}} Z_{tk} \geq 1$, where $A_{s \cdot}$ represents
which video frames are temporally close to the caption time of the text token~$s$.
It thus then enforces that at least one temporally close video frame is assigned
to the main action $k$.
} 
where $A$ encodes the temporal alignment constraints between video and text (type I).
Second, it includes the event ordering constraints within each video input (type II).
Finally, it encodes the fact that each event is assigned to exactly one time interval within each video (type III).
The last two constraints are encoded in the set of constraints $\mathcal{Z}$.
To summarize, let $\mathcal{\tilde{Z}}$ denote the resulting (discrete) feasible space for $Z$ i.e. $\mathcal{\tilde{Z}} := \{Z \in \mathcal{Z} \, | \,AZ \geq R \}$.
%
%

%
%
%
%
%
%

%
%
%

%
We are then left with a problem in $Z$ which is still hard to solve because the set $\tilde{\mathcal{Z}}$ is not convex. 
To approximately optimize $h$ over $\tilde{\mathcal{Z}}$, we follow the strategy of~\cite{Bojanowski14weakly,Bojanowski15weakly}.  First, we optimize $h$ over the relaxed $\text{conv}(\tilde{\mathcal{Z}})$ by using the Frank-Wolfe algorithm to get a fractional solution $Z^* \in \text{conv}(\tilde{\mathcal{Z}})$. We then find a feasible candidate $\hat{Z} \in \tilde{\mathcal{Z}}$ by using a rounding procedure. We now give the details of these steps.  

First we note that the linear oracle of the Frank-Wolfe algorithm can be solved separately for each video $n$.
Indeed, because we solve a linear program, there is no quadratic term that brings dependence between different videos in the objective, and moreover all the constraints are blockwise in $n$.
%
Thus, in the following, we will give details for one video only by adding an index $n$ to $\tilde{\mathcal{Z}}$, to $Z$ and to $T$.

The linear oracle of the Frank-Wolfe algorithm can be solved via an efficient dynamic program.
%
Let us suppose that the linear oracle corresponds to the following problem: 
\begin{equation}
    \min_{Z_n \in \tilde{\mathcal{Z}}_n} \text{Tr}(C_n^\top Z_n),
    \label{eq:linear}
\end{equation}
where $C_n \in \mathbb{R}^{T_n \times K}$ is a cost matrix that arises by computing the gradient of $h$ with respect to $Z_n$ at the current iterate. The goal of the dynamic program is to find which entries of $Z_n$ are equal to 1, recalling that $(Z_n)_{tk}=1$ means that the step $k$ was assigned to time interval $t$. From the constraint of type III (unique prediction per step), we know that each column $k$ of $Z_n$ has exactly one $1$ (to be found). From the ordering constraint (type II), we know that if $(Z_n)_{tk} = 1$, then the only possible locations for a 1 in the $(k+1)$-th column is for $t' > t$ (i.e. the pattern of 1's is going downward when traveling from left to right in $Z_n$). Note that there can be ``jumps'' in between the time assignment for two subsequent steps $k$ and $k+1$. In order to encode this possibility using a continuous path search in a matrix, we insert dummy columns into the cost matrix $C$.
We first subtract the minimum value from $C$ and then insert columns filled with zeros in between every pair of columns of $C$.
In the end, we pad $C$ with an additional row filled with zeros at the bottom.
The resulting cost matrix $\tilde{C}$ is of size $(T_n+1) \times (2K+1)$ and is illustrated (as its transpose) along with the corresponding update rules in Figure~\ref{fig:dp}.

\begin{figure}[tbp]
    \centering
    \includegraphics[width=\linewidth]{figs/reduced_dp.pdf}

    %

    \caption{
        Illustration of the dynamic programming solution to the linear program~\eqref{eq:linear}.
        The drawing shows a possible cost matrix~$\tilde{C}$ and an optimal path in red.
        The gray entries in the matrix $\tilde{C}$ correspond to the values from the matrix \(C\). The white entries have minimal cost and are thus always preferred over any gray entry. 
        Note that we display~$\tilde{C}$ in a transpose manner to better fit on the page.
}
    \label{fig:dp}

    %

\end{figure}

The problem that we are interested in is subject to the additional linear constraints given by the clustering of text transcripts (constraints of type I).
These constraint can be added by constraining the path in the dynamic programming algorithm.
This can be done for instance by setting an infinite alignment cost outside of the constrained region.

At the end of the Frank-Wolfe optimization algorithm, we obtain a continuous solution $Z^*_n$ for each $n$.
By stacking them all together again, we obtain a continuous solution $Z^*$. From the definition of $h$, we can also look at the corresponding model $W^*(Z^*)$ defined by equation~\eqref{eq:appAWopt} which again is shared among all videos.
All $Z_n^*$ have to be rounded in order to obtain a feasible point for the initial, non relaxed problem.
Several rounding options were suggested in~\cite{Bojanowski15weakly}; it turns out that the one which uses $W^*$ gives better results in our case.
%
More precisely, in order to get a good feasible binary matrix $\hat{Z}_n \in \tilde{Z}_n$, we solve the following problem: $\min_{Z_n \in \tilde{\mathcal{Z}}_n} \ \|Z_n - X_n W^*\|_F^2$.
By expanding the norm, we notice that this corresponds to a simple linear program over $\tilde{\mathcal{Z}}_n$ as in equation~\eqref{eq:linear} that can be solved using again the same dynamic program detailed above.
Finally, we stack these rounded matrices $\hat{Z}_n$ to obtain our predicted assignment matrix $\hat{Z} \in \tilde{\mathcal{Z}}$.



\begin{figure*}[ht!]
       \centering
%
%
	 \includegraphics[width=\linewidth]{figs/dataset_all_v2.pdf} %
%
     \caption{\small Illustration of our newly collected dataset of instructions videos. 
     Examples of transcribed narrations together with still frames from the corresponding videos are shown for the 5 tasks of the dataset: 
     %
     {\em Repotting a plant}, {\em Performing CPR}, {\em Jumping cars}, {\em Changing a car tire} and {\em Making coffee}. 
     The dataset contains challenging real-world videos performed by many different people, captured in uncontrolled settings in a variety of outdoor and indoor environments.  
%
     }
     \label{fig:datasetApp}
 \end{figure*}

\section{Experimental protocol}
\label{subsec:details_experiments}

In this section, we give more details about the setting for our experiments on the time localization of events with results given in Figure~\ref{tab:exp-localization}.

\subsection{Supervised experiments.} 
Here, we describe in more details how we obtained the scores for the supervised approach depicted in yellow in Figure~\ref{tab:exp-localization}.
We first divided the $N$ input videos in 5 different folds.
One fold is kept for the test set while the 4 other are used as train/validation dataset.
With the 4 remaining folds, we perform a 4-fold cross validation in order to choose the hyperparameter $\lambda$.
Once the hyper parameter is fixed, we retrain a model on the 4 folds and evaluate it on the test set.
By iterating over the five possible test folds, we report variation in performance with error bars in Figure~\ref{tab:exp-localization}.

\paragraph{Training phase.}
The goal of this phase is to learn classifiers $W$ for the visual steps.
To that end, we minimize the cost defined in~\eqref{eq:videocost} under the ground truth annotations constraints.
This is very close to our setting, and in practice we can use exactly the same framework as in problem~\eqref{eq:hFWproblem} by simply replacing the constraints coming from the text by the constraints coming from the ground truth annotations.
\paragraph{Testing phase.}
At test time, we simply use the classifiers $W$ to perform least-square prediction of $Z_{\text{test}}$ under ordering constraints.
Performance are evaluated with the F1 score.

%

\subsection{Error bars for Frank-Wolfe methods.}
We explain here how we obtained the error bars of Figure~\ref{tab:exp-localization} in the main paper for the unsupervised approaches.
Let us first recall that the Frank-Wolfe algorithm is used to solve a continuous relaxation of problem~\eqref{eq:hFWproblem}.
To obtain back an integer solution, we round the continuous solution using the rounding method described at the end of Section~\ref{subsec:fw_dp}.
This rounding procedure is performed at each iteration of the optimization method.
When the stopping criterion of the Frank-Wolfe scheme is reached (fixed number of iterations or target sub-optimality in practice), we have as many rounded solutions as number of iterations.
Our output integer solution is then the integer point that achieves the lowest objective.
Note that we are only guaranteed to diminish objective in the continuous domain and \emph{not} for the integer points, therefore there are no guarantees that this solution is the last rounded point.
In order to illustrate the variation of the performance with respect to the optimization scheme, we defined our error bars as being the interval with bounds determined by the minimal performance and the maximal performance obtained \emph{after} visiting the best rounded point (the output solution).
This notably explains why the error bars of Figure~\ref{tab:exp-localization} are not necessarily symmetric.
Overall, the observed variation is not very important, thus highlighting the stability of the procedure.

%
%
%
%
%
%
%
%
%
%
%
%


\section{Qualitative results}
\label{sec:qual_res}

In Section~\ref{subsec:script_disc}, we give detailed results of script discovery for the five different tasks.
In Section~\ref{subsec:action_loc}, we present detailed results for the action localization experiment.

\subsection{Script discovery}
\label{subsec:script_disc}
Table~\ref{tab:script_res} shows the automatically recovered sequences of steps for the five tasks considered in this work. 
%
%
        The results are shown for setting the maximum number of discovered steps, $K = \{7,10,12,15\}$. 
        Note how our method automatically selects less than $K$ steps in some cases.
         These are the automatically chosen $k\leq K$ steps that are the most salient in the aligned narrations as described in Section~\ref{subsec:model_text}.  
		 This is notably the case for the {\em Repotting a plant} task. 
		 Even for $K\leq 12$, the algorithm recovers only 6 steps that match very well the seven ground truth steps for this task. 
		 This saliency based task selection is important because it allows for a better precision at high $K$ without lowering much the recall.       

Please note also how the steps and their ordering recovered by our method correspond well to the ground truth steps for each task. 
        For {\em CPR}, our method recovers fine-grained steps e.g.~{\em tilt head}, {\em lift chin}, which are not included in the main ground truth steps, but nevertheless could be helpful in some situations.
	    For {\em Changing tire}, we also recover more detailed actions such as~{\em remove jack} or {\em put jack}.
%
%
	    In some cases, our method recovers repeated steps. %
	    For example, for {\em CPR} our method learns that one has to alternate between {\em giving breath} and {\em performing compressions} even if this alternation was not annotated in the the ground truth.
%
%
	     Or for  {\em Jumping Cars} our method learns that cables need to be connected twice (to both cars).
%
%
	    
	    These results demonstrate that our method is able to automatically discover meaningful scripts describing very different tasks.
	    The results also show that the constraint of a single script providing an ordering of events is a reasonable prior for a variety of different tasks. 
%
 
%
%

\subsection{Action localization}
\label{subsec:action_loc}
Examples of the recovered instruction steps for all five tasks are shown in Figure~\ref{fig:qualitative_results_changing_tire}--\ref{fig:qualitative_results_cpr}.
Each row shows one recovered step. 
For each step, we first show the clustered direct object relations, followed by representative example frames localizing the step in the videos. Correct localizations are shown in green. Some steps are incorrectly localized in some videos (red), but often look visually very similar.  Note how our method correctly recovers the main steps of the task and localizes them in the input videos.
Those results have been obtained by imposing $K\leq 10$ in our method.
The video on the project website illustrates action localization for the five tasks.


%
\bibliographystylesup{ieee}
\bibliographysup{biblio}

%

%
%

\newpage

\setlength{\tabcolsep}{2pt}

\begin{table*}
\footnotesize %

     \begin{subtable}{0.48\textwidth}
     \centering
\resizebox{0.95\textwidth}{!}{	
    \begin{tabular}{lrrrr >{\centering\hspace{0.5pt}}m{0cm}}
    \toprule
GT (\textbf{11}) & $K\leq 7$ &  $K\leq 10$ & $K\leq 12$ & $K\leq 15$\\
\cmidrule{1-5}
\textit{put brake on}  & &      &   &    \\ 
\textit{get tools out}  & & \textbf{get tire}     & \textbf{get tire}  & \textbf{get tire}   \\ 

     
\textit{start loose} & \textbf{loosen nut} & \textbf{loosen nut}     & \textbf{loosen nut}   &   \textbf{loosen nut}   \\ 

\textit{} &  &  & \textbf{} &  \textbf{lift car} \\

\textit{}          & \textbf{put jack} & \textbf{put jack}     &  \textbf{put jack} &   \textbf{put jack} \\  

\textit{}  &   &     &  \textbf{raise vehicle} &  \textbf{raise vehicle}\\  

\textit{jack car}  & \textbf{jack car}  & \textbf{jack car}     &  \textbf{jack car} &  \textbf{jack car}\\  


\textit{unscrew wheel} & \textbf{remove nut}   & \textbf{remove nut}     &  \textbf{remove nut}  & \textbf{remove nut} \\  


\textit{remove wheel} & & \textbf{take wheel}     &  \textbf{take wheel} & \textbf{take wheel} \\  


\textit{put wheel}  & \textbf{take tire}  & \textbf{take tire}     & \textbf{take tire} &  \textbf{take tire} \\  


\textit{screw wheel} & & \textbf{put nut}     & \textbf{put nut} &    \textbf{put nut} \\  


\textit{lower car}  &  \textbf{lower jack} & \textbf{lower jack}     & \textbf{lower jack} &   \textbf{lower jack} \\  

\textit{}  &  & & & \textbf{remove jack}\\  

\textit{tight wheel}  & \textbf{tighten nut}  & \textbf{tighten nut}     &  \textbf{tighten nut}   &    \textbf{tighten nut}   \\  
\textit{put things back} &  &  & \textbf{take tire} &  \textbf{take tire} \\
\midrule
Precision  & 0.85  &  0.9    & 0.83 & 0.71   \\
Recall     & 0.54  &  0.9    & 0.9  & 0.9   \\

        \bottomrule
    \end{tabular}
}
     \caption{Changing a tire}
     \end{subtable}%
     \hspace*{\fill}%
	      \begin{subtable}{0.48\textwidth}
     \centering
\resizebox{0.95\textwidth}{!}{
    \begin{tabular}{lrrrr >{\centering\hspace{0.5pt}}m{0cm}}
    \toprule
GT (\textbf{10}) & $K\leq 7$ &  $K\leq 10$ & $K\leq 12$ & $K\leq 15$\\
\cmidrule{1-5}
\textit{grind coffee} & \textbf{•} & \textbf{•} & \textbf{•} & \textbf{•} \\
\textit{put filter} & \textbf{•} & \textbf{•} & \textbf{•} & \textbf{•} \\
\textit{add coffee} & \textbf{} & \textbf{put coffee} & \textbf{put coffee} & \textbf{put coffee} \\
\textit{even surface} & \textbf{•} & \textbf{•} & \textbf{•} & \textbf{•} \\
\textit{} & \textbf{} & \textbf{fill chamber} & \textbf{fill chamber} & \textbf{fill chamber} \\
\textit{} & \textbf{} & \textbf{} & \textbf{} & \textbf{make noise} \\
\textit{fill water} & \textbf{fill water} & \textbf{fill water} & \textbf{fill water} & \textbf{fill water} \\
\textit{screw top} & \textbf{} & \textbf{put filter} & \textbf{put filter} & \textbf{put filter} \\
\textit{} & \textbf{} & \textbf{} & \textbf{} & \textbf{fill basket} \\
\textit{} & \textbf{} & \textbf{see steam} & \textbf{see steam} & \textbf{see steam} \\
\textit{put stove} & \textbf{take minutes} & \textbf{take minutes} & \textbf{take minutes} & \textbf{take minutes} \\
\textit{•} & \textbf{make coffee} & \textbf{make coffee} & \textbf{make coffee} & \textbf{make coffee} \\
\textit{see coffee} & \textbf{see coffee} & \textbf{see coffee} & \textbf{see coffee} & \textbf{see coffee} \\
\textit{withdraw stove} & \textbf{•} & \textbf{•} & \textbf{•} & \textbf{turn heat} \\
\textit{pour coffee} & \textbf{make cup} & \textbf{make cup} & \textbf{make cup} & \textbf{make cup} \\
\textit{} & \textbf{•} & \textbf{•} & \textbf{•} & \textbf{pour coffee} \\
\midrule
Precision  & 0.8 &  0.67   & 0.67 &    0.54\\
Recall     & 0.4  &  0.6 & 0.6  &   0.7\\

        \bottomrule
    \end{tabular}
}
     \caption{Making coffee}
     \end{subtable}%
\vspace{3mm}
\hspace*{\fill}
%
\footnotesize %
 \begin{subtable}{0.48\textwidth}
     \centering
\resizebox{0.95\textwidth}{!}{
    \begin{tabular}{lrrrr >{\centering\hspace{0.5pt}}m{0cm}}
    \toprule
GT (\textbf{7}) & $K\leq 7$ &  $K\leq 10$ & $K\leq 12$ & $K\leq 15$\\
\cmidrule{1-5}
\textit{cover hole}      & \textbf{•}&  \textbf{•}   & \textbf{•}  & \textbf{take piece}   \\ 
    & \textbf{•}&  \textbf{•}   & \textbf{•}  & \textbf{keep soil}   \\ 
        & \textbf{•}&  \textbf{•}   & \textbf{•}  & \textbf{stop soil}   \\ 
        
\textit{take plant}   & \textbf{take plant}&  \textbf{take plant}  & \textbf{take plant}  & \textbf{take plant}  \\ 
\textit{put soil}     & \textbf{use soil}& \textbf{use soil}&\textbf{use soil}&\textbf{use soil}\\
\textit{loosen root}  & \textbf{loosen soil}&  \textbf{loosen soil}&\textbf{loosen soil}&\textbf{loosen soil}\\
\textit{place plant}  & \textbf{place plant}&   \textbf{place plant}&  \textbf{place plant}&  \textbf{place plant}\\
\textit{add top}      & \textbf{add soil}& \textbf{add soil}& \textbf{add soil}& \textbf{add soil}\\
& \textbf{•}&  \textbf{•}   & \textbf{•}  & \textbf{fill pot}   \\
& \textbf{•}&  \textbf{•}   & \textbf{•}  & \textbf{get soil}   \\ 
& \textbf{•}&  \textbf{•}   & \textbf{•}  & \textbf{give drink}   \\ 
\textit{water plant}  & \textbf{water plant}&  \textbf{water plant}&\textbf{water plant}&\textbf{water plant}\\
& \textbf{•}&  \textbf{•}   & \textbf{•}  & \textbf{give watering}   \\ 
\midrule
Precision  & 1    &  1      & 1    &  0.54  \\
Recall     & 0.86 &  0.86   & 0.86 &  1 \\

        \bottomrule
    \end{tabular}
}
     \caption{Repot a plant}
     \end{subtable}        
     \hspace*{\fill}%
     \begin{subtable}{0.48\textwidth}
          \centering
     \resizebox{0.95\textwidth}{!}{
      \footnotesize
    \begin{tabular}{lrrrr >{\centering\hspace{0.5pt}}m{0cm}}
    \toprule
GT (\textbf{7}) & $K\leq 7$ &  $K\leq 10$ & $K\leq 12$ & $K\leq 15$\\
\cmidrule{1-5}
\textit{open airway}  & \textbf{open airway}&    \textbf{open airway} & \textbf{open airway} & \textbf{open airway}   \\ 
\textit{check response}  & \textbf{•}&    \textbf{•}  &  \textbf{•} & \textbf{•}   \\ 
\textit{call 911}  & \textbf{•}&    \textbf{•}  &  \textbf{•} & \textbf{•}   \\ 
\textit{check breathing}  & \textbf{•}&    \textbf{•}  &  \textbf{•} & \textbf{•}   \\ 
\textit{check pulse}  & \textbf{}&    \textbf{put hand}  &  \textbf{put hand}  & \textbf{put hand}   \\ 
\textit{•}  & \textbf{tilt head} &    \textbf{tilt head}  &  \textbf{tilt head} & \textbf{tilt head}   \\  
\textit{}  &  \textbf{lift chin} &    \textbf{lift chin}  &  \textbf{lift chin} & \textbf{lift chin}   \\  
\textit{give breath}  & \textbf{give breath}&    \textbf{give breath}  &  \textbf{give breath}& \textbf{give breath}   \\ 
\textit{give compression}  & \textbf{do compr.}&    \textbf{do compr.}  &  \textbf{do compr.} & \textbf{do compr.}   \\ 
\textit{}  & \textbf{open airway}&    \textbf{open airway}  &  \textbf{open airway} & \textbf{open airway}   \\  
\textit{}  & \textbf{•}&    \textbf{start compr.}  &  \textbf{start compr.} & \textbf{start compr.}   \\  
\textit{}  & \textbf{•}&    \textbf{}  &  \textbf{} & \textbf{continue cpr}   \\ 
\textit{}  & \textbf{•}&    \textbf{do compr.}  &  \textbf{do compr.} & \textbf{do compr.}   \\ 
\textit{}  & \textbf{•}&    \textbf{•}  &  \textbf{} & \textbf{put hand}   \\ 
\textit{}  & \textbf{•}&    \textbf{give breath}  &  \textbf{give breath} & \textbf{give breath}   \\ 

\midrule
Precision  &  0.	5 &  0.4      &  0.4 &  0.33  \\
Recall     &  0.43 &  0.57    & 0.57 & 0.57   \\
\bottomrule
    \end{tabular}
   } 
     \caption{Performing CPR}
     \end{subtable}
   
   
%
     
%
%
%
%
%
\centering
\footnotesize %

     \begin{subtable}{0.48\textwidth}
     \centering
\resizebox{0.95\textwidth}{!}{     
    \begin{tabular}{lrrrr >{\centering\hspace{0.5pt}}m{0cm}}
    \toprule
GT (\textbf{12}) & $K\leq 7$ &  $K\leq 10$ & $K\leq 12$ & $K\leq 15$\\
\cmidrule{1-5}
\textit{get cars}  & \textbf{}&    \textbf{}  &  \textbf{} & \textbf{}   \\ 
\textit{open hood}  & \textbf{}&    \textbf{}  &  \textbf{} & \textbf{}   \\ 
\textit{}  & \textbf{}&    \textbf{}  &  \textbf{} & \textbf{have terminal}   \\  
\textit{}  & \textbf{}&    \textbf{}  &  \textbf{attach cab.} & \textbf{attach cab.}   \\ 
\textit{connect red A}  & \textbf{connect cable}&    \textbf{conn. cable}  &  \textbf{conn. cable} & \textbf{conn. cable}   \\
\textit{}  & \textbf{}&    \textbf{}  &  \textbf{} & \textbf{conn. clamp}   \\  
\textit{}  & \textbf{charge battery}&    \textbf{charge batt.}  &  \textbf{charge batt.} & \textbf{charge batt.}   \\ 
\textit{connect red B}  & \textbf{connect end}&    \textbf{conn. end}  &  \textbf{conn. end} & \textbf{conn. end}   \\ 
\textit{connect black A}  & \textbf{}&    \textbf{}  &  \textbf{conn. cab.} & \textbf{conn. cab.}   \\ 
\textit{connect ground}  & \textbf{•}&    \textbf{•}  &  \textbf{have cab.} & \textbf{have cab.}   \\ 
\textit{start car A}  & \textbf{start car}&    \textbf{start car}  &  \textbf{start car} & \textbf{start car}   \\ 
\textit{start car B}  & \textbf{•}&    \textbf{•}  &  \textbf{start vehicle} & \textbf{start veh.}   \\ 
\textit{}  & \textbf{•}&    \textbf{•}  &  \textbf{start engine} & \textbf{start eng.}   \\ 
\textit{remove ground}  & \textbf{remove cable}&    \textbf{rem. cable}  &  \textbf{rem. cable} & \textbf{rem. cable}   \\ 
\textit{remove black A}  & \textbf{disconnect cable}&    \textbf{disc. cable}  &  \textbf{disc. cable} & \textbf{disc. cable}   \\ 
\textit{remove red B}  & \textbf{}&    \textbf{}  &  \textbf{} & \textbf{}   \\ 
\textit{remove red A}  & \textbf{}&    \textbf{}  &  \textbf{} & \textbf{}   \\ 
        
        \midrule
Precision  & 0.83  &  0.83   &  0.72 &  0.69  \\
Recall     & 0.42  &  0.42   & 0.67 &  0.67 \\
\bottomrule
    \end{tabular}
}
     \caption{Jumping cars}
     \end{subtable}%

%
 \caption{\small
  Automatically recovered sequences of steps for the five tasks considered in this work. 
        Each recovered step is represented by one of the aligned direct object relations  (shown in bold). 
        Note that most of the recovered steps correspond well to the ground truth steps (showed in italic).
        The results are shown for setting the maximum number of discovered steps, $K = \{7,10,12,15\}$. 
        Note how our method automatically selects less than $K$ steps in some cases.
         These are the automatically chosen $k\leq K$ steps that are the most salient in the aligned narrations as described in Sec.~\ref{subsec:model_text}.  
%
        }    
    \label{tab:script_res} 
\end{table*}


\setlength{\tabcolsep}{6pt}




\begin{figure*}[!ht]
%
\includegraphics[width=\linewidth]{figs/qual_res_changing_tire_v2}
%
\caption{\footnotesize {\bf Examples of the recovered instruction steps for the task ``Changing the car tire".} 
%
%
}

\label{fig:qualitative_results_changing_tire} 
\end{figure*}

\begin{figure*}[!ht]
%
\includegraphics[width=\linewidth]{figs/qual_res_jump_car_v2}
%
\caption{\footnotesize {\bf Qualitative results for the task ``Jumping cars".}
%
}



\label{fig:qualitative_results_jump_car} 
\end{figure*}


\begin{figure*}[!ht]
%
\includegraphics[width=\linewidth]{figs/qual_res_repot_v2}
%
\caption{\footnotesize {\bf Qualitative results for the task ``Repot a plant".}
%
}

\label{fig:qualitative_results_repot} 
\end{figure*}


\begin{figure*}[!ht]
%
\includegraphics[width=\linewidth]{figs/qual_res_coffee_v2}
%
\caption{\footnotesize {\bf Qualitative results for the task ``Making coffee".}
%
}
\label{fig:qualitative_results_coffee} 
\end{figure*}


\begin{figure*}[!ht]
%
\includegraphics[width=\linewidth]{figs/qual_res_cpr_v2}
%
\caption{\footnotesize {\bf Qualitative results for the task ``Performing CPR".}
%
}

\label{fig:qualitative_results_cpr} 
\end{figure*}





\end{document}
\newcommand{\highlight}[1]{\textcolor{red}{#1}}

\newcommand{\cov}[2]{\text{Cov}\left[#1, #2\right]}
\newcommand{\dw}{\nabla_{\w}}
\newcommand{\fhat}{\hat{f}}
\newcommand{\Jhat}{\hat{J}}
\newcommand{\g}{{\bf g}}
\newcommand{\gb}{\bar{g}}
\newcommand{\mutil}{\tilde{\mu}}
\newcommand{\shat}{\hat{s}}
\newcommand{\sign}{\text{sign}}
\newcommand{\sqnorm}[1]{\left|\left|#1\right|\right|^2} 
\newcommand{\tr}{\text{Tr}}
\newcommand{\vt}{{\bf v}_t}
\newcommand{\vard}[1]{\text{Var}_d[#1]} 
\newcommand{\vectornorm}[1]{\left|\left|#1\right|\right|}
\newcommand{\w}{{\bf w}}
\newcommand{\we}{\tilde{\w}}
\newcommand{\wstar}{\w^*}
\newcommand{\wtil}{{\tilde{\bf w}}}
\newcommand{\x}{{\bf x}}
\newcommand{\y}{{\bf y}}
\newcommand{\abold}{{\bf a}}
\newcommand{\bbold}{{\bf b}}


\newcommand{\yhat}{\hat{y}}

\newcommand{\A}{{\mathcal A}}
\newcommand{\B}{{\mathcal B}}
\newcommand{\C}{{\mathcal C}}
\newcommand{\D}{{\mathcal D}}
\newcommand{\E}{{\mathbf E}}
\newcommand{\Efull}{\mathbb{E}}
\newcommand{\F}{{\mathcal F}}
\newcommand{\N}{{\mathbf N}}
\newcommand{\R}{{\mathbb{R}}}

\newcommand{\INDSTATE}[1]{\STATE \hspace{2mm} #1}

\newcommand{\al}[1]{{\color{blue}{#1}}}
\newcommand{\bri}[1]{{\color{red}{#1}}}


\newtheorem{mylemma}{Lemma}
\newtheorem{mytheorem}{Theorem}

\newcolumntype{S}{>{\centering\arraybackslash} m{.10\linewidth} }
\newcolumntype{T}{>{\centering\arraybackslash} m{.28\linewidth} }
\newcolumntype{U}{>{\centering\arraybackslash} m{.08\linewidth} }
\newcolumntype{V}{>{\centering\arraybackslash} m{.37\linewidth} }
\newcolumntype{W}{>{\centering\arraybackslash} m{.1\linewidth} }

\newcommand{\note}[1]{{\textbf{\color{red}#1}}}\vspace{-2mm}
\section{Discussion}
\vspace{-3mm}
We introduce the first provably convergent
algorithm for the TRW objective over the marginal polytope, 
under the assumption of exact MAP oracles.
%
%
We quantify the gains obtained both from marginal inference over $\MARG$ \textit{and} from tightening
over the spanning tree polytope. 
%
%
We give heuristics that improve the scalability of Frank-Wolfe
when used for marginal inference.
The runtime cost of iterative MAP calls (a reasonable rule of thumb is to assume 
an approximate MAP call takes roughly the same time as a run of TRBP) is worthwhile particularly in 
%
%
cases such as the Chinese Characters where $\LOCAL$ is loose. 
Specifically, our algorithm is appropriate for domains where marginal inference is hard but there exist
efficient MAP solvers capable of handling non-submodular potentials.
Code is available at {\small \url{https://github.com/clinicalml/fw-inference}}. 

Our work creates a flexible, modular framework for optimizing a broad
class of variational objectives, not simply TRW, with guarantees of convergence. We hope that this
will encourage more research on building better entropy approximations.
%
%
%
The framework we adopt is more generally applicable
to optimizing functions whose gradients tend to infinity at the boundary of the
domain.
%
%

%
%
%
%

Our method to deal with gradients that diverge at the boundary
bears resemblance to barrier functions used in interior point methods insofar as they bound
the solution away from the constraints. Iteratively decreasing $\shrinkAmount$ in our framework
can be compared to decreasing the strength of the barrier, enabling the iterates to get closer to the
facets of the polytope, although its worthwhile to note that we have an \emph{adaptive} method of doing so.
\documentclass[twoside,11pt]{article}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

\usepackage{jmlr2e, amsmath, color,array, booktabs,bm,algorithm, algorithmic}

\DeclareGraphicsExtensions{.pdf}

\usepackage{tikz}

\usetikzlibrary{arrows,shapes,backgrounds,patterns,fadings,decorations.pathreplacing,decorations.pathmorphing}
\tikzset{>=stealth'}
\tikzstyle{graphnode} = [circle,draw=black,minimum size=22pt,text centered, text width=22pt, inner sep=0pt]
\tikzstyle{var}   =[graphnode,fill=white]
\tikzstyle{obs}   =[graphnode,fill=black,text=white]
\tikzstyle{fac}   =[rectangle,draw=black,fill=black!25,minimum size=5pt]
\tikzstyle{facprior} =[rectangle,draw=black,fill=black,text=white,minimum size=5pt]
\tikzstyle{edge}  =[draw=white,double=black,thick,-]
\tikzstyle{prior} =[rectangle, draw=black, fill=black, minimum size=
5pt, inner sep=0pt]
\tikzstyle{dirprior} = [circle, draw=black, fill=black, minimum
size=5pt, inner sep=0pt]

% Definitions of handy macros can go here
\def\argmin{\qopname\relax n{argmin}}

\renewcommand{\vec}{\boldsymbol}
\newcommand{\mat}[1]{\mathsf{#1}}
\newcommand{\Trans}{^\intercal}
\newcommand{\Exp}{\mathbb{E}}
\newcommand{\Ent}{\mathbb{H}}
\newcommand{\II}{\mathbb{I}}
\newcommand{\I}{\boldsymbol{I}}
\renewcommand{\O}{\mathcal{O}}

\def\x{{\mathbf x}}
\def\y{{\mathbf y}}
\def\c{{\mathbf c}}
\def\w{{\mathbf w}}
\def\r{{\mathbf r}}
\def\e{{\mathbf e}}
\def\l{{\mathbf l}}
\def\u{{\mathbf u}}
\def\a{{\mathbf a}}
\def\m{{\mathbf m}}
\def\boldmu{\bm{\mu}}
\def\boldeta{\bm{\eta}}
\def\boldtheta{\bm{\theta}}
\def\regionA{\mathcal{A}}
\def\authorrefmark#1{$^{#1}$}
\newcommand{\normal}[2]{\ensuremath{\mathcal{N}\left(#1,#2\right)}}
\newcommand{\reals}{\mathsf{I\!R}}
\newcommand{\N}{\mathcal{N}}
\renewcommand{\d}{\,\mathrm{d}}
\newcommand{\KL}{\text{KL}}


\def\vecx{\vec{x}}
\def\vecc{\vec{c}}
\def\veca{\vec{a}}
\def\vecp{\vec{p}}
\def\vecmu{\vec{\mu}}
\def\vecy{\vec{y}}
\def\sfP{\mathsf{P}}
\def\sfQ{\mathsf{Q}}
\def\sfV{\mathsf{V}}
\newcommand{\given}{\mid}
\newcommand{\wo}{\setminus}

\newcommand{\todo}[1]{\textcolor{red}{[#1]}}
\newcommand{\phtodo}[1]{\textcolor{blue}{[#1]}}
\newcommand{\sljtodo}[1]{\textcolor{green}{[#1]}}

%%%% PHILIPP'S COMMENTS%%%%%%%%%
\newcommand{\phcomment}[1]{\textcolor{blue}{[#1]}}
%%%%%%%%%%%%%%%%%%%%%%%%%




\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}

% Heading arguments are {volume}{year}{pages}{submitted}{published}{author-full-names}

\jmlrheading{X}{2011}{XX-XX}{04/11}{X/11}{John P. Cunningham, Philipp Hennig, and Simon Lacoste-Julien}

% Short headings should be running head and authors last names

\ShortHeadings{Gaussian Probabilities and Expectation Propagation}{Cunningham, Hennig, Lacoste-Julien}
\firstpageno{1}



\begin{document}

%\todo{ {\huge \bf DRAFT NOTES FOR JPC, PH, SLJ}
%\begin{enumerate}
%\item This draft (13/04/2011) is final and nearly ready to submit.  In your reading and response, please consider the question, ``are we ready to submit?"  Are you signed off on me submitting this, pending incorporation of your changes, or do you want to see it again and check it again?  If I am given approval to submit, I will do so 15 Friday.
%\item minor point: Some feel the introduction is too much like a laundry list in terms of the positive aspects of Gaussians that it introduces.  Some like that list.  Should we cut it down?  I am happy to do so.  Default: No.
%\item minor point:  We have all noted that the paper is lengthy.  You guys have also noted that the trapezoid probabilities are confusing.  Should we cut those out?  We can do so pretty much losslessly and it would save a page or two.  I am inclined to do so.  Your thoughts?  Default: No.
%\item minor point: There have been several suggestions of a metric to quantify the region in the whitened space.  Currently we use condition number for simplicity.  Simon has suggested l1 and l2 norms of the scaled Gram matrix.  The story told by those metrics (and corresponding figures) is just about the same, at some points a bit more confusing.  I want to keep Figure 4 panel D to make that point about eccentricity of the region and dismiss it (ie, i prefer not to change that one).  For Figure 5 and 6, I am inclined to keep them as is, as it doesn't require much explanation.  I have added sentences saying other metrics look similar.  See attached panelDoptions.pdf workup of those.  Do we particularly care to make this change?  Default: no change.    
%\item Appendices:  I am still open to any restructuring.  If you want to do so, please let me know.  Default: nothing more.
%\item We have all been working really hard on this.  Thank you very much guys.  Let's submit this soon.  
%\item Appendix C: I know I know, I still haven't finished it.  Sorry.  I will send it along Thursday.  Hopefully you won't have read that far by then.
%\item ALL: This draft (of 08/04/2011) reflects almost all the opinions/edits we have to date.  It starts from Philipp's draft and John's draft.  I tried to model-average the two, from presentation choices (of EP for example, where we went with Philipp's) down to notation choices ($\c_i$ instead of $\xi_i$, for example) and even dialect (British English...).  Simon has gone through an earlier draft and most all of his changes are in here (save the big appendix rewrites... see below).  Zoubin and Carl have added several ideas to the results, presentation, and discussion; these have been incorporated.  The paper is big and there are many new parts, so please read carefully and perhaps like a reviewer who hasn't seen it before.
%\item ALL: please focus on clarification of the message.  We really want to avoid people thinking this is a paper about EPMGP vs. Genz.  It is about using EP in a novel way to get a new method for an important problem (Gaussian probabilities).   Also it is about investigating EP in a way not usually done.  Third we demonstrate choices for the applied researcher who needs to calculate Gaussian probabilities.  Etc...
%\item ALL: My opinion is that we need to focus on the discussion.  It has seen the least revisions/considerations, etc, and it is important for delivering the theoretical contribution of this work.
%\item ALL: I am not wedded to many of these particulars, but I am driven to finish this soon.  Thus, with apologies for beating a dead horse, please prioritise and specify edits.
%\item JPC: make a proper figure for Figure \ref{fig:white} (currently cartoon)
%\item JPC: write appendix \ref{sec:minpoly}.
%\item JPC: write appendix \ref{sec:deriv}.
%\item PH: edit main text as much as desired
%\item SLJ: make factor graph figures for discussion
%\item SLJ: add your subsection in red in discussion
%\item PH: as time permits hack up the appendices (only place where version control could be an issue, see next)
%\item SLJ: as time permits hack up the appendices (only place where version control could be an issue, see previous).  NOTE: Simon, as you and I discussed, I went through and made your small edits to the readability of these appendices.  I \emph{did not} change the structure broadly.  I do agree that your suggestions are mostly superior to what is there, but I will leave it to you to make those big changes if you have the time. 
%\end{enumerate}
%}
%\sljtodo{Simon please make comments in green}
%\phtodo{Philipp please make comments in blue}


\newpage

\title{Gaussian Probabilities\\ and Expectation Propagation}


\author{%
\name John P. Cunningham
\email jpc74@cam.ac.uk \\
\addr Department of Engineering\\
University of Cambridge\\
Cambridge, UK\\
\AND
\name Philipp Hennig
\email{phennig@tuebingen.mpg.de} \\
\addr Max Planck Institute for Intelligent Systems\\
T\"ubingen, Germany\\
\AND
\name Simon Lacoste-Julien
\email{sl522@cam.ac.uk} \\
\addr Department of Engineering\\
University of Cambridge\\
Cambridge, UK\\
}

\editor{Editor Name}

\maketitle

\begin{abstract}% <- trailing '%' for backward compatibility of .sty file
  While Gaussian probability \emph{densities} are omnipresent in applied mathematics, Gaussian cumulative \emph{probabilities} are hard to calculate in any but the univariate case. 
  %Numerical integration methods provide reliable results in principle, but many applications require analytic answers, in particular derivatives with respect to the distribution's parameters. 
  We study the utility of Expectation Propagation (EP) as an approximate integration method for this problem. For rectangular integration regions, the approximation is highly accurate. We also extend the derivations to the more general case of polyhedral integration regions. However, we find that in this polyhedral case, EP's answer, though often accurate, can be almost arbitrarily wrong. We consider these unexpected results empirically and theoretically, both for the problem of Gaussian probabilities and for EP more generally.  These results elucidate an interesting and non-obvious feature of EP not yet studied in detail.  \end{abstract}

\begin{keywords}
Multivariate Normal Probabilities, Gaussian Probabilities, Expectation Propagation, Approximate Inference
\end{keywords}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% MAIN TEXT %%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% INTRODUCTION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:introduction}

This paper studies approximations to definite integrals of Gaussian (also known as normal or central) probability distributions. We define the Gaussian distribution $p_0(\x) = \N(\x;\m,K)$ as
%
\begin{equation} 
\label{eqn:gaussian} 
p_0(\x) =  \frac{1}{(2\pi)^{\frac{n}{2}}|K|^{\frac{1}{2}}} \exp\left\{ -\frac{1}{2}(\x - \m)^T K^{-1}(\x - \m)\right\}, \end{equation}
%
\noindent where $\x\in\reals^{n}$ is a vector with $n$ real valued elements, $\m\in\reals^{n}$ is the mean vector, and $K\in\reals^{n\times n}$ is the symmetric, positive semidefinite covariance matrix. The Gaussian is perhaps the most widely used distribution in science and engineering. Whether this popularity is reflective of a fundamental character (often afforded to this distribution because of its role in the Central Limit Theorem) is debatable, but at any rate, a battery of convenient analytic characteristics make it an indispensible tool for many applications.   Multivariate Gaussians induce Gaussian marginals and conditionals on all linear subspaces of their domain, and are closed under linear transformations of their domain. Under Gaussian beliefs, expectations can be evaluated for a number of important function classes, such as linear functions, quadratic forms, exponentials of the former, trigonometric functions, and linear combinations of such functions. Gaussians form an exponential family (which also implies closure, up to normalisation, under multiplication and exponentiation, and the existence of an analytic conjugate prior---the normal-Wishart distribution). The distribution's moment generating function and characteristic function have closed-form. Historically, much importance has been assigned to the fact that the Gaussian maximises differential entropy for given mean and covariance. More recently, the fact that the distribution can be analytically extended to the infinite-dimensional limit---the Gaussian Process \citep{rasmussenBook}---has led to a wealth of new applications. Yet, despite all these great aspects of Gaussian \emph{densities}, Gaussian \emph{probabilities} are difficult to calculate: even the cumulative distribution function (cdf) has no closed-form expression and is numerically challenging in high-dimensional spaces. More generally, we consider the probability that a draw from $p(\x)$ falls in a region $\regionA\subseteq\reals^{n}$, which we will denote as
\begin{equation}
\label{eqn:cumdensity}
F(\regionA) = \mathrm{Prob}\left\{\x\in \regionA\right\} = \int_\regionA p(\x)\d\x =
\int_{\ell_1(\x)}^{u_1(\x)} \cdots \int_{\ell_n(\x)}^{u_n(\x)} p(\x) \d
x_n \cdots \d x_1,
\end{equation}
where $\ell_1,{\ldots} ,\ell_n$ and $u_1,{\ldots} ,u_n$ denote the upper and lower bounds of the region $\regionA$.  In general, $\u(\x)$ and $\l(\x)$ may be nonlinear functions of $\x$ defining a valid region $\regionA$, but the methods presented here discuss the case where $\u$ and $\l$ are linear functions of $\x$, meaning that $\regionA$ forms a (possibly unbounded) polyhedron. The probability $F(\regionA)$ generalises the cdf, which is recovered by setting the upper limits $\u=(u_i)_{i=1,\dots,n}$ to a single point in $\reals^n$ and the lower limits to $l_1={\ldots} =l_n = -\infty$. Applications of these multivariate Gaussian probabilities are widespread. They include statistics \citep{genz92, joeMVNcdfJASA1995, Hothorn05unbiasedrecursive} (where a particularly important use case is probit analysis \citep{AshfordSowden1970,SicklesTabuman1986,Gibbons1996}), economics \citep{Boyle05pricingoptions}, mathematics \citep{Hickernell99theasymptotic}, biostatistics \citep{thiebautCMPB2004,zhaoCSDA2005}, medicine \citep{lesaffre1991}, environmental science \citep{Buccianti_computationalinvestigations}, computer science \citep{Klerk_onapproximate}, neuroscience \citep{Pillow04maximumlikelihood}, machine learning \citep{liaoICML2007}, and more (see for example the applications listed in \citet{gassmann2002}).  For the machine learning researcher, two popular problems that can be cast as Gaussian probabilities include the Bayes Point Machine \cite[]{herbrichBook} and Gaussian Process classification \cite[]{rasmussenBook, KussRasmussen2005}, which will we discuss more specifically later in this work.

Univariate Gaussian probabilities can be so quickly and accurately calculated \citep[\emph{e.g.}][]{cody69} that the univariate cumulative density function is available with machine-level precision in many statistical computing packages ({\it e.g.}, {\tt normcdf} in {\sc matlab}, {\tt CDFNORM} in {\sc spss}, {\tt pnorm} in {\sc r}, to name a few).  Unfortunately, no similarly powerful algorithm exists for the multivariate case. There are some known analytic decompositions of Gaussian integrals into lower-dimensional forms \citep{Placket1954,Curnow1962,Lazard2003}, but such decompositions have very high computational complexity \citep{Huguenin2009}. In statistics, sampling based methods have found application in specific use cases \citep{LermanManski1981,McFadden1989,Pakes1989}, but the most efficient general, known method is numerical integration \citep{genz92, drezner89, drezner94, genz99, genz99b, genz02, genz04}. A recent book \citep{genzBook} gives a good overview. These algorithms make a series of transformations to the Gaussian $p_0(\x)$ and the region $\regionA$, using the Cholesky factor of the covariance $K$, the univariate Gaussian cdf and its inverse, and randomly generated points.  These methods aim to restate the calculation of $F(\regionA)$ as a problem that can be well handled by quasi-random or lattice point numerical integration. Many important studies across many fields have been critically enabled by these and other algorithms \citep{joeMVNcdfJASA1995, Hothorn05unbiasedrecursive, Boyle05pricingoptions, Hickernell99theasymptotic, thiebautCMPB2004, zhaoCSDA2005, Buccianti_computationalinvestigations, Klerk_onapproximate, Pillow04maximumlikelihood, liaoICML2007}.   In particular, the methods of Genz represent the state of the art, and we will use this method as a basis for comparison.  We note that the numerical accuracy of these methods is generally high and can be increased by investing additional computational time (more integration points), though by no means to the numerical precision of the univariate case.  As such, achieving high accuracy invokes substantial computational cost.  Also, applications such as Bayesian model selection require \emph{analytic} approximations of $F(\regionA)$, usually because the goal is to optimise $F(\regionA)$ with respect to the parameters $\{\m,K\}$ of the Gaussian, which requires the corresponding derivatives.  These derivatives and other features (to be discussed) are not currently offered by numerical integration methods.  Thus, while there exist sophisticated methods to compute Gaussian probabilities with high accuracy, there remains significant work to be done to address this important problem.  

In Section \ref{sec:ep}, we will develop an analytic approximation to $F(\regionA)$ by using \emph{Expectation Propagation (EP)} \cite[]{minka01phd, minkaUAI01,minkaMSFTTR2005, opperTAP2000} as an approximate integration method. We first give a brief introduction to EP (Section \ref{sec:expect-prop}), then develop the necessary methods for integration over \emph{hyperrectangular} regions $\regionA$ in Section \ref{sec:rect-integr-regi}, which is the case most frequently computed (and which includes the cdf).  In Section \ref{sec:polyh-integr-regi}, we then describe how to generalise these derivations to \emph{polyhedral} $\regionA$. However, while these polyhedral extensions are conceptually elegant, it turns out that they do not always lead to an accurate algorithm. In Section \ref{sec:results}, we compare EP's analytic approximations to numerical results. For rectangular $\regionA$, the approximations are of generally high quality. For polyhedral $\regionA$, they are often considerably worse. This shortcoming of EP will come as a surprise to many readers, because the differences to the rectangular case are inconspiciously straightforward.  Indeed, we will demonstrate that hyperrectangles are in fact not a fundamental distinction, and we will use intuition gained from the polyhedral case to build pathological cases for hyperrectangular cases also.  We study this interesting result in Section \ref{sec:discussion} and give some insight into this problem. Our overall empirical result is that EP provides reliable analytic approximations for rectangular Gaussian integrals, but should be used only with caution on more general regions. This has implications for some important applications, such as Bayesian generalised regression, where EP is often used.

This work bears connection to previous literature in machine learning.  Most obviously, when considering the special case of Gaussian probabilities over hyperrectangular regions (Section \ref{sec:rect-integr-regi}), our algorithm is derived directly from EP, yet distinct in two ways.  First, conceptually, we do not use EP for approximate inference, but instead we put integration bounds in the place of likelihood terms, thereby using EP as a high dimensional integration scheme.  Second, because of this conceptual change, we are dealing with unnormalised EP (as the ``likelihood" factors are not distributions).  From this special case, we extend the method to more general integration regions, which involves rank one EP updates.  Rank one EP updates have been previously discussed, particularly in connection to the Bayes Point Machine \cite[]{minkaUAI01, herbrichBook, minkaTR2008}, but again their use for Gaussian probabilities has not been investigated. 

Thus, our goal here is to build from EP and the general importance of the Gaussian probability problem, to offer three contributions: first, we give a full derivation that focuses EP on the approximate integration problem for multivariate Gaussian probabilities.  Second, we perform detailed numerical experiments to benchmark our method and other methods, so that this work may serve as a useful reference for future researchers confronted with this ubiquitous and challenging computation.  Third, we discuss empirically and theoretically some features of EP that have not been investigated in the literature, which has importance for EP well beyond Gaussian probabilities.

The remainder of this paper is laid out as follows.  In Section \ref{sec:ep}, we discuss the EP algorithm in general and its specific application when the approximating distribution is Gaussian, as is our case of interest here.  In Section \ref{sec:epmgp}, we apply EP specifically to Gaussian probabilities, describing the specifics and features of our algorithm: Expectation Propagation for Multivariate Gaussian Probabilities (EPMGP).  In Section \ref{sec:othermgp}, we describe other existing methods for calculating Gaussian probabilities, as they will be compared to EPMGP in the results section.  In Section \ref{sec:testcases}, we describe the probabilities that form the basis of our numerical computations.  Section \ref{sec:results} gives the results of these experiments, and Section \ref{sec:discussion} discusses the implications of this work and directions for future work.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Approximate Integration Through Approximate Moment Matching: Expectation Propagation}
\label{sec:ep}

The key step to motivate this approach is to note that we can cast the Gaussian probability problem as one of integrating an intractable and unnormalised distribution.  Defining $p(\x)$ as 
%
\begin{gather}
\label{eqn:trunc2}
p(\x) = \begin{cases} 
p_0(\x) & \x \in \regionA \\
~~0~~ & \mathrm{otherwise,} 
\end{cases}
\end{gather}

\noindent we see that the normaliser of this distribution is our probability of interest $F(\regionA) = \mathrm{Prob}\left\{\x\in \regionA\right\} = \int_\regionA p_0(\x)\d\x = \int p(\x) \d\x$.

Expectation Propagation \cite[]{minka01phd, minkaUAI01,minkaMSFTTR2005, opperTAP2000} is a method for finding an approximate unnormalised distribution $q(\x)$ to replace an intractable true unnormalised distribution $p(\x)$. The EP objective function is involved, but EP is \emph{motivated} by the idea of minimising Kullback-Leibler (KL) divergence \cite[]{CoverandThomas} from the true distribution to the approximation. We define this KL-divergence as 
\begin{equation}
  \label{eq:1}
  D_\KL(p \| q) = \int p(\x) \log \frac{p(\x)}{q(\x)} \d \x + \int q(\x) \d \x -
  \int p(\x) \d \x
\end{equation}
which is a form of the divergence that allows unnormalised distributions and reverts to the more popular form if $p$ and $q$ have the same indefinite integral. 

KL-divergence is a natural measure for the
quality of the approximation $q(\x)$.  If we choose $q(\x)$ to be
a high-dimensional Gaussian, then the choice of $q(\x)$ that
minimises $D_\KL(p \parallel q)$ is
the $q(\x)$ with the same zeroth, first, and
second moments as $p(\x)$.  If we are trying
to calculate $F(\regionA)$, then we
equivalently seek the zeroth moment of $p(\x)$.  As such, trying to minimise this global KL
divergence is an appropriate and sensible method to calculate Gaussian probabilities.  Appendix~\ref{sec:KL} reviews the equivalence between moment matching and minimising KL-divergence in this problem. 

 Unfortunately, minimising
global KL-divergence directly is in many cases intractable.  This
fact motivated the creation of the EP algorithm, which seeks to approximately do
this global KL minimisation by
iteratively minimising the KL-divergence of local, single factors
of $q(\x)$ with respect to $p(\x)$.  Since $F(\regionA)$ is the zeroth moment of $p(\x)$ as defined in Equation (\ref{eqn:cumdensity}), any algorithm trying to minimise this global KL-divergence objective, or an approximation thereof, is also a candidate method for the (approximate) calculation of Gaussian probabilities.







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% EP GENERAL SECTION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Expectation Propagation}
\label{sec:expect-prop}

The following sections review EP and introduce its use for Gaussian probability calculations.  It is a prototypical aspect of Bayesian inference that an intractable distribution $p(\x)$ is a product of a prior distribution $p_0(\x)$ and one or more likelihood functions or factors $t_i(\x)$: 

\begin{equation}
  \label{eq:2}
  p(\x) = p_0(\x) \prod_i t_i(\x).
\end{equation}

\noindent Note that our Gaussian probability problem has this unnormalised form, as it can be written as: 

\begin{equation}
  \label{eqn:cumdensityEP}
  F(\regionA) = \int_\regionA p_0(\x)\d\x = \int p(\x)\d\x = \int p_0(\x) \prod_i t_i(\x) \d\x
\end{equation}
where $t_i(\x)$ is an indicator function defined in a particular direction, namely a ``box function":

\begin{equation}
\label{eqn:truefactor}
t_i(\x)  =  \II \bigl\{ l_i < \c_i^T\x < u_i \bigr\} = 
\begin{cases}
1 & ~~ l_i < \c_i^T\x < u_i \\
0 & ~~ \mathrm{otherwise}. 
\end{cases}
 \end{equation}

The above form is our intractable problem of interest\footnote{This box function notation may seem awkward vs. the more conventional halfspace definitions of polyhedra, but we will make use of this definition.  Further, halfspaces can be recovered by setting any $u_i = \infty$, so this definition is general.}, and we assume without loss of generality that the $\c_i$ have unit norm.  An important clarification for the remainder of this work is to note that most of the distributions discussed will be unnormalised.  By the previous construction, the factors $t_i(\x)$ are unnormalised, and thus so is $p(\x)$ (as will be the EP approximation $q(\x)$).  Though the steps we present will all be valid for unnormalised distributions, we make this note to call out this somewhat atypical problem setting.

   The approach of the EP algorithm is to replace each intractable $t_i(\x)$ with a tractable unnormalised Gaussian $\tilde{t}_i(\x)$.  Our specific algorithm thus yields a nice geometric interpretation.  We want to integrate a Gaussian over a polyhedron defined by several box functions, but this operation is intractable.  Instead, EP allows us to replace each of those intractable box truncation functions with soft Gaussian truncations $\tilde{t}_i(\x)$.   Then, since we know that these exponential family distributions are simple to multiply (multiplication essentially amounts to summing the natural parameters), our problem reduces to finding $\tilde{t}_i(\x)$.

This perspective motivates the EP algorithm, which approximately
minimises $D_\KL(p \| q)$ by iteratively constructing an unnormalised approximation
$q(\x)$. At any point in the
iteration, EP tracks a current approximate $q(\x)$ in an
exponential family, and a set of \emph{approximate factors} (or \emph{messages}) $\tilde{t}_i$, also
in the family. The factors are updated by constructing a
\emph{cavity} distribution
%
\begin{equation}
  \label{eq:5}
  q^{\wo i}(\x) = \frac{q(\x)}{\tilde{t}_i(\x)}
\end{equation}
%
(this division operation on unnormalised distributions is also well-defined for members
of exponential families - it amounts to subtracting natural parameters), and then
\emph{projecting} into the exponential family
%
\begin{equation}
  \label{eq:6}
  \tilde{t}_i(\x)q^{\wo i}(\x) = \operatorname{proj} [t_i(\x)q^{\wo i}(\x)].
\end{equation}
%
where this projection operation (\cite{PowerEP}, an M-projection from information geometry \cite[]{kollerBook}) is defined as setting $\tilde{t}_i(\x)$ to the unnormalised member of the exponential family minimizing $D_\KL(t_i q^{\wo i}\| \tilde{t}_i q^{\wo i})$.   Intuitively, to update an approximate factor $\tilde{t}_i(\x)$, we first remove its effect from the current approximation (forming the cavity), and then we include the effect of the true factor $t_i(\x)$ by an M-projection, updating $\tilde{t}_i(\x)$ accordingly.  As derived in Appendix~\ref{sec:KL}, this projection means matching the sufficient statistics of $\tilde{t}_i(\x)q^{\wo i}(\x)$ to those of $t_i(\x) q^{\wo i}(\x)$. In particular for Gaussian $\tilde{t}_i(\x)$, matching sufficient statistics is equivalent to matching \emph{zeroth}, first and second moments.   We now restrict this general prescription of EP to the Gaussian case, as it will clarify the simplicity of our Gaussian probability algorithm.

\subsubsection{Gaussian EP with rank-one factors}
\label{sec:gaussian-ep}

In cases where the approximating family is Gaussian, the EP derivations can be specified further without having to make additional assumptions about the true $t_i(\x)$. Because it will lend clarity to Sections \ref{sec:rect-integr-regi} and \ref{sec:polyh-integr-regi}, we give that analysis here.  To orient the reader, it is important to be explicit about the numerous uses of ``Gaussian" here.  When we say ``Gaussian EP," we mean \emph{any} EP algorithm where the approximating $q(\x)$ (the prior $p_0(\x)$ and approximate factors $\tilde{t}_i(\x)$) are Gaussian.  The true factors $t_i(\x)$  can be arbitrary.  In particular, here we discuss Gaussian EP, and only in Section \ref{sec:epmgp} do we discuss Gaussian EP as used for multivariate Gaussian probability calculations (with box function factors $t_i(\x)$).  It is critical to note that all steps in this next section are invariant to changes in the form of the true $t_i(\x)$, so what follows is a general description of Gaussian EP.  The unnormalised Gaussian approximation is 
 
\begin{equation}
q(\x) = p_0(\x)\prod_i \tilde{t}_i(\x) = p_0(\x)\prod_i \tilde{Z}_i\mathcal{N}(\x; \tilde{\mu}_i,\tilde{\sigma}_i^2) = Z \N(\x; \mu, \Sigma).
\end{equation}

We will require the cavity
distribution for the derivation of the updates, which is defined as

\begin{equation}
q^{\wo i}(\x) = \frac{q(\x)}{\tilde{t}_i(\x)} = Z^{\wo i} \N(\x ; \u^{\wo i} , V^{\wo i})
\end{equation}

The above step is the \emph{cavity} step of EP (Equation \ref{eq:5}).  We now must do the projection operation of Equation \ref{eq:6}, which involves moment matching the approximation $\tilde{t}_i(\x)q^{\wo i}(\x)$ to the appropriate moments of $t_i(\x)q^{\wo i}(\x)$:

\begin{eqnarray}
\label{eqn:moments}
\hat{Z}_i & = & \int t_i(\x) q^{\wo i}(\x) \d\x  \\
\hat{\u}_i & = & \frac{1}{\hat{Z}_i}\int \x t_i(\x) q^{\wo i}(\x) \d\x  \\
\hat{V}_i & = & \frac{1}{\hat{Z}_i}\int (\x - \hat{\u}_i)(\x - \hat{\u}_i)^T t_i(\x) q^{\wo i}(\x) \d\x. 
\end{eqnarray}

These updates are useful and tractable if the individual $t_i(\x)$ have some simplifying structure such as low rank, as in Equation \ref{eqn:truefactor}.  In particular, as is the case of interest here, if the $t_i(\x)$ have rank one structure, then these moments have simple form:

\begin{eqnarray}
\label{eqn:r1moments}
\hat{Z}_i & = & \int t_i(\x) q^{\wo i}(\x) \d\x  \nonumber \\
\hat{\u}_i & = & \hat{\mu}_i   \c_i \nonumber \\
\hat{V}_i & = & \hat{\sigma}_i^2 \c_i \c_i^T,
\end{eqnarray}

\noindent where $\{\hat{Z}_i,\hat{\mu}_i,\hat{\sigma}_i^2\}$ depend on the factor $t_i(\x)$ and $\c_i$ is the rank one direction as in Equation \ref{eqn:truefactor}.   The important thing to note is that these parameters $\{\hat{Z}_i,\hat{\mu}_i,\hat{\sigma}_i^2\}$ are the only place in which the actual form of the $t_i(\x)$ enter in.   This can also be derived from differential operators, as was done in a short technical report \cite[]{minkaTR2008}.

To complete the projection step, we must update the approximate factors $\tilde{t}_i(\x)$ such that the moments of $\tilde{t}_i(\x)q^{\wo i}(\x)$ match $\{\hat{Z}_i,\hat{\mu}_i,\hat{\sigma}_i^2\}$, as doing so is the KL minimisation over this particular factor $\tilde{t}_i(\x)$.  We derive the simple approximate factor updates using basic properties of the Gaussian (again just subtracting natural parameters).  We have

\begin{eqnarray}
\label{eqn:site}
 \tilde{t}_i(\x) & = &
\tilde{Z}_i\mathcal{N}(\c_i^T\x; \tilde{\mu}_{i},\tilde{\sigma}^2_{i}),~~~\\
\mathrm{where}~~~\tilde{\mu}_{i} & = &
\tilde{\sigma}^2_{i}(\hat{\sigma}^{-2}_i\hat{\mu}_i -
{\sigma}^{-2}_{\wo i}{\mu}_{\wo i}),~~~~~~
\tilde{\sigma}^2_{i} = (\hat{\sigma}^{-2}_i -
\sigma^{-2}_{\wo i})^{-1},\nonumber\\
\tilde{Z}_i & = & \hat{Z}_i\sqrt{2\pi}\sqrt{\sigma^2_{\wo i} +
\tilde{\sigma}^2_i}\mathrm{exp}\Bigl\{\frac{1}{2}(\mu_{\wo i} -
\tilde{\mu}_i)^2/(\sigma^2_{\wo i} + \tilde{\sigma}_i^2)\Bigr\},\nonumber
\end{eqnarray}

\noindent and where cavity parameters $\{\sigma_{\wo i}^2,\mu_{\wo i} \} ~= ~\{ \c_i^TV^{\backslash i}\c_i ~,~ \c_i^T\u^{\backslash i} \}$ are fully derived in Appendix \ref{sec:rank1cavity} as:

\begin{eqnarray}
\label{eqn:cavityepmgp}
\mu_{\wo i} = \sigma_{\wo i}^2 \Bigl( \frac{\c_i^T\boldmu}{\c_i^T\Sigma\c_i} - \frac{\tilde{\mu}_i}{\tilde{\sigma}_i^2} \Bigr)
,~~~\mathrm{and}~~~
\sigma^2_{\wo i}  = \bigl( (\c_i^T\Sigma \c_i)^{-1} - \tilde{\sigma}^{-2}_i\bigr)^{-1}.
\end{eqnarray}

\noindent These
equations are all standard manipulations of Gaussian random variables.   Now, by the definition of the approximation, we can calculate the new approximation $q(\x)$ as the product $q(\x) = p_0(\x)\prod_i \tilde{t}_i(\x)$:

\begin{eqnarray}
\label{eqn:epmgppost}
 q(\x) & = &
Z\mathcal{N}(\boldmu,\Sigma),~~~\\
\mathrm{where}~~~\boldmu & = &
\Sigma\Bigl(K^{-1}\m + \overset{m}{\sum_{i=1}}\frac{\tilde{\mu}_i}{\tilde{\sigma}^2_i}\c_i \Bigr),~~~~~~\Sigma = \Bigl(K^{-1} +
\overset{m}{\sum_{i=1}} \frac{1}{\tilde{\sigma}^2_i}\c_i\c_i^T\Bigr)^{-1},~~~~~~\nonumber
\end{eqnarray}

\noindent where $p_0(\x) = \mathcal{N}(\x; \m,K)$ as in Equation \ref{eqn:gaussian}.  These forms can be efficiently and numerically stably calculated in quadratic computation time via steps detailed in Appendix \ref{sec:epmgpstable}.  Lastly, once the algorithm has converged, we can calculate the normalisation constant of $q(\x)$.  While this step is again general to Gaussian EP, we highlight it because it is our quantity of interest - the approximation of the probability $F(\regionA)$:

\begin{eqnarray}
\label{eqn:logZ}
\log Z & = &
-\frac{1}{2}\left(\m^TK^{-1}\m
+\log\lvert K \rvert \right) \nonumber \\
& &
+ \overset{m}{\sum_{i=1}}\left( \log \tilde{Z}_i
- \frac{1}{2}\left(\frac{\tilde{\mu}_i^2}{\tilde{\sigma}_i^2}
+ \log \tilde{\sigma}^2_i + \log(2\pi)\right)\right) \nonumber \\
& &
+ \frac{1}{2}\left(\boldmu^T\Sigma^{-1}\boldmu
+ \log\lvert \Sigma \rvert \right)
%& \equiv & Z_\text{init} + \sum_i Z_\text{messages} - Z_\text{final}
\end{eqnarray}

\noindent In the above we have broken this equation up into three lines to clarify that the normalisation term $\log Z$ has contribution from the prior $p_0(\x)$ (first line), the approximate factors (the second line), and the full approximation $q(\x)$ (third line).  An intuitive interpretation is that the integral of $q(\x)$ is equal to the product of all the contributing factor masses (the prior and the approximate factors $\tilde{t}_i$), divided by what is already counted by the normalisation constant of the final approximation $q(\x)$.  Similar reparameterisations of these calculations help to ensure numerical stability and quicker, more accurate computation.  We derive that reparameterisation in the second part of Appendix \ref{sec:epmgpstable}.   Taking stock, we have derived Gaussian EP in a way that requires only rank one forms.  These factors may be of any functional form, but in the following section we show the box function form of the Gaussian probability problem leads to a particularly fast and accurate computation of Gaussian EP.

At this point it may be helpful to clarify again that EP has two levels of approximation.  First, being an approximate inference framework, EP would ideally choose an approximating $q(\x)$ from a tractable family (such as the Gaussian) such that $D_\KL(p\| q)$ is minimised.  That global problem being intractable, EP makes a second level of approximation in not actually minimising global $D_\KL(p\| q)$, but rather minimising the local $D_\KL(t_i q^{\wo i}\| \tilde{t}_i q^{\wo i})$. Because there is no obvious connection between these global and local divergences, it is difficult to make quantitative statements about the quality of the approximation provided by EP without empirical evaluation. However, EP has been shown to provide excellent approximations in many applications \citep{minka01phd, KussRasmussen2005, herbrich2007trueskilltm, stern2009matchbox}, often outperforming competing methods. These successes have deservedly established EP as a useful tool for applied inference, but they may also mislead the applied researcher into trusting its results more than is warranted by theoretical underpinnings. This paper, aside from developing a tool for approximate Gaussian integration, also serves to demonstrate a non-obvious way in which EP can yield poor results. The following two sections introduce our use of EP for approximate Gaussian integration that performs very well (Sections \ref{sec:rect-integr-regi} and \ref{sec:results}), followed by an ostensibly straightforward extension  (Section \ref{sec:polyh-integr-regi}), which turns out not to perform particularly well (Section \ref{sec:results}), for a subtle reason (Section \ref{sec:discussion}).




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% EPMGP SECTION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{EP Multivariate Gaussian Probability (EPMGP) Algorithm}
\label{sec:epmgp}

This section introduces the EPMGP algorithm, which uses Expectation Propagation to calculate multivariate Gaussian probabilities.  We consider two cases: axis-aligned hyperrectangular integration regions $\regionA$ and more general poyhedral integration regions $\regionA$.   Because of our general Gaussian EP derivation above, for each case we need only calculate the parameters that are specific to the choices of factor $t_i(\x)$, namely  $\{\hat{Z}_i,\hat{\mu}_i,\hat{\sigma}_i^2\}$ from Equation \ref{eqn:r1moments}.  This approach simplifies the presentation of this section and clarifies what is specific to Gaussian EP and what is specific to the Gaussian probability problem.

\subsection{Rectangular Integration Regions}
\label{sec:rect-integr-regi}

\begin{figure}[ht]
  \centering
  \begin{tikzpicture}[node distance=2cm]
    \node[fac] (p) at (0,0) {}; 
    \node[anchor=east] at (p.west) {$p_0(\x) = \mathcal{N}(\x; \m, K) $}; 
    \node[var, right of=p] (x) {$\x$} edge (p);
    \node[fac, right of=x] (t) {} edge (x);
    \node[anchor=west] at (t.east) {$t_i(\x)= \II\{l_i < x_i < u_i\}$};
    \draw (3,-1) rectangle (9,1);
    \node[anchor=south east] at (9,-1) {$i=1,\dots,n$};
  \end{tikzpicture}
  \caption{Graphical model (factor graph) for the rectangular Gaussian integration
    problem. In the language of Bayesian inference, the unconstrained
    Gaussian forms a ``prior'', the integration limits form
    ``likelihood'' factors, which give a ``posterior'' distribution on
    $\x$, and we are interested in the normalisation constant of that
    distribution. Note, however, that this inference terminology is
    used here without the usual philosophical interpretations of prior
    knowledge and subsequently aquired data.  Note also that in the rectangular integration case there are exactly $n$ such factors, corresponding to each dimension of the problem.}
  \label{fig:fac_rect}
\end{figure}

This section derives an EP algorithm for Gaussian integration over rectangular regions. Without loss of generality, we will assume that the rectangular region $\regionA$ is axis-aligned. Otherwise, there exists a unitary map $ U$ such that in the rotated coordinate system $\x\to U\x$ the region is axis aligned and the transformation morphs the Gaussian distribution $p_0(\x)=\N(\x; \m,K)$ into another Gaussian $p_0'(\x) = \N(U\x; U\m,UKU^T)$. Hence, the integration boundaries are not functions of $\x$, and the integral $F(\regionA)$ can be written using a product of $n$ factors $t_i(x_i) = \II\{l_i<x_i<u_i\}$: 

\begin{equation}
  \label{eq:7}
  F(\regionA) = \int_{l_1} ^{u_1} \cdots \int_{l_n} ^{u_n} \N(\x;\m,K) \d\x = \int \N(\x;\m,K) \prod_{i=1} ^n t_i(x_i) \d x_i.
\end{equation}

EP's iterative steps thus correspond to the incorporation of individual box functions of upper and lower integration limits into the approximation, where the incorporation of a factor implies approximating the box function with a univariate Gaussian aligned in that direction.  Thanks to the generality of the derivations in Section \ref{sec:gaussian-ep}, the remaining derivations for this setting are conveniently short.  The required moments for Equation \ref{eqn:r1moments} (the EP projection step) are calculated exactly \cite[]{jawitz2004} using the error function\footnote{These erf($\cdot$) computations also require a numerically stable implementation, though the details do not add insight to this problem and are not specific to this problem (unlike Appendix \ref{sec:epmgpstable}).  Thus we defer those details to the EPMGP code package available at {\tt <url to come with publication>}.}:
%
\begin{eqnarray}
\label{eqn:hat}
\hat{Z}_i & = &
\frac{1}{2}\Bigl(\mathrm{erf}(\beta) -  \mathrm{erf}(\alpha)\Bigr), \\
\hat{\mu}_i & = & \mu_{\wo i} +
\frac{1}{\hat{Z}_i}\frac{\sigma_{\wo i}}{\sqrt{2\pi}}\bigl(\mathrm{exp}\{-\alpha^2\}
- \mathrm{exp}\{-\beta^2\}\bigr),~~~\mathrm{and} \\
\label{eqn:hat3}
\hat{\sigma}_i^2 & = &   \mu_{\wo i}^2 + \sigma_{\wo i}^2 +
\frac{1}{\hat{Z}_i}\frac{\sigma_{\wo i}}{\sqrt{2\pi}}\bigl((l_i +
\mu_{\wo i})\mathrm{exp}\{-\alpha^2\} - (u_i +
\mu_{\wo i})\mathrm{exp}\{-\beta^2\}\bigr) - \hat{\mu}_i^2, 
\end{eqnarray}
%
\noindent where we have everywhere above used the shorthand $\alpha = \frac{l_i -
\mu_{\wo i}}{\sqrt{2}\sigma_{\wo i}}$ and $\beta = \frac{u_i -
\mu_{\wo i}}{\sqrt{2}\sigma_{\wo i}}$, and  $\{\mu_{\wo i},\sigma^2_{\wo i}\}$ are: 


\begin{eqnarray}
\label{eqn:cavityepmgpAxis}
\mu_{\wo i} & = &  \sigma_{\wo i}^2 \Bigl( \frac{\mu_i}{\Sigma_{ii}} - \frac{\tilde{\mu}_i}{\tilde{\sigma}_i^2} \Bigr)
,~~~\mathrm{and}~~~
\sigma^2_{\wo i} = \bigl( \Sigma_{ii}^{-1} - \tilde{\sigma}^{-2}_i\bigr)^{-1}.
\end{eqnarray}

The above is simply the form of  Equation \ref{eqn:cavityepmgp} with the vectors $\c_i$ set to the cardinal axes $\e_i$.  These parameters are the forms required for Gaussian EP to be used for hyperrectangular Gaussian probabilities.  



%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Polyhedral Integration Regions}
\label{sec:polyh-integr-regi}

\begin{figure}[ht]
  \centering
  \begin{tikzpicture}[node distance=2cm]
    \node[fac] (p) at (0,0) {}; 
    \node[anchor=east] at (p.west) {$p_0(\x) = \mathcal{N}(\x; \m, K) $}; 
    \node[var, right of=p] (x) {$\x$} edge (p);
    \node[fac, right of=x] (t) {} edge (x);
    \node[anchor=west] at (t.east) {$t_i(\x)= \II\{l_i < \c_i^T\x < u_i\}$};
    \draw (3,-1) rectangle (9,1);
    \node[anchor=south east] at (9,-1) {$i=1,\dots,m$};
  \end{tikzpicture}
  \caption{Factor graph for the polyhedral Gaussian integration
    problem. Note the onstensibly very similar setup to the rectangular problem in Figure \ref{fig:fac_rect}. The only difference is a replacement of axis-aligned box functions with box functions on the axis defined by $\c_i$, and a change to the number of restrictions, from $n$ to some different $m$.}
  \label{fig:fac_polyhedral}
\end{figure}

The previous section assumed axis-aligned integration limits. While a fully general set of integration limits is difficult to analyse, the derivations of the previous section can be extended to \emph{polyhedral} integration regions. This extension is straightforward: each factor truncates the distribution along the axis orthogonal to the vector $\c_i$ (which we assume has unit norm without loss of generality) instead of the cardinal axes as in the previous section.  The notational overlap of $\c_i$ with the Gaussian EP derivation of Section \ref{sec:gaussian-ep} is intentional, as these factors of Figure \ref{fig:fac_polyhedral} are precisely our general rank one factors for EP. An important difference to note in Figure \ref{fig:fac_polyhedral} is that there are $m$ factors, where $m$ can be larger or smaller (or equal to) the problem dimensionality $n$, unlike in the hyperrectangular case where $m=n$.  To implement EP, we find the identical moments as shown in Equation \ref{eqn:hat}. In fact the only difference is the cavity parameters $\{\mu_{\wo i},\sigma^2_{\wo i}\}$, which have the general form of Equation \ref{eqn:cavityepmgp}:

  \begin{eqnarray}
\label{eqn:cavityepmgpPoly}
\mu_{\wo i} & = &  \sigma_{\wo i}^2 \Bigl( \frac{\c_i^T\boldmu}{\c_i^T\Sigma\c_i} - \frac{\tilde{\mu}_i}{\tilde{\sigma}_i^2} \Bigr)
,~~~\mathrm{and}~~~
\sigma^2_{\wo i} = \bigl( (\c_i^T\Sigma \c_i)^{-1} - \tilde{\sigma}^{-2}_i\bigr)^{-1}.
\end{eqnarray}

From this point of view, it might seem like there is very little difference between the axis-aligned hyperrectangular setup of the previous section and the more general polyhedral setup of this section.  However, our experiments in Section \ref{sec:results} indicate that EP can have very different performance between these two cases, a finding discussed at length in subsequent sections.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Implementation}

To summarise, we have an EP algorithm that calculates $q(\x) = Z\mathcal{N}(\x; \boldmu, \Sigma)$ such that $D_{KL}(p \| q)$ is hopefully small.  Though there is no proof for when EP methods converge to the global KL minimiser, our results (in the following sections) suggest that EPMGP does converge to a fixed point reasonably close to this optimum in the hyperrectangular integration case.  In the general polyhedral case, performance is often excellent but can also be considerably worse, such that results in this case should be treated with caution.   The EPMGP algorithm typically converges in fewer than 10 EP iterations (of all $m$ factors) and is insensitive to initialisation and factor ordering.   EPMGP pseudocode is given in Algorithm \ref{alg:epmgp}, and our MATLAB implementation is available at {\tt <url to come with publication>}.
%
% pseudocode here
\begin{algorithm}
\caption{EPMGP: Calculate $F(\regionA)$, the probability of $p_0(\x)$
on the region $\regionA$.}
\label{alg:epmgp}
\begin{algorithmic}[1]
\STATE Initialise with any $q(\x)$ defined by $Z,\boldmu,\Sigma$ (typically the parameters of $p_0(\x)$).
\STATE Initialise messages $\tilde{t}_{i}$ with zero precision.
\WHILE{$q(\x)$ has not converged}
\FOR{$i\gets 1:m$}
\STATE form cavity local $q_{\wo i}(\x)$ by Equation~\ref{eqn:cavityepmgp} (stably calculated by \ref{eqn:cavityepmgpnatural}).
\STATE calculate moments of $q_{\wo i}(\x)t_i(\x)$ by Equations~\ref{eqn:hat}-\ref{eqn:hat3}.
\STATE choose $\tilde{t}_i(\x)$ so $q_{\wo i}(\x)\tilde{t}_i(\x)$
matches above moments by Equation~\ref{eqn:site}.
\ENDFOR
\STATE update $\boldmu,\Sigma$ with new $\tilde{t}_i(\x)$ (stably using Equations~\ref{eqn:sigmanew} and \ref{eqn:munew}).
\ENDWHILE
\STATE calculate $Z$, the total mass of $q(\x)$ using Equation \ref{eqn:logZ} (stably using Equation~\ref{eqn:logZstable}).
\STATE \textbf{return} $Z$, the approximation of $F(\regionA)$.
\end{algorithmic}
\end{algorithm}



%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%
\subsection{Features of the EPMGP algorithm}
\label{sec:advantages}

Before evaluating the numerical results of EPMGP, it is worth noting a few attractive features of the EP approach, which may motivate its use in many applied settings.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Derivatives with respect to the distribution's parameters}
\label{sec:deriv-with-resp}

A common desire in machine learning and statistics is model selection, which involves optimising (or perhaps integrating) model parameters over an objective function.  In the case of Gaussian probabilities, researchers may want to maximise the likelihood $F(\regionA)$ by varying $\{\m,K\}$ subject to some other problem constraints.  Numerical integration methods do not offer this feature currently.  Because the EP approach yields the analytic approximation shown in Equation \ref{eqn:logZ}, we can take derivatives of $\log Z$ so as to approximate $\partial F(\regionA) / \partial \m$ and $\partial F(\regionA) / \partial K$ for use in a gradient solver.  Those derivatives are detailed in Appendix \ref{sec:deriv}.

As a caveat, it is worth noting that numerical integration methods like Genz were likely not designed with these derivatives in mind.  Thus, though it has not been shown in the literature, perhaps an implementation could be derived to give derivative approximations via a clever reuse of integration points (though it is not immediately clear how to do so, as it would be in a simple Monte Carlo scheme).  It is an advantage of the EPMGP algorithm that it can calculate these derivatives naturally and analytically, but we do not claim that it is necessarily a fundamental characteristic of the EP approach unshared by any other approaches.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Fast runtime and low computational overhead}

By careful consideration of rank one updates and a convenient reparameterisation, we can derive a quadratic run time factor update as opposed to a naive cubic runtime (in the dimensionality of the Gaussian $n$).  This quadratic factor update is done at each iteration, in addition to a determinant calculation in the $\log Z$ calculation, leading to a complexity of $\mathcal{O}(n^3)$ in theory (or $\mathcal{O}(mn^2 + n^3)$ in the case of polyhedral regions).  Numerical integration methods such as Genz also require a Cholesky factorisation of the covariance matrix $K$ and are thus $\mathcal{O}(n^3)$ in theory, and we have not found a more detailed complexity analysis in the literature.  Further, the Genz method uses potentially large numbers of sample points (we use $5\times 10^5$ to derive our high accuracy estimates), which can present a serious runtime and memory burden.    As such, we find in practice that the EP-based algorithm always runs as fast as the Genz methods, and typically runs one to three orders of magnitude faster (the hyperrectangular EPMGP runs significantly faster than the general EPMGP, as the factor updates can be batched for efficiency in languages such as MATLAB).  Furthermore, we find that EP always converges to the same fixed point in a small number of steps (typically 10 or fewer iterations through all factors).    Thus, we claim there are computational advantages to running the EPMGP algorithm over other numerical methods.

There are three caveats to the above claims.  First, since Genz numerical integration methods have a user-selected number of sample points, there is a clear runtime-accuracy tradeoff that does not exist with EP.  EP is a deterministic approximation, and it has no guarantees about convergence to the correct $F(\regionA)$ in the limit of infinite computation.  Claims of computational efficiency must be considered in light of these two different characteristics.  Second, since both EP and Genz algorithms are theoretically $\mathcal{O}(n^3)$, calculated runtime differences are highly platform dependent.  Our implementations use MATLAB, the runtime results of which should not be trusted as reflective of a fundamental feature of the algorithm.  Thirdly, since it is unclear if Genz's method  (available from author) was written with computational efficiency in mind, a more rigorous comparison seems unfair.  Nonetheless, Section \ref{sec:results} presents runtime/accuracy comparisons for a handful of representative cases so that the applied researcher may be further informed about these methods.

Despite these caveats, the EPMGP algorithm runs very fast and stably up to problems of high dimension, and it always converges quickly to what appears to be a unique mode of the EP energy landscape (empirical observation).  Thus, we claim that runtime is an attractive feature of EP for calculating Gaussian probabilities.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Tail probabilities}

In addition to fast runtime, careful reparameterisation and consideration of the EP parameters leads to a highly numerically stable implementation that is computed in log space.  The lengthy but simple details of these steps are presented in Appendix \ref{sec:epmgpstable}.  Importantly, this stability allows arbitrarily small ``tail" probabilities to be calculated, which are often important in applied contexts.  In our testing the EPMGP algorithm can calculate tail probabilities with $\log Z < -10^5$, that is, $F(\regionA) \approx 10^{-50000}$.  This is vastly below the range of Genz and other methods, which snap to $\log Z = -\infty$ anywhere below roughly $\log Z \approx -150$.  These numbers reflect our own testing of the two methods.  To be able to present results from both methods, we present the results of Section \ref{sec:results} over a much smaller range of probabilities, in the range of $\log Z \approx -20$ to $\log Z = 0$ ($F(\regionA)$ from $10^{-9}$ to $1$).

Again, it is worth noting that the Genz method was likely not designed with tail probabilities in mind, and perhaps an implementation could be created to handle tail probabilities (though it is not obvious how to do so).  Thus, it is an advantage of the EPMGP algorithm that it can calculate tail probabilities naturally, but we do not claim that it is necessarily a unique characteristic of the EP approach.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{The fundamental similarity between hyperrectangular and polyhedral integration regions}
\label{sec:white}
We here note important similarities between the hyperrectangular and polyhedral cases.  Though brief, this section forms the basis for much of the discussion and exploration of the strengths and weaknesses of EP.  Since we have developed EP for arbitrary polyhedral Gaussian probabilities, we can somewhat simplify our understanding of the problem.  We can always whiten the space of the problem via a substitution $\y = L^{-1}(\x - \m)$ where $K = LL^T$.   In this transformed space, we have a standard $\mathcal{N}(0,I)$ Gaussian, and we have a transformed set of polyhedral faces $L^T\c_i$ (which must also be normalised as the $\c_i$ are).   This whitening is demonstrated in Figure \ref{fig:white}.   

Thus, in trying to understand sources of error, we can then attend to properties of this transformed region.  Intuition tells us that, the more hyperrectangular this transformed region is, the better performance, as hyperrectangular cases (in a white space) decompose into a product of solvable univariate problems.  On the other hand, the more redundant/colinear/coloured those faces are, the higher the error that we should expect.    We will thus consider metrics on the geometry of transformed integration region that exists in the whitened space. Further, we then see that hyperrectangular cases are in fact not a fundamental distinction.  Indeed they are simply polyhedra where $m=n$, that is, the number of polyhedral constraints equals the number of dimensions of the problem.  This seemingly small constraint, however, often makes the difference between an algorithm that performs well and one that does not, as the results will show.  

\begin{figure}
\centering
\hspace{0.0cm}
%\epsfig{figure=white.eps,width=6in}
\includegraphics[width=6in]{white}
%\vspace{-0.3cm}
\caption{\small{The effect of whitening the Gaussian probability based on the covariance of the Gaussian.  Panel A shows a Gaussian probability problem in original form, with the integration region in red.  Panel B shows the same problem when we have whitened (and mean-centered) space with respect to the Gaussian.  We can then think of these problems only in terms of the geometry of the ``coloured" integration region in the whitened space.}}
\label{fig:white} % caption for the whole figure
\end{figure}

Finally, it is important to connect this change to our EP approach.  Because each step of EP minimises a KL-divergence on distributions, and because KL is invariant to
invertible change of variables, each step of EP is invariant to invertible change of variables.  This invariance requires that the factors, approximate factors, etc. are all transformed accordingly, so that the overall distribution is unchanged (as noted in \cite{seeger08epexpfam}, Section 4). For Gaussian EP, the approximate factors are Gaussian, so the family is invariant to invertible \emph{linear} transformations.  Thus we can equivalently analyse our Gaussian EP algorithm for multivariate Gaussian probabilities in the white space, where $p_0(\x)$ is a spherical Gaussian as in Panel B of Figure \ref{fig:white}.   Considering the whitened space and transformed integration region is conceptually useful and theoretically sound.  This consideration allow us to maintain two views which have different focus: covariance structure in coloured space and a rectangular region (in the hyperectangular case), and geometry of the transformed polyhedron in whitened space.  Much more will be made of this notion in the Results and Discussion.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Other Multivariate Gaussian Probability algorithms}
\label{sec:othermgp}

This section details other methods to which we compare EPMGP.  First, we describe the algorithms due to Genz (Section \ref{sec:Genz}), which represent the state of the art for calculating Gaussian probabilities.  Second, we briefly describe Monte Carlo-based methods (Section \ref{sec:mcmc}).  Third, in some special cases there exist exact ({\it i.e.}, exact to machine precision) methods; we describe those in brief in Section \ref{sec:specialcases}.  EPMGP and the algorithms discussed here will be the algorithms studied in numerical simulation.

\subsection{Genz Numerical Integration Method}
\label{sec:Genz}

The Genz method (first described in \cite{genz92}, but the book \cite{genzBook} is an excellent current reference) is a sophisticated numerical integration
method for calculating Gaussian probabilities.
The core idea is to make a series of transformations of the region $\regionA$ and the Gaussian
$p_0(\x)$ with the hope that this transformed region can be accurately
integrated numerically. This approach performs three
transformations to $F(\regionA)$.  It first 
whitens the Gaussian integrand $p_0(\x)$ (via a Cholesky factorisation of
$K$, which changes the
integration bounds), and secondly it removes the integrand
altogether by transforming the integration bounds with a function
using the cdf and inverse cdf of a
univariate Gaussian.  Finally, a further transformation is done
(using points uniformly drawn from the $[0,1]$ interval) to
set the integration region to the unit box (in $\reals^n$).  Once this is
done, \cite{genz92} makes intelligent choices on dimension ordering
to improve accuracy.  With all orderings and
transformations completed, numerical integration is
carried out over the unit hypercube, using either quasi-random integration
rules \cite[]{nieder72,cranley76} or lattice-point rules
\cite[]{cranley76, nuyens04}.   The original algorithm
\cite[]{genz92} reported, ``it is possible to reliably compute
moderately accurate multivariate normal probabilities for practical
problems with as many as ten variables.'' Further developments
including \cite[]{genz99, genz99b, genz02, genz04} have improved the
algorithm considerably and have generalised it to general polyhedra.
Our use of this algorithm was enabled by the author's MATLAB code
{\tt QSILATMVNV}, which is directly analogous to EPMGP for axis-aligned hyperrectangles.  The author's more general algorithm for arbitrary polyhedra is available from the same code base as {\tt QSCLATMVNV}: this is directly comparable to EPMGP.  These functions run a vectorised,
lattice-point version of the algorithm from \cite[]{genz92}, which we found always
produced lower error estimates (this algorithm approximates the
accuracy of its result to the true probability) than the vectorised, quasi-random
{\tt QSIMVNV} (with similar run-times).  We also found that these vectorised versions
had lower error estimates and significantly faster run-times than
the non-vectorised {\tt QSIMVN}.  Thus, all results shown as ``Genz" were gathered using {\tt QSCLATMVNV}, which was available at the time of this
report at: {\tt
http://www.math.wsu.edu/faculty/genz/software/software.html}.  The
software allows the user to define the number of lattice points used
in the evaluation.  For the Genz methods to which EPMGP was compared, we used $5 \times 10^5$ points.  We
found that many fewer points increased the error
considerably, and many more points increased run-time with little
improvement to the estimates.  We
note also that several statistical software packages have bundled
some version of the Genz method into their product ({\it e.g.}, {\tt mvncdf}
in MATLAB, {\tt pmvnorm} in R).  Particular choices and calculations
outside the literature are typically made in those implementations
(for example, MATLAB does not allow $n>25$), so we have
focused on just the code available from the author himself.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Sampling methods}
\label{sec:mcmc}

It is worth briefly noting that Monte Carlo methods are not particularly well suited to this problem.  In the simplest case, one could use a rejection sampler, drawing samples from $p_0(\x)$ and rejecting them if outside the region $\regionA$ (and calculating the fraction of kept samples).  This method requires huge numbers of samples to be accurate, and it scales very poorly with dimension $n$.  The problem with this simple Monte Carlo approach is that the draws from $p_0(\x)$ neglect the location of the integration region $\regionA$, so the rejection sampler can be terribly inefficient.  We also considered importance sampling from the region $\regionA$ (uniformly with a ``pinball" or ``hit-and-run" MCMC algorithm \cite[]{herbrichBook}).  However, this approach has the complementary problem of wasting samples in parts of $\regionA$ where $p_0(\x)$ is small.  Further, this scheme is inappropriate for open polyhedra.  In an attempt to consider both $\regionA$ and $p_0(\x)$, we used elliptical slice sampling \cite[]{murrayESS} to draw samples from $p(\x) = p_0(\x)\prod_i t_i(\x)$ (the familiar Bayesian posterior).  We then calculated the empirical mean and covariance of these samples and used that posterior Gaussian as the basis of an importance sampler with the true distribution.  Though better than naive Monte Carlo, this and other methods are deeply suboptimal in terms of runtime and accuracy, particularly in light of the Genz algorithms, which are highly accurate for calculating the ``true" value, which allows a baseline for comparison to EPGMP.  Thus we will not consider samplers further in this paper.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Special case methods}
\label{sec:specialcases}

It is also useful to consider special cases where we can analytically find the true probability $F(\regionA)$.  The most obvious is to consider hyperrectangular integration regions over a diagonal covariance $K$.  Indeed both EPMGP and Genz methods return the correct $F(\regionA)$ to machine precision, since these problems trivially decompose to univariate probabilities.  While a good sanity check, this finding is not instructive.  Instead, here we consider a few cases where the integration region yields a geometric solution in closed form.

\subsubsection{Orthant Probabilities} 

Orthants generalise quadrants to high dimension, for example the nonnegative orthant $\{ \x \in \reals^n : x_i > 0~ \forall i \}$.   For zero-mean Gaussians of low dimension, calculating the probability $F(\regionA)$ when $\regionA$ is an orthant can be solved geometrically.  In $\reals^2$, any orthant (quadrant) can be whitened by the Gaussian covariance $K$, which transforms the orthant into a wedge.  The angle subtended by that wedge, divided by $2\pi$, is the desired probability $F(\regionA)$.  With that intuition in $\reals^2$, researchers have also derived exact geometric solutions in $\reals^3, \reals^4,$ and $\reals^5$ \cite[]{genzBook, sinnKeller2010}.  We can thus compare EP and Genz methods to the true exact answers in the case of these orthant probabilities.  Orthant probabilities also have particularly important application in applied settings such as probit analysis \citep{AshfordSowden1970,SicklesTabuman1986,Gibbons1996}, so knowing the performance of various methods here is valuable.

\subsubsection{Trapezoid Probabilities}

Trapezoid probabilities are another convenient construction where we can know the true answer.   If we choose our Gaussian to be zero-mean with a diagonal covariance K, and we choose $\regionA$ to be a hyperrectangle that is symmetric about zero (and thus the centroid is at zero), then this probability decomposes simply into a product of univariate probabilities.  We can then take a non-axis cut through the center of this region.   If we restrict this cut to be in only two dimensions, then the region in those two dimensions is now a trapezoid (or a triangle if the cut happens to pass through the corner, but this happens with zero probability).   By the symmetry of the Gaussian and this region, we have also cut the probability in half.  Thus, we can calculate the simple rectangular probability and half it to calculate the probability of this generalised trapezoid.   

The virtue of this construction is that this trapezoidal region appears as a nontrivial, general polyhedron to the EPMGP and Genz methods, and thus we can use this simple example to benchmark those methods.  Unlike orthant probabilities, trapezoids are a contrived example so that we can test these methods.  We do not know of any particular application of these probabilities, so these results should be seen solely for vetting of the EP and Genz algorithms.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Cases for Comparing Gaussian Probability Algorithms}
\label{sec:testcases}

The following section details our procedure for drawing example integration regions $\regionA$ and example Gaussians $p_0(\x)$ for the numerical experiments.  Certainly understanding the behavior of these algorithms across the infinite space of parameter choices is not possible, but our testing in several different scenarios indicates that the choices below are representative.

\subsection{Drawing Gaussian distributions $p_0(\x) = \mathcal{N}(\x; \m,K)$}

To define a Gaussian $p_0(\x)$, we first randomly
drew one positive-valued $n$-vector from an exponential distribution with mean
10 ($\lambda = 0.1$, $n$ independent draws from this distribution),
and we call the corresponding diagonal matrix the matrix of eigenvalues $S$.  We then
randomly draw an $n\times n$ 
orthogonal matrix $U$ (orthogonalising a matrix of iid $\mathcal{N}(0,1)$ random variables with the singular
value decomposition suffices), and we form the Gaussian covariance matrix
$K = USU^T$.  This procedure produces a good spread of differing
covariances $K$ with quite different eigenvalue 
spectra and condition numbers. We
note that this procedure produced a more interesting range of
$K$ - in particular, a better spread of condition numbers - than using draws from a Wishart distribution.  We
then set the mean of the Gaussian $\m=0$ without loss of
generality (note that, were $\m\neq 0$, we could equivalently
pick the region $\regionA$ to be shifted by $\m$, and then the
problem would be unchanged; instead, we leave $\m=0$, and we
allow the randomness of $\regionA$ to suffice).   For orthant probabilities, no change to this procedure is required.  For trapezoid probabilities, a diagonal covariance is required, and thus we set $U = I$ in the above procedure.

\subsection{Drawing integration regions $\regionA$}

Now that we have the Gaussian $p_0(\x) = \mathcal{N}(\x; \m,K)$,
we must define the region $\regionA$ for the Gaussian probability
$F(\regionA)$.  

\subsubsection{Axis-aligned hyperrectangular regions}  

The hyperrectangle $\regionA$ can be defined by two
$n$-vectors: the upper and lower bounds $\u$ and $\l$, where each entry corresponds to a dimension of the problem.  To make
these vectors, we first randomly drew a point from the Gaussian
$p_0(\x)$ and defined this point as an interior point of
$\regionA$.  We then added and subtracted randomly chosen lengths to
each dimension of this point to form the bounds $u_i$ and $l_i$.  Each length from the interior point was chosen uniformly between 0.01 and the dimensionality $n$.  Having the region size scale with the dimensionality $n$ helped to prevent the probabilities $F(\regionA)$ from becoming vanishingly small with increasing dimension (which, as noted previously, is handled fine by EP but problematic for other methods).  

\subsubsection{Arbitrary polyhedral regions}

The same procedure is repeated for drawing the interior point and the bounds $l_i$ and $u_i$ for all $m$ constraints.  The only difference is that we also choose axes of orientation, which are unit-norm vectors $\c_i \in \reals^n$ for each of the $m$ factors.  These vectors are drawn from a uniform density on the unit hypersphere (which is achieved by normalising $n$ iid $\mathcal{N}(0,1)$ draws).  The upper and lower bounds are defined with respect to these axes.

With these parameters defined, we now have a randomly
chosen $\regionA$ and the Gaussian $p_0(\x)$, and we can test all methods as previously described.  This
procedure yields a variety of Gaussians and regions, so we believe our results
are strongly representative. 

It is worth noting here that arbitrary polyhedra are not uniquely defined by the ``intersection of boxes" or more conventional ``intersection of halfspaces" definition.  Indeed, there can be inactive, redundant constraints which change the representation but not the polyhedron itself.  We can define a minimal representation of any polyhedron $\regionA$ to be the polyhedron where all constraints are active and unique.  This can be achieved by pulling in all inactive constraints until they support the polyhedron $\regionA$ and are thus active, and then we can prune all repeated/redundant constraints.  These steps are solvable in polynomial time as a series of linear programs, and they bear important connection to problems considered in centering (analytical centering, Chebyshev centering, etc.) for cutting-plane methods and other techniques in convex optimisation ({\it e.g.}, see \cite{boydBook}).  These steps are detailed in Appendix \ref{sec:minpoly}.   To avoid adding unnecessary complexity, we do not minimalise polyhedra in our results, as that can mask other features of EP that we are interested in exploring.  Anecdotally, we report that while minimalising polyhedra \emph{will} change EP's answer (a different set of factors), the effect is not particularly large in these randomly sampled polyhedral cases.  In our empirical and theoretical discussion of the shortcomings of EP, we will heavily revisit this notion of constraint redundancy.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results}
\label{sec:results}

We have thus far claimed that EP provides a nice framework for Gaussian probability calculations, and it offers a number of computational and analytical benefits (runtime, tail probabilities, derivatives).   We have further claimed that existing numerical integration methods, of which the Genz method is state of the art, can reliably compute Gaussian probabilities to high accuracy, such that they can be used as ``ground truth" when evaluating EP.  Thirdly, we claim that this Gaussian probability calculation elucidates a pathology in EP that is not often discussed in the literature.  Giving evidence to those claims is the purpose of this Results section, which will then be discussed from a theoretical perspective in Section \ref{sec:discussion}.

The remainder of this results section is as follows:  we first demonstrate the accuracy of the EP method in the hyperrectangular case across a range of dimensionalities (Section \ref{sec:axisresults}).  Using the same setup, we then demonstrate a higher error rate for the general polyhedral case in Section \ref{sec:generalresults}, testing the sensitivity of this error to both dimensionality $n$  and the number of polyhedral constraints $m$.  In Section \ref{sec:specialresults}, we evaluate the empirical results for the special cases where the true Gaussian probability can be calculated by geometric means, as introduced in the previous Section \ref{sec:specialcases}, which allows us to compare the EP and Genz approaches.  We next test the empirical results for the commonly tested case of equicorrelated Gaussian probabiliites (Section \ref{sec:equicorrresults}).  The equicorrelation and special case results point out the shortcomings of the EP approach.  We can then proceed to construct pathological cases that clarify where and why EP gives poor approximations of Gaussian probabilities, which is done in Section \ref{sec:pathologicalresults}.  This final section gives the reader guidelines about when EP can be effectively used for Gaussian probabilities, and it provides a unique perspective and leads to a theoretical discussion about negative aspects of EP rarely discussed in the literature.


%%%%%%%%%%%%%%%%%
\subsection{EP results for hyperrectangular integration regions}
\label{sec:axisresults}

%
\begin{figure}
\centering
\hspace{0.0cm}
%\epsfig{figure=results_scenario2.eps,width=6in}
\includegraphics[width=6in]{results_scenario2}
%\vspace{-0.3cm}
\caption{\small{Empirical results for EPMGP with hyperrectangular integration regions.  Each point represents a random Gaussian $p_0(\x)$ and a random region $\regionA$, as described in Section \ref{sec:testcases}.  In all panels the colour coding corresponds to the dimensionality of the Gaussian integral (chosen to be $n = \{2,3,4,5,10,20,50,100\}$.  All panels show the relative log error of the EP method with respect to the high accuracy Genz method.  Panel A plots the errors of EPMGP by dimensionality (note log-log scale).  The horizontal offset at each dimension is added jitter to aid the reader in seeing the distribution of points (panel A only).  The blue line and error bars plot the median and $\{25\%,75\%\}$ quartiles at each dimension $n$.  The black dashed line indicates the $1\% \log Z$ error.  Finally, the black line below the EP results is the median error estimate given by Genz.  Since this line is almost always a few orders of magnitude below the EP distribution, it is safe to use the high accuracy Genz method as a proxy to ground truth.  Panel B plots the errors by the value of $\log Z$ itself.  On this panel we also plot a $1\%$ error for $Z\approx F(\regionA)$.  Panel C repeats the same errors but plotted by the condition number of the covariance $K$.  Panel D plots the errors vs the maximum eccentricity of the hyperrectangle, namely the largest width divided by the smallest width.}}
\label{fig:axisresults} % caption for the whole figure
\end{figure}
%

Figure \ref{fig:axisresults} shows the empirical results for the EPMGP algorithm in the case of hyperrectangular integration regions.  For each dimension $n = \{2,3,4,5,10,20,50,100\}$, we used the
above procedure to choose a random Gaussian $p_0(\x)$ and a random region $\regionA$.   This procedure was repeated 1000 times at each $n$.  Each choice of $p_0(\x)$ and $\regionA$ defines a single Gaussian probability problem, and we can use EPMGP and Genz methods to approximate the answer.  We calculate the relative error between these two estimates.  Each of these errors is plotted as a point in Figure \ref{fig:axisresults}.  We plot only 250 of the 1000 points at each dimensionality, for purposes of visual clarity in the figures.  The four panels of Figure \ref{fig:axisresults} plot the same data on different $x$-axes, in an effort to demonstrate what aspects of the problem imply different errors.  As described in the figure caption, Panel A plots the errors of EPMGP by dimensionality $n$.  Horizontal offset at each dimension is added jitter to aid the reader in seeing the distribution of points (panel A only).  The black dashed line indicates the $1\% \log Z$ error.  Finally, the black line below the EP results is the median error estimate given by Genz.  Since this line is almost always a few orders of magnitude below the distribution of EP errors, it is safe to use the high accuracy Genz method as a proxy to ground truth.   

Panel A demonstrates that the EP method gives a reliable estimate, typically with errors on the order of $10^{-4}$, of Gaussian probabilities with hyperrectangular integration regions.  The shape of the blue curve in Panel A seems to indicate that error peaks around $n=10$.  We caution the reader not to over-interpret this result, as this feature may be just as much about the random cases chosen (as described in Section \ref{sec:testcases}) as a fundamental feature of Gaussians.  Nonetheless, we do notice across many test cases that error decreases at higher dimension, perhaps in part due to the concentration of mass of high dimensional Gaussians on the $1\sigma$ covariance ellipsoid.

Panel B plots the errors by the value of $\log Z$ itself, where we use the $\log Z$ calculated from Genz.  On this panel we can also plot a $1\%$ error for $Z\approx F(\regionA)$, as the log scale can suggest misleadingly large or small errors.  A priori we would not expect any significant trend to be caused by the actual value of the probability, and indeed there seems to be a weak or nonexistent correlation of error with $\log Z$.  Panel C repeats the same errors but plotted by the condition number of the covariance $K$.   In this case perhaps we would expect to see trend effects.  Certainly the EP algorithm should be correct to machine precision in the case of a hyperrectangular region and a white covariance, since this problem decomposes into $n$ one-dimensional problems.  Indeed we do see this accuracy in our tests.  Increasing the eccentricity of the covariance, which can be roughly measured by the condition number, should and does increase the error, as shown in Panel C.  This effect can be seen more clearly in later figures (when we test equicorrelation matrices).  One might also wonder about the condition number of the correlation, as this value would neglect axis-scaling effects.  We have also tested this and do not show this panel as it looks nearly identical to Panel C.  

Panel D plots the errors vs the maximum eccentricity of the hyperrectangle, namely the largest width divided by the smallest width.  As with Panel B, it is unclear why the eccentricity of a hyperrectangular region should have bearing on the accuracy of the computation, and indeed there appears to be little effect.  The colour coding helps to clarify this lack of effect, because each dimensionality seems to have insensitivity to error based on changing eccentricity.  Thus any overall trend appears more due to the underlying explanatory factor of dimensionality than due to the region eccentricity itself.  

Finally, as an implementation detail, we discuss the Genz error estimate (as seen in the black line of Panel A).  It is indeed true that this error estimate can sometimes be significant, but that happens rarely enough ($<10\%$) that it has little bearing on the trends shown here.  To be strict, one can say that the lowest $10\%$ of EP errors are conservative, noisy errors: they could be significantly smaller, but not significantly larger.  Due to the log scale of these plots, the larger EP errors (the ones we care about) are reliable.  Furthermore, we note that the Genz errors increase significantly at higher dimension $n>100$.  The EP method can compute larger dimensions without trouble, but the Genz method achieves a high enough error estimate that many more, often a majority, of EP errors are to be considered conservative, noisy estimates.  Empirically, it seems the errors in higher dimensions continue their downward trend, but they are noisier measurements, so we do not report them to avoid over penalising the EP method (as they could be much smaller).

Overall, the empirical results shown here indicate that EPMGP is a successful candidate algorithm for Gaussian probabilities over hyperrectangular integration regions.  The error rate is non-zero but generally quite low, with median errors less $10^{-4}$ and individual errors rarely in excess of 1\% across a range of dimensions, which may be acceptable in many applied settings.   


%%%%%%%%%%%%%%%%%%%%
\subsection{EP results for general polyhedral cases}
\label{sec:generalresults}

%
\begin{figure}
\centering
\hspace{0.0cm}
%\epsfig{figure=results_scenario1.pdf,width=6in}
\includegraphics[width=6in]{results_scenario1}
%\vspace{-0.3cm}
\caption{\small{Empirical results for EPMGP with polyhedral integration regions.  This figure has largely the same description as Figure \ref{fig:axisresults}.  The only difference from that previous figure is that panel A depicts as a black line the median errors from EPMGP in the hyperrectangular case.  This demonstrates the consistently higher errors of general polyhedral EPMGP.  Panels B and C have the same description as Figure \ref{fig:axisresults}.  Panel D plots the condition number of the transformed region.  As discussed in the text, any Gaussian probability problem can be whitened to consider a Gaussian with identity covariance.  This step merely linearly transforms the region $\regionA$.  Panel D shows the errors plotted vs. the condition number of the integration region transformed appropriately by the Gaussian covariance $K$.}}
\label{fig:generalresults} % caption for the whole figure
\end{figure}
%
We have previously shown how the EP framework readily can be extended to arbitrary polyhedral regions with only a minor change to the calculation of cavity parameters (Equation \ref{eqn:cavityepmgpPoly}).  Figure \ref{fig:generalresults} shows the empirical results of this change, which are perhaps surprisingly large.  In the same way we drew 1000 test cases at each dimensionality $n$.  The only difference is that we also drew $m = n$ random polyhedral faces as described in Section \ref{sec:testcases}, instead of the fixed hyperrectangular faces in the previous case.  

Figure \ref{fig:generalresults} has the same setup as \ref{fig:axisresults}, with only a few exceptions.  First, the black line in Panel A shows the median EP errors in the axis aligned case ({\it i.e.}, the blue line in Figure \ref{fig:axisresults}, Panel A).  The purpose of this line is to show that indeed for the same Gaussian cases, the error rate of polyhedral EPMGP is typically an order of magnitude or two higher than in the hyperrectangular case.  Genz errors are not shown (as they were in Figure \ref{fig:axisresults}), but we note that they tell the same story as in the hyperrectangular case.  The only other change is Panel D.  In Section \ref{sec:white}, we introduced the concept of whitening the space to consider just the region integrated over a standard Gaussian.   We consider this region as a matrix with columns equal to $L^T\c_i$ (normalised), the transformed polyhedral faces.  If this region were hyerrectangular (in the transformed space), then this matrix would be $I$, and the method would be perfectly accurate (decomposed problem).  Thus, to test the transformed region's proximity to $I$, we can use the condition number.  We do so in Panel D, where we plot errors against that value.  As in Panel C, there appears to be some legitimate trend based on the condition number of the transformed region.  This finding confirms our intuition that less hyperrectangular transformed integration regions will have higher error.  There are a number of other metrics on the transformed region that one might consider that attempt to account for colinearity (non-orthogonality) in the transformed region constraints.  We have produced similar plots with Frobenius ($l_2$) and $l_1$ norms of the elements of the scaled Gram matrix (instead of condition number), and we report that the trends found there are similar to these figures: there is some upward trend in error with a metric of colinearity.
%
\begin{figure}
\centering
\hspace{0.0cm}
%\epsfig{figure=results_scenario4.eps,width=6in}
\includegraphics[width=6in]{results_scenario4}
%\vspace{-0.3cm}
\caption{\small{Empirical results for EPMGP with polyhedral integration regions.  Panel A calculates the same error metric as in the previous figures for $n=10$ dimensional Gaussians with varying numbers of polyhedral constraints $m = \{2,4,8,10,12,16,32,64\}$.  Note the difference with Figure \ref{fig:generalresults} where dimensionality $n$ is swept (and $m=n$).  Other panels are as previously described in Figure \ref{fig:generalresults}.}}
\label{fig:generalresultspoly} % caption for the whole figure
\end{figure}
%

Figure \ref{fig:generalresults} only shows cases where the number of polyhedral constraints is the same as the dimensionality of the Gaussian ($m=n$).  The sensitivity of error to the number of constraints $m$ is also an important question.  Figure \ref{fig:generalresultspoly} gives these empirical results.  We use the same setup as Figure \ref{fig:generalresults} with only one change: Panel A plots errors by number of polyhedral face constraints $m$ instead of by Gaussian dimension $n$.  In this figure all cases use $n=10$ dimensions, and we show polyhedral sizes of $m = \{2,4,8,10,12,16,32,64\}$.  It seems from Panel A that larger numbers of constraints/polyhedral faces do imply larger error, trending up by roughly an order of magnitude or two.  This is an interesting finding that will be revisited in the theoretical discussion: for a given dimension $n$, more polyhedral constraints (more factors) implies higher EP error.  The errors shown in B and C seem largely invariant to the condition number of the covariance and the value $\log Z$ itself.  However, Panel D still shows the error in the whitened space and indicates again some upward trend, as we would expect.

Figure \ref{fig:generalresults} and \ref{fig:generalresultspoly} show error rates that may in many cases be unacceptable.  Though EP still performs well often, the reliability of the algorithm is not what it is in the hyperrectangular case of Figure \ref{fig:axisresults}.   We now test special cases where true answers are known, which will further our understanding of the strengths and weaknesses of the EP approach.

%%%%%%%%%%%%%%%
\subsection{EP and Genz results for special cases with known answers}
\label{sec:specialresults}

Thus far we have relied on the Genz method to provide the baseline of comparisons.  Though we have argued that this step is valid, it is still useful to consider the handful of special cases where the true probability can be calculated from geometric arguments.  We do so here, and those results are shown in Figure \ref{fig:specialresults}.
%
\begin{figure}
\centering
\hspace{0.0cm}
%\epsfig{figure=results_scenario3.eps,width=6in}
\includegraphics[width=6in]{results_scenario3}
%\vspace{-0.3cm}
\caption{\small{Empirical results for the special cases where an analytical true answer can be calculated.  Panels A and B show results for orthant probabilities of dimension $n = \{2,3,4,5\}$ (See Section \ref{sec:specialcases}).   Panel A shows relative $\log Z$ error, and Panel B shows relative $Z$ error.  The coloured points indicate EP errors, and the grayscale points indicate the errors of the Genz method.  Though not shown in this figure, the Genz estimates of error only slightly and occasionally overlap the coloured distribution of EP errors, which again clarifies that Genz can be effectively used as a proxy to ground truth. Panels C and D plot errors for trapezoid probabilities (see Section \ref{sec:specialcases}), again with EP and Genz errors.  Note also the three pairs of points in Panel A and C which are circled in blue.  These are three representative examples: one each of an orthant probability in $n=2$, an orthant probability in $n=5$, and a trapezoid probability in $n=16$.  We use these representative examples in the following Figure \ref{fig:runtimeresults} to evaluate the runtime/accuracy tradeoff of the EP and Genz methods.}}
\label{fig:specialresults} % caption for the whole figure
\end{figure}
%

Panels A and B of Figure \ref{fig:specialresults}  show errors for both the EP method (colour) and the Genz method (grayscale).  The two panels are the usual errors plotted as a function of condition number of covariance (Panel A) and the true probability $\log Z$ (Panel B).   The four cases shown are orthant probabilities at $n = \{2,3,4,5\}$.  There are a few interesting features worthy of mention.  First, we note that there is a clear separation between the Genz errors and the EP errors (in log-scale no less).  This finding helps to solidify the earlier claim that the Genz numerical answer can be used as a reasonable proxy to ground truth.  We can also view this figure with the Genz error estimates as error bars on the grayscale points.  We have done so and report that the majority of the coloured points remain above those error bars, and thus the error estimate reinforces this difference between Genz and EP.  We do not show those error bars for clarity of the figure.

Second, it is interesting to note that there is significant structure in the EP errors with orthant probabilities when plotted by $\log Z$ in Panel B.  Each dimension has a ``V" shape of errors.  This can be readily explained by reviewing what an orthant probability is.  For a zero-mean Gaussian, an orthant probability in $\reals^2$ is simply the mass of one of the quadrants.  If that quadrant has a mass of $\log Z = \log 0.25$, then the correlation must be white, and hence EP will produce a highly accurate answer (the problem decomposes).  Moving away from white correlation, EP will produce error.  This describes the shape of the red curve for $n=2$, which indeed is minimised at $\log 0.25 \approx -1.39$.  The same argument can be extended for why there should be a minimum in $\reals^3$ at $\log Z = \log 0.125$, and so on.  It is interesting to note that the Genz errors have structure of their own in this view, which is presumably related to the same fact about orthant probabilities.  Investigating the pattern of Genz errors further is beyond the scope of this work, however. 

Panels C and D plot the same figures as Panels A and B, but they do so with trapezoid probabilities.   As described above, trapezoid probabilities involve mean-centered hyperrectangular integration regions and Gaussians with diagonal covariance.  There is then an additional polyhedral constraint that cuts two dimensions of the region through the origin such that the probability is halved.  This appears to EP and to Genz as an arbitrary polyhedron in any dimensionality.  We show here results for $n = \{2,4,16\}$.  Panels C and D clarify that EP works particularly poorly in this case, often with significant errors.  This finding highlights again that EP with general polyhedral regions can yield unreliable results.  
%
\begin{figure}
\centering
\hspace{0.0cm}
%\epsfig{figure=results_runtime.eps,width=6in}
\includegraphics[width=6in]{results_runtime}
%\vspace{-0.3cm}
\caption{\small{Runtime analysis of the EPMGP and Genz algorithms.   The three pairs of lines are three representative examples, which were circled in blue in Figure \ref{fig:specialresults}.  They include orthant probabilities at dimension $n=2$ and $n=5$, and a trapezoid probability at $n=16$.  Each point in this Figure is the median run time of 100 repeated trials (both methods give the same error on each repetition).  The Genz method has a clear runtime/accuracy tradeoff, as more integration points significantly increases accuracy and computational burden.  For Genz we swept the number of integration points from 50 to $10^6$.  The EP method was run with a fixed maximum number of iterations (from 1 to 20).  Because the method converges quickly, rarely in more than 10 steps, these lines typically appear flat.  Note that EP runs one to three orders of magnitude more quickly than Genz, but more computation will not make EP more accurate.  }}
\label{fig:runtimeresults} % caption for the whole figure
\end{figure}
%

In Figure \ref{fig:specialresults}, Panels A and C also show 3 pairs of points circled in blue.  Those points are three representative test cases - an orthant probability in $n=2$, an orthant probability in $n=5$, and a trapezoid probability in $n=16$.  We chose these as representative cases to demonstrate runtime/accuracy tradeoffs, which is shown in Figure \ref{fig:runtimeresults}.   This figure is offered for the applied researcher who may have runtime/accuracy constraints that need to be balanced.  Panel A shows the familiar $\log Z$ errors, and Panel B shows the same cases with $Z$ error.  Each colour gives the error plotted as a function of runtime for one of the pair of points circled in Figure \ref{fig:specialresults}.  For the Genz method, the lines are formed by sweeping the number of integration points used from 50 to $10^6$.  For the EP method (results in bold), we vary the number of EP iterations allowed, from 2 to 20.  Since the EP method rarely takes more than 10 iterations to converge numerically, the bold lines are rather flat.   Each point in these panels is the median runtime of 100 repeated runs (the results and errors were identical for each case, as both methods are deterministic).

Figure \ref{fig:runtimeresults} shows that EPMGP runs one to three orders of magnitude faster than the Genz method.  However, since EP is a Gaussian approximation and not a numerical computation of the correct answer, one can not simply run EPMGP longer to produce a better answer.  The Genz method on the other hand has this clear runtime/accuracy tradeoff.   Our results throughout the other figures use the high accuracy Genz result ($5\times 10^5$ integration points - the rightmost point on the Genz lines) and the fully converged EP method (again the rightmost points on the EP lines).  Finally, we note that we chose three representative examples, but these are highly reflective of the typical runtime/accuracy tradeoff we have seen across many more test cases.  


%%%%%%%%%%%%%%%
\subsection{EP results for equicorrelated Gaussian probabilities}
\label{sec:equicorrresults}


Another interesting case for study is the case of Gaussians with equicorrelated covariance matrices.  These Gaussians have covariances with all diagonal entries equal to 1, and all off-diagonal entries equal to $\rho$, where $-1/(n-1) \le \rho \le 1$ (these bounds ensure a positive semidefinite matrix).    This case is interesting primarily because it allows us to look at a parameterised family of covariance matrices that include the identity matrix ($\rho = 0$, where we believe EP should produce a highly accurate answer) and highly elongated matrices ($\rho \rightarrow 1$ or $\rho \rightarrow -1/(n-1)$, where Figures \ref{fig:generalresults} and \ref{fig:specialresults} suggest that EP should do poorly).  This case is also important as it connects to previous literature on the subject of Gaussian probabilities, where equicorrelated matrices are a common test case (such as \cite{gassmann2002, shervish1984}).

%
\begin{figure}
\centering
\hspace{0.0cm}
%\epsfig{figure=results_equicorr.eps,width=6in}
\includegraphics[width=6in]{results_equicorr}
%\vspace{-0.3cm}
\caption{\small{ Empirical results for EPMGP over equicorrelated Gaussians.  Historically, a common test case for Gaussian probability algorithms has been to test error on correlation matrices of equal correlation (see {\it e.g.}, \cite{gassmann2002, shervish1984}).  All other quantities - the region $\regionA$ and Gaussian mean $\m$ - are still drawn randomly as previously described.  Here we test error for both the hyperrectangular case (panels A and B) and the polyhedral case (panels C and D), showing relative error in $Z$ (panels A and C) and relative error in $\log Z$ (panels B and D).  We show three different dimensionalities $n = \{2,4,8\}$ coloured as red, green, and black, respectively.  100 random cases were run at each correlation value.  Each error bar shows the median and $\{25\%,75\%\}$ quartiles.  The $x$-axis sweeps the correlation coefficient $\rho$ throughout its range for each dimension (which is the $[-1/(n-1) , 1]$ interval; matrices outside this range are not valid, positive semidefinite correlation matrices). }}
\label{fig:resultsequicorr} % caption for the whole figure
\end{figure}
%

Each panel tests the three cases of $n = \{2,4,8\}$, with the lines colour coded as in previous figures.  The $x$-axis of these figures sweeps the valid range of $\rho$ values.  At each $\rho$ value, which fully specifies a Gaussian covariance, we draw the region $\regionA$ and Gaussian mean $\m$ in the usual way, as previously described.  We do this 100 times at each value of $\rho$, calculating the EP and Genz results, from which we can compute errors in $\log Z$ and $Z$ in the usual way.  Again the line and error bars at each point show the median and $\{25\%,75\%\}$ quartiles.   Panels A and B show these results with hyperrectangular integration regions, and Panels C and D show the results with general polyhedral regions.

Panels A and B behave as intuition suggests: when the covariance $K$ is nearly white, the error is minimal.  As the correlation becomes larger and the matrix more eccentric, error increases.  Also in the hyperrectangular case we see that errors become meaningful - around $1\%$ - only in the most eccentric cases where the matrix is nearly singular. 

Panels C and D appear to have a much flatter error, which perhaps disagrees with intuition.  The error seems relatively insensitive to the eccentricity of the correlation structure.  Errors do have a slight downward trend towards 0, but it is by no means significant.  This finding can be readily explained by returning to the notion of the whitened space (Figure \ref{fig:white} and Panel D of Figures \ref{fig:generalresults} and \ref{fig:generalresultspoly}).  This result also helps us transition into understanding pathologies of EP.  Recall that the problem can be whitened by the covariance of the Gaussian, such that the Gaussian is standardised to identity covariance and the region carries all notions of eccentricity.  If we revisit Panels A and B in this light, we see that the EP method is doing well when the transformed region is close to hyperrectangular ($\rho \approx 0$), but then error grows as the region becomes coloured/non-rectangular ($\rho \rightarrow 1$ or $\rho \rightarrow -1/(n-1)$).   The intuition and the results are unchanged in Panels A and B.  Turning to Panels C and D, we can imagine the same effect as $\rho$ is changed.  However, because we are in the case of polyhedral integration regions, the region we draw is \emph{already} coloured before we whiten the space.  Thus, when we whiten the space, we will equally often make a region less hyperrectangular and more hyperrectangular.  In this view, sweeping this correlation matrix should have minimal average effect on the error.  This lack of effect is indeed what we see in Panels C and D of Figure \ref{fig:resultsequicorr}.  

%%%%%%%%%%%%%%%
\subsection{Contrived, pathological cases to illustrate shortcomings of EP}
\label{sec:pathologicalresults}


%
\begin{figure}
\centering
\hspace{0.0cm}
%\epsfig{figure=results_pathology.eps,width=6in}
\includegraphics[width=6in]{results_pathology}
%\vspace{-0.3cm}
\caption{\small{Empirical results from intentionally pathological cases that explore the shortcomings of the EP approach.  All panels show the problem of integrating $\mathcal{N}(0,I)$ Gaussian in $n=2$ dimensions over the $[-1,1] \times [-1,1]$ box.  Panel depicts the error as repeated box factors are added to the problem.  The true integral $F(\regionA)$ is unchanged, but the EP error grows arbitrarily bad.  Here EP consistently \emph{underestimates} the true $F(\regionA)$.  Panel B depicts the error as extra factor mass is added.  Though the integration region is unchanged (still the $[-1,1] \times [-1,1]$ box that is the intersection of larger boxes), the factors have extra mass truncated by other factors.  EP can not ignore this extra mass and categorically \emph{overestimates} the true probability.  Here it is important to note that the error in Panel B does not start at 0 (when there is no extra mass), because that case corresponds to 2 repeated boxes.  This case is shown as blue dots in Panels A and B to make that connection (the blue dots are the same problem).  The error starts as an underestimate, and it crosses 0 error at the point at which the redundancy effect (underestimator from Panel A) and extra mass (overestimator from Panel B) compensate for each other.  Both Panels A and B can be ``fixed" by removing redundant or inactive constraints: minimalising the polyhedron as described in Appendix \ref{sec:minpoly}.  However, Panel C shows that these redundancy and extra mass problems can not always be corrected.  Here we show rotated boxes (each rotated by $\pi/1000$ with respect to the previous box), which define the region uniquely and with no inactive constraints.  There are still significant errors due to these effects.  Panel C underestimates the probability, which is sensible because there is more redundancy than there is extra mass in this pathological case.  Though all these examples are contrived, they illustrate serious empirical issues with EP that are not widely discussed.}}
\label{fig:resultspathology} % caption for the whole figure
\end{figure}
%

The above results already suggest that certain Gaussian probabilities may be ill-suited to EP.   In particular, we gained intuition from viewing the problem in the whitened space, where the colinearity of the integration region can be seen to imply greater or lesser error when using EP.  Now we want to use that intuition to intentionally create pathological cases for the EP method.  Here we will introduce our terminology for the issues that cause EP to give poor results, which will provide a transition to a more theoretical discussion of the shortcomings of EP in the subsequent Discussion.  Figure \ref{fig:resultspathology} introduces these pathological cases.  In all cases, we want to integrate the $\mathcal{N}(0,I)$ Gaussian in $n=2$ dimensions over the $[-1,1] \times [-1,1]$ box, which is simply the product of two axis-aligned univariate box functions ($t_1(x_1)= \II\{-1 < x_1 < 1\}$ and $t_2(x_2)= \II\{-1 < x_2 < 1\}$).  We note that this choice of Gaussian and integration region is convenient, but that the effects shown here exist in all integration regions that we have tested, to varying extents.  This case is a common and representative case in terms of error magnitudes.  The $y$-axis shows the errors as usual, and the $x$-axis is a feature chosen to create large errors in the EP method.  

First, when looking at a transformed integration region, it seems plausible that some of the error from EP derives from the fact that approximate factors $\tilde{t}_i(\x)$ are not orthogonal, and thus they may double count the mass in some way when forming the EP approximation $q(\x)$.   An extreme, contrived example of this would be adding two repeats of identical factors $t_i(\x)$.  Though the integration region is unchanged, EP is not invariant to this change, as it still must make an approximate factor for each true factor.  This fact has been noted previously in Power-EP \cite[]{PowerEP}, a connection which we explore deeply in the Discussion.   Panel A describes this pathological case, which we call the ``redundancy" issue.   As the cartoon in Panel A depicts, along the $x$-axis we use EP to solve the same probability with increasing numbers of redundant factors.  We begin with a single box at left, where EP gets the exact answer as expected, since this problem decomposes to a product of univariate computations.  Moving right, we add up to 1000 copies of the same integration region, and we see that the error grows to an answer that is arbitrarily bad.  In mathematical terms, we go from using EP to solve $\int p_0(\x) t(x_1)t(x_2) d\x$, to using EP to solve $\int p_0(\x) t(x_1)^{1000}t(x_2)^{1000} d\x$.  Though the true $F(\regionA)$ is unchanged, EP returns a wildly different answer.    More specifically, EP in this redundancy case is \emph{underestimating} the true probability, which is expected and fully explained in the Discussion.  

Second, by returning again to the transformed integration region, it also seems plausible that some of the error from EP derives from the approximate factors accounting for mass inappropriately.  For example, when two $t_i(\x)$ are not orthogonal, the moment matching of the cavity and the factor  to the cavity and the approximate factor will include mass that is outside the integration region.  Panel B of Figure \ref{fig:resultspathology} elucidates this concept.   The integration region of interest here is  still the $[-1,1] \times [-1,1]$ box, but that can be described by the intersection two boxes of any size, as the cartoon in Panel B shows.   However, during the EP iterations, EP must consider the mass that exists in each true box factor \emph{individually}.  Because the Gaussian family has infinite support, there will always be nonzero mass outside the desired intersection that is  the $[-1,1] \times [-1,1]$ box, and EP approximate factors will incorporate this mass.  Hence we expect that EP will not be invariant to this change, and further that EP should overestimate the true probability when there is ``extra mass," which is the term we use to describe this pathology of EP.  Indeed Panel B shows precisely this.  As extra mass is increased along the $x$-axis, error increases and overestimates the true probability.  There are two interesting features here: first, note that the case where there is no extra mass - at left - is still a case where there are two redundant boxes, and hence we expect the underestimation error from Panel A (these corresponding points are circled in blue in Panels A and B).   As extra mass is added, the EP result increases, the error crosses zero, and the EP result becomes a significant overestimate of the true probability.  The second interesting feature is that the error levels off more quickly than in the redundancy case.  This is not surprising, as the exponential decay of the Gaussian implies that the extra mass issue can not arbitrarily punish EP - adding larger and larger boxes gives diminishing additional probability mass that EP must incorporate.

Finally, one might suggest that the problems of Panels A and B are entirely avoidable with appropriate preprocessing.  Specifically, we can pull in all inactive polyhedral constraints, which would remove the extra mass issue of Panel B.  Further, we can prune all redundant constraints, which would remove the redundancy issue of Panel A.   We have discussed this previously in terms of a minimal representation of the polyhedron $\regionA$, and these steps are detailed in Appendix \ref{sec:minpoly}.  However, these trimming and pruning operations will not remove these issues in anything other than these specific cases, as can be seen in Panel C.  Here we essentially replicate the result of Panel A.  However, instead of precisely repeating the box, we add a sequence of slightly rotated boxes.  Thus, this polyhedron is a minimal description of the integration region, and yet still we have identical issues as Panel A and B.  One can imagine a series of heuristic choices to circumvent this issue, but the fact remains that EP will produce poor results for many simpler cases (see Figures \ref{fig:generalresults} and \ref{fig:specialresults}, for example the trapezoid case), and thus we feel an investigation of these error modes is warranted and an interesting EP contribution.

From this figure the reader might correctly suggest that both the redundancy issue of Panel A and the extra mass issue of Panel B are two sides of the same coin: EP enforces a Gaussian approximation to a hard box-function factor, which is a highly non-Gaussian form.  Thus redundancy and extra mass generally go hand in hand: if there are constraints that are not orthogonal, they will each have to consider both mass already considered by another factor (redundancy) and mass that lies outside the actual polyhedron (extra mass).   Nonetheless, keeping these two ideas separate in these results will help us consider the theoretical underpinnings and point towards possible ways to improve the EP estimate of Gaussian probability and EP more generally, which is the subject of the Discussion section.

\subsection{Final note: other moments}

A novelty of our work here is that we have brought an approximate inference framework to bear on the problem of approximate integration.   While the problem of calculating Gaussian probabilities - the zeroth moment of $p(\x)$ - is interesting in its own right, many researchers will ask how well other moments, namely the ``posterior" mean and covariance  of the approximation $q(\x)$ match the true moments.  Throughout our results we calculated these moments with EP also.  While demonstrating this claim rigorously is beyond the scope of this work (which focuses on the zeroth moment), we report anecdotally that we see similar accuracy and error regimes for the mean and covariance estimates from EP.    As Genz methods do not return these moments, we calculated posterior means and covariances with elliptical slice sampling \cite[]{murrayESS}, which further complicates this claim due to the nontrivial error rate and computational burden of this sampling method.   Nonetheless, to answer this common question and broaden the implications of these results beyond simply the zeroth moment, we note our finding that other EP moments follow similar trends.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}
\label{sec:discussion}

This paper has introduced a number of concepts and results around both Gaussian probabilities and Expectation Propagation.  Here we discuss these findings.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{EP allows approximate integration and can be used to calculate Gaussian probabilities}

At the highest level, this paper introduces a novel approach to calculating multivariate Gaussian probabilities, which are a difficult and important problem throughout science and applied statistics.   We used an approximate inference framework - Expectation Propagation - in an unexpected way to perform this Gaussian integration.  From the perspective of Gaussian probabilities, this work introduces EPMGP as a method that can be used to give good, often highly accurate, numerical approximations.  Our results show that under a variety of circumstances, relative EP errors rarely exceed $1\%$, and median errors are typically two to four orders of magnitude smaller.  While we spent much of the results exploring the regions of poor performance (which we continue to explore below), the general strength of the method should not be neglected.  The EPMGP algorithm also has some nice features such as analytical derivatives, attractive computational properties, and a natural ability to compute tail probabilities.  On the other hand, existing numerical methods, in particular the Genz method, typically demonstrate high numerical accuracy and a clear runtime/accuracy tradeoff, which we have also demonstrated here.   Furthermore, as we noted, one can readily construct cases wherein the EP method will perform poorly.  Thus we again note that the purpose here is not to compare EPMGP to the Genz method, as those approaches share different properties, but rather to analyse the methods for Gaussian probability calculations across a range of scenarios so that applied researchers can make an informed and practical choice about which methods suit particular needs.   

For machine learning researchers, another value of this work is the use of a popular approximate inference framework for an application other than approximate inference.  Exploring the generality of the machine learning toolset for other purposes is important for understanding broader implications of our field's technologies.

Perhaps the most interesting aspect of this work is that the Gaussian probability problem, by its simple geometric interpretation, allows clear investigation into the strengths and weaknesses of EP.  This work helps us explore empirically and theoretically the circumstances of and reasons behind EP's sometimes excellent and sometimes poor performance, which bears relevance to applications well beyond just Gaussian probabilities.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{The strengths and weaknesses of EP: empirical evidence}

Section \ref{sec:results} presents extensive empirical results of the performance of EP for the Gaussian probability problem.  Figures \ref{fig:axisresults} and \ref{fig:generalresults} indicate that the EP algorithm works well for hyperrectangular regions but has considerably lower accuracy (one to two orders of magnitude) for general polyhedral regions.  While this discrepancy may seem fundamental, we showed in Figure \ref{fig:white} that in fact hyperrectangular and polyhedral regions are closely related in a whitened space, where all but trivial cases are non-rectangular polyhedra.  We have introduced the intuitive concepts of extra mass and redundancy, which suggest why EP should be error prone as the integration region (in the whitened space) gets further from rectangular.    Figure \ref{fig:generalresultspoly} provided a first example of this feature.  For a fixed dimensionality $n$, increasing the number of polyhedral faces $m$ increases the redundancy, since each subsequently added constraint will have increasing colinearity with the existing constraint set.  Panel A of Figure \ref{fig:generalresultspoly} shows this increasing error as a function of $m$.  We also investigated cases where the Gaussian problem has special structure, such as orthant and trapezoid integration regions in Figure \ref{fig:specialresults} and equicorrelated Gaussians in Figure \ref{fig:resultsequicorr}.  Focusing now on the redundancy and extra mass issues that we have introduced, Panel B of Figure \ref{fig:specialresults} and all of Figure \ref{fig:resultsequicorr} are particularly useful.  They both demonstrate considerable EP error trend as the Gaussian moves away from white.  Considered from the perspective of the space whitened with respect to the Gaussian, these figures demonstrate that EP error trends reliably with how non-rectangular or coloured the integration region is.  We can concretely see the extra mass and redundancy issues causing errors in this view.  Finally, Figure \ref{fig:resultspathology} presents pointed (albeit contrived) examples to clarify the effect of these issues.   

In total, our empirical evidence suggests that EP can often be used to calculate Gaussian probabilities quickly and to high accuracy, and further that consideration of the Gaussian and integration region at hand can help predict when this computation will not be reliable.  We introduced the intuitive concepts of redundancy and extra mass, which are helpful in understanding why problems come about.  We attempt to condense these intuitions into a more rigorous theoretical understanding in the following section.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{The strengths and weaknesses of EP: theoretical underpinnings}

We now delve into a more theoretical investigation of EP, first by considering a factor graph representation, and second by considering the divergences that EP is trying to minimise in various problem settings.

\subsubsection{Factorisation perspective}

As we explained in section~\ref{sec:white}, we can equivalently represent any hyperrectangular or polyhedral integration region as a polyhedral region in the whitened space where the Gaussian has identity covariance. This standardisation enables us to focus on the geometry of the region.  If this transformed region is actually axis-aligned, we obtain a fully factorised distribution. The factor graph representing this situation is shown in Panel A of Figure~\ref{fig:factorization-view}, where we use one node for each dimension to explicitly show the factorisation. Because the distribution is fully factorised, the integration also factorises as a product of univariate integrals, and so local moment matching in this case also yields global moment matching.   Mathematically, the global KL-divergence can now be directly related to the local divergences such that $D_\KL(p\| q) = \sum_i D_\KL(t_i p'_{0i}\| \tilde{t}_i p'_{0i})$, where $p'_{0i}$ is the $i$th dimension of the whitened Gaussian $p'_0(\x)$ (standard univariate normals as in Figure \ref{fig:factorization-view}).  Accordingly, the KL minimisation problem separates, and EP will yield the exact zeroth moment (to within the machine precision of the error function).  To be clear, EP is still making an approximation (the Gaussian approximation $q$), but we know that this $q$ will have the exact zeroth, first, and second moments of $p$.  And thus the Gaussian probability will be correct.

On the other hand, non axis-aligned constraints in the whitened space can be seen as linking multiple nodes together, thereby inducing dependencies in the factor graph and breaking the equivalence between local moment matching and global moment matching. These non axis-aligned constraints could either arise from the original integration region where a constraint links variables together, or from a non-diagonal Gaussian covariance which colours the integration region when space is whitened with respect to that covariance.  We show an example of these dependencies in Panel B of Figure~\ref{fig:factorization-view}. This factor graph is obtained by taking a bidiagonal prior covariance matrix on a polyhedral region for $n=3$ and transforming it to the whitened space as in Figure \ref{fig:white}. The polyhedral region is an axis-aligned hyperrectangle plus an additional factor constraint that links all dimensions.  The covariance matrix induces pairwise factors in the whitened space, and the polyhedral constraint induces a factor on all variables. These non axis-aligned factors give rise to loops\footnote{We note in passing that the EP factor formulation that we are using is different than the standard one used on factor graphs which replaces each factor with a product of univariate approximate factors -- see for example Section 10.7.2 of~\cite{bishopBook} -- and which reduces to loopy belief propagation in the case of discrete variables. Our formulation keeps the full approximate factor $\tilde{t}_i(\x)$, much as when EP is applied on the Bayesian posterior of an i.i.d. model.} in the factor graph which we expect to reduce the accuracy of EP, as seen in Figure~\ref{fig:generalresults} for polyhedral regions.

%
\begin{figure}
\centering
\hspace{0.0cm}
  \begin{tikzpicture}[node distance=2cm]
  % left side. Panel A
    \node[anchor=north west] at (-10,2.5) {\LARGE{\bf A}};
	% first independent factor...
    \node[fac] (p1) at (-8.5,1) {}; 
    \node[anchor=south] at (p1.north) {$\mathcal{N}(x_1) $}; 
    \node[var, below of=p1] (x1) {$x_1$} edge (p1);
   \node[fac, below of=x1] (t1) {} edge (x1);
    \node[anchor=north] at (t1.south) {$t_1(x_1)$};
	% second independent factor...
    \node[fac] (p2) at (-6.5,1) {}; 
    \node[anchor=south] at (p2.north) {$\mathcal{N}(x_2) $}; 
    \node[var, below of=p2] (x2) {$x_2$} edge (p2);
   \node[fac, below of=x2] (t2) {} edge (x2);
    \node[anchor=north] at (t2.south) {$t_2(x_2)$};
	% the ellipsis...
	\node[anchor=north west] at (-5.7,-1) {\huge{...}};
	% nth independent factor...
    \node[fac] (pn) at (-4,1) {}; 
    \node[anchor=south] at (pn.north) {$\mathcal{N}(x_n) $}; 
    \node[var, below of=pn] (xn) {$x_n$} edge (pn);
   \node[fac, below of=xn] (tn) {} edge (xn);
    \node[anchor=north] at (tn.south) {$t_n(x_n)$};
	%
	% now the right side, panel B.
    \node[anchor=north west] at (-2.5,2.5) {\LARGE{\bf B}};
   	% first dependent factor...
    \node[fac] (pb1) at (-1,1) {}; 
    \node[anchor=south] at (pb1.north) {$\mathcal{N}(x_1) $}; 
    \node[var, below of=pb1] (xb1) {$x_1$} edge (pb1);
	% second independent factor...
    \node[fac] (pb2) at (1.5,1) {}; 
    \node[anchor=south] at (pb2.north) {$\mathcal{N}(x_2) $}; 
    \node[var, below of=pb2] (xb2) {$x_2$} edge (pb2);
	% 3rd independent factor...
    \node[fac] (pb3) at (4,1) {}; 
    \node[anchor=south] at (pb3.north) {$\mathcal{N}(x_3) $}; 
    \node[var, below of=pb3] (xb3) {$x_3$} edge (pb3);
	%    now the factors
	   \node[fac] (tb1) at (0.25, 0) {} edge (xb1) edge (xb2);
	    \node[anchor=south] at (tb1.north) {$t_1(x_1,x_2)$};
	   \node[fac] (tb2) at (2.75, 0) {} edge (xb2) edge (xb3);
	    \node[anchor=south] at (tb2.north) {$t_2(x_2,x_3)$};
	   \node[fac, below of=xb3] (tb3) {} edge (xb3);
    	  \node[anchor=north] at (tb3.south) {$t_3(x_3)$};
	   \node[fac] (tb4) at (1.0, -3) {} edge (xb1) edge (xb2) edge (xb3);
	    \node[anchor=north] at (tb4.south) {$t_4(x_1,x_2,x_3)$};
%	
  \end{tikzpicture}
%\epsfig{figure=factorization.pdf,width=6in}
%\vspace{-0.3cm}
\caption{\small{Factorisation perspective. Panel A represents the factor graph for an axis-aligned region in the whitened space ($\mathcal{N}(x_i)$ is a standard Gaussian on $x_i$, which defines the whitened prior $p_0'(\x)$). Panel B represents an example of a region which is not axis-aligned in the whitened space: we show a polyhedral region with $n=3$ which had a bidiagonal prior covariance structure, yielding dependent factors in the whitened space.}}
\label{fig:factorization-view} % caption for the whole figure
\end{figure}
%

Little is formally known about the accuracy of EP on loopy graphs.  In the case of Gaussian probabilities, fortunately we can use the geometry of the integration region to interpret the effect of additional graph edges. In particular, factors having several variables in common correspond to constraints which have a non-zero dot product (somewhat co-linear).  Thus, these factors create approximate redundancy (Panel~C of Figure~\ref{fig:resultspathology}) and extra mass (Panel~B of the same figure).   While these EP issues exist in all EP problem setups (not only Gaussian probabilities), the geometrical concepts available here provide a unique opportunity to understand the accuracy of EP which would not be apparent in standard factor graphs.  We formalise these concepts in the next section.  

\subsubsection{Alpha-divergence perspective}

The redundancy and extra mass problems can be viewed in a unifying way as changing the effective \emph{alpha-divergence} that EP is minimising, as we explain in this section. The alpha-divergence $D_{\alpha}(p \parallel q)$ is a generalisation of the KL-divergence.  The relationship of alpha-divergences to EP and other variational schemes has been studied: \cite{minkaMSFTTR2005} shows that several message passing algorithms on factor graphs can be interpreted as the local minimisation of different alpha-divergences. Following the notation from~\cite{minkaMSFTTR2005}, $\KL (q \parallel p)$, the Hellinger distance, and $\KL (p \parallel q)$ corresponds to $\alpha=0$, $\alpha=0.5$, and $\alpha=1$ respectively. Hence EP can be interpreted as the minimisation of local alpha-divergences with $\alpha=1$. 

The Power-EP algorithm~\cite[]{PowerEP}\footnote{We warn the reader that the notation for alpha-divergence varies between papers (and in particular between~\cite{PowerEP} and~\cite{minkaMSFTTR2005}). We use the notation from~\cite{minkaMSFTTR2005} -- the corresponding $\alpha'$ used in~\cite{PowerEP} is obtained as $\alpha' = 2 \alpha - 1$.} is an extension of EP where the factors are raised to a specific power $\alpha_i$ before doing moment matching. One of the motivations for this algorithm is that raising the factor $t_i$ to a power can make the projection more tractable.  That paper also notes that running EP on a model where each factor $t_i$ is repeated $n_i$ times is equivalent to running Power-EP (with the factors unrepeated) with $\alpha_i = 1/n_i$.  Importantly, the Power-EP algorithm can also be derived as the minimisation of local $\alpha_i$-divergences. 

This repetition of factors is precisely the ``redundancy" issue that we created by repeating constraints $n_i$ times (as in Panel~A of Figure~\ref{fig:resultspathology}). Our EP algorithm in the pure redundant case can thus be interpreted as running Power-EP with $\alpha_i = 1/n_i$.  This relationship is useful to us for four reasons. 

First, the zeroth moment $Z$ is always \emph{underestimated} when doing global minimisation of \emph{exclusive} alpha-divergence with $\alpha<1$ \cite[]{minkaMSFTTR2005}.  Also, $Z$ is only correctly estimated for $\alpha=1$ (KL-divergence), and it is \emph{overestimated} for \emph{inclusive} alpha-divergences which have $\alpha>1$.   This provides a theoretical explanation for the systematic underestimation of $Z$ in the redundancy case of Panel~A, Figure~\ref{fig:resultspathology}:  EP in this case is the same as Power-EP with $\alpha_i = 1/n_i < 1$.  As in regular EP, a subtle aspect here is that Power-EP is doing \emph{local} alpha-divergence minimisation, whereas the overestimation/underestimation of the zeroth moment $Z$ holds for \emph{global} alpha-divergence minimisation, and the relationship between the two is not yet fully understood.   However, our construction in Panel A of Figure \ref{fig:resultspathology} was a fully factorised case, so there is direct correspondence between global and local divergences, and as such we did systematically observe underestimation in these experiments.  We also add that our experiments with Panel C of this figure and other similar cases indicate similar underestimation even when there is not direct global-local correspondence. 

Second, this relationship between EP and alpha-divergence gives us a notion of \emph{effective} alpha-divergence $\alpha_\text{eff}$ for each factor. In particular, running EP with the factor $t_i$ repeated $n_i$ times is the same as the local alpha-divergence minimisation by Power-EP with $\alpha_\text{eff} = 1/n_i$. When the constraints are almost-repeated (they have a large dot product, but are not fully colinear, such as in Panel~C of Figure~\ref{fig:resultspathology}), we lose the exact correspondence between EP and Power-EP, but we could still imagine that EP would correspond to Power-EP with a suitable $\frac{1}{n_i} < \alpha_\text{eff}<1$ in this case as well (using a continuity argument). 

Third, we can now think of \emph{correcting} for this $\alpha_\text{eff}$ by using Power-EP with $\alpha' = 1/\alpha_\text{eff}$ instead of standard EP. In the case of exactly repeated constraints, this means using $\alpha'_i = n_i$.  Indeed, our additional experiments (not shown) demonstrate that Power-EP does yield back the correct answer in this simple case. In other words, using this notion of correcting for $\alpha_{\text{eff}}$, the errors of Panel A in Figure \ref{fig:resultspathology} disappear.   In the case of almost-repeated constraints, we can still imagine correcting the effect by choosing a suitable $\alpha_\text{eff}$ correction term and using Power-EP.  Our early investigation of this correction has given some promising results.   An important and interesting direction of future work is characterising $\alpha_\text{eff}$ from geometric quantities such as dot products of constraints. 

Fourth and finally, the extra mass issue can also be viewed in terms of alpha-divergences, though the connection is not as rigorous as the redundancy issue.  The extra mass problem is one of inclusivity: the Gaussian approximate factor is including mass that it should not (as that mass lies outside the true polyhedron), which is a property of \emph{inclusive} alpha-divergences as mentioned previously, {\it i.e.}, divergences with $\alpha > 1$.  These we would expect to overestimate the true probability, and indeed this is what we see in Panel B of Figure \ref{fig:resultspathology}.  We can also consider correcting for these $\alpha_{\text{eff}}$ with Power EP.  The situation is slightly more complicated here as the extra mass problem involves understanding how $\alpha_{\text{eff}}$ changes with the decay of the Gaussian distribution, but our initial attempts at these corrections indicate that the overestimation error can be significantly reduced.

To summarise, both the redundancy and extra mass problems can be cast in the language of effective alpha-divergence. We know that minimising $D_{\alpha}$ only returns moment matching when $\alpha = 1$ (KL-divergence), and thus any efforts to drive $\alpha_{\text{eff}}$ closer to 1 should improve all the performance results seen here.  We have noted our success in removing errors entirely in the simplest cases such as Panel A of Figure~\ref{fig:resultspathology} and in removing errors significantly in other slightly more complicated scenarios.  Thus we feel that a judicious choice of $\alpha$, to correct for the effective $\alpha$ in a Power-EP framework, is a fruitful direction of future work for both the theoretical understanding of and the empirical performance of EP across many applications.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Broader implications for EP}

%minka01phd, minkaUAI01,minkaMSFTTR2005, 

Having delved deeply into EP for Gaussian probabilities, it is worth stepping back to consider the breadth of these findings.

First we draw connections to other common machine learning applications.  In the noise-free Bayes Point Machine model \cite[]{herbrichBook, minkaUAI01}, the model evidence is the integral of a white Gaussian over a polyhedral cone (intersection of halfspaces going through the origin), and so is a special case of our Gaussian probability framework.  EP was analysed for the Bayes Point Machine in \cite{minka01phd,minkaUAI01} and was found to have good performance.  On the other hand, its accuracy was only evaluated on a synthetic example with 3 points in 2 dimensions, which in our terminology would be $\{m=3 , n=2\}$, which forms but a small part of our empirical analyses.   Furthermore, prediction accuracy on a test set was used to evaluate EP on real world examples, but as ground truth was unavailable, the estimation accuracy remains unclear.  \cite{minka01phd} observed our redundancy problem in a synthetic example: the author notes repetitions of the same datapoints (and so repeating the halfspace constraint in our formulation) yielded much poorer approximations of the evidence or the mean.   By comparing to numerical integration methods, we have provided a more extensive analysis of EP's accuracy for this problem.  Further, we anticipate that a method for estimating $\alpha_{\text{eff}}$ (and using Power-EP) could improve the performance of Bayes Point Machines when the data is highly redundant.

Another important existing connection point is Gaussian Process classification \cite[]{rasmussenBook, KussRasmussen2005}.  When a probit likelihood (namely a univariate Gaussian cdf) is used, the data likelihood becomes a Gaussian probability problem.   The same is true of probit regression \cite[]{ochi1984}, where changing the representation of the more standard prior and likelihood reveals a high-dimensional Gaussian probability.  Though these problems are usually not considered as Gaussian probabilities, it is another point of broad importance for EP and for Gaussian probabilities.  Furthermore, as EP is often used in a different way in these problems, exploring the similarities between EP for Gaussian probabilities and EP for problems that can be cast as Gaussian probabilities is an interesting subject for future investigation.

We have revealed some interesting empirical and theoretical properties for EP as applied to approximate Gaussian integration, but understanding how much of these issues stems from EP itself and how much from the application is an important clarification.   Gaussian probabilities are a particularly instructive application, as they encourage the geometric intepretation that we have maintained throughout this paper.  However, the issues of redundancy and extra mass are not specific to strict box functions, and the interpretation in terms of $\alpha_{\text{eff}}$ is no less applicable in the case of smooth functions.   To test this question, we could repeat the setup of Figure \ref{fig:resultspathology} but using smooth functions such as probit curves.  This problem would then bear similarity to Gaussian Process classification, and we could stress test the sensitivity of EP (in terms of extra mass and redundancy) to changing amounts of smoothness in the probit, where one end of this continuum is our Gaussian probability results.  Though this remains a subject of interesting further work, we anticipate a result consistent with our findings here.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Future Directions}

This work has explored both Gaussian probabilities and Expectation Propagation, and there are opportunities for valuable future work in both.  We feel the most interesting direction for further investigation regards EP, specifically the theoretical discussion of alpha-divergences as in the section above.  Any insights into understanding $\alpha_{\text{eff}}$ based on the geometry of the distributions involved, and further methods by which to correct for that $\alpha_{\text{eff}}$ via Power-EP approaches, could have broad implications for the continued success of EP as a powerful approximate inference framework.  Here we have explored the geometrically and functionally simple case of box functions (which give Gaussian probablities), but future work should explore this for a variety of other factors (likelihood terms) as well.

The other focus for future work could be improvements to EPMGP specifically.   For example, EP can often be improved by updating multiple approximate factors together.  Here we considered a rank-one factor update, but also highly redundant factors could be considered in a rank-two update together.  Because some of these low dimensional Gaussian probabilities can be calculated (such as orthants), these updates will be tractable in certain cases. 

Finally, it is worth noting that our EP approach to approximate integration highlights the connections between approximate inference and integration, so some interesting connections here could be explored.  For example, we noted that the Genz methods do not currently offer the ability to compute tail probabilities, calculate higher moments than the zeroth, or offer analytical derivatives.   The absence of these features should not in principle be fundamental to the numerical integration approach, so reconsidering the Genz methods in the light of approximate inference could yield novel techniques for machine learning.  Furthermore, hybrid approaches could be considered, drawing strengths from EP and numerical integration approaches.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}
\label{sec:conclusion}

We have presented an algorithm constructing analytic approximations to polyhedral integrals on multivariate Gaussian distributions. Such approximations are of value for a number of applications in many fields, particularly in hyperrectangular integration cases. We presented extensive numerical results for this case and the more general polyhedral case.  We showed an interesting result that a small generalisation of the same algorithm is considerably less reliable. We explored the empirical and theoretical reasons behind these performance differences, which elucidated some features of EP not often discussed.  These results also reveal Gaussian Probabilities as a useful testbed for analysing EP.  As Expectation Propagation is becoming a more popular approximate inference method, this cautionary result suggests that EP should not be trusted blindly in new applications. Nonetheless, the uses of this EP framework will hopefully enable future work that uses the important object that is Gaussian probability.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Acknowledgements should go at the end, before appendices and references
\acks{We thank Tom Minka, Thore Graepel, Ralf Herbrich, Carl Rasmussen, Zoubin
  Ghahramani, Ed Snelson, and David Knowles for helpful discussions. JPC was supported by the UK
  Engineering and Physical Sciences Research Council (EPSRC
  EP/H019472/1); PH and SLJ were supported by a grant from Microsoft Research
  Ltd.}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% END OF MAIN TEXT  %%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\appendix



\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\appendix

\section{Gaussian EP cavity distributions for rank one factors $t_i(\x)$}
\label{sec:rank1cavity}

The purpose of this appendix is to derive the forms of the EP cavity distributions given in Equation \ref{eqn:cavityepmgp}.  The only specific requirement is that the factors are rank one, that is: $t_i(\x) = t_i(\c_i^T\x)$. We begin with the high dimensional Gaussian cavity distribution:
\begin{equation}
q^{\wo i}(\x) ~~=~~ Z^{\wo i}\mathcal{N}(\x; \u^{\backslash i}, V^{\backslash i}) ~~~\propto~~~ p_0(\x) \prod_{j\ne i} \tilde{t}_j(\x)
\end{equation}
where
%
\begin{equation}
V^{\backslash i} = \left(K^{-1}  + \sum_{j\ne i} \frac{1}{\tilde{\sigma}_j^2} \c_j\c_j^T\right)^{-1}
\end{equation}
%
and
%
\begin{equation}
\u^{\backslash i} = V^{\backslash i}\left(K^{-1}\m  + \sum_{j\ne i} \frac{\tilde{\mu}_j}{\tilde{\sigma}_j^2} \c_j\right)
\end{equation}
using standard properties of the normal distribution ({\it e.g.}, \cite{rasmussenBook}).  Alternatively, as is often done for EP, we can consider subtracting the $i$th approximate factor from the overall posterior approximation $q(\x)$.  That is, since
\begin{equation}
q(\x) = Z\mathcal{N}(\x; \boldmu, \Sigma) ~~~\propto~~~ p_0(\x) \overset{m}{\prod_{j=1}} \tilde{t}_j(\x),
\end{equation}
we can say that $q^{\wo i}(\x)$ has parameters:
%
\begin{equation}
\label{eqn:Vslash}
V^{\backslash i} = \left(\Sigma^{-1}  - \frac{1}{\tilde{\sigma}_i^2} \c_i\c_i^T\right)^{-1}
\end{equation}
%
and
%
\begin{equation}
\label{eqn:mslash}
\u^{\backslash i} = V^{\backslash i}\left(\Sigma^{-1}\boldmu  - \frac{\tilde{\mu}_i}{\tilde{\sigma}_i^2} \c_i\right).
\end{equation}

Note that these steps are just like standard EP, save that we are subtracting rank one factors $\tilde{t}_i(\x)$ instead of the more common axis-aligned factors.  We will use this cavity to moment match $q^{\wo i}(\x)\tilde{t}_i(\x)$ to $q^{\wo i}(\x)t_i(\x)$.  Since our $t_i(\x)$ are rank one, we can equivalently marginalise out all other dimensions and moment match in a single dimension, which simplifies computation and presentation.  This equivalence of marginal moment matching with the high-dimensional moment matching for EP requires that the exponential family used for the approximate factors is closed under marginalisation (which is the case for Gaussians), as noted in Section 3 of \cite{seeger08epexpfam}.  Accordingly, we want to form a univariate cavity distribution by marginalising over all dimensions orthogonal to the univariate factor of interest:
\begin{equation}
q_{\wo i}(\c_i^T\x) = \int_{\backslash \c_i;\x} q^{\wo i}(\x') d\x'
\end{equation}
where the integration region $\backslash \c_i;\x$ indicates marginalisation over all dimensions orthogonal to $\c_i$ (an $n-1$ dimensional integral) and we move the cavity-defining ``$\wo i$" superscript down to the subscript to indicate going from high dimension to single dimension.  In standard EP, this notation would be $\backslash \e_i$ for the axis defined by the unit vector $\e_i$; in other words, we marginalise all variables but the $i$th.  Mathematically, $\backslash \c_i;\x$ is the $n-1$ dimensional affine space perpendicular to $\c_i$ and defined by a particular choice of $\c_i^T\x$, that is $\backslash \c_i;\x = U_{\c_i} + (\c_i^T\x) \c_i$ and $U_{\c_i} = \bigl\{ \x'\in\reals^n \mid \c_i^T\x' = 0 \bigr\}$ (note that $\c_i$ is assumed to be normalised).

To do this marginalisation in closed form, we do a simple change of variable. We exploit the fact that, for any unit norm vector $\c_i$, there exists an orthonormal basis $\{\c_i, \a_2,...,\a_n\}$ of $\reals^n$.  We define the orthogonal matrix
\begin{equation}
A = \begin{bmatrix} \c_i & \a_2 & \hdots & \a_n  \end{bmatrix}, ~~~~\mathrm{with}~~~~  A^TA = AA^T = I,
\end{equation}
and change the coordinates for our integration of $q^{\wo i}(\x)$ with the change of variable $\y = A^T \x'$. Since $A$ is orthogonal, the Jacobian determinant is one. The Gaussian $q^{\wo i}(\x')= Z^{\wo i} \mathcal{N}(\x'; \u^{\wo i},V^{\wo i})$ simply becomes $Z^{\wo i} \mathcal{N}(\y; A^T \u^{\wo i},A^T V^{\wo i} A)$, and so:
\begin{eqnarray}
q_{\wo i}(\c_i^T\x) & = & \int_{\backslash \c_i;\x} q^{\wo i}(\x') d\x' \\
& = & \int_{\backslash \e_1;(\c^T\x)\e_1}  Z^{\wo i} \mathcal{N}(\y; A^T \u^{\wo i},A^T V^{\wo i} A) d\y \\
& = & Z^{\wo i} \mathcal{N}(\c_i^T \x; \c_i^T\u^{\backslash i} , \c_i^TV^{\backslash i}\c_i)
\end{eqnarray}
where the second to last equality follows from the fact that integrating out all dimensions orthogonal to $\c_i$ is equivalent to marginalising all but the first axis ($\e_1$ of $A^T\x$), by our definition of the matrix $A$.  Since those orthogonal dimensions all normalise to 1, the high dimensional integral becomes univariate.  The last equality follows again from the definition of $A$, and thus we are left with a simple univariate cavity distribution.  By using the matrix inversion lemma and the definitions from Equations \ref{eqn:Vslash} and \ref{eqn:mslash}, we can derive simple  forms for these cavity parameters:

\begin{eqnarray}
\sigma_{\wo i}^{2} & = &  \c_i^TV^{\backslash i}\c_i \\
& = & \c_i^T(\Sigma^{-1}  - \frac{1}{\tilde{\sigma}_i^2} \c_i\c_i^T)^{-1}\c_i \\
& = & \c_i^T(\Sigma + \Sigma \c_i ( \tilde{\sigma}_i^{2}  - \c_i^T \Sigma \c_i )^{-1} \c_i^T \Sigma) \c_i \\
& = & \c_i^T\Sigma\c_i\Bigl( 1 +  \frac{ \c_i^T \Sigma \c_i }{ \tilde{\sigma}_i^{2}  - \c_i^T \Sigma \c_i }\Bigr)\\
& = & \frac{\tilde{\sigma}_i^{2}\c_i^T\Sigma\c_i}{\tilde{\sigma}_i^{2}  - \c_i^T \Sigma \c_i }\\
& = & \bigl( (\c_i^T\Sigma \c_i)^{-1} - \tilde{\sigma}^{-2}_i\bigr)^{-1}
\end{eqnarray}

\noindent which matches the form of Equation \ref{eqn:cavityepmgp}.  So too, for the mean parameter of the univariate cavity distribution, we have:

\begin{eqnarray}
\mu_{\wo i} & = & \c_i^T\u^{\backslash i} \\
& = & \c_i^TV^{\backslash i}(\Sigma^{-1}\boldmu  - \frac{\tilde{\mu}_i}{\tilde{\sigma}_i^2} \c_i) \\
& = & \c_i^TV^{\backslash i}\Sigma^{-1}\boldmu - \frac{\tilde{\mu}_i}{\tilde{\sigma}_i^2} \sigma_{\wo i}^2 \\
& = & \c_i^T(\Sigma + \Sigma \c_i ( \tilde{\sigma}_i^{2}  - \c_i^T \Sigma \c_i )^{-1} \c_i^T \Sigma) \Sigma^{-1}\boldmu  - \frac{\tilde{\mu}_i}{\tilde{\sigma}_i^2} \sigma_{\wo i}^2  \\
& = & \c_i^T\boldmu + \c_i^T\Sigma \c_i ( \tilde{\sigma}_i^{2}  - \c_i^T \Sigma \c_i )^{-1} \c_i^T\boldmu - \frac{\tilde{\mu}_i}{\tilde{\sigma}_i^2} \sigma_{\wo i}^2 \\
& = & \c_i^T\boldmu\Bigl( 1 +  \frac{ \c_i^T \Sigma \c_i }{ \tilde{\sigma}_i^{2}  - \c_i^T \Sigma \c_i }\Bigr) - \frac{\tilde{\mu}_i}{\tilde{\sigma}_i^2} \sigma_{\wo i}^2\\
& = & \frac{\c_i^T\boldmu}{\c_i^T\Sigma\c_i} \sigma_{\wo i}^2 - \frac{\tilde{\mu}_i}{\tilde{\sigma}_i^2} \sigma_{\wo i}^2\\
& = & \sigma_{\wo i}^2 \Bigl( \frac{\c_i^T\boldmu}{\c_i^T\Sigma\c_i} - \frac{\tilde{\mu}_i}{\tilde{\sigma}_i^2} \Bigr),
\end{eqnarray}

\noindent which again matches the result of Equation \ref{eqn:cavityepmgp}.  These cavity parameters $\{\mu_{\wo i},\sigma_{\wo i}^2\}$ are intuitively sensible.  Comparing to the standard EP cavity form ({\it e.g.}, \cite{rasmussenBook}), we see that instead of using the axis aligned univariate marginal defined by $\{\mu_i,\sigma^2_i\}$ (the $i$th entry and diagonal entry of $\mu$ and $\Sigma$, the parameters of the approximation $q(\x)$), the cavities over arbitrary polyhedra require the univariate projection onto the axis $\c_i$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage



\section{Efficient and stable EPMGP implementation}
\label{sec:epmgpstable}

EP gives us a simple form for calculating the parameters of the approximation $q(\x)$ (from which we use the normalising constant $Z$ as the Gaussian probability approximation).   However, there are two potential problems that deserve investigation.  First, each update of $\{\boldmu,\Sigma\}$ naively requires $\mathcal{O}(n^3)$ computation, which may be needlessly burdensome in terms of run time and may introduce numerical instabilities.  Second, as factor parameters $\tilde{\sigma}_i^2$ can tend to infinity ({\it i.e.}, that factor $t_i(\x)$ has no effect on the MGP calculation), we want to reparametrise the calculation for numerical stability.   This reparameterisation is also critical for calculating tail probabilities.

We introduce the natural parameters $\tilde{\tau}_i = \frac{1}{\tilde{\sigma}_i^2}$ and $\tilde{\nu}_i = \frac{\tilde{\mu}_i}{\tilde{\sigma}_i^2}$, and we rewrite Equation \ref{eqn:cavityepmgp} in terms of these parameters, as is standard for EP.  In particular the cavity parameters are calculated as:
%
\begin{eqnarray}
\label{eqn:cavityepmgpnatural}
\mu_{\wo i} & = &  \sigma_{\wo i}^2 \Bigl( \frac{\c_i^T\boldmu}{\c_i^T\Sigma\c_i} - \tilde{\nu}_i \Bigr)
~~~\mathrm{and}~~~
\sigma^2_{\wo i} = \bigl( (\c_i^T\Sigma \c_i)^{-1} - \tilde{\tau}_i\bigr)^{-1}.
\end{eqnarray}
%
We note that we do not reparameterise the cavity parameters (just the site parameters within that calculation), as the cavity parameters remain well behaved and numerically stable.  Next, we rewrite the parameters of $q(\x)$ from Equation \ref{eqn:epmgppost} as  
\begin{equation}
\boldmu  = 
\Sigma\Bigl(K^{-1}\m + \overset{m}{\sum_{i=1}}\tilde{\nu}_i\c_i \Bigr)~~~~~~\mathrm{and}~~~~~\Sigma = \Bigl(K^{-1} +
\overset{m}{\sum_{i=1}} \tilde{\tau}_i\c_i\c_i^T\Bigr)^{-1}
\end{equation}

Again these updates have cubic run time complexity.  Let $\Delta\tilde{\tau}_i = \tilde{\tau}_i^{\mathrm{new}} - \tilde{\tau}_i^{\mathrm{old}}$.  Now we consider updating $\Sigma_{\mathrm{old}}$ to $\Sigma_{\mathrm{new}}$ with an update  $\Delta\tilde{\tau}_i$:

\begin{eqnarray}
\label{eqn:sigmanew}
\Sigma_{\mathrm{new}} & = & \Bigl(K^{-1} + \overset{m}{\sum_{j\ne i}} \tilde{\tau}_j^{\mathrm{old}}\c_j\c_j^T + \tilde{\tau}_i^{\mathrm{new}}\c_i\c_i^T \Bigr)^{-1} \\
& = & \Bigl(K^{-1} + \overset{m}{\sum_{i=1}} \tilde{\tau}_i^{\mathrm{old}}\c_i\c_i^T + \Delta\tilde{\tau}_i\c_i\c_i^T \Bigr)^{-1} \\
& = & (\Sigma_{\mathrm{old}}^{-1} + \Delta\tilde{\tau}_i\c_i\c_i^T )^{-1} \\
& = & \Sigma_{\mathrm{old}} - \Sigma_{\mathrm{old}} \c_i \Bigl(\frac{1}{\Delta\tilde{\tau}_i} + \c_i^T \Sigma_{\mathrm{old}} \c_i\Bigr)^{-1} \c_i^T\Sigma_{\mathrm{old}} \\
& = & \Sigma_{\mathrm{old}} - \Biggl(\frac{\Delta\tilde{\tau}_i }{ 1 +  \Delta\tilde{\tau}_i \c_i^T\Sigma_{\mathrm{old}}\c_i }\Biggr) (\Sigma_{\mathrm{old}} \c_i)(\Sigma_{\mathrm{old}} \c_i)^T,
\end{eqnarray}

\noindent which is conveniently just a rank one (quadratic run time) update of the previous covariance.  By a similar set of steps, we derive an update the mean of $q(\x)$, which gives:

\begin{equation}
\label{eqn:munew}
\boldmu_{\mathrm{new}} = \boldmu_{\mathrm{old}} +    \Biggl( \frac{\Delta\tilde{\nu}_i - \Delta\tilde{\tau}_i \c_i^T\boldmu_{\mathrm{old}} }{ 1 + \Delta\tilde{\tau}_i \c_i^T\Sigma_{\mathrm{old}} \c_i} \Biggr) \Sigma_{\mathrm{old}} \c_i,
\end{equation}

\noindent which again involves only quadratic computations and uses numerically stable natural parameters for the approximate factor $\tilde{t}_i(\x)$ terms.

The final term that warrants consideration is the normalisation constant $Z$, which in fact is our approximation to the high dimensional Gaussian probability and the entire point of the EPMGP algorithm.  Starting from Equation \ref{eqn:logZ} and including the definition of $\tilde{Z}_i$ from Equation \ref{eqn:site}, that term is:

\begin{eqnarray}
\log Z &= & 
-\frac{1}{2}\log\lvert K \rvert 
-\frac{1}{2}\overset{m}{\sum_{i=1}} \log \tilde{\sigma}^2_i
+ \frac{1}{2}\log\lvert \Sigma \rvert 
-\frac{1}{2}\m^TK^{-1}\m \nonumber 
+\frac{1}{2}\boldmu^T\Sigma^{-1}\boldmu
- \frac{1}{2}\overset{m}{\sum_{i=1}}\frac{\tilde{\mu}_i^2}{\tilde{\sigma}_i^2}  
\nonumber \\ && ~~
+ \overset{m}{\sum_{i=1}}\log
\tilde{Z}_i - \frac{m}{2}\log(2\pi) \nonumber \\
& = & 
-\frac{1}{2}\log\lvert K \rvert 
-\frac{1}{2}\overset{m}{\sum_{i=1}} \log \tilde{\sigma}^2_i
+ \frac{1}{2}\log\lvert \Sigma \rvert 
-\frac{1}{2}\m^TK^{-1}\m \nonumber 
+\frac{1}{2}\boldmu^T\Sigma^{-1}\boldmu
- \frac{1}{2}\overset{m}{\sum_{i=1}}\frac{\tilde{\mu}_i^2}{\tilde{\sigma}_i^2}\nonumber
\\ && ~~
+ \overset{m}{\sum_{i=1}}\log
\hat{Z}_i + \frac{m}{2}\log(2\pi) + \frac{1}{2}\overset{m}{\sum_{i=1}}\log (\sigma_{\wo i}^2 + \tilde{\sigma}^2_i) 
+ \frac{1}{2}\overset{m}{\sum_{i=1}}\frac{(\mu_{\wo i} - \tilde{\mu}_i)^2}{(\sigma_{\wo i}^2 + \tilde{\sigma}^2_i)}- \frac{m}{2}\log(2\pi) \nonumber \\
& = & 
-\frac{1}{2}\log\lvert K \rvert 
-\frac{1}{2}\overset{m}{\sum_{i=1}} \log \tilde{\sigma}^2_i
+ \frac{1}{2}\log\lvert \Sigma \rvert 
-\frac{1}{2}\m^TK^{-1}\m \nonumber 
+\frac{1}{2}\boldmu^T\Sigma^{-1}\boldmu
- \frac{1}{2}\overset{m}{\sum_{i=1}}\frac{\tilde{\mu}_i^2}{\tilde{\sigma}_i^2}\nonumber
\\ && ~~
+ \overset{m}{\sum_{i=1}}\log
\hat{Z}_i + \frac{1}{2}\overset{m}{\sum_{i=1}}\log (\sigma_{\wo i}^2 + \tilde{\sigma}^2_i) 
+ \frac{1}{2}\overset{m}{\sum_{i=1}}\frac{(\mu_{\wo i} - \tilde{\mu}_i)^2}{(\sigma_{\wo i}^2 + \tilde{\sigma}^2_i)} \nonumber \\
& = & 
-\frac{1}{2}\log\lvert K \rvert 
+ \frac{1}{2}\log\lvert \Sigma \rvert 
-\frac{1}{2}\m^TK^{-1}\m \nonumber 
+\frac{1}{2}\boldmu^T\Sigma^{-1}\boldmu
- \frac{1}{2}\overset{m}{\sum_{i=1}}\frac{\tilde{\mu}_i^2}{\tilde{\sigma}_i^2}
\nonumber \\ && ~~
+ \overset{m}{\sum_{i=1}}\log
\hat{Z}_i + \frac{1}{2}\overset{m}{\sum_{i=1}}\log (1 + \tilde{\sigma}^{-2}_i\sigma_{\wo i}^2) 
+ \frac{1}{2}\overset{m}{\sum_{i=1}}\frac{(\mu_{\wo i} - \tilde{\mu}_i)^2}{(\sigma_{\wo i}^2 + \tilde{\sigma}^2_i)} \nonumber \\
& = & 
-\frac{1}{2}\log\lvert K \rvert 
+ \frac{1}{2}\log\lvert \Sigma \rvert 
-\frac{1}{2}\m^TK^{-1}\m 
+\frac{1}{2}\boldmu^T\Sigma^{-1}\boldmu
\nonumber \\ && ~~
+ \overset{m}{\sum_{i=1}}\log
\hat{Z}_i + \frac{1}{2}\overset{m}{\sum_{i=1}}\log (1 + \tilde{\sigma}^{-2}_i\sigma_{\wo i}^2) 
+ \frac{1}{2}\overset{m}{\sum_{i=1}}\frac{(\mu_{-i}^2 - 2\mu_{\wo i}\tilde{\mu}_i - \tilde{\mu}^2_i\tilde{\sigma}^{-2}_i\sigma^2_{\wo i})\tilde{\sigma}^{-2}_i}{(1 + \tilde{\sigma}^{-2}_i\sigma_{\wo i}^2) } 
\nonumber \\
& = & 
-\frac{1}{2}\log\lvert K \rvert 
+ \frac{1}{2}\log\lvert \Sigma \rvert 
-\frac{1}{2}\m^TK^{-1}\m \nonumber 
+\frac{1}{2}\boldmu^T\Sigma^{-1}\boldmu
\\ && ~~
+ \overset{m}{\sum_{i=1}}\log
\hat{Z}_i + \frac{1}{2}\overset{m}{\sum_{i=1}}\log (1 + \tilde{\tau}_i\sigma_{\wo i}^2) 
+ \frac{1}{2}\overset{m}{\sum_{i=1}}\frac{\mu_{\wo i}^2\tilde{\tau}_i - 2\mu_{\wo i}\tilde{\nu}_i - \tilde{\nu}^{2}_i\sigma^2_{\wo i}}{(1 + \tilde{\tau}_i\sigma_{\wo i}^2) }, 
\end{eqnarray}

\noindent where the second equation is just expanding the $\log \tilde{Z}_i$ terms, the fourth equation combines the second and eighth terms of the previous equation, the fifth equation combines the fifth and the eighth terms of the previous equation, and the final equation rewrites those in terms of natural parameters. These tedious but straightforward steps enable calculation of $\log Z$ using only the natural parameters of the approximate factors.  Doing so yields a numerically stable implementation.  

Finally, we can go a few steps further to simplify the run time and the numerical stability of calculating $\log Z$.  Since we often want to calculate small probabilities with EPMGP, and do so quickly, these extra steps are important.  Noting the definition of $\Sigma$ (as in Equation \ref{eqn:epmgppost}), we write:

\begin{eqnarray}
\label{eqn:logZstable}
\log Z &= & 
-\frac{1}{2}\log\lvert K \rvert 
+ \frac{1}{2}\log\lvert \Sigma \rvert 
-\frac{1}{2}\m^TK^{-1}\m \nonumber 
+\frac{1}{2}\boldmu^T\Sigma^{-1}\boldmu
\\ && ~~
+ \overset{m}{\sum_{i=1}}\log
\hat{Z}_i + \frac{1}{2}\overset{m}{\sum_{i=1}}\log (1 + \tilde{\tau}_i\sigma_{\wo i}^2) 
+ \frac{1}{2}\overset{m}{\sum_{i=1}}\frac{\mu_{\wo i}^2\tilde{\tau}_i - 2\mu_{\wo i}\tilde{\nu}_i - \tilde{\nu}^{2}_i\sigma^2_{\wo i}}{(1 + \tilde{\tau}_i\sigma_{\wo i}^2) }, \nonumber \\
&= & 
-\frac{1}{2}\log\lvert K \rvert 
- \frac{1}{2}\log\Bigl\lvert     \Bigl(K^{-1} + \overset{m}{\sum_{i=1}} \tilde{\tau}_i\c_i\c_i^T\Bigr)  \Bigr\rvert 
-\frac{1}{2}\m^TK^{-1}\m \nonumber 
+\frac{1}{2}\boldmu^T\Bigl( K^{-1} + \overset{m}{\sum_{i=1}} \tilde{\tau}_i\c_i\c_i^T\Bigr ) \boldmu
\\ && ~~
+ \overset{m}{\sum_{i=1}}\log
\hat{Z}_i + \frac{1}{2}\overset{m}{\sum_{i=1}}\log (1 + \tilde{\tau}_i\sigma_{\wo i}^2) 
+ \frac{1}{2}\overset{m}{\sum_{i=1}}\frac{\mu_{\wo i}^2\tilde{\tau}_i - 2\mu_{\wo i}\tilde{\nu}_i - \tilde{\nu}^{2}_i\sigma^2_{\wo i}}{(1 + \tilde{\tau}_i\sigma_{\wo i}^2) }, \nonumber \\
&= & 
-\frac{1}{2}\log\Bigl\lvert     \Bigl(I + \overset{m}{\sum_{i=1}} \tilde{\tau}_i(L^T\c_i)(L^T\c_i)^T\Bigr)  \Bigr\rvert 
-\frac{1}{2}\m^TK^{-1}\m 
+\frac{1}{2}\boldmu^TK^{-1}\boldmu 
+\frac{1}{2}\overset{m}{\sum_{i=1}} \tilde{\tau}_i(\c_i^T\boldmu)^2 \nonumber
\\ && ~~
+ \overset{m}{\sum_{i=1}}\log
\hat{Z}_i + \frac{1}{2}\overset{m}{\sum_{i=1}}\log (1 + \tilde{\tau}_i\sigma_{\wo i}^2) 
+ \frac{1}{2}\overset{m}{\sum_{i=1}}\frac{\mu_{\wo i}^2\tilde{\tau}_i - 2\mu_{\wo i}\tilde{\nu}_i - \tilde{\nu}^{2}_i\sigma^2_{\wo i}}{(1 + \tilde{\tau}_i\sigma_{\wo i}^2) },
\end{eqnarray}

\noindent where $K = LL^T$ is the Cholesky factorisation of the prior covariance $K$.  We can precompute $L$ at the beginning of the algorithm, and hence the terms $\m^TK^{-1}\m$ and $\boldmu^TK^{-1}\boldmu$ are also easy to solve.  Then, all terms are highly numerically stable, and only that first log determinant has cubic run time.  Importantly, we never have to invert or solve $\Sigma$, and we are able to operate solely in the space of natural parameters for the factor approximations.

To sum up, these steps give us fast and numerically stable implementations for calculating the updates to $q(\x)$ and for calculating the final normalising constant, which is the result of our EPMGP algorithm.   Our simple MATLAB implementation can be downloaded from {\tt <url to come with publication>}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage

\section{Derivatives of the EP result $\log Z$}
\label{sec:deriv}

The main text has claimed that an advantage of EP methods is that they provide an analytical approximation to the Gaussian (log) probability $\log Z$ such that derivatives can be taken with respect to parameters of the Gaussian $p_0(\x) = \mathcal{N}(\x; \m, K)$.  We produce those derivatives here as reference.  Throughout applied uses of EP, derivatives with respect to $\log Z$ are often interesting, as $\log Z$ in a standard approximate inference model corresponds to the marginal likelihood of the observed data. One often wants to optimise (and thus take derivatives of) this quantity with respect to parameters of the prior $p_0(\x)$.  Two useful references for discussion and application of these derivatives are \cite{seeger08epexpfam, rasmussenBook}.  Most importantly, we leverage the result from \cite{seeger08epexpfam} (Section 2.1), where it is shown that the converged messages - the approximate factor parameters $\{\tilde{Z}_i,\tilde{\mu}_i,\tilde{\sigma}_i^2\}$ - have 0 derivative with respect to parameters of the prior $p_0(\x)$.  This consideration is sometimes described as a distinction between \emph{explicit} dependencies (where $\log Z$ actually has the prior terms $\{\m,K\}$) and \emph{implicit} dependencies where a message parameter such as $\tilde{Z}_i$ might depend implicitly on $\{\m,K\}$ (as in \cite{rasmussenBook}).  The fact that EP converges to a fixed point of its objective function, and thus makes the latter implicit derivatives equal 0, simplifies our task considerably. 

The simplest place to begin is with the original form of the normaliser in Equation \ref{eqn:logZ}.  We note that the second line of that equation is entirely due to the approximate factors, so by \cite{seeger08epexpfam}, we need only consider the first and third lines, yielding:

\begin{equation}
\label{eqn:dlogZ}
\log Z  ~~~ = ~~~ 
\frac{1}{2}\boldmu^T\Sigma^{-1}\boldmu
+ \frac{1}{2}\log\lvert \Sigma \rvert
-\frac{1}{2}\m^TK^{-1}\m 
-\frac{1}{2}\log\lvert K \rvert  + \mathrm{constants}.
\end{equation}

Typically, these terms can be simplified into the familiar form of a normaliser of a product of Gaussians ({\it e.g.} A.7 in \cite{rasmussenBook}).   However, because the factor contributions need not be full rank, that standard treatment requires a bit more detail.  First, we revisit the definition of $\{\boldmu, \Sigma\}$:

\begin{equation}
\boldmu  =
\Sigma\Bigl(K^{-1}\m + \r \Bigr),
~~~~~~\Sigma = \Bigl(K^{-1} + RR^T \Bigr)^{-1},
\end{equation}

\noindent where we have hidden several of the approximate factor parameters with:

\begin{equation}
\label{eqn:bterms}
\r  = \overset{m}{\sum_{i=1}}\frac{\tilde{\mu}_i}{\tilde{\sigma}^2_i}\c_i,
~~~~~~
RR^T = \overset{m}{\sum_{i=1}} \frac{1}{\tilde{\sigma}^2_i}\c_i\c_i^T.
\end{equation}

\noindent $RR^T$ is a singular value decomposition of the matrix above (in general $R$ will not be square).  This simplification allows us to focus only on the parts of the equation that are dependent on $\{\m,K\}$.  Thus we can rewrite $\log Z$ as:

\begin{eqnarray}
\label{eqn:dlogZ2}
\log Z  ~~~&=&~~~ 
-\frac{1}{2}\log\lvert K \rvert 
+ \frac{1}{2}\log\lvert (K^{-1} + RR^T)^{-1} \rvert + \mathrm{constants}
\\ && ~~~~~
+ \frac{1}{2}(K^{-1}\m + \r)^T(K^{-1} + RR^T)^{-1}(K^{-1}\m + \r)
-\frac{1}{2}\m^TK^{-1}\m
\nonumber
\\
%~~~& = & ~~~
%-\frac{1}{2}\log\lvert (I + R^TKR) \rvert 
%\\ && ~~~~~
%+ \frac{1}{2}(K^{-1}\m + \r)^T(K^{-1} + RR^T)^{-1}(K^{-1}\m + \r)
%-\frac{1}{2}\m^TK^{-1}\m
%\nonumber
%\\
~~~& = & ~~~
-\frac{1}{2}\log\lvert (I + R^TKR) \rvert + \mathrm{constants}
\\ && ~~~~~
-\frac{1}{2} (\m -K\r)^T\Bigl(R(I + R^TKR)^{-1}R^T\Bigr)(\m - K\r) + \r^T(\m + \frac{1}{2}K\r)
\nonumber
\end{eqnarray}

\noindent where the second line comes from combining the two determinants and using Sylvester's determinant rule, and the third line comes from the typical algebra of using the matrix inversion lemma to rearrange these terms.  Some readers may prefer the second or the third line;  we use the third as it prevents any explicit inversions of $K$ and allows significant computational savings if the number of columns of $R$ is smaller than that of $K$.

With that setup, we consider two possible and common derivative quantities of interest.  First and most simply, we may want the gradient with respect to the Gaussian mean $\m$:

\begin{equation}
\nabla_{\m} \log Z = - \Bigl(R(I + R^TKR)^{-1}R^T\Bigr)(\m - K\r) + \r
\end{equation}

\noindent This form has a closed form solution and is instructive for two reasons.  First, we note that if $RR^T$ is not full rank, then $\m$ is underdetermined.  This is intuitively reasonable, as a subrank $RR^T$ means that the polyhedron is open in some dimension ({\it e.g.} a slab), and thus there should be a linear subspace of optimal positions for the Gaussian.  Second, this solution is interesting in its connections to centering algorithms as previously mentioned (see for example Appendix \ref{sec:minpoly}).  This optimised mean provides a center for a $K$ shaped Gaussian within the polyhedron $\regionA$.  Understanding the geometric implications of this centering approach is an interesting question, but well beyond the scope of this work. 

Second,  we can consider parameterised $K$.  We presume there is some parameter $\theta$ (such as a kernel hyperparameter) on the covariance $K = K(\theta)$.  Then we see: 

\begin{eqnarray}
\label{eqn:dlogZ3}
\frac{\partial \log Z}{\partial \theta}
~~& = & ~~~
\frac{\partial}{\partial \theta} \Biggl[ -\frac{1}{2}\log\lvert (I + R^TKR) \rvert    \nonumber
\\ && ~~~~~~~~~~
-\frac{1}{2} (\m -K\r)^T\Bigl( R(I + R^TKR)^{-1}R^T\Bigr)(\m - K\r) + \r^T(\m + \frac{1}{2}K\r) \Biggr]
\nonumber
\\
&& \nonumber
\\
~~& = & ~~~
-\frac{1}{2}\text{trace}\left( R(I + R^TKR)^{-1}R^T\frac{\partial K}{\partial \theta} \right)   \nonumber
\\ && ~~~
+ \r^T \frac{\partial K}{\partial \theta} \Bigl(R(I + R^TKR)^{-1}R^T\Bigr)(\m - K\r)  \nonumber
\\ && ~~~
+ \frac{1}{2} (\m -K\r)^T\Bigl(R(I + R^TKR)^{-1}R^T\frac{\partial K}{\partial \theta}R(I + R^TKR)^{-1}R^T\Bigr)(\m - K\r)  \nonumber
\\ && ~~~
+ \frac{1}{2}\r^T\frac{\partial K}{\partial \theta}\r.
\end{eqnarray}

\noindent Within this form, the matrix $R(I + R^TKR)^{-1}R^T$ reverts to the familiar inverse sum of covariances when $R$ is full rank.  Thus, though this form may look different, it is simply a subrank version of the familiar product of Gaussians marginalisation constant as often used in marginal likelihood derivatives for EP.

Finally, we note that we have only focused on derivatives with respect to parameters of the Gaussian $p_0(\x)$.  One might also want to take parameters with respect to the region $\regionA$.  The convenience of ignoring implicit dependencies would no longer hold in such a case, so taking these derivatives would involve taking more complicated derivatives of the approximate factors.  As this is an aside to the current report, we leave that investigation for future work.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage

\section{Minimalising polyhedral regions $\regionA$}
\label{sec:minpoly}

Our box-function definition of polyhedra, or similarly the more conventional halfspace definition, does not in general uniquely specify a polyhedral region $\regionA$.   For any polyhedron defined by $\regionA = \prod_i t_i(\x)$ (our conventional definition throughout the paper), we can always add another constraint $t_j(\x)$ such that $\regionA = \prod_i t_i(\x) = t_j(\x) \prod_i t_i(\x)$.   This redundant constraint could be colinear with an existing $t_i(\x)$ factor that includes at least the $[l_i,u_i]$ interval, but it can also be any constraint such that the entire projection of $\regionA$ onto $\c_j$, the rank one dimension of $t_j(\x)$, is contained inside $[l_j,u_j]$.  Such factors are typically called inactive constraints.  

Importantly, these inactive constraints do not change the polyhedron and therefore do not change the probability $F(\regionA)$ of any Gaussian across this region.  On the other hand, inactive constraints do change the representation and thus the EP approximation.  Given our discussion of the extra mass and redundancy issues (Sections \ref{sec:results} and \ref{sec:discussion}), removing these inactive constraints can potentially improve the accuracy of our EP algorithms.  To do so, we define a minimal representation of any polyhedron $\regionA$ to be the polyhedron where all constraints are active and unique.  This can be achieved by pulling in all inactive constraints until they support the polyhedron $\regionA$ and are thus active, and then we can prune all repeated/redundant constraints.  These steps are solvable in polynomial time as a series of linear programs, and they bear important connection to problems considered in centering (analytical centering, Chebyshev centering, etc.) for cutting-plane methods and other techniques in convex optimisation ({\it e.g.}, see \cite{boydBook}).    The first problem one might consider is to find a point on the interior of the polyhedron, which can be solved by introducing a slack variable $t$ into the linear program:  

\begin{equation}
\label{eqn:lp0}
\begin{split}
\underset{\{\x,t\}}{\mathrm{minimise}} & ~~~~t   \\
\text{subject\thinspace to} & ~~~-\c_i^T\x + l_i  <  t ~~\forall~ \text{$i$ = 1{\ldots}$m$}\\
& ~~~~~~\c_i^T\x - u_i  <  t ~~\forall~ \text{$i$ = 1{\ldots}$m$}.
\end{split}
\end{equation}

This program will either return an interior point $\x$ and a negative slack $t$ or will return a positive $t$, which is a certificate that the polyhedron is in fact empty.  Standard LP solvers can be used to find this result.  Next, we can consider each factor $t_j(\x)$ to test if it is inactive.  First to consider the lower bound, for all $j$ we solve: 

\begin{equation}
\label{eqn:lp1}
\begin{split}
\underset{\x}{\mathrm{minimise}} & ~~~~\c_j^T\x   \\
\text{subject\thinspace to} & ~~~-\c_i^T\x + l_i  <  t ~~\forall~ \text{$i$ = 1{\ldots}$m$}\\
& ~~~~~~\c_i^T\x - u_i  <  t ~~\forall~ \text{$i$ = 1{\ldots}$m$}.
\end{split}
\end{equation}

\noindent Having solved for the lower bounds, we then solve the upper bound problems for all $j$: 

\begin{equation}
\label{eqn:lp2}
\begin{split}
\underset{\x}{\mathrm{maximise}} & ~~~~\c_j^T\x   \\
\text{subject\thinspace to} & ~~~-\c_i^T\x + l_i  <  t ~~\forall~ \text{$i$ = 1{\ldots}$m$}\\
& ~~~~~~\c_i^T\x - u_i  <  t ~~\forall~ \text{$i$ = 1{\ldots}$m$}.
\end{split}
\end{equation}

With the answers for these $2m$ LPs, we can now make all constraints active.  If the solution to Equation \ref{eqn:lp1} is larger than $l_j$, then we know that the lower bound $l_j$ is inactive.  In words, this means that the other constraints have cut off $\regionA$ inside of $l_j$ in direction $\c_j$.  Equivalently, if the solution to Equation \ref{eqn:lp2} is smaller than the upper bound $u_j$, we know $u_j$ is inactive.  If both lower and upper bounds are found to be inactive, the factor $t_j(\x)$ is entirely redundant and can be pruned from the representation without changing $\regionA$.  Since we know redundancy can lead to EP underestimates, pruning can be useful (see for example Panel A of Figure \ref{fig:resultspathology}).  If only one of the lower or upper bounds is found to be inactive by Equation \ref{eqn:lp1} or \ref{eqn:lp2}, then this bound can be tightened to {\it activate} it, at which point it will support the polyhedron $\regionA$.   The value it should be tightened to is the result of the corresponding LP that found that bound to be inactive.  This tightening operation would prevent errors such as those seen in Panel B of Figure \ref{fig:resultspathology}.  One small nuisance is simple repeated active factors, as each will technically appear active.  However, this case can be dealt with by checking for such repetitions in preprocessing, and thus is not a difficulty in practice.  

With these steps done, we have a minimal representation of the polyhedron $\regionA$.  This set of operations is convenient because using it as a preprocessing step will ensure that each polyhedron is uniquely represented, and thus results from EP and other such approaches (as with centering algorithms) are consistent.  However,  in our results and our presentation we do not minimalise polyhedra to avoid adding additional complexity.  Furthermore, as in Figure \ref{fig:resultspathology}, doing so can mask other features of EP that we are interested in exploring.  Anecdotally, we report that while minimalising polyhedra \emph{will} change EP's answer, the effect is not particularly large in these randomly sampled polyhedral cases.   
   



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage


\section{Minimising KL-divergence does moment matching}
\label{sec:KL}

In this section, we review the equivalence between moment matching and minimising the unnormalised KL-divergence in an exponential family. This fact is often mentioned without proof in the literature (as for example in~\cite{minkaMSFTTR2005}) -- we provide here the derivation for completeness. In particular, this implies that minimising the KL-divergence corresponds to matching the zeroth, first, and second moments of $q(\x)$ to $p(\x)$, when the approximating family is Gaussian.  As approximately minimising the KL-divergence is the goal of EP, this result will show that EPMGP is an appropriate choice to solve the Gaussian probability calculation (zeroth moment). We provide the general derivation here.

First, as defined in the main text, $p(\x)$ and $q(\x)$ do not normalise to 1. Thus, we use the general definition of the KL-divergence for non-negative unnormalised distributions $p(\x)$ and $q(\x)$:
\begin{equation}
\label{eqn:KLfg}
D_{KL}\bigl(p(\x)\parallel q(\x)\bigr) = \int p(\x) \log \frac{p(\x)}{q(\x)} d\x
~~+~~\int q(\x)d\x ~~-~~ \int p(\x)d\x,
\end{equation}
as in \cite[]{zhu95infogeom, minkaMSFTTR2005}.  Note that the typically-seen normalised KL-divergence ({\it e.g.}, \cite{CoverandThomas}) is recovered when both $p(\x)$ and $q(\x)$ normalise to 1.

We are interested in the distribution $q(\x)$ that minimises $D_{KL}\bigl(p(\x) \parallel q(\x)\bigr)$ when $q(\x)$ is restricted to be in an (unnormalised) exponential family, and where $p(\x)$ could be arbitrary. The exponential families are flexible parametric families of distributions which contain many familiar families such as the Gaussian, Dirichlet, Poisson, etc.~\cite[]{bishopBook,seeger03phd}. Reusing the notation from~\cite{seeger03phd}, an exponential family is defined by specifying the sufficient statistics vector function $\phi(\x)$\footnote{Strictly speaking, the base measure $\mu$ for the density also needs to be specified (e.g. the counting measure for a Poisson distribution). For simplicity of presentation, we will show our derivations with the Lebesgue measure (appropriate for the Gaussian distribution e.g.), though the result holds for any exponential family in the natural parameterisation.}, and so the unnormalised $q(\x)$ takes the form:
%
\begin{equation}
\label{eqn:expfam2}
q(\x) = Z_q\exp\Bigl\{\boldtheta^T\phi(\x) -
\Phi(\boldtheta)\Bigr\},
\end{equation}
%
where $\Phi(\boldtheta) =
\log\int\exp\Bigl\{\boldtheta^T\phi(\x)\Bigr\}d\x$ is the log-partition function (the usual normaliser) and $Z_q$ is an extra parameter accounting for the unnormalised $q(\x)$. Substituting this form of $q(\x)$ in~\eqref{eqn:KLfg}, we obtain:
\begin{align}
\label{eqn:KLpq}
D_{KL}\bigl(p(\x) \parallel q(\x)\bigr)
& = \int p(\x) \left(\Phi(\boldtheta) - \boldtheta^T\phi(\x) - \log{Z_q} \right)d\x +Z_q-\int p(\x)d\x - H[p] \\
& = Z_p \Phi(\boldtheta) - \boldtheta^T \int \phi(\x) p(\x) d\x  - Z_p \log{Z_q} + (Z_q-Z_p) - H[p],
\end{align}
where $H[p] = -\int p(\x) \log{p(\x)} d\x$ is the entropy of $p$ (a constant with respect to $q$), and we defined the normalisation constant of $p$ as $Z_p = \int p(\x) d\x$. To minimise this KL, we
first take the derivative with respect to the normaliser of $q(\x)$,
namely $Z_q$:
\begin{eqnarray}
\label{eqn:dZ}
\frac{d}{dZ_q}D_{KL}\bigl(p(\x) \parallel q(\x)\bigr)
& = & -~\frac{Z_p}{Z_q} + 1 = 0 \\
\label{eqn:Zstar}
& \implies & Z_q^* = Z_p = \int p(\x)d\x.
\end{eqnarray}
The second derivative is $1/Z_q^2 > 0$, so this is the unique global minimum.

From this, we see that minimising the KL-divergence with respect to the normalising constant $Z_q$ matches the zeroth moments, as we expected.  Indeed, it is this equation that motivates the entire EPMGP approach. We obtain the optimal natural parameter $\boldtheta^*$ of $q(\x)$ by setting its derivative of the KL to zero:
%
\begin{eqnarray}
\label{eqn:dtheta}
\frac{d}{d\boldtheta}D_{KL}\bigl(p(\x) \parallel q(\x)\bigr)
& = & Z_p \nabla_{\boldtheta}\Phi(\boldtheta)- \int p(\x)\phi(\x)d\x = \mathbf{0} \\
& \implies & \nabla_{\boldtheta}\Phi(\boldtheta) = \int \frac{p(\x)}{Z_p}\phi(\x)d\x \label{eqn:p-moment}.
\end{eqnarray}
It is a standard fact of the exponential family that the gradient of the log-partition function is equal to the expected sufficient statistics; to see this, from the definition of $\Phi(\cdot)$ (after Equation~\ref{eqn:expfam2} above), we have:
%
\begin{eqnarray}
\label{eqn:dphi}
\nabla_{\boldtheta}\Phi(\boldtheta)
& = & \frac{\int\phi(\x)\exp\Bigl\{\boldtheta^T\phi(\x)\Bigr\}d\x}
{\int\exp\Bigl\{\boldtheta^T\phi(\x)\Bigr\}d\x} \\
& = & \int\phi(\x)\exp\Bigl\{\boldtheta^T\phi(\x) -
\Phi(\boldtheta)\Bigr\}d\x \\ \label{eqn:Eqphi}
& = & \int \frac{q(\x)}{Z_q}\phi(\x)d\x.
\end{eqnarray}
Combining~\eqref{eqn:Eqphi} with~\eqref{eqn:p-moment}, we get that the expected sufficient statistics of $q$ need to match those of $p$, namely, the higher-order moment matching condition, as we wanted to show. The second derivative with respect to $\boldtheta$ of the KL-divergence is the Hessian of the log-partition function, which is strictly positive definite if the exponential family is \emph{minimal}~\cite[]{wainwright08variational} and in this case gives a unique $\boldtheta^*$; otherwise, the moment matching condition still gives a global minimum, but there can be multiple parameters with this property.  For the Gaussian family, the sufficient statistics are a vector of all elements $x_i$ and all pairs of elements $x_i x_j$. This means that the moment matching condition is to match the zeroth, first and second order statistics.

To summarise, these final equations~\eqref{eqn:Zstar}, \eqref{eqn:Eqphi} and ~\eqref{eqn:p-moment} tell us that, to uniquely minimise the global KL-divergence between the truncated distribution $p(\x)$ (or any target distribution) and the Gaussian $q(\x)$, we do two things: first, we set $Z$ to be the total mass (zeroth moment) of $p(\x)$; and second, we set $\boldtheta$, the natural parameters of $q(\x)$ such that the mean (first moment) and covariance (second moment) of $q(\x)$ equal exactly the first and second moments of $p(\x)$. 

As it pertains to calculating Gaussian probabilities, minimising the KL-divergence calculates the zeroth moment of $p(\x)$, which is exactly $F(\regionA)$, the probability of interest.  As such, EPMGP is a sensible choice for multivariate Gaussian probability calculations.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage

\bibliography{epmgp}

\end{document}







\section{Correction and Local Search Steps in Algorithm \ref{alg:algInfadaptive}}

Algorithm~\ref{alg:algReopt} details the $\CORRECTION$ procedure used in line~16 of Algorithm~\ref{alg:algInfadaptive} to implement the correction step of the FCFW algorithm.
It uses the modified Frank-Wolfe algorithm (FW with away steps), as detailed in Algorithm~\ref{alg:MFW}.
Algorithm~\ref{alg:algLocalSearch} depicts the $\LOCALSEARCH$ procedure used in line~17 of Algorithm~\ref{alg:algInfadaptive}. The local search is performing FW over $\Meps$ for
a fixed $\shrinkAmount$ using the iterated conditional mode algorithm as an approximate
FW oracle. This enables the finding in a cheap of way of more vertices to augment
the correction polytope $V$.


\begin{algorithm}
	\caption{Re-Optimizing over correction polytope $V$ using MFW, $f$ is the negative TRW objective}
	\label{alg:algReopt}
	\begin{algorithmic}[1]
		\STATE $\CORRECTION(\x^{(0)},V,\shrinkAmount,\vrho)$
		\STATE Let $f(\cdot) := \text{-TRW}(\cdot;\vtheta,\vrho)$; we use MFW to optimize over the contracted correction polytope $\conv(V_\shrinkAmount)$ 
		where $V_\shrinkAmount := (1-\shrinkAmount) V + \shrinkAmount \unif$.
		\STATE Let $\epsilon$ be the desired accuracy of the approximate correction.
		\STATE Let $\bm{\alpha}^{(0)}$ be such that $\x^{(0)} = \sum_{\vv \in V_\shrinkAmount} \alpha^{(0)}_\vv \vv$.
		\STATE $\x^{(\mathrm{new)}} \leftarrow \textbf{MFW}(\x^{(0)}, \bm{\alpha}^{(0)}, V_\shrinkAmount, \stopCrit)$ \algComment{see Algorithm~\ref{alg:MFW}}
		\STATE \textbf{return} $\x^{(\mathrm{new)}}$
	\end{algorithmic}
\end{algorithm}

\begin{algorithm}
	\caption{Local Search using Iterated Conditional Modes, $f$ is the negative TRW objective}
	\label{alg:algLocalSearch}
	\begin{algorithmic}[1]
		\STATE $\LOCALSEARCH(\x^{(0)},\vv_{\mathrm{init}},\shrinkAmount,\vrho)$
		\STATE $\s^{(0)}\leftarrow \vv_{\mathrm{init}}$
		\STATE $V\leftarrow {\emptyset}$
		\FOR{$k=0\ldots \MAXITS$}
		 \STATE $\tilde{\theta} = \nabla f(\xk;\vtheta,\vrho)$
		 \STATE $\snext\leftarrow $ICM$(-\tilde{\theta},\sk)$ \quad\emph{\small (Approximate FW search using ICM; \\ \hspace{35mm} we initialize ICM at previously found vertex $\sk$)}
		 \STATE $\snext_{(\shrinkAmount)} \leftarrow (1-\shrinkAmount)\s^{(k+1)}+\shrinkAmount\unif$
		 \STATE $V\leftarrow V \cup \{\snext\}$
		 \STATE $\dkeps\leftarrow\snext_{(\shrinkAmount)}-\x^{(k)}$
		 \STATE Line-search: $\stepsize_k \in \displaystyle\argmin_{\stepsize \in [0,1]} \textstyle f\left(\x^{(k)} + \stepsize \dkeps\right)$
 		 \STATE Update $\x^{(k+1)} := \x^{(k)} + \stepsize_k  \dkeps$  \algComment{FW update}
		\ENDFOR 
		\STATE \textbf{return} $\xnext,V$
	\end{algorithmic}
\end{algorithm}

%
%
%
%
%
%
%

\section{Comparison to perturbAndMAP}
%

\textbf{Perturb \& MAP.} We compared the performance between our method and perturb \& MAP for 
inference on $10$ node Synthetic cliques. We expand on the method we used to evaluate perturbAndMAP in Figure~\ref{fig:syntheticComplete_tightening_l1} 
and~\ref{fig:syntheticComplete_tightening_logz}. 
We re-implemented the algorithm to estimate the partition function in Python (as described in \citet{hazan2012partition}, Section 4.1) 
and used toulbar2~\citep{allouche2010toulbar2}
to perform MAP inference over an inflated graph where every variable maps to
five new variables. 
The log partition function is estimated as the mean energy of 10 exact MAP calls
on the expanded graph where the single node potentials are perturbed by draws from the Gumbel distribution.
To extract marginals, we fix the value of a variable to every assignment, estimate the log partition function 
of the conditioned graph and compute beliefs based on averaging the results of adding the unary potentials
to the conditioned values of the log partition function. 
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%



%
\section{Correction Steps for Frank-Wolfe over $\MARG$}
Recall that the correction step is done over the correction polytope, 
the set of all vertices of $\MARG$ encountered thus far in the algorithm.
On experiments conducted over $\MARG$, we found that using a better correction algorithm often \emph{hurt} performance.
This potentially arises in other constrained optimization problems
where the gradients are unbounded at the boundaries of the polytope. We found that better correction steps
over the correction polytope (the convex hull of the vertices explored by the MAP solver, denoted $V$ in Algorithm \ref{alg:algInfadaptive}), often resulted in a solution at or near a boundary of the marginal polytope (shared
with the correction polytope).
This resulted in the iterates becoming too small. We know that the Hessian of $\TRW$ is ill conditioned
near the boundaries of the marginal polytope. Therefore, we hypothesize that this is because the 
gradient directions obtained
when the iterates became too small are simply less informative.
Consequently, the optimization over $\MARG$ suffered. We found that the duality gap over $\MARG$ 
would often increase after a correction step when this phenomenon occurred.
The variant of our algorithm based on $\Meps$ is less sensitive to this issue since the restriction 
of the polytope bounds the smallest marginal and therefore also controls the quality of the gradients obtained.

\section{Additional Experiments}

For experiments on the $10$ node synthetic cliques, we can also track the average number of ILP calls required to converge to a fixed 
duality gap
for any $\theta$. This is depicted in Figure~\ref{fig:syntheticILPvstheta}. 
Optimizing over $\TREEPOL$ realized three to four times as many MAP calls as the first iteration of inference. 

Figure~\ref{fig:chineseChar_appendix} depicts additional examples from the Chinese Characters test set. Here, we also visualize results
from a wrapper around TRBPs implementation in libDAI~\citep{Mooij_libDAI_10} that performs tightening over $\TREEPOL$. 
Here too we find few gains over optimizing over $\mathbb{L}$.

Figure~\ref{fig:MvsM_eps_l1_1},~\ref{fig:MvsM_eps_l1_2} depicts the comparison of convergence of algorithm variants 
over $\MARG$ and $\Meps$ (same setup as Figure~\ref{fig:M_M_eps},~\ref{fig:M_M_eps2}. Here, we plot~$\zeta_{\mu}$.
%
%
%
%
%
%
%
%
%
%
%
%
%

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{./images/chineseChar/appendix_chinese.pdf}
\caption{Chinese Characters : Additional Experiments. TRBP (opt) denotes our implementation of tightening over $\TREEPOL$ using a wrapper over libDAI~\citep{Mooij_libDAI_10}}
\label{fig:chineseChar_appendix}
\end{figure}

\begin{figure}
\centering
\subfigure[\small{$\zeta_{\mu}$: $5\times5$ grid, $\MARG$ vs $\MARG_{\shrinkAmount}$}]{
	\includegraphics[height=6cm,width=4cm,keepaspectratio]{./images/M_M_eps/gridWi5Tr_M_M_eps_l1_maxrho1.pdf}
\label{fig:MvsM_eps_l1_1}
}
\centering
\subfigure[\small{$\zeta_{\mu}$: $10$ node clique, $\MARG$ vs $\MARG_{\shrinkAmount}$}]{
\includegraphics[height=6cm,width=4cm,keepaspectratio]{./images/M_M_eps/synthetic_M_M_eps_l1_maxrho1.pdf}
\label{fig:MvsM_eps_l1_2}
}
\centering
\subfigure[\small{Average ILP calls versus $\theta$: $10$ node clique}]{
\includegraphics[height=4cm,width=5cm,keepaspectratio]{./images/Synthetic/SyntheticILPcallsVsTheta.pdf}
\label{fig:syntheticILPvstheta}
}
\caption{Figure~\ref{fig:MvsM_eps_l1_1},~\ref{fig:MvsM_eps_l1_2} depict $\zeta_{\mu}$ corresponding to the experimental setup in Figure~\ref{fig:M_M_eps},~\ref{fig:M_M_eps2} respectively. Figure \ref{fig:syntheticILPvstheta} explores the average number of ILP calls taken to convergence with and without optimizing over $\vrho$} 
\vspace{-3mm}
\end{figure}
\section{Bounding the Sub-optimality for Fixed $\shrinkAmount$ \label{sec:theory_fixed_eps} Variant}
The pseudocode for optimizing over~$\DOMeps$ for a fixed~$\shrinkAmount$ is given in Algorithm~\ref{alg:adaptive_eps} (by ignoring the step~10 which updates $\shrinkAmount$).
It is stated with a stopping criterion $\stopCrit$, but it can alternatively
be run for a fixed number of $K$ iterations.
The following theorem bounds the suboptimality of the iterates with respect to the 
true optimum $\x^*$ over $\DOM$. If one can compute the constants in the theorem,
one can choose a target contraction amount $\shrinkAmount$ to guarantee
a specific suboptimality of $\stopCrit'$; otherwise, one can choose $\shrinkAmount$
using heuristics. Note that unlike the adaptive-$\shrinkAmount$ variant, this algorithm
does not converge to the true solution as $K \rightarrow \infty$ unless $\x^*$ happens
to belong to $\DOMeps$. But the error can be controlled by choosing $\shrinkAmount$ small enough.

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\begin{theorem}[Suboptimality bound for fixed-$\shrinkAmount$ algorithm]
  \label{thm:convergence_fixed_eps}
  Let $f$ satisfy the properties in Problem~\ref{prop:generic_fxn} and suppose its gradient is Lipschitz continuous on the contractions $\DOMeps$ as in Property~\ref{prop:prop_bounded_grad}. Suppose further that $f$ is finite on the boundary of $\DOM$. 
  
  Then $f$ is uniformly continuous on $\DOM$ and has a \emph{modulus of continuity} function $\modContinuity$ quantifying its level of continuity, i.e. 
  $|f(\x) - f(\x')| \leq \modContinuity(\|\x - \x'\|) \,\, \forall \x, \x' \in \DOM$, with $\modContinuity(\sigma) \downarrow 0$ as $\sigma \downarrow 0$.
  
  Let $\x^*$ be an optimal point of $f$ over $\DOM$. The iterates $\xk \in \DOMeps$ of the FCFW algorithm as described in Algorithm~\ref{alg:adaptive_eps} for a fixed $\shrinkAmount > 0$ has sub-optimality over~$\DOM$ bounded as:
	\begin{equation} \label{eq:rate_fixed_delta}
	f(\xk) - f(\x^*) \leq \frac{2\mathcal{C}_{\shrinkAmount}}{(k+2)}+\modContinuity \left( \shrinkAmount \diam(\DOM)\right), 
	\end{equation}
	 where $C_{\shrinkAmount} \leq \diam(\DOMeps)^2 L_{\shrinkAmount}$. Note that different norms can be used in the definition of $\modContinuity(\cdot)$ and $C_\shrinkAmount$.
\end{theorem}
\begin{proof}
Let $\xopteps$ be an optimal point of $f$ over $\DOMeps$. As $f$ has a Lipschitz continuous gradient over $\DOMeps$, we can use any standard convergence result of the Frank-Wolfe algorithm to bound the suboptimality of the iterate $\xk$ over $\DOMeps$. Algorithm~\ref{alg:adaptive_eps} (with a fixed~$\shrinkAmount$) describes the FCFW algorithm which guarantees at least as much progress as the standard FW algorithm (by step~15 and~20a), and thus we can use the convergence result from~\citet{jaggi2013revisiting} as
already stated in~\eqref{eqn:fw_theorem_convergence_original}: $f(\xk)-f(\xopteps) \leq \frac{2C_{\shrinkAmount}}{(k+2)}$ with $C_{\shrinkAmount} \leq \diam(\DOMeps)^2 L_{\shrinkAmount}$, where $L_{\shrinkAmount}$ comes from Property~\ref{prop:prop_bounded_grad}. This gives the first term in~\eqref{eq:rate_fixed_delta}. Note that if the function $f$ is \emph{strongly} convex, then the FCFW algorithm has also a linear convergence rate~\citep{lacoste2015MFW}, though we do not cover this here.

We now need to bound the difference $f(\xopteps) - f(\xopt)$ coming from the fact that we are not optimizing
over the full domain, and giving the second term in~\eqref{eq:rate_fixed_delta}. We let $\xoptshift$ be the contraction of $\xopt$ on $\DOMeps$ towards $\unif$, i.e.
$\xoptshift := (1-\shrinkAmount) \xopt + \shrinkAmount \unif$.\footnote{Note that without a strong convexity assumption on $f$, the optimum over $\DOMeps$, $\xopteps$, could be quite far from the optimum over $\DOM$, $\xopt$, which is why we need to construct this alternative close point to $\xopt$.} Note that $\| \xoptshift - \xopt \| = \delta \| \xopt - \unif \| \leq  \delta \diam(\DOM)$, and thus can be made arbitrarily small by letting $\shrinkAmount \downarrow 0$. Because $\xoptshift \in \DOMeps$, we have that $f(\xoptshift) \geq f(\xopteps)$ as $\xopteps$ is optimal over~$\DOMeps$.
Thus $f(\xopteps) - f(\xopt) \leq f(\xoptshift) - f(\xopt) \leq \modContinuity(\| \xoptshift - \xopt\|)$ by the uniform continuity of~$f$ (that we explain below). Since~$\modContinuity$ is an increasing function, we have $\modContinuity(\| \xoptshift - \xopt\|) \leq \modContinuity(\delta \diam(\DOM))$, giving us the control on the second term of~\eqref{eq:rate_fixed_delta}. See Figure~\ref{fig:fixed_eps_diagram} for an illustration of the four points considered in this proof.

Finally, we explain why $f$ is uniformly continuous. As $f$ is a (lower semi-continuous) convex function, it is continuous at every point where it is finite. As $f$ is said to be finite at its boundary (and it is obviously finite in the relative interior of $\DOM$ as it is continuously differentiable there), then $f$ is continuous over the whole of $\DOM$. As $\DOM$ is compact, this means that $f$ is also uniformly continuous over $\DOM$.
\end{proof}

\begin{figure} 
\centering
\includegraphics[width=.5\textwidth]{./images/theory/fixed_eps_diagram.pdf}
\vspace{-2mm}
\caption{Illustration of the four points considered for the error analysis of the fixed-$\shrinkAmount$ variant}
\label{fig:fixed_eps_diagram}
\vspace{-3mm}
\end{figure}

We note that the modulus of continuity function $\modContinuity$ quantifies the level of continuity of $f$. For a Lipschitz continuous function, we have $\modContinuity(\sigma) \leq L \sigma$. If instead we have $\modContinuity(\sigma) \leq C \sigma^\alpha$ for some $\alpha \in [0,1]$, then $f$ is actually $\alpha$-H\"{o}lder continuous. We will see in Section~\ref{sec:trw_weak_lip} that the TRW objective is not Lipschitz continuous, but it is  $\alpha$-H\"{o}lder continuous for any $\alpha < 1$, and so is ``almost'' Lipschitz continuous. From the theorem, we see that to get an accuracy of the order $\stopCrit$, we would need $(\shrinkAmount \diam(\DOM))^\alpha < \stopCrit$, and thus a contraction of $\shrinkAmount < \frac{\stopCrit^{(1/\alpha)}}{\diam(\DOM)}$.
\documentclass{article}
\pdfoutput=1
\usepackage{geometry}     
\geometry{a4paper}

\oddsidemargin .25in        
\evensidemargin .25in
\marginparwidth 0.07 true in
\topmargin -0.5in
\textheight 9.65 true in
\textwidth 6 true in        
\widowpenalty=10000
\clubpenalty=10000

\parindent 0pt
\topsep 4pt plus 1pt minus 2pt
\partopsep 1pt plus 0.5pt minus 0.5pt
\itemsep 2pt plus 1pt minus 0.5pt%
\parsep 2pt plus 1pt minus 0.5pt
\parskip .5pc



\usepackage[round]{natbib}


\usepackage[utf8]{inputenc} %
\usepackage[T1]{fontenc}    %
\usepackage{hyperref}       %
\usepackage{url}            %
\usepackage{bbm, bm,color}
\usepackage{mathdots}
\usepackage{booktabs}       %
\usepackage{amsfonts}       %
\usepackage{nicefrac}       %
%
\usepackage{amsmath,amsthm}
\usepackage{graphicx}
%
\usepackage{caption}
\usepackage{subcaption}
%
%
%
\usepackage{enumitem}
\usepackage{algcompatible} %
\usepackage{wrapfig}
\usepackage{multicol}
\usepackage{algorithm}

\renewcommand\algorithmiccomment[1]{%
    \hfill \textit{// #1}%
}


\usepackage{color}
%
\hypersetup{ %
    pdftitle={},
    pdfauthor={},
    pdfkeywords={},
    pdfborder=0 0 0,
	pdfpagemode=UseNone,
    colorlinks=true,
    linkcolor=blue, %
    citecolor=blue, %
    filecolor=blue, %
    urlcolor=blue, %
    pdfview=FitH,
    pdfauthor={Simon Lacoste-Julien},
} 

%

%
\newlength{\Sfloatsep}
\setlength{\Sfloatsep}{\floatsep}
\newlength{\Stextfloatsep}
\setlength{\Stextfloatsep}{\textfloatsep}
\newlength{\Sintextsep}
\setlength{\Sintextsep}{\intextsep}

%
%

%



%
%

%
%
%
%
%
%

\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\renewcommand{\L}{\mathcal{L}}
\newcommand{\X}{\mathcal{M}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\Z}{\mathcal{Z}}
\newcommand{\M}{\X \times \Y}
\newcommand{\prodscal}[2]{\left\langle#1,#2\right\rangle}
\newcommand{\x}{\bm{x}}
\newcommand{\y}{\bm{y}}
\newcommand{\z}{\bm{z}}
\renewcommand{\u}{\bm{u}}
\newcommand{\s}{\bm{s}}
\newcommand{\xt}{\bm{x}^{(t)}}
\newcommand{\xtt}{\bm {x}^{(t+1)}}
\newcommand{\zt}{\bm{z}^{(t)}}
\newcommand{\ztt}{\bm {z}^{(t+1)}}
\newcommand{\st}{\bm{s}^{(t)}}
\newcommand{\stt}{\bm{s}^{(t+1)}}
\newcommand{\vt}{\bm{v}^{(t)}}
\newcommand{\dt}{\bm{d}^{(t)}}
\newcommand{\yt}{\bm{y}^{(t)}}
\newcommand{\ytt}{\bm{y}^{(t+1)}}
\newcommand{\xtm}{\widehat{\bm{x}}^{(t)}}
\newcommand{\ytm}{\widehat{\bm{y}}^{(t)}}
\newcommand{\wt}{w_t}
\newcommand{\wtt}{w_{t+1}}
\newcommand{\gn}{\nabla \L(\xt,\yt)}
\newcommand{\gnx}{\nabla_x\L(\xt,\yt)}
\newcommand{\gny}{\nabla_y\L(\xt,\yt)}
\newcommand{\minimax}{\underset{ \x  \in \X}{ \min \;} \underset{ \y \in \Y}{ \max \;}    \L(\x,\y) }
\newcommand{\maximin}{  \underset{ \y \in \Y}{\max \;}\underset{\x \in \X}{ \min} \; \L(\x,\y) }
\renewcommand{\SS}{ \bm{S}}
\newcommand{\St}{ \bm{S}^{(t)}}
\newcommand{\Xt}{\bm{X}^{(t)}}
\newcommand{\Xtt}{\bm{X}^{(t+1)}}
\newcommand{\gap}{g_{t}}
\newcommand{\et}{\qquad \text{and} \qquad}


%
%
%


%
\newtheorem*{rep@theorem}{\rep@title}
\newcommand{\newreptheorem}[2]{%
\newenvironment{rep#1}[1]{%
 \def\rep@title{#2 \ref{##1}}%
 \begin{rep@theorem}}%
 {\end{rep@theorem}}}

\newreptheorem{lemma}{Lemma'}
\newreptheorem{proposition}{Proposition'}
\newreptheorem{theorem}{Theorem'}
\newreptheorem{remark}{Remark'}

%
%
%

\newtheorem{definition}{Definition}
\newtheorem{theorem}[definition]{Theorem}
\newtheorem{proposition}[definition]{Proposition}
\newtheorem{lemma}[definition]{Lemma}
\newtheorem{corollary}[definition]{Corollary}
\newtheorem{observation}[definition]{Observation}
\newtheorem{conjecture}[definition]{Conjecture}
\newtheorem{remark}[definition]{Remark}
\newtheorem{problem}{Problem}
\newtheorem{claim}{Claim}
\newtheorem{open}{Open Problem}
\newtheorem*{example}{Example}

%
\DeclareMathOperator*{\conv}{conv}
\DeclareMathOperator{\diam}{diam}
\DeclareMathOperator*{\E}{\rm E}
\DeclareMathOperator*{\argmin}{\arg\min}
\DeclareMathOperator*{\argmax}{\arg\max}
\DeclareMathOperator{\lmax}{\lambda_{\max}}
\DeclareMathOperator{\lmin}{\lambda_{\min}}
\DeclareMathOperator*{\supp}{supp}
\DeclareMathOperator*{\diag}{diag}
\DeclareMathOperator*{\sign}{sign}
\DeclareMathOperator*{\tr}{Tr}
\DeclareMathOperator*{\rk}{Rk}
\DeclareMathOperator*{\card}{card}
\DeclareMathOperator*{\relint}{\mathop{relint}}

%
\providecommand{\abs}[1]{\left\lvert#1\right\rvert}
\providecommand{\norm}[1]{\left\lVert#1\right\rVert}
\providecommand{\dualnorm}[1]{\norm{#1}_*}
\providecommand{\frobnorm}[1]{\norm{#1}_{Fro}}
\providecommand{\tracenorm}[1]{\norm{#1}_{tr}}
\providecommand{\opnorm}[1]{\norm{#1}_{op}}
\providecommand{\maxnorm}[1]{\norm{#1}_{\max}}
\providecommand{\latGrNorm}[1]{\norm{#1}_{\groups}}

\newcommand{\ball}{\mathcal{B}}
%

\newcommand{\bigO}{O}
\newcommand{\Sym}{\mathbb{S}}

%
\newcommand{\domain}{\mathcal{M}} %
\newcommand{\stepsize}{\gamma}
\newcommand{\stepmax}{\stepsize_{\textnormal{\scriptsize max}}} %
\newcommand{\stepbound}{\stepsize^\textnormal{\scriptsize B}} %
\newcommand{\FW}{{\hspace{0.05em}\textnormal{FW}}}
\newcommand{\PW}{{\hspace{0.05em}\textnormal{PFW}}}
\newcommand{\away}{{\hspace{0.06em}\textnormal{\scriptsize A}}}
\newcommand{\Cf}{C_{\hspace{-0.08em}f}}
\newcommand{\CfA}{C_{\hspace{-0.08em}f}^-}
\newcommand{\CfAFW}{C_{\hspace{-0.08em}f}^\away}
\newcommand{\strongConvFW}{\mu_{\hspace{-0.08em}f}^\FW}
\newcommand{\strongConvAFW}{\mu_{\hspace{-0.08em}f}^\away}
\newcommand{\distToBoundary}{{\delta_{\x^*\!,\domain}}}
\newcommand{\dirW}{\mathop{dirW}}
\newcommand{\dd}{\bm{d}}
\newcommand{\uu}{\bm{u}}
\newcommand{\vv}{\bm{v}} %
\newcommand{\atoms}{\mathcal A}
\newcommand{\groups}{\mathcal G}
\newcommand{\row}{\text{row}}
\newcommand{\col}{\text{col}}
\newcommand{\lft}{\text{left}}
\newcommand{\rgt}{\text{right}}
\newcommand{\cst}{\mathrm{cst}}
%
\newcommand{\mapprox}{\nu} %
\newcommand{\CfTotal}{C_f^{\text{\tiny{prod}}}}
\DeclareMathOperator*{\lmo}{LMO_{\!\Vertices}}
%
\newcommand{\A}{\mathcal{A}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\bv}{\bm{b}}
\newcommand{\err}{\bm{e}} %

%
\newcommand{\VVertices}{\mathcal{V_\A}\times \mathcal{V_\B}}
\newcommand{\Vertices}{\mathcal{A}\times \mathcal{B}} %
\newcommand{\Coreset}{\mathcal{S}}
\renewcommand{\SS}{\mathcal{S}}
\renewcommand{\aa}{\bm{\alpha}}
\renewcommand{\r}{\bm{r}}
\newcommand{\PdirW}{\mathop{PdirW}}
\newcommand{\PWidth}{\mathop{PW\!idth}}
\newcommand{\innerProd}[2]{\left\langle #1 , #2 \right\rangle}
\newcommand{\innerProdCompressed}[2]{\langle #1 , #2 \rangle}
\newcommand{\C}{\mathcal{C}}
\newcommand{\proj}{\bm{P}}
\newcommand{\K}{\bm{K}}
\newcommand{\Kface}{\mathcal{K}}


\newcommand{\xdim}{d}  %
\newcommand{\ydim}{p}  %

\newcommand{\strongConvGeneralized}{\tilde{\mu}_{\hspace{-0.08em}f}}

\DeclareMathOperator{\ddim}{dim}

%
\newcommand{\Spectahedron}{\mathcal{S}}
\newcommand{\SpectahedronLeOne}{\mathcal{S}_{\le 1}} %
\newcommand{\SpectahedronLeT}{\mathcal{S}_{t}} %
\newcommand{\MaxCutPolytope}{\boxplus}
\newcommand{\FourPointPSD}{\Spectahedron^4_{\text{sparse}}}
\newcommand{\FourPointPSDplus}{\Spectahedron^{4+}_{\text{sparse}}}
\newcommand{\FourPointPSDminus}{\Spectahedron^{4-}_{\text{sparse}}}
\newcommand{\LOneBall}{\diamondsuit}
\newcommand{\signVec}{\mathbf{s}}
\newcommand{\id}{\mathbf{I}} %
\newcommand{\indic}{\mathbbm{1}} %
\newcommand{\ind}{\mathbf{1}} %
\newcommand{\0}{\mathbf{0}} %
\newcommand{\unit}{\mathbf{e}} %
\newcommand{\one}{\mathbf{1}} %
\newcommand{\zero}{\mathbf{0}}
\newcommand\SetOf[2]{\left\{#1\vphantom{#2}\right.\left|\vphantom{#1}\,#2\right\}}
\newcommand{\ignore}[1]{}%


\newcommand{\todo}[1]{\marginpar[\hspace*{4.5em}\textbf{TODO}\hspace*{-4.5em}]{\textbf{TODO}}\textbf{TODO:} #1}
\newcommand{\note}[1]{{\textbf{\color{red}#1}}}
\newcommand{\remove}[1]{} %



%

\title{Convergence Rate of Frank-Wolfe for Non-Convex Objectives}

\date{June 28, 2016}

%
%
%
%
%
%
%
%
%

\author{
Simon Lacoste-Julien \\
INRIA - SIERRA team\\
ENS, Paris 
}

%
%
%
%
%




\begin{document}
\maketitle

\begin{abstract}
We give a simple proof that the Frank-Wolfe algorithm obtains a stationary point at a rate of $O(1/\sqrt{t})$ on non-convex objectives with a Lipschitz continuous gradient. Our analysis is affine invariant and is the first, to the best of our knowledge, giving a similar rate to what was already proven for projected gradient methods (though on slightly different measures of stationarity).
\end{abstract}

%
%
%

\section{Introduction}
We consider the optimization problem:
\begin{equation} \label{eq:problem}
\min_{\x \in \domain} f(\x)
\end{equation}
where $f:\R^d\rightarrow\R$ is a continuously differentiable function over the domain $\domain$ that is \emph{convex} and \emph{compact}, but $f$ is potentially \emph{non-convex}. The Frank-Wolfe (FW) optimization algorithm proposed by~\citet{Frank:1956vp} (also known as conditional gradient method~\citep{demyanov1970approximate}), is a popular first-order method to solve~\eqref{eq:problem} while only requiring access to a \emph{linear minimization oracle} over~$\domain$, i.e., the ability to compute efficiently $\text{LMO}(\r) := \argmin_{\s \in \domain} \innerProd{\s}{\r}$.\footnote{Ties can be broken arbitrarily in this paper.} It has recently enjoyed a surge in popularity thanks to its ability to cheaply exploit the structured constraint sets $\domain$ appearing in machine learning applications, see~\citet{jaggi2013revisiting,lacoste2015global} and references therein. See also~\citet{lan2013FW} for a related survey.

We give the Frank-Wolfe algorithm with adaptive step sizes in Algorithm~\ref{alg:FW} (either with line-search or with a step size that minimizes an affine invariant quadratic upper bound). As~$\domain$ is convex, the iterates~$\xt$ stay in the feasible set~$\domain$ during the algorithm. For a convex function~$f$ with Lipschitz continuous gradient, the FW algorithm obtains a global suboptimality smaller than $\frac{2 C}{t+2}$ after~$t$ iterations~\citep[Theorem~1]{jaggi2013revisiting}, where $C \geq \Cf$ is the constant used in Algorithm~\ref{alg:FW} for the adaptive step size, and $\Cf$ is called the \emph{curvature constant} of~$f$ (defined in~\eqref{eq:Cf} below). On the other hand, we are not aware of any rates proven for Algorithm~\ref{alg:FW} in the case where $f$ is \emph{non-convex}. Examples of recent applications where the FW algorithm is run on a non-convex objective include multiple sequence alignment~\citep[Appendix B]{Alayrac16unsupervised} and multi-object tracking~\citep[Section~5.1]{chari15pairwise}. To talk about rates in the non-convex setting, we need to define a measure of non-stationarity for our iterates.

Consider the ``Frank-Wolfe gap'' of~$f$ at~$\xt$:
\begin{equation} \label{eq:FWgap}
g_t := \max_{\s \in \domain} \prodscal{\s-\xt}{-\nabla f(\x^{(t)})} .
\end{equation}
This quantity is a standard one appearing in the analysis of FW algorithms, and is computed for free during the FW algorithm (see Line~\ref{line:FWgap} in Algorithm~\ref{alg:FW}).
A point $\xt$ is a stationary point for the constrained optimization problem~\eqref{eq:problem} if and only if $g_t = 0$. Moreover, we always have $g_t \geq 0$. The FW gap is thus a meaningful measure of non-stationarity, generalizing the more standard $\| \nabla f(\xt) \|$ that is used for unconstrained optimization. An appealing property of the FW gap is that it is \emph{affine invariant}~\citep{jaggi2013revisiting}, that is, it is invariant to an affine transformation of the domain~$\domain$ in problem~\eqref{eq:problem} and is not tied to any specific choice of norm, unlike the criterion~$\| \nabla f(\xt) \|$. As the FW algorithm is also affine invariant~\citep{jaggi2013revisiting}, it is important that we state our convergence results in term of affine invariant quantities. In this paper, we show in Theorem~\ref{thm:gConvergence} below that the minimal FW gap encountered during the FW algorithm is $O(1/\sqrt{t})$ after $t$ iterations, that is:
\begin{equation}
\min_{0 \leq k \leq t} g_k  \leq \frac{\max\{2 h_0, C\} }{\sqrt{t+1}}  \quad \text{for $t \geq 0$} \, ,
\end{equation}
where $h_0 := f(\x^{(0)}) - \min_{\x \in \domain} f(\x)$ is the initial global suboptimality.

Another nice property of the FW gap $g_t$ is the following \emph{local suboptimality} property. If $\xt$ lies in a convex subset $\domain' \subseteq \domain$ on which $f$ is convex, then $g_t$ upper bounds the suboptimality with respect to the constrained minimum on~$\domain'$, that is, $g_t \geq f(\xt) - \min_{\x \in \domain'} f(\x)$ (by convexity).

\begin{figure}
\centering
\begin{minipage}[t]{.7\textwidth}
  \null
  \begin{algorithm}[H]
    \caption{Frank-Wolfe algorithm (with adaptive step sizes)}\label{alg:FW}
    \begin{algorithmic}[1]
      \STATE Let $\x^{(0)} \in \X$
      \FOR{$t=0 \ldots T$} 
      \STATE Compute $\st := \displaystyle\argmin_{\s \in \X} \prodscal{\s}{\nabla f(\x^{(t)})}$
      \STATE Let $\dd_t := \st - \xt$ \COMMENT{FW update direction}
      \STATE Compute $g_t := \left\langle \dd_t, - \nabla f(\x^{(t)}) \right\rangle$ \COMMENT{FW gap} \label{line:FWgap}
      \STATE \textbf{if} $ \gap \leq \epsilon$ \textbf{then} \textbf{return} $\x^{(t)}$ \COMMENT{approximate stationary pt.}
      \STATE Option I: Line-search: $\stepsize_t \in \displaystyle\argmin_{\stepsize \in [0,1]} \textstyle  f\left(\x^{(t)} + \stepsize \dd_t\right)$
      \STATE Option II: Set $\stepsize_t := \min\{ \frac{g_t}{C}, 1 \} \quad$ for some $C \geq \Cf$ given in~\eqref{eq:Cf}
      \STATE Update $\xtt := \xt + \stepsize_t \dd_t$
      \ENDFOR
      \STATE \textbf{return} $\x^{(T)}$
    \end{algorithmic}
  \end{algorithm}
\end{minipage}%
\end{figure}

\vspace{-3mm}
\section{Result}
\vspace{-2mm}
Before stating our convergence result, we review the usual affine invariant constant appearing in the convergence rates for FW methods. The \emph{curvature constant} $C_{f}$ of a continuously differentiable
function $f:\R^d\rightarrow\R$, with respect to a compact domain $\domain$, is defined as:
\begin{equation}\label{eq:Cf}
  \Cf := \sup_{\substack{\x,\s\in \domain,  ~\stepsize\in[0,1],\\
                      \y = \x+\stepsize(\s-\x)}} \textstyle
           \frac{2}{\stepsize^2}\big( f(\y)-f(\x)-\innerProd{\nabla f(\x)}{\y-\x}\big) \ .
\end{equation}
The assumption of bounded curvature $\Cf$ closely corresponds to a Lipschitz
assumption on the gradient of~$f$. %
More precisely, if~$\nabla f$ is $L$-Lipschitz continuous on $\domain$ with
respect to some arbitrary chosen norm $\norm{.}$ in dual pairing, i.e.
$\norm{\nabla f(\x) - \nabla f(\y)}_* \leq L \norm{\x-\y}$, then
\begin{equation} \label{eq:CfBound}
\Cf \le L \diam_{\norm{.}}(\domain)^2  \ ,
\end{equation}
where $\diam_{\norm{.}}(.)$ denotes the $\norm{.}$-diameter, see \citep[Lemma
7]{jaggi2013revisiting}. These quantities were normally defined in the context of convex optimization, but these bounds did not use convexity anywhere.

\begin{theorem}[Convergence of FW on non-convex objectives] \label{thm:gConvergence}
Consider the problem~\eqref{eq:problem} where $f$ is a continuously differentiable function that is potentially non-convex, but has a finite curvature constant~$\Cf$ as defined by~\eqref{eq:Cf} over the \emph{compact} convex domain~$\domain$. Consider running the Frank-Wolfe algorithm~\ref{alg:FW} with line-search (option I; then take $C := \Cf$ below) or with the step size that minimizes a quadratic upper bound (option II), for any $C \geq \Cf$. Then the minimal FW gap $\tilde{g}_t := \displaystyle \min_{0 \leq k \leq t} g_k$ encountered by the iterates during the algorithm after $t$ iterations satisfies: \vspace{-1mm}
\begin{equation} \label{eq:gBound}
\tilde{g}_t \leq \frac{\max\{2 h_0, C\} }{\sqrt{t+1}}  \quad \text{for $t \geq 0$} \, ,
\end{equation}
where $h_0 := f(\x^{(0)}) - \displaystyle \min_{\x \in \domain} f(\x)$ is the initial global suboptimality. It thus takes at most $O(\frac{1}{\epsilon^2})$ iterations to find an approximate stationary point with gap smaller than $\epsilon$.
\end{theorem}

The main idea of the proof is fairly simple and follows the spirit of the ones used for the gradient descent method. Basically, during FW, the objective $f$ is decreased by a quantity related to the gap $g_t$ at each iteration. As the maximum progress is bounded by the global minimum on~$\domain$ of~$f$, the gap~$g_t$ cannot always stay big, and the initial suboptimality $h_0 = f(\x^{(0)}) - \min_{\x \in \domain} f(\x)$ will control how big the gap can stay.

\begin{proof}
Let $\x_\stepsize
:=  \x^{(t)} + \stepsize \dd_t$ be the point obtained by moving with
step size $\stepsize$ in direction $\dd_t$, where $\dd_t := \st - \xt$ is the FW direction as defined 
by Algorithm~\ref{alg:FW}. By using $\s := \st$, $\x := \x^{(t)}$ and $\y := \x_\stepsize$ in the definition of the
curvature constant~$\Cf$~\eqref{eq:Cf}, and solving for $f(\x_\stepsize)$, we
get an affine invariant version of the standard descent lemma \citep[see e.g. (1.2.5) in][]{Nesterov:2004:lectures}:
\begin{equation} \label{eq:descentLemmaCf}
f(\x_\stepsize) \leq f(\x^{(t)}) + \stepsize \left\langle  \nabla
f(\x^{(t)}), \dd_t \right\rangle + \frac{\stepsize^2}{2} \Cf, \quad \text{valid $\forall
\stepsize \in [0,1]$}.
\end{equation}
Replacing the value of the FW gap $g_t$ in the above equation and substituting $C \geq \Cf$, we get:
\begin{equation} \label{eq:htStepsize}
f(\x_\stepsize) \leq f(\x^{(t)}) - \stepsize g_t + \frac{\stepsize^2}{2} C, \quad \text{valid $\forall
\stepsize \in [0,1]$}.
\end{equation}
We consider the best feasible step size that minimizes the quadratic upper bound on the RHS of~\eqref{eq:htStepsize}: $\stepsize^* := \min\{\frac{g_t}{C}, 1\}$. This is the same step size as used in option II of the algorithm ($f(\x^{(t+1)}) = f(\x_{\stepsize^*})$).  
In option I, the step size $\stepsize_t$ is obtained by line-search, and so
$f(\x^{(t+1)}) = f(\x_{\stepsize_t}) \leq  f(\x_\stepsize)$ $\forall
\stepsize \in [0,1]$ and thus $f(\x^{(t+1)}) \leq f(\x_{\stepsize^*})$. In both cases, we thus have:
\begin{equation} \label{eq:ht}
f(\x^{(t+1)}) \leq f(\x^{(t)}) - \min\left\{ \frac{{g_t}^2}{2 C} \, , \, g_t - \frac{C}{2} \indic_{ \{g_t > C \}} \right\} \, ,
\end{equation}
where $\indic_{ \{ \cdot \}}$ is an indicator function used to consider both possibilities of $\stepsize^* = \min\{\frac{g_t}{C}, 1\}$ in the same equation.
By recursively applying~\eqref{eq:ht}, we get:
\begin{equation} \label{eq:htPlusOne}
f(\x^{(t+1)}) \leq f(\x^{(0)}) - \sum_{k=0}^{t} \min\left\{ \frac{{g_k}^2}{2 C} \, , \, g_k - \frac{C}{2} \indic_{ \{g_k > C \}}\right\}.
\end{equation}
Now let $\tilde{g}_t := \min_{0\leq k \leq t} g_k$ be the minimal gap seen so far. Inequality~\eqref{eq:htPlusOne} then becomes:
\begin{equation} \label{eq:htInequality}
f(\x^{(t+1)})  \leq f(\x^{(0)}) - (t+1) \min\left\{ \frac{{\tilde{g}_t}^2}{2 C} \, ,\, \tilde{g}_t - \frac{C}{2} \indic_{ \{\tilde{g}_t > C \}} \right\}.
\end{equation}
We consider the two possibilities for the result of the $\min$. In both cases, we use the fact that $f(\x^{(0)}) - f(\x^{(t+1)}) \leq f(\x^{(0)}) - \min_{\x \in \domain} f(\x) =: h_0$ by definition and solve for $\tilde{g}_t$ in~\eqref{eq:htInequality}. In case that $\tilde{g}_t \leq C$, the first argument of the $\min$ is smaller and we get the claimed rate on $\tilde{g}_t$:
\begin{equation} \label{eq:gtRate}
\tilde{g}_t \leq \sqrt{ \frac{2 h_0 C}{t+1} } \,  .
\end{equation} 
In case that $\tilde{g}_t > C$ (in the first few iterations), we get that the initial condition $h_0$ is forgotten at a faster rate for $\tilde{g}_t$:
\begin{equation} \label{eq:gtRateInit}
\tilde{g}_t \leq  \frac{h_0}{t+1} + \frac{C}{2} \, .
\end{equation}
We note that this case is only relevant when $h_0 > C/2$; neither of the bounds~\eqref{eq:gtRate} and~\eqref{eq:gtRateInit} are then dominating each other as the inequality~\eqref{eq:gtRateInit} has a faster $O(1/t)$ rate but with the worse constant~$h_0$. We can also show that $\tilde{g}_t \leq C$ (for any $t \geq 0$) when $h_0 \leq C/2$. Indeed,
as we assumed that $\tilde{g}_t  > C$ to get~\eqref{eq:gtRateInit}, we have that~\eqref{eq:gtRateInit} then implies:
\begin{align}
C &< \frac{h_0}{t+1} + \frac{C}{2} \notag \\
t+1 &< \frac{2 h_0}{C} .  \label{eq:boundT}
\end{align}
If $h_0 \leq \frac{1}{2} C$, \eqref{eq:boundT} then yields a contradiction as $t \geq 0$, implying that $\tilde{g}_{t} \leq C$ for all $t\geq 0$ in this case.

From this analysis, we can summarize the bounds as:
\begin{equation} \label{eq:fullBound}
\tilde{g}_t \leq \left\{
\begin{aligned}
\frac{h_0}{t+1} + \frac{C}{2} \quad &\text{for} \quad t+1 \leq \frac{2 h_0}{C}   \, , \\
\sqrt{ \frac{2 h_0 C}{t+1}} \quad &\text{otherwise} \, .
\end{aligned} \right.
\end{equation}
We obtain the theorem statement by simplifying the first option in~\eqref{eq:fullBound} by using that it only happens when $h_0 > \frac{C}{2}$ and $t+1 \leq \frac{2 h_0}{C}$, and thus:
\begin{align*}
\frac{h_0}{t+1} + \frac{C}{2} &= \frac{h_0}{\sqrt{t+1}} \left( \frac{1}{\sqrt{t+1}} + \frac{C}{2h_0} \sqrt{t+1} \right) \\
&\leq \frac{h_0}{\sqrt{t+1}} \left( \frac{1}{\sqrt{t+1}} + \sqrt{\frac{C}{2h_0}} \right) \\
&\leq \frac{h_0}{\sqrt{t+1}} \left( \frac{1}{\sqrt{t+1}} + 1 \right) \\
&\leq \frac{2h_0} {\sqrt{t+1}} \quad \text{for $t\geq 0$}.
\end{align*}
By using $\sqrt{2h_0 C} \leq \max \{ 2 h_0, C \}$, we get the theorem statement.
\end{proof}

\section{Related work}

\paragraph{FW methods.} The only convergence rate for a FW-type algorithm on non-convex objectives that we are aware of is given in Theorem~7 of~\citet{yu2014GCG},\footnote{In this paper, they generalize the Frank-Wolfe algorithm to handle the (unconstrained) minimization of $f(\x) + g(\x)$, where $g$ is a non-smooth convex function for which $\min_{\x} \innerProd{\x}{\r} + g(\x)$ can be efficiently computed. The standard FW setup is recovered when $g$ is the characteristic function of a convex set~$\domain$, but they can also handle other types of $g$, such as the $\ell_1$ norm for example.} but they only cover non-adaptive step size versions (which does not apply to Algorithm~\ref{alg:FW}) and they can only obtain slower rates than~$O(1/\sqrt{t})$. 
\citet[Section 2.2]{bertsekas1999nonlinear} shows that any limit point of the sequence of iterates for the standard FW algorithm converges to a stationary point (though no rates are given). His proof only requires the function $f$ to be continuously differentiable.\footnote{We note that the gradient function being continuous on a compact set implies that it is uniformly continuous, with a \emph{modulus of continuity} function that characterizes its level of continuity. Different rates are obtained by assuming different levels of uniform continuity of the gradient function. The more standard one is assuming Lipschitz-continuity of the gradient, but other (slower) rates could be derived by using various growth levels of the modulus of continuity function.} He basically shows that the sequence of directions $\dd_t$ obtained by the FW algorithm is \emph{gradient related}, and then get the stationarity point convergence guarantees by using~\citep[Proposition 2.2.1]{bertsekas1999nonlinear}.
\citet[Note 5.5]{dunn1979rates} generalizes the standard rates for the FW method (in terms of global suboptimality) when running it on a class of quasi-convex functions of the form: $f(\x) := h(G(\x))$ where $h : \R \rightarrow\R$ is a strictly increasing real function with a continuous derivative, and $G : \R^d \rightarrow\R$ is a convex function with Lipschitz continuous gradient. These functions are quite special though: they are \emph{invex}, that is, all their stationarity points are also global optima.

\paragraph{Unconstrained gradient methods.} Our~$O(1/\sqrt{t})$ rate is analogous to the ones derived for projected gradient methods. In the unconstrained setting, \citet[Inequality~(1.2.15)]{Nesterov:2004:lectures} showed that the gradient descent method with line-search or a fixed step size of $\frac{1}{L}$, where $L$ is the Lipschitz constant of the gradient function, had the following convergence rate to a stationary point:
\begin{equation} \label{eq:gradientConvergence}
\min_{0 \leq k \leq t} \| \nabla f(\x^{(k)}) \| \leq \frac{\sqrt{2 h_0 L}}{\sqrt{t+1}} \quad \text{for $t \geq 0$} \, .
\end{equation}
We see that this rate is very similar to the one we give in~\eqref{eq:gtRate} in the proof of Theorem~\ref{thm:gConvergence}.
\citet{Cartis2010} also showed that the $O(1/\sqrt{t})$ rate was tight for the gradient descent method for an unconstrained objective. It is unclear though whether their example could be adapted to also show a lower bound for the FW method in the constrained setting, as their unidimensional example has a stationarity point only at~$+\infty$, which thus does not apply to a compact domain.

\paragraph{Constrained gradient methods.} In the constrained setting, several measures of non-stationarity have been considered for projected gradient methods. \citet{Cartis2012a} consider the \emph{first-order criticality measure} of~$f$ at~$\x$ which is similar to the FW gap~\eqref{eq:FWgap}, but replacing the maximization over $\domain$ in its definition to the more local $\domain \cap \mathbb{B}(\x)$, where $\mathbb{B}(\x)$ is the unit ball around~$\x$. This measure appears standard in the trust region method literature~\citep{Conn1993}. \citet{Cartis2012a} present an algorithm that gives a $O(1/\sqrt{t})$ rate on this measure. \citet{ghadimi2016nonconvexPG} considers instead the norm of the gradient mapping as a measure of non-stationarity.\footnote{The gradient mapping is defined for the more general proximal optimization setting, but we consider it here for the simple projected gradient setup. For a step size $\stepsize$, the gradient mapping is defined as $\frac{1}{\stepsize}(\x - \x^+)$ where $\x^+ := \text{Proj}_\domain(\x - \stepsize \nabla f(\x))$. If we let the $\stepsize \rightarrow 0$, then the gradient mapping becomes simply the negative of the projection of $-\nabla f(\x)$ on the solid tangent cone to~$\domain$ at~$\x$. When $\domain$ is the full space (unconstrained setting), then the gradient mapping becomes simply $\nabla f(\x)$. We use the ``gradient mapping'' terminology from~\citep[Definition 2.2.3]{Nesterov:2004:lectures} but with the notation from~\citep{ghadimi2016nonconvexPG}.} They show in~\citet[Corollary~1]{ghadimi2016nonconvexPG} that the simple projected gradient method with $\frac{1}{L}$ step size gives the same rate as given by~\eqref{eq:gradientConvergence} in the unconstrained setting, but using the norm of the gradient mapping on the LHS instead. They also later showed in~\citet{ghadimi2016accelerated} that the accelerated projected gradient method of Nesterov gave also the same $O(1/\sqrt{t})$ rate, but with a slightly better dependence on the Lipschitz constant.

\clearpage

%
\bibliographystyle{abbrvnat}
\bibliography{references}
\end{document}
%
%
%

%
%
%

\documentclass{article}

%
\usepackage{times}
%
\usepackage{graphicx} %
%
%

%
\usepackage{natbib}
\usepackage{multibib} %
\newcites{sup}{Supplementary References}

%
%
%
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
%
%
\usepackage{subcaption}
\usepackage{booktabs}       %
\usepackage{array}
 \usepackage{multirow}
 \usepackage[table,xcdraw]{xcolor}
\usepackage{algorithm,algcompatible} %
\algnewcommand\algorithmicinput{\textbf{input}}
\algnewcommand\INPUT{\item[\algorithmicinput]}
\algnewcommand{\algorithmicreturn}{\textbf{return}}%
\algnewcommand{\RETURN}[1]{\algorithmicreturn~\ref{#1}}%
\renewcommand\algorithmiccomment[1]{%
    \hfill \textit{// #1}%
}

\usepackage{enumitem}

%
\usepackage{amsmath, amssymb, amsthm, bm, color}
\usepackage{notation}
\usepackage{nicefrac}       %


%
%
%
%
%
%

%
%
\newcommand{\theHalgorithm}{\arabic{algorithm}}

%
%
%
%
\usepackage[nohyperref, accepted]{icml2016} %

\usepackage{calc} %

\usepackage{hyperref}


%
\definecolor{mydarkblue}{rgb}{0,0.08,0.45}

\hypersetup{ %
    pdftitle={Minding the Gaps for Block Frank-Wolfe Optimization of Structured SVMs},
    pdfauthor={Anton Osokin, Jean-Baptiste Alayrac, Isabella Lukasewitz, Puneet K. Dokania, Simon Lacoste-Julien},
    pdfsubject={Proceedings of the International Conference on Machine Learning 2016},
    pdfkeywords={adaptive sampling, structured output prediction, Frank-Wolfe, block-coordinate, structured SVM, machine learning, ICML, convex optimization},
    pdfborder=0 0 0,
    pdfpagemode=UseNone,
    colorlinks=true,
    linkcolor=mydarkblue,
    citecolor=mydarkblue,
    filecolor=mydarkblue,
    urlcolor=mydarkblue,
    pdfview=FitH
}

%
%
\renewcommand{\baselinestretch}{.97}
\parskip .3pc  

%
\newlength{\Sfloatsep}
\setlength{\Sfloatsep}{\floatsep} %
\newlength{\Stextfloatsep}
\setlength{\Stextfloatsep}{\textfloatsep}  %
\newlength{\Sintextsep}
\setlength{\Sintextsep}{\intextsep}  %

%
%
%
%


%
%
\icmltitlerunning{Minding the Gaps for Block Frank-Wolfe Optimization of Structured SVMs}

\begin{document}

\twocolumn[
\icmltitle{Minding the Gaps for Block Frank-Wolfe Optimization of Structured SVMs}

%
%
%
%
\icmlauthor{Anton Osokin$^{*,1}$ \enspace Jean-Baptiste Alayrac$^{*,1}$}{first.lastname@inria.fr}
\vspace{1mm}
\icmlauthor{Isabella Lukasewitz$^{1}$ \enspace Puneet K. Dokania$^{2}$ \enspace Simon Lacoste-Julien$^{1}$}{}
%
\icmladdress{$^{1}\!$ INRIA -- \'Ecole Normale Sup\'erieure, Paris, France \quad $^{2}\!$ INRIA -- CentraleSup\'elec, Ch\^atenay-Malabry, France}
\vspace{-3mm}
\icmladdress{{\small $^*$ Both authors contributed equally.}}
%
%


%
%
%
\icmlkeywords{adaptive sampling, structured output prediction, Frank-Wolfe, block-coordinate, structured SVM, machine learning, ICML, convex optimization}

%
\vspace*{1.5em}
]

\begin{abstract}
In this paper, we propose several improvements on the block-coordinate Frank-Wolfe (BCFW) algorithm from~\citet{lacosteJulien13bcfw} recently used to optimize the structured support vector machine (SSVM) objective in the context of structured prediction, though it has wider applications. 
The key intuition behind our improvements is that the estimates of block gaps maintained by BCFW reveal the block suboptimality that can be used as an \emph{adaptive} criterion.
First, we sample objects at each iteration of BCFW in an adaptive non-uniform way via gap-based sampling.
Second, we incorporate pairwise and away-step variants of Frank-Wolfe into the block-coordinate setting.
Third, we cache oracle calls with a cache-hit criterion based on the block gaps.
Fourth, we provide the first method to compute an approximate regularization path for SSVM\@.
Finally, we provide an exhaustive empirical evaluation of all our methods on four structured prediction datasets.
\end{abstract}

%
%
\section{Introduction \label{sec:intro}}
%
%

One of the most popular learning objectives for structured prediction is the structured support vector machine~\citep{Taskar2003,Tsochantaridis2005}, which generalizes the classical binary SVM to problems with structured outputs. 
In this paper, we consider the $\ell_2$-regularized $\ell_1$-slack structured SVM, to which we will simply refer as SSVM.
The SSVM method consists in the minimization of the regularized structured hinge-loss on the labeled training set.
The optimization problem of SSVM is of significant complexity and, thus, hard to scale up.
In the literature, multiple optimization methods have been applied to tackle this problem, including cutting-plane methods~\citep{Tsochantaridis2005,Joachims:2009ex} and stochastic subgradient methods~\citep{Ratliff:2007subgradient}, among others. 

Recently, \citet{lacosteJulien13bcfw} proposed the block-coordinate Frank-Wolfe method (BCFW), which is currently one of the state-of-the-art algorithms for SSVM.\footnote{Independently, \citet{branson13} proposed their SVM-IS algorithm which is equivalent to BCFW in some scenarios.}
In contrast to the classical (batch) Frank-Wolfe algorithm~\citep{Frank:1956vp}, BCFW is a randomized block-coordinate method that works on \emph{block-separable} convex compact domains.
%
In the case of SSVM, BCFW operates in the dual domain and iteratively applies Frank-Wolfe steps on the blocks of dual variables corresponding to different objects of the training set.
Distinctive features of BCFW for SSVM include optimal step size selection leading to the absence of the step-size parameter, convergence guarantees for the primal objective, and ability to compute the duality gap as a stopping criterion. 



Notably, the duality gap obtained by BCFW can be written as a sum of block gaps, where each block of dual variables corresponds to one training example.
In this paper, we exploit this property and improve the BCFW algorithm in multiple ways. First, we substitute the standard uniform sampling of objects at each iteration with an adaptive non-uniform sampling.
Our procedure consists in sampling objects with probabilities proportional to the values of their block gaps, giving one of the first fully \emph{adaptive} sampling approaches in the optimization literature that we are aware of.
This choice of sampling probabilities is motivated by the intuition that objects with higher block gaps potentially can provide more improvement to the objective.
We analyze the effects of the gap-based sampling on convergence and discuss the practical trade-offs.

Second, we apply pairwise~\citep{Mitchell:1974uy} and away~\citep{Wolfe:1970wy} steps of Frank-Wolfe to the block-coordinate setting.
This modification is motivated by the fact that batch algorithms based on these steps have linear convergence rates~\citep{LacosteJulien2015linearFW} whereas convergence of standard Frank-Wolfe is sublinear.

Third, we cache oracle calls and propose a gap-based criterion for calling the oracle (cache miss vs.\ cache hit).
Caching the oracle calls was shown do deliver significant speed-ups when the oracle is expensive, e.g., in the case of cutting-plane methods~\cite{Joachims:2009ex}.

Finally, we propose an algorithm to approximate the regularization path of SSVM, i.e., solve the problem for all possible values of the regularization parameter.
Our method exploits block gaps to construct the breakpoints of the path and leads to an $\epsilon$-approximate path.

\textbf{Contributions.} Overall, we make the following contributions:
(i) adaptive non-uniform sampling of the training objects;
(ii) pairwise and away steps in the block-coordinate setting;
(iii) gap-based criterion for caching the oracle calls;
(iv) regularization path for SSVM. The first three contributions are general to BCFW and thus could be applied to other block-separable optimization problems where BCFW could or have been used such as video co-localization~\citep{joulin2014video}, multiple sequence alignment~\citep[App.~B]{alayrac2016MSA}
%
or structured submodular optimization~\citep{jegelka2013reflection}, among others.

This paper is organized as follows. In Section~\ref{sec:background}, we describe the setup and review the BCFW algorithm. In Section~\ref{sec:contributions}, we describe our contributions: adaptive sampling (Section~\ref{sec:gap_sampling}), pairwise and away steps (Section~\ref{sec:pairwise_away_steps}), caching (Section~\ref{sec:caching}).
In Section~\ref{sec:reg_path}, we explain our algorithm to compute the regularization path.
We discuss the related work in the relevant sections of the paper. 
Section~\ref{sec:experiments} contains the experimental study of the methods.
The code and datasets are available at our project webpage.\footnote{\resizebox{0.92\columnwidth}{!}{ \url{http://www.di.ens.fr/sierra/research/gapBCFW/}}}

%
%
%
%
%
%
%
%
%
%


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\section{Background \label{sec:background}}

\subsection{Structured Support Vector Machine (SSVM) \label{sec:ssvm}}
In structured prediction, we are given an input~$\inputvarv\in\inputdomain$, and the goal is to predict a structured object~$\outputvarv\in\outputdomain(\inputvarv)$ (such as a sequence of tags). In the standard setup for structured SVM (SSVM)~\citep{Taskar2003,Tsochantaridis2005}, we assume that prediction is performed with a linear model $h_{\weightv}(\inputvarv) = \argmax_{\outputvarv\in\outputdomain(\inputvarv)}\langle\weightv, \featuremapv(\inputvarv,\outputvarv) \rangle$ parameterized by the weight vector~$\weightv$ where the structured feature map $\featuremapv(\inputvarv,\outputvarv) \in \R^d$ encodes the relevant information for input/output pairs. We reuse below the notation and setup from~\citet{lacosteJulien13bcfw}. Given a labeled training set~$\data = \{(\inputvarv_i,\outputvarv_i)\}_{i=1}^n$, the parameters~$\weightv$ are estimated by solving a convex non-smooth optimization problem\vspace{-1.5mm}
\begin{equation}
    \label{eq:svmstruct_nslack_primal_nonsmooth}
    \min_{\weightv} \quad \frac{\regularizerweight}{2}\norm{\weightv}^2 +
    \frac1n \sum_{i=1}^n \tilde{H}_i(\weightv)\vspace{-1.5mm}
\end{equation}
where $\regularizerweight$ is the regularization parameter and~$\tilde{H}_i(\weightv)$ is the structured hinge loss defined using the \emph{loss-augmented decoding} subproblem (or \emph{maximization oracle}):
%
%
\begin{equation}\label{eq:subproblem_loss_augm}
\text{\parbox[t]{3em}{`max \\oracle'}} \quad 
  \tilde{H}_i(\weightv) := \max_{\outputvarv\in\outputdomain_i} \
    \underbrace{%
    \errorterm_i(\outputvarv)
    - \langle \weightv,
    \featuremapdiffv_i(\outputvarv)
    \rangle
    }_{=:\, H_i(\outputvarv;\weightv)}.
\end{equation}
%
%

Here $\featuremapdiffv_i(\outputvarv):= \featuremapv(\inputvarv_i,\outputvarv_i) - \featuremapv(\inputvarv_i,\outputvarv)$, $\outputdomain_i := \outputdomain(\inputvarv_i)$, and $\errorterm_i(\outputvarv) := \errorterm(\outputvarv_i,\outputvarv)$ denotes the task-dependent structured error of predicting an output~$\outputvarv$ instead of the observed output~$\outputvarv_i$ (e.g., a Hamming distance between two sequences).

\paragraph{Dual formulation.} The negative of a Fenchel dual for objective~\eqref{eq:svmstruct_nslack_primal_nonsmooth} can be written as\vspace{-1mm}
\begin{align}
    \label{eq:svmstruct_nslack_dual} %
    \min_{\substack{ \dualvarv\in\R^{m} \\  \dualvarv \succcurlyeq 0}} \quad  f(\dualvarv) \;:=&  \;\;
    \frac{\regularizerweight}{2}
    \big\| A\dualvarv \big\|^2
    - \bv^\transpose\dualvarv
    \\[-3mm]
    \text{s.t.} \quad &  \;
      \textstyle\sum_{\outputvarv \in \outputdomain_i}  \dualvar_i(\outputvarv) = 1 ~~~\forall i\in[n] \ \notag
    %
\end{align}
where $\dualvar_i(\outputvarv)$, $i\in[n]:=\{1,\ldots,n\}$, $\outputvarv \in \outputdomain_i$ are the dual variables.
The matrix~$A\in\R^{d\times m}$ consists of the $m := \sum_i m_i = \sum_i |\outputdomain_i|$ columns $A := \SetOf{\frac1{\regularizerweight n} \featuremapdiffv_i(\outputvarv) \in\R^d}{i\in[n],\outputvarv \in \outputdomain_i}$, and the vector $\bv \in \R^m$ is given by
$\bv:= \left(\frac1n \errorterm_i(\outputvarv) \right)_{i\in[n],\outputvarv\in\outputdomain_i}$.

In SSVM (as for the standard SVM), the Karush-Kuhn-Tucker (KKT) optimality conditions can give the primal variables~\mbox{$
\weightv(\dualvarv) = A\dualvarv  = \sum_{i,\,\outputvarv \in \outputdomain_i} \dualvar_i(\outputvarv)  \frac{\featuremapdiffv_i(\outputvarv)}{\regularizerweight n}
$} corresponding to the dual variables $\dualvarv$ (see, e.g., \citep[App.~E]{lacosteJulien13bcfw}).
The gradient of $f$ then takes a simple form $\nabla f(\dualvarv) = \regularizerweight A^\transpose A\dualvarv - \bv = \regularizerweight A^\transpose \weightv - \bv$; its \mbox{$(i,\outputvarv)$-th} component equals $-\frac{1}{n} H_i(\outputvarv; \weightv)$.
%

\subsection{Block Coordinate Frank-Wolfe method (BCFW)\label{sec:bcfw}}

%
\begin{algorithm}[t]
    \caption{Block-Coordinate Frank-Wolfe (BCFW) algorithm for structured SVM}%
    \label{alg:FW_product_SVM}
\begin{algorithmic}[1]
        %
        %
        \STATE Let $\weightv^{(0)}\!:=\!{\weightv_i}^{(0)}\!:=\!\0$; \; $\ell^{(0)}\!:=\!{\ell_i}^{(0)}\!:=\!0$
        %
        %
       \FOR{$k:=0,\dots,\infty$}s
                \STATE Pick $i$ at random in $\{1,\ldots,n\}$ \label{alg:FW_product_SVM:randomSample}
                \STATE Solve $\outputvarv_i^* := \displaystyle\argmax_{\outputvarv\in\outputdomain_i} \ H_i(\outputvarv;\weightv^{(k)})$ \label{alg:FW_product_SVM:max_oracle}
                %
                \STATE Let $\weightv_{\sv} := \frac1{\regularizerweight n} \featuremapdiffv_i(\outputvarv_i^*)$ \;
                and \; $\ell_{\sv} := \frac1n \errorterm_i(\outputvarv_i^*)$
                %
                %
               \STATE Let $g_i^{(k)} :=  \regularizerweight (\weightv_i^{(k)}-\weightv_{\sv})^\transpose \weightv^{(k)} - \ell_i^{(k)} + \ell_{\sv}$\label{alg:FW_product_SVM:block_gap}
               \STATE {\small Let $\stepsize := \frac{ g_i^{(k)}}{ \regularizerweight \|\weightv_i^{(k)}-\weightv_{\sv}\|^2}$~and clip to $[0,1]$}\label{alg:FW_product_SVM:line_search}
               %
               %
                \STATE Update ${\weightv_i}^{(k+1)}:= (1-\stepsize){\weightv_i}^{(k)}+\stepsize \,\weightv_{\sv}$
                \STATE {\small~~~~~~~and~ ${\ell_i}^{(k+1)}:= (1-\stepsize){\ell_i}^{(k)}+\stepsize\, \ell_{\sv}$}
              %
                \STATE Update $\weightv^{(k+1)}\;:= \weightv^{(k)} + {\weightv_i}^{(k+1)} - {\weightv_i}^{(k)}$
                \STATE {\small~~~~~~~and~~ $\ell^{(k+1)}:= ~\ell^{(k)}+{\ell_i}^{(k+1)} \ \  - {\ell_i}^{(k)}$}
              %
              %
        \ENDFOR
\end{algorithmic}
\end{algorithm}


We give in Alg.~\ref{alg:FW_product_SVM} the BCFW algorithm from~\citet{lacosteJulien13bcfw} applied to problem~\eqref{eq:svmstruct_nslack_dual}. It exploits the block-separability of the domain~$\domain := \simplex_{|\outputdomain_1|}\times\mathellipsis\times\simplex_{|\outputdomain_n|}$ for problem~\eqref{eq:svmstruct_nslack_dual} and sequentially applies the Frank-Wolfe steps to the blocks of the dual variables~$\dualvarv_{(i)}\in\domain^{(i)} := \simplex_{|\outputdomain_i|}$.
%
%
%
%

While BCFW works on the dual~\eqref{eq:svmstruct_nslack_dual} of SSVM, it only maintains explicitly the primal variables via the relationship $\weightv(\dualvarv)$. 
Most importantly, the Frank-Wolfe linear oracle on block $i$ at iterate $\dualvarv^{(k)}$ is equivalent to the max oracle~\eqref{eq:subproblem_loss_augm} at the corresponding weight vector $\weightv^{(k)}:= A\dualvarv^{(k)}$ \citep[App.~B.1]{lacosteJulien13bcfw} (see line~\ref{alg:FW_product_SVM:max_oracle} of Alg.~\ref{alg:FW_product_SVM}):
%
\begin{equation}
\label{eq:block_max_oracle}
\!\!\!\!\displaystyle\max_{\sv_{(i)}\in \domain^{(i)}} \!\!\big\langle \sv_{(i)}, -\nabla_{(i)} f(\dualvarv^{(k)}) \big\rangle
=
\frac1n\max_{\outputvarv\in\outputdomain_i} H_i(\outputvarv;\weightv^{(k)}).
\end{equation}
Here, the operator~$\nabla_{(i)}$ denotes the partial gradient corresponding to the block~$i$, i.e., $\nabla f = (\nabla_{(i)} f)_{i=1}^n$. Note that each~$\argmax$ of the r.h.s. of~\eqref{eq:block_max_oracle}, $\outputvarv_{(i)}^*$, corresponds to a corner~$\sv_{(i)}^*$ of the polytope~$\domain^{(i)}$ maximizing the l.h.s. of~\eqref{eq:block_max_oracle}.

As the objective~\eqref{eq:svmstruct_nslack_dual} is quadratic, the optimal step size that yields the maximal improvement in the chosen direction $\sv_{(i)}^*- \dualvarv^{(k)}_{(i)}$ can be found analytically (Line~\ref{alg:FW_product_SVM:line_search} of Alg.~\ref{alg:FW_product_SVM}).

%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\subsection{Duality gap \label{sec:duality_gap}}
At each iteration, the batch Frank-Wolfe algorithm~\citep{Frank:1956vp}, \citep[Section~3]{lacosteJulien13bcfw} computes the following quantity, known as the \emph{linearization duality gap} or \emph{Frank-Wolfe gap}:
\begin{equation}\label{eq:duality_gap}
  g(\dualvarv) := \max_{\sv \in \domain} \,\langle \dualvarv - \sv, \nabla f(\dualvarv) \rangle
  %
  = \langle \dualvarv - \sv^*, \nabla f(\dualvarv) \rangle .
\end{equation}
It turns out that this Frank-Wolfe gap exactly equals the Lagrange duality gap between the dual objective~\eqref{eq:svmstruct_nslack_dual} at a point~$\dualvarv$ and the primal objective~\eqref{eq:svmstruct_nslack_primal_nonsmooth} at the point~$\weightv(\dualvarv)=A\dualvarv$ \citep[App.~B.2]{lacosteJulien13bcfw}.

Because of the separability of~$\domain$, the Frank-Wolfe gap~\eqref{eq:duality_gap} can be represented here as a sum of block gaps~$g_i(\dualvarv)$, $g(\dualvarv) = \sum_{i=1}^n g_i(\dualvarv)$, where 
\begin{equation}
\label{eq:block_gap}
g_i(\dualvarv) := \max_{\sv_{(i)}\in \domain^{(i)}} \left\langle\dualvarv_{(i)} - \sv_{(i)}, \nabla_{(i)} f(\dualvarv) \right\rangle.
\end{equation}
Block gaps can be easily computed using the quantities maintained by Alg.~\ref{alg:FW_product_SVM} (see line~\ref{alg:FW_product_SVM:block_gap}).

%
%
%
%
%
%
%
%



%


%
%
%
%
%
%
%
%
%
%

Finally, we can rewrite the block gap in the form
\begin{equation}\label{eq:duality_gap_withH}
g_i(\dualvarv)
\!=\!
\frac{1}{n}  \bigg(\max_{\outputvarv \in \outputdomain_i} H_i(\outputvar; \weightv) -\!\!\sum_{\outputvarv \in \outputdomain_i}\dualvar_i(\outputvarv)H_i(\outputvarv; \weightv)  \bigg)
\end{equation}
providing understandable intuition of when the block gap equals zero. This is the case when all the support vectors, i.e., labelings corresponding to  $\dualvar_i(\outputvarv) > 0$, are tied solutions of the max oracle~\eqref{eq:block_max_oracle}.







%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\subsection{Convergence of BCFW \label{sec:convergence_bcfw}}
\citet{lacosteJulien13bcfw} prove the convergence of the BCFW algorithm at a rate $\bigO(\frac{1}{k})$.

\begin{theorem}[\citet{lacosteJulien13bcfw}, Theorem~2] \label{thm:convergence_FW_product}
For each $k\ge 0$, the iterate\footnote{Note that Alg.~\ref{alg:FW_product_SVM} does not maintain iterates~$\dualvarv^{(k)}$ explicitly.
They are stored in the form of~$\weightv^{(k)} = A \dualvarv^{(k)}$\!.} 
$\dualvarv^{(k)}$ of %
Alg.~\ref{alg:FW_product_SVM} satisfies
$
\E\big[f(\dualvarv^{(k)})\big] - f(\dualvarv^*) \le \frac{2n}{k+2n}\big(\CfTotal+ h_0\big) \, ,
$
where $\dualvarv^*\in \domain$ is a solution of the problem~(\ref{eq:svmstruct_nslack_dual}), $h_0 := f(\dualvarv^{(0)}) - f(\dualvarv^*)$ is the suboptimality at the starting point of the algorithm, $\CfTotal := \sum_{i=1}^n \Cf^{(i)}$ is the sum of the curvature constants\footnote{For the definition of curvature constant, see Definition~\ref{def:curvature_const} in App.~\ref{app:descent_lemma} or \citep[App.~A]{LacosteJulien2015linearFW}} of $f$ with respect to the domains~$\domain^{(i)}$ of individual blocks.
The expectation is taken over the random choice of the block~$i$ at iterations $1,\dots,k$ of the algorithm.
%
%
%
%
%
\end{theorem}

The proof of Theorem~\ref{thm:convergence_FW_product} crucially depends on a standard \emph{descent lemma} applied to a block, stating that at each iteration of BCFW, for any picked block~$i$ and any scalar~$\stepsize \in [0,1]$, the following inequality holds:
\begin{equation}
\label{eq:block_descent_line_search}
  f(\dualvarv^{(k+1)})
  \le f(\dualvarv^{(k)}) - \stepsize g_i(\dualvarv^{(k)}) + \frac{\stepsize^2}{2}  \Cf^{(i)}.
\end{equation}
We rederive inequality~\eqref{eq:block_descent_line_search} as Lemma~\ref{lem:block_step_improvement} in App.~\ref{app:descent_lemma}.
Note that~$\dualvarv^{(k+1)} \in \domain$ is defined by a line search, which is why the bound~\eqref{eq:block_descent_line_search} holds for any scalar~$\stepsize \in [0,1]$.

%
%
%
%
%
%
%
%
%

Taking the expectation of~\eqref{eq:block_descent_line_search} w.r.t.\ the random choice of block~$i$ (sampled uniformly on~$[n]$), we get the inequality
\begin{equation}
\label{eq:expected_descent_line_search}
\E\big[ f( {\scriptstyle \dualvarv^{(k+1)}})\,|\, {\scriptstyle \dualvarv^{(k)} }\big]
  \le f(\dualvarv^{(k)}) - \frac{\stepsize}{n} g(\dualvarv^{(k)}) + \frac{\stepsize^2}{2n} \CfTotal
\end{equation}
which can be used to get the convergence theorem.
%
%
%
%
%
%
%

\section{Block gaps in BCFW \label{sec:contributions}}
%
%
%
In this section, we propose three ways to improve the BCFW algorithm: adaptive sampling (Sec.~\ref{sec:gap_sampling}), pairwise and away steps (Sec.~\ref{sec:pairwise_away_steps}) and caching (Sec.~\ref{sec:caching}).

%
%
\subsection{Adaptive non-uniform sampling \label{sec:gap_sampling}}

\setlength{\textfloatsep}{\Stextfloatsep} %
\begin{figure*}
\centering
\begin{subfigure}[b]{0.28\textwidth}
\includegraphics[width=\textwidth]{figures/gap1_gap_vs_iter.pdf}
\caption{Convergence plots\label{fig:gapSampling_a}\vspace{-3.5mm}}
\end{subfigure}
\qquad 
\begin{subfigure}[b]{0.28\textwidth}
\includegraphics[width=\textwidth]{figures/gap2_ratio_vs_iter.pdf}
\caption{Quality of gap estimates\label{fig:gapSampling_b}\vspace{-3.5mm}}
\end{subfigure}
\qquad 
\begin{subfigure}[b]{0.28\textwidth}
\includegraphics[width=\textwidth]{figures/gap3_theoreticalSpeedup_vs_iter.pdf}
\caption{Theoretical improvement\label{fig:gapSampling_c}\vspace{-3.5mm}}
\end{subfigure}
\caption{Plot~(a) shows exploitation/staleness trade-off for the gap sampling approach. We report the duality gap against the number of effective passes over the data for uniform sampling and for gap sampling with the different frequencies of batch passes updating the gap estimates (every pass over data, every 5, 10, 100 passes, no batch updates). Plot~(b) shows the quality of heuristic gap estimates obtained by the same methods. We report the ratio of the heuristic gap estimate to the true gap value. Plot~(c) shows the factor of improvement of exact gap sampling predicted by Theorem~\ref{thm:convTheoremGapSampling} for real gaps appearing during a run of BCFW with either uniform or gap sampling.\vspace{-3.5mm}
    \label{fig:gapSampling}}
\end{figure*}

\paragraph{Motivation.}
When optimizing finite sums such as~\eqref{eq:svmstruct_nslack_primal_nonsmooth}, it is often the case that processing some summands does not lead to significant progress of the algorithm. 
%
At each iteration, the BCFW algorithm selects a training object and performs the block-coordinate step w.r.t.\ the corresponding dual variables. 
If these variables are already close to being optimal, then BCFW does not make significant progress at this iteration. Usually, it is hard to identify whether processing the summand would lead to an improvement without actually doing computations on it. The BCFW algorithm obtains at each iteration the block gap~\eqref{eq:block_gap} quantifying the suboptimality on the block. In what follows, we use the block gaps to randomly choose a block (an object of the training set) at each iteration in such a way that the blocks with larger suboptimality are sampled more often (the sampling probability of a block is proportional to the value of the current gap estimate).

%

\paragraph{Convergence.}
%
%
%
%
%
%
%

Assume that at iteration~$k$ of Alg.~\ref{alg:FW_product_SVM}, we have the probability~$p_i^{(k)}$ 
%
of sampling block~$i$.
By minimizing the descent lemma bound~\eqref{eq:block_descent_line_search} w.r.t.\ $\stepsize$ for each~$i$ independently under the assumption that $g_i(\dualvarv^{(k)}) \leq \Cf^{(i)}$, and then taking the conditional expectation w.r.t.\ $i$, we get
\begin{equation}
\label{eq:expectedImprovement}
\E\big[ f( {\scriptstyle \dualvarv^{(k+1)}})\,|\, {\scriptstyle \dualvarv^{(k)} }\big]
  \le f(\dualvarv^{(k)}) - \frac{1}{2} \sum_{i=1}^n \p_i^{(k)} \frac{g_i^2(\dualvarv^{(k)})}{\Cf^{(i)}}.
\end{equation}
%
Intuitively, by adapting the probabilities $p_i^{(k)}$, we can obtain a better bound on the expected improvement of~$f$.
In the ideal scenario, one would choose deterministically the block~$i$ with the maximal value of $g_i^2(\dualvarv^{(k)})/\Cf^{(i)}$.

In practice, the curvature~$\Cf^{(i)}$ is unknown, and having access to all $g_i( {\scriptstyle \dualvarv^{(k)} })$'s at each step is prohibitively expensive.
However, the values of the block gaps obtained at the previous iterations can serve as estimates of the block gaps at the current iteration.
We use them in the following \emph{non-uniform} gap sampling scheme:
$
p_i^{(k)} \propto {g}_i(\dualvarv^{(k_i)}).
$ 
 where $k_i$ records the last iteration at which the gap~$i$ was computed.
%
%
%
%
Alg.~\ref{alg:FW_product_SVM_gapsampling} in App.~\ref{app:algorithms} summarizes the method.

We also motivate this choice by Theorem~\ref{thm:convTheoremGapSampling} below which shows that BCFW with (exact) gap sampling converges with a better constant in the rate than BCFW with uniform sampling when the gaps are non-uniform enough (and is always better when the curvatures~$\Cf^{(i)}$'s are uniform). See the proof and discussion in App.~\ref{app:proof_gap_sampling}.

\begin{theorem}\label{thm:convTheoremGapSampling}
Consider the same notation as in Theorem~\ref{thm:convergence_FW_product}.
Assume that at each iterate~$\dualvarv^{(k)}$, BCFW with gap sampling (Alg.~\ref{alg:FW_product_SVM_gapsampling}) has access to the exact values of the block gaps. 
Then, at each iteration, it holds that
$
\E \big[f(\dualvarv^{(k)})\big] - f(\dualvarv^*) \le \frac{2n}{k+2n}\big(\CfTotal \nonuniformityTotal + h_0\big)
$
%
where the constant~$\nonuniformityTotal$ is an upper bound on $\E \Big[ \frac{\nonuniformity(\Cf^{(:)})}{\nonuniformity(\bm{g}_{:}(\dualvarv^{(k)}))^3} \Big]$. The \emph{non-uniformity measure} $\nonuniformity(\bm{x})$ of a vector~$\bm{x}\in\R^n_+$ is defined as
$
\nonuniformity(\bm{x}) 
:=  
\sqrt{1+n^2\Var \big[\bm{p}\big]}
%
$
where~$\bm{p} := \frac{\bm{x}}{\Vert \bm{x} \Vert_1}$ is the probability vector obtained by normalizing~$\bm{x}$.
%
\end{theorem}

%
%
%
%
%
%
%

%


%
%
%
%
%
%
%
%
%
%
%
\vspace{-2mm}
\paragraph{Adaptive procedure.} 
Note that this procedure is \emph{adaptive}, meaning that the criterion for choosing an object to optimize changes during the optimization process.
Our adaptive approach differs from more standard techniques that sample proportional to the Lipschitz constants, as e.g., in~\citet{Nesterov:2012fa}.
%
In App.~\ref{app:toy_example}, we illustrate the advantage of this property by constructing an example where the convergence of gap sampling can be shown \emph{tightly} to be $n$ times faster than when using Lipschitz sampling.

\vspace{-2mm}
\paragraph{Exploitation versus staleness trade-off.}
In practice, having access to the exact block gaps is intractable because it requires a full pass over the dataset after every block update.
However, we have access to the estimates of the block gaps computed from past oracle calls on each block.
%
%
%
Notice that such estimates are outdated, i.e., might be quite far from the current values of the block gaps. We call this effect~``staleness''.
One way to compensate staleness is to refresh the block gaps by doing a full gap computation (a pass over the dataset) after several block-coordinate passes. These gap computations were often already done during the optimization process, e.g., to monitor convergence.

We demonstrate the exploitation/staleness trade-off in our exploratory experiment reported in Figure~\ref{fig:gapSampling}.
On the OCR dataset~\citep{Taskar2003}, we run the gap sampling algorithm with a gap computation pass after 1, 5, 10 and 100 block-coordinate passes (Gap 1, Gap 5, Gap 10, Gap 100) and without any gap computation passes (Gap Inf).
As a baseline, we use BCFW with uniform sampling (Uniform). Figure~\ref{fig:gapSampling_a} reports the duality gap after each number of effective passes over the data.\footnote{An effective pass consists in $n$ calls to the max oracle.}
Figure~\ref{fig:gapSampling_b} shows the ratio of the exact value of the duality gap to the heuristic gap estimate defined as the sum of the current gap estimates.
We observe that when the gap computation is never run, the gap becomes significantly underestimated and the algorithm does not converge.
On another extreme, when performing the gap computation after each pass of BCFW, the algorithm wastes too many computations and converges slowly. Between the two extremes, the method is not very sensitive to the parameter (we have tried 5, 10, 20, 50) allowing us to always use the value of~10.

Comparing adaptive methods to BCFW with uniform sampling, we observe a faster convergence.
Figure~\ref{fig:gapSampling_c} reports the improvement of gap sampling at each iteration w.r.t. uniform sampling that is predicted by Theorem~\ref{thm:convTheoremGapSampling}. 
Specifically, we report the quantity $\nicefrac{\nonuniformity(\bm{g}_{:}(\dualvarv^{(k)}))^3}{\nonuniformity(\Cf^{(:)})}$ with the block gaps estimated at the runs of BCFW with both uniform and gap sampling schemes. To estimate the curvature constants $\Cf^{(i)}$, we use the upper bounds proposed by~\citet[App.~A]{lacosteJulien13bcfw}: $\frac{4R_i^2}{\regularizerweight n^2}$ where
$R_i:= \max_{\outputvarv \in \outputdomain_i}\norm{\featuremapdiffv_i(\outputvarv)}_2$.
We approximate $R_i$ by picking the largest value~$\norm{\featuremapdiffv_i(\outputvarv)}_2$ corresponding to a labeling~$\outputvarv$ observed within the run of BCFW.

%


%
%
%
%


\paragraph{Related work.}
Non-uniform sampling schemes have been used over the last few years to improve the convergence rates of well known randomized algorithms~\cite{Nesterov:2012fa,needell2014,ZhaoImportanceSampling_ICML15}.
%
Most of these approaches use the Lipschitz constants of the gradients to sample more often functions for which gradient changes quickly.
This approach has two main drawbacks.
First, Lipschitz constants are often unknown and heuristics are needed to estimate them.
Second, such schemes are not adaptive to the current progress of the algorithm.
%
To the best of our knowledge, the only other approach that uses an \emph{adaptive} sampling scheme to guide the optimization with convergence guarantees is the one from~\citet{Csiba15adaSDCA}, in the context of the stochastic dual coordinate ascent (SDCA) algorithm.
A cyclic version of BCFW has been analyzed by~\citet{beck2015cyclicBCFW} while~\citet{wang2014parallelBCFW} analyzed its mini-batch form.

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


\subsection{Pairwise and away steps \label{sec:pairwise_away_steps}}
%
\paragraph{Motivation.}
In the batch setting, the convergence rate of the Frank-Wolfe algorithm is known to be sublinear
%
when the solution is on the boundary~\citep{Wolfe:1970wy}, as is the case for SSVM\@.
Several modifications have been proposed in the literature to address this issue.
All these methods replace (or complement) the FW step with a step of another type: pairwise step~\citep{Mitchell:1974uy}, away step~\citep{Wolfe:1970wy}, fully-corrective step~\citep{Holloway:1974:FCFW} (see~\citet{LacosteJulien2015linearFW} for a recent review and the proof that all these methods have a linear rate on the objective~\eqref{eq:svmstruct_nslack_dual} despite not being strongly convex).
A common feature of these methods is the ability to remove elements of the active set (support vectors in the case of SSVM) in order to reach the boundary, unlike FW which oscillates while never completely reaching the boundary. As we expect the solution of SSVM to be sparse, these variants seem natural in our setting.
In the rest of this section, we present the pairwise steps in the block-coordinate setting (the away-step version is described in Alg.~\ref{alg:aFW_product_SVM_away_steps} of App.~\ref{alg:aFW_product_SVM_away_steps}).
%

%
%
%
%
%
%

\paragraph{Pairwise steps.}
A (block) pairwise step consists in \emph{removing} mass from the \emph{away corner} on block~$i$ and transferring it to the 
\emph{FW corner} obtained by the max oracle~\eqref{eq:block_max_oracle}. The away corner is the element of the active set~$\activeS_i := \{\outputvarv \in \outputdomain_i \ | \ \alpha_i(\outputvarv)>0\} \subseteq \outputdomain_i$ worst aligned with the current descent direction, which can be found by solving~$\outputvarv_i^{\awayv} := \argmin\nolimits_{\outputvarv\in\activeS_i} H_i(\outputvarv;\weightv)$.
This does not require solving a combinatorial optimization problem because the size of the active set is typically small, e.g., bounded by the number of iterations performed on the block~$i$.
Analogously to the case of BCFW, the optimal step size~$\stepsize$ for the pairwise step can be computed explicitly by
clipping $\frac{\regularizerweight (\weightv_{\awayv}-\weightv_{\sv})^\transpose \weightv^{(k)} + \ell_{\sv} - \ell_{\awayv}}{ \regularizerweight \|\weightv_{\awayv}-\weightv_{\sv}\|^2}$ to the segment $[0,\dualvar_i^{(k)}(\outputvarv_i^{\awayv})]$ where the upper bound~$\dualvar_i^{(k)}(\outputvarv_i^{\awayv})$ corresponds to the mass of the away corner before the step and the quantities 
$\weightv_{\sv} := \frac1{\regularizerweight n} \featuremapdiffv_i(\outputvarv_i^*)$, $\ell_{\sv} := \frac1n \errorterm_i(\outputvarv_i^*)$ and 
$\weightv_{\awayv} := \frac1{\regularizerweight n} \featuremapdiffv_i(\outputvarv_i^a)$, $\ell_{\awayv} := \frac1n \errorterm_i(\outputvarv_i^a)$ represent the FW and away corners.
Alg.~\ref{alg:pFW_product_SVM_pairwise} in App.~\ref{app:algorithms} summarizes the block-coordinate pairwise Frank-Wolfe (BCPFW) algorithm.

In contrast to BCFW, the steps of BCPFW cannot be expressed in terms of the primal variables~$\weightv$ only, thus it is required to explicitly store the dual variables~$\dualvarv_i$. Storing the dual variables is feasible, because they are extremely sparse, but still can lead to computational overheads caused by the maintenance of the data structure.

The standard convergence analysis for pairwise and away-step FW cannot be easily extended to BCFW. We show the geometric decrease of the objective in Theorem~\ref{thm:BCPFW} of App.~\ref{app:BCPFWconvergence} only when \emph{no} block would have a drop step (a.k.a.\ `bad step'); a condition that cannot be easily analyzed due to the randomization of the algorithm. We believe that novel proof techniques are required here, even though we did observe empirically a linear convergence rate when $\regularizerweight$ is big enough.

\paragraph{Related work.}
\citet[Alg.~4]{nanculef2014} used the pairwise FW algorithm on the dual of binary SVM (in batch mode, however). It is related to classical working set algorithms, such as the SMO algorithm used to train SVMs~\cite{platt1999SMO}, also already applied on SSVMs in~\citet[Ch.~6]{taskar04thesis}.
\citet{franc2014fasole} recently proposed a version of pairwise FW for the block-coordinate setting.
Their SDA-WSS2 algorithm uses a different criterion for choosing the away corner than BCPFW:
instead of minimizing~$H_i$ over the active set~$\activeS_i$, they compute the improvement for all possible away corners and pick the best one.
Their FASOLE algorithm also contains a version of gap sampling in the form of variable shrinking: if a block gap becomes small enough, the block is not visited again, until all the counters are reset.

%
%
%
%
%
%

%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%

%

%

%
%
%
%
%
%


\subsection{Caching \label{sec:caching}}

\paragraph{Motivation.}
At each step, the BCFW and BCPFW algorithms call the max oracle to find the Frank-Wolfe corner.
In cases where the max oracle is expensive, this step becomes a computational bottleneck.
A natural idea to overcome this problem consists in using a ``cheaper oracle'' most of the time hoping that the resulting corner would be good enough.
Caching the results of the max oracle implements this idea by reusing the previous calls of the max oracle to store potentially promising corners.

%
%
%
%
%
%
%

\paragraph{Caching.}
The main principle of caching consists in maintaining a working set $\cache_i \subset \outputdomain_i$ of labelings/corners for each block $i$, where $|\cache_i|\ll|\outputdomain_i|$.
A \emph{cache oracle} obtains the \emph{cache corner} defined as a corner from the working set best aligned with the descent direction, i.e., $\outputvarv_i^c := \argmax_{\outputvarv\in\cache_i} \ H_i(\outputvarv;\weightv)$. 
If the obtained cache corner passes a \emph{cache hit criterion}, i.e., there is a \emph{cache hit}, we do a Frank-Wolfe (or pairwise) step based on the cache corner.
A step defined this way is equivalent to the corresponding step on the convex hull of the working set, which is a subset of the block domain~$\outputdomain_i$.
If a cache hit criterion is not satisfied, i.e., there is a \emph{cache miss}, we call the (possibly expensive) max oracle to obtain a Frank-Wolfe corner over the full domain $\outputdomain_i$.
Alg.~\ref{alg:FW_product_SVM_caching} in App.~\ref{app:algorithms} summarizes the BCFW method with caching.

Note that, in the case of BCPFW, the working set~$\cache_i$ is closely related to the active set~$\activeS_i$. 
On the implementation side, we maintain both sets in the same data structure and keep $\activeS_i \subseteq \cache_i$.

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\paragraph{Cache hit criterion.}
An important part of a caching scheme is the criterion deciding whether the cache look up is sufficient or the max oracle needs to be called.
Intuitively, we want to use the cache whenever it allows optimization to make large enough progress.
We use as measure of potential progress the inner product between the candidate direction and the negative gradient (which would give the block gap $g_i$~\eqref{eq:block_gap} if the FW corner is used). For a cache step, it gives $\cachegap_i^{(k)} := \regularizerweight(\weightv_i^{(k)}-\weightv_{\cachev})^\transpose \weightv^{(k)}-{\ell_i}^{(k)}+\ell_{\cachev}$, which is defined by quantities~$\weightv_{\cachev} = \frac{ \featuremapdiffv_i(\outputvarv_i^c)}{\regularizerweight n}$, $\ell_{\cachev}=\frac1n L_i(\outputvarv_i^c)$ similar to the ones defining the block gap.
The quantity~$\cachegap_i^{(k)}$ is then compared to a \emph{cache hit threshold} defined as~$\max(\cacheF g_i^{(k_i)},\frac{\cacheN}{n} g^{(k_0)})$ where~$k_i$ identifies the iteration when the max oracle was last called for the block~$i$, $k_0$ is the index of the iteration when the full batch gap was computed, $\cacheF>0$ and $\cacheN>0$ are cache parameters. 

The following theorem gives a safety convergence result for BCFW with caching (see App.~\ref{app:caching_theorem} for the proof).
\begin{theorem} \label{thm:cacheTheorem}
Consider the same notation as in Theorem~\ref{thm:convergence_FW_product}. Let~$\tilde{\cacheN} := \frac1n\cacheN \leq 1$. The iterate $\dualvarv^{(k)}$ of %
Alg.~\ref{alg:FW_product_SVM_caching} satisfies
$
\E\big[f(\dualvarv^{(k)})\big] - f(\dualvarv^*) \le \frac{2n}{\tilde{\cacheN}k+2n}\big(\frac{1}{\tilde{\cacheN}}\CfTotal+ h_0\big)
$
for $k\geq0$.
%
%
\end{theorem}
Note that the convergence rate of Theorem~\ref{thm:cacheTheorem} differs from the original rate of  BCFW (Theorem~\ref{thm:convergence_FW_product}) by the  constant~$\tilde{\cacheN}$.
If~$\tilde{\cacheN}$ equals one the rate is the same, but the criterion effectively prohibits cache hits.
If~$\tilde{\cacheN} < 1$ then the convergence is slower, meaning that the method with cache needs more iterations to converge, but the oracles calls might be cheaper because of the cache hits.

%
%
%
%
%
%
%

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.22\textwidth}
        \includegraphics[width=0.9\textwidth]{figures/cacheIllustration.pdf}
        \caption{Regimes of the cache\label{fig:caching_a}\vspace{-2mm}}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.22\textwidth}
        \includegraphics[width=0.9\textwidth]{figures/ocr_large_BCFW_uniform_cache1_gapCheck10_lambda1e-3_different_cache_regimes.pdf}
        \caption{Convergence plots\label{fig:caching_b}\vspace{-2mm}}
    \end{subfigure}
    \caption{ Plot~(a) illustrates different regimes induced by the cache parameters~$\cacheF$ and~$\cacheN$. Plot~(b) shows the evolution of the duality gap within BCFW with gap sampling and with cache parameters in different regimes. \label{fig:caching} \vspace{-3mm}}
\end{figure}

%
\paragraph{Effect of $F$ and $\nu$.}
The parameter~$\nu$ controls the global component and acts as a safety parameter to ensure convergence (Theorem~\ref{thm:cacheTheorem}).
The parameter~$F$ controls, instead, the local (block-dependent) component of the criterion.
Figure~\ref{fig:caching} illustrates the effect of the parameters on OCR dataset~\citep{Taskar2003} and motivates their choice.
At one extreme, if either $F$ or $\nu$ are too large the cache is almost never hit.
At another extreme, if both values are small the cache is hit almost always, thus the method almost stops calling the oracle and does not converge.
Between the two extremes, one of the components usually dominates.
We observe empirically that the regime with the local component dominating leads to faster convergence.
Our experiments show that the method is not very sensitive to the choice of the parameters, so, in what follows, we use values $F=0.25$ and $\nu=0.01$.

%
%
%
%
%
%
%

%
%

\vspace{-1mm}
\paragraph{Related work.}
In the context of SSVM, the idea of caching was successfully applied to the cutting plane methods by~\citet{Joachims:2009ex}, and, recently, to BCFW by~\citet{shah2015caching}. 
In contrast to~\citet{shah2015caching}, our method chooses whether to call the oracle or to use the cache in an adaptive way by looking at the gap estimates of the current blocks.
In the extreme case, when just one block is hard and requires computation and all the rest are easy, our method would be able to call an oracle on the hard block and to use the cache everywhere else. This will result to $n$ times less oracle calls, compared to their strategy.

\section{Regularization path \label{sec:reg_path}}
According to the definition of~\citet{EfronRegPathLasso_04}, a regularization path is a set of minimizers of a regularized objective in the form of~\eqref{eq:svmstruct_nslack_primal_nonsmooth} for all possible values of the regularization parameter~$\regularizerweight$.
%
Similarly to LASSO and binary SVM, the general result of~\citet[Proposition~1]{Rosset07_regPathTheory} is applicable to the case of SSVM and implies that the exact regularization path is piecewise linear in~$1/\regularizerweight$.
However, recovering the exact path is, up to our knowledge, intractable in the case of SSVM.
%
In this paper, we construct an $\epsilon$-approximate regularization path, meaning that, for each feasible~$\regularizerweight$, we have a corresponding primal variables~$\weightv$ which is $\epsilon$-approximate, i.e., the suboptimality $f_{\regularizerweight}(\weightv) - f_{\regularizerweight}^*$ does not exceed~$\epsilon$. 
We use a piecewise \emph{constant} approximation except for the first piece which is linear. 
The approximation is represented by a set of breakpoints~$\{\regularizerweight_j\}_{j=0}^{J+1}$, $\regularizerweight_0 \!=\! +\infty$, $\regularizerweight_{J+1} \!=\! 0$, $\regularizerweight_{j+1} \!\leq\! \regularizerweight_j$, and a set of parameter vectors $\{\weightv^{j}\}_{j=1}^{J}$ with the following properties: for each $\regularizerweight \in [\regularizerweight_{j+1}, \regularizerweight_{j}]$, $j \geq 1$, the vector~$\weightv^{j}$ is $\epsilon$-approximate; for $\regularizerweight \geq \regularizerweight_{1}$, the vector $\frac{\regularizerweight_1}{\regularizerweight}\weightv^{1}$ is $\epsilon$-approximate.

Our algorithm consists of two steps:
(1) at the initialization step, we find the maximal finite breakpoint~$\regularizerweight^\infty := \regularizerweight_{1}$ and the vector $\weightv^\infty:=\weightv^{1}$;
(2) at the induction step, we compute a value~$\regularizerweight_{j+1}$ and a vector $\weightv^{j+1}$ given quantities~$\regularizerweight_{j}$ and $\weightv^{j}$. 
At both steps of our algorithm, we explicitly maintain dual variables $\dualvarv$ that correspond to $\weightv$.
Alg.~\ref{alg:rpAlgorithmSSVM} in App.~\ref{app:algorithms} presents the complete procedure.

%
 
\paragraph{Initialization of the regularization path.}
First, note that, for~$\regularizerweight = \infty$, the KKT conditions for \eqref{eq:svmstruct_nslack_primal_nonsmooth} and~\eqref{eq:svmstruct_nslack_dual} imply that $\weightv=\0$ is a solution of the problem~\eqref{eq:svmstruct_nslack_primal_nonsmooth}.
In what follows, we provide a finite value for~$\regularizerweight^\infty$ and explicitly construct~$\dualvarv^\infty$ and $\weightv^\infty$ such that $\frac{\lambda^\infty}{\lambda}\weightv^\infty$ is $\epsilon$-approximate for~$\regularizerweight \geq \regularizerweight^\infty$.

Let 
$
\tilde{\outputvarv}_i = \argmax_{\outputvarv \in \mathcal{Y}_i} H_i(\outputvarv; \0) = \argmax_{\outputvarv \in \mathcal{Y}_i} L_i(\outputvarv)
$
be the output of the max oracle for~$\weightv = \0$.
First, we construct a dual point~$\dualvarv^\infty \in \domain$ by setting $\dualvar^\infty_i(\tilde{\outputvarv}_i) = 1$. For any value of~$\regularizerweight^\infty$, the corresponding weight vector can be easily computed:
$
\weightv^\infty =  \frac{1}{\regularizerweight^\infty n} \sum_{i=1}^n \featuremapdiffv_i (\tilde{\outputvarv_i}).
$
Identity~\eqref{eq:duality_gap_withH} provides the duality gap:\\[-5mm]
\begin{align*}
g(\dualvarv^\infty, \regularizerweight^\infty, \weightv^\infty) = \frac1n \sum_{i=1}^{n} \Big( &\max_{\outputvarv \in \mathcal{Y}_i } \big( L_i(\outputvarv) - \langle\weightv^\infty, \featuremapdiffv(\outputvarv) \rangle \big) \nonumber \\
&- L_i(\tilde{\outputvarv_i}) + \langle(\weightv^\infty, \featuremapdiffv(\tilde{\outputvarv_i})\rangle \Big).
\end{align*}\\[-5mm]
The inequality $\max_x( f(x) + g(x) ) \leq \max_x f(x) + \max_x g(x)$ and the equality $ \max_{\outputvarv \in \mathcal{Y}_i } L_i(\outputvarv) =L_i(\tilde{\outputvarv_i})$ bound the gap:\\[-5mm]
\begin{multline}
%
g(\dualvarv^\infty, \regularizerweight^\infty, \weightv^\infty) \leq  \frac1n \sum_i \Big( \max_{\outputvarv \in \mathcal{Y}_i } (- \langle\weightv^\infty, \featuremapdiffv(\outputvarv)\rangle ) +\\  \langle\weightv^\infty, \featuremapdiffv(\tilde{\outputvarv_i})\rangle \Big) \nonumber 
= \frac{1}{n \regularizerweight^\infty} \sum_{i=1}^n \theta_i + \frac{1}{\regularizerweight^\infty} \norm{\tilde{\featuremapdiffv}}^2
\end{multline}\\[-4mm]
where the quantities $\theta_i = \max_{\outputvarv \in \outputdomain_i}  \big( - \langle\tilde{\featuremapdiffv}, \featuremapdiffv(\outputvarv)\rangle \big)$ and $\tilde{\featuremapdiffv} := \frac1n \sum_i \featuremapdiffv_i (\tilde{\outputvarv_i})$ are easily computable. 
To ensure that $g(\dualvarv^\infty, \regularizerweight, \frac{\regularizerweight^\infty}{\regularizerweight}\weightv^\infty) \leq\epsilon$ for $\regularizerweight \geq \regularizerweight^\infty$, we can now set\\[-3mm]
\begin{equation*}
\regularizerweight^\infty := \frac{1}{\epsilon} \bigg(\|\tilde{\featuremapdiffv}\|^2 + \frac1n \sum_{i=1}^n \theta_i \bigg).
\end{equation*}
 
\vspace{-3mm}\paragraph{Induction step.}
%
%

We utilize the intuition that the expression~\eqref{eq:duality_gap_withH} provides control on the Frank-Wolfe gap for different values of~$\regularizerweight$ if the primal variables~$\weightv$ and, consequently, the results of the max oracles stay unchanged. 
Proposition~\ref{thm:reg_path} formalizes this intuition.
\begin{proposition}
    \label{thm:reg_path}
    Assume that $L_i(\outputvarv_i) = 0$, $i=1,\dots,n$, i.e., the loss on the ground truth equals zero. Let $\regPathStep\!:=\!\frac{\regularizerweight^{\text{new}}}{\regularizerweight^{\text{old}}}\!<\!1$.
    Then, setting $\dualvar_i(\outputvarv) := \regPathStep \dualvar_i^{\text{old}}(\outputvarv)$, $\outputvarv \neq \outputvarv_i$,
    and $\dualvar_i(\outputvarv_i):=1-\sum_{\outputvarv \neq \outputvarv_i} \dualvar_i(\outputvarv)$, we then have $\weightv^{\text{new}} = \weightv^{\text{old}}$ and
    \begin{equation}
    g(\dualvarv,\: \regularizerweight^{\text{new}}) = g(\dualvarv^{\text{old}}\!\!\!,\: \regularizerweight^{\text{old}}) + (1 - \regPathStep) \Delta(\dualvarv^{\text{old}}\!\!\!,\: \regularizerweight^{\text{old}})
    \label{eq:ssvm_rp_gapnew}
    \end{equation}
    where\\[-4.5mm]
    \begin{equation}
    \notag
    \Delta(\dualvarv^{\text{old}}\!\!\!,\: \regularizerweight^{\text{old}}) := \frac1n \sum_{i=1}^n\sum_{\outputvarv \in \outputdomain_i} \dualvar_i^{\text{old}}(\outputvarv) H_i(\outputvarv; \weightv^{\text{old}}).
    \end{equation}
\end{proposition}
\begin{proof}
    \vspace{-2mm}Consider the problem~\eqref{eq:svmstruct_nslack_dual} for both~$\regularizerweight^{\text{new}}$ and~$\regularizerweight^{\text{old}}$\!\!\!.\:
    Since~$\featuremapdiffv_i(\outputvarv_i)=\bm{0}$ and $A^{\text{new}}=\frac{1}{\regPathStep}A^{\text{old}}$, we have that $\weightv^{\text{old}} = A^{\text{old}} \dualvarv^{\text{old}} = A^{\text{new}} \dualvarv = \weightv^{\text{new}}$.
    The assumption~$L_i(\outputvarv_i) = 0$ implies equalities~$H_i(\outputvarv_i; \weightv^{\text{old}})=0$.
    Under these conditions, the equation~\eqref{eq:ssvm_rp_gapnew} directly follows from the computation of $g(\dualvarv,\: \regularizerweight^{\text{new}})-g(\dualvarv^{\text{old}}\!\!\!,\: \regularizerweight^{\text{old}})$ and the equality~\eqref{eq:duality_gap_withH}.
\end{proof}

Assume that for the regularization parameter~$\regularizerweight^{\text{old}}$ the primal-dual pair $\dualvarv^{\text{old}}$\!, $\weightv^{\text{old}}$ is $\factorRegPath\epsilon$-approximate, $0<\factorRegPath<1$, i.e., $g(\dualvarv^{\text{old}}\!\!,\: \regularizerweight^{\text{old}}) \leq \factorRegPath\epsilon$.
Proposition~\ref{thm:reg_path} ensures that $g(\dualvarv,\: \regularizerweight^{\text{new}}) \leq \epsilon$ whenever\\[-0.15cm]
\begin{equation}\label{eq:reg_path_weight}
\regPathStep 
= 
1 - \frac{\epsilon - g(\dualvarv^{\text{old}}\!,\: \regularizerweight^{\text{old}})}{\Delta(\dualvarv^{\text{old}}\!,\: \regularizerweight^{\text{old}})}
\leq 1 - \frac{\epsilon(1-\factorRegPath)}{\Delta(\dualvarv^{\text{old}}\!,\: \regularizerweight^{\text{old}})}.
\end{equation}
Having~$\factorRegPath<1$ ensures that $\regPathStep < 1$, i.e., we get a new break point $\regularizerweight^{\text{new}} < \regularizerweight^{\text{old}}$.
If the equation~\eqref{eq:reg_path_weight} results in $\regPathStep\leq 0$ then we reach the end of the regularization path, i.e., $\weightv^{\text{old}}$ is $\epsilon$-approximate for all $0 \leq \regularizerweight < \regularizerweight^{\text{old}}$.

To be able to iterate the induction step, we apply one of the algorithms for the minimization of the SSVM objective for~$\regularizerweight^{\text{new}}$ to obtain~$\factorRegPath\epsilon$-approximate pair $\dualvarv^{\text{new}}$, $\weightv^{\text{new}}$\!. Initializing from~$\dualvarv$, $\weightv^{\text{old}}$ provides fast convergence in practice.

%
%
%
%
%
%
%
%
%








%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

 

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
\paragraph{Related work.} Due to space constraints, see App.~\ref{app:relatedWorkRegPath}.

\begin{figure*}[t]
    \centering
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=0.85\textwidth]{figures/legend_compact_oneline_BCPFW.pdf}
    \end{subfigure}
    \begin{subfigure}[b]{0.29\textwidth}
        \includegraphics[width=\textwidth]{figures/gapVsPass_ocrLarge_lambda_1e-03.pdf}
        \includegraphics[width=\textwidth]{figures/gapVsTime_ocrLarge_lambda_1e-03.pdf}
        \caption{OCR-large, $\regularizerweight=0.001$\label{fig:exp1_main_a}\vspace{-2mm}}
    \end{subfigure}
    \qquad
    \begin{subfigure}[b]{0.29\textwidth}
        \includegraphics[width=\textwidth]{figures/gapVsPass_horseMedium_lambda_1e+01.pdf}
        \includegraphics[width=\textwidth]{figures/gapVsTime_horseMedium_lambda_1e+01.pdf}
        \caption{HorseSeg-medium, $\regularizerweight=10$\label{fig:exp1_main_b}\vspace{-2mm}}
    \end{subfigure}
    \qquad
    \begin{subfigure}[b]{0.29\textwidth}
        \includegraphics[width=\textwidth]{figures/gapVsPass_lspSmall_lambda_1e+02.pdf}
        \includegraphics[width=\textwidth]{figures/gapVsTime_lspSmall_lambda_1e+02.pdf}
        \caption{LSP-small, $\regularizerweight=100$\label{fig:exp1_main_c}\vspace{-2mm}}
    \end{subfigure}
    \caption{Summary of the results of Section~\ref{subsec:exp1}: the duality gap against the number of effective passes over data (top) and time (bottom).
        %
        %
        %
        %
        %
        \label{fig:exp1_main} \vspace{-4.5mm}  
    }
\end{figure*}


%


%
%
%

%
%



%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\section{Experiments}\label{sec:experiments}
The experimental evaluation consists of two parts:
Section~\ref{subsec:exp1} compares the different algorithms presented in Section~\ref{sec:contributions};
Section~\ref{subsec:reg_path} evaluates our approach on the regularization path estimation. %

%
\textbf{Datasets.}
We evaluate our methods on four datasets for different structured prediction tasks: OCR~\citep{Taskar2003} for handwritten character recognition, CoNLL~\citep{Sang2000} for text chunking, HorseSeg~\citep{kolesnikov2014closed} for binary image segmentation and LSP~\citep{Johnson10} for pose estimation.
The models for OCR and CoNLL were provided by~\citet{lacosteJulien13bcfw}. We build our model based on the one by~\citet{kolesnikov2014closed} for HorseSeg, and the one by~\citet{Chen_NIPS14} for LSP.
For OCR and CoNLL, the max oracle consists of the Viterbi algorithm~\citep{Viterbi67}; for HorseSeg~-- in graph cut~\citep{boykov2004graphcut}, for LSP~-- in belief propagation on a tree with messages passed by a generalized distance transform~\citep{felzenszwalb2005distTrans}.
Note that the oracles of HorseSeg and LSP require positivity constraints on a subset of the weights in order to be tractable. 
The BCFW algorithm with positivity constraints is derived in App.~\ref{app:pos_const}.
We provide a detailed description of the datasets in App.~\ref{app:dataset_table} with a summary in Table~\ref{tab:dataset}.

The problems included in our experimental study vary in the number of objects~$n$ (from $100$ to $25,\!000$), in the number of features~$d$ (from $10^2$ to $10^6$), and in the computational cost of the max oracle (from $10^{-4}$ to $2$ seconds).

%
%
%
%
%


%
\subsection{Comparing the variants of  BCFW \label{subsec:exp1}}




%
In this section, we evaluate the three modifications of BCFW presented in Section~\ref{sec:contributions}.
We compare 8 methods obtained by all the combinations of three binary dimensions: 
gap-based vs.\ uniform sampling of objects, BCFW vs.\ BCPFW, caching oracle calls vs.\ no caching.
%
%

We report the results of each method on 6 datasets (including 3 sizes of HorseSeg) for three values of the regularization parameter~$\regularizerweight$: the value leading to the best test performance, a smaller and a larger value. For each setup, we report the duality gap against both number of oracle calls and elapsed time.
We run each method~$5$ times with different random seeds influencing the order of sampled objects and report the median (bold line), minimum and maximum values (shaded region).
We summarize the results in Figure~\ref{fig:exp1_main} and report the rest in App.~\ref{app:exp1}.

%
%
%
%
%
%
%

First, we observe that, aligned with our theoretical results, gap sampling always leads to faster convergence (both in terms of time and the number of effective passes).
The effect is stronger when $n$ is large (Figure~\ref{fig:exp1_main_b}).
%
Second, caching always helps in terms of number of effective passes, but an overhead caused by maintaining the cache is significant when the max oracle is fast (Figure~\ref{fig:exp1_main_a}).
In the case of expensive oracle (Figure~\ref{fig:exp1_main_c}), the cache overhead is negligible.
Third, the pairwise steps (BCPFW) lead to an improvement to get smaller values of duality gaps.
The effect is stronger when the problem is more strongly convex, i.e., $\regularizerweight$ is bigger. However, maintaining the active sets results in computational overheads, which sometimes are significant.
Note that the overhead of cache and active sets are shared, because they are maintained in the same data structure.
Using a cache also greatly limits the memory requirements of BCPFW, because, when the cache is hit, the active set is guaranteed not to grow.

\textbf{Recommendation.} For off-the-shelf usage, we recommend to use the BCPFW + gap sampling + cache method when oracle calls are expensive, and the BCFW + gap sampling method when oracle calls are cheap.

%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%

\vspace{-2mm}
\subsection{Regularization path}\label{subsec:reg_path}
In this section, we evaluate our regularization path algorithm presented in Section~\ref{sec:reg_path}.
\begin{figure}
\vspace{-1mm}
     \begin{tabular}{c@{$\:\:$}c}
         \includegraphics[width=0.23\textwidth]{figures/horseSmall_bcPfwGapCache_time_noHeuristic} &
         \includegraphics[width=0.23\textwidth]{figures/horseSmall_bcPfwGapCache_numpass_noHeuristic}\\[-0.3cm]
        \end{tabular}
        \caption{
            \label{fig:exp2_horseSmall} On HorseSeg-small, we compare $\epsilon$-approximate regularization path against grid search with/without warm start. 
            We report the cumulative running time (left) and the cumulative effective number of passes (right) required to get to each value of the regularization parameter~$\regularizerweight$.
            \vspace{-2mm} 
        }
\end{figure}
We compare an $\epsilon$-approximate regularization path with $\epsilon=0.1$ against the standard grid search approach with/without warm start (we use a grid of 31 values of $\regularizerweight$: $2^{15},2^{14},\dots,2^{-15}$).
In Figure~\ref{fig:exp2_horseSmall}, we report the cumulative elapsed time and cumulative number of effective passes over the data required by the three methods to reach a certain value of~$\regularizerweight$ on the HorseSeg-small dataset (starting from the initialization value for the path method and the maximum values of the grid for the grid search methods).
The methods and additional experiments are detailed in App.~\ref{app:exp2}.

\paragraph{Interpretation.} First, we observe that warm start speeds up the grid search.
Second, the cost of computing the full regularization path is comparable with the cost of grid search.
However, the regularization path algorithm finds solutions for all values of $\regularizerweight$ without the need to predefine the grid.
%
%
%
%

\section*{Acknowledgments} 
%
This work was partially supported by the MSR-Inria Joint Center and a Google Research Award.
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


%

%
\bibliography{icml2016_bcfw}
\bibliographystyle{icml2016}


%
\clearpage
\appendix

%
%
%

\twocolumn[
\icmltitle{Supplementary Material\\
Minding the Gaps for Block Frank-Wolfe optimization of structured SVMs}
]

\paragraph{Outline} 
\begin{description}[itemsep=0mm,topsep=0cm,leftmargin=0cm,font=\normalfont]
\renewcommand\labelitemi{--}
\item[Appendix~\ref{app:relatedWorkRegPath}:] related work on regularization paths.
\item[Appendix~\ref{app:descent_lemma}:] review of the block descent lemma for BCFW.
\item[Appendix~\ref{app:toy_example}:] toy example showing that our adaptive gap sampling technique can be~$n$ times faster than alternatives.
\item[Appendix~\ref{app:algorithms}:] details of the proposed algorithms.
\item[Appendix~\ref{app:proof_gap_sampling}:] the proof of Theorem~\ref{thm:convTheoremGapSampling} on convergence of BCFW with gap sampling.
\item[Appendix~\ref{app:caching_theorem}:] the proof of Theorem~\ref{thm:cacheTheorem} on convergence of BCFW with caching.
\item[Appendix~\ref{app:BCPFWconvergence}:] the proof of Theorem~\ref{thm:BCPFW} on convergence of BCPFW and BCAFW (with conditions).
\item[Appendix~\ref{app:pos_const}:] generalization of BCFW to the case of SSVM with box constraints on the weights.
\item[Appendix~\ref{app:dataset_table}:] the detailed description of the datasets used in our experiments.
\item[Appendix~\ref{app:exp1}:] the full experimental comparison of different variants of BCFW.
\item[Appendix~\ref{app:exp2}:] the full experimental study of our regularization path method.
\end{description}

\section{Related work for regularization path} \label{app:relatedWorkRegPath}

\citet{EfronRegPathLasso_04}, in their seminal paper, introduced the notion of regularization path and showed that the regularization path of  LASSO~\citesup{TibshiraniLasso_96} is piecewise linear.
\citetsup{HastieRegPathSVM_04} proposed the path following method to compute the exact regularization path for the binary SVM with L2-regularization.
Exact path following algorithms suffer from numerical instabilities as they repeatedly invert a potentially badly-conditioned matrix~\citepsup{Allgower931_pathFollowing}.
In addition, \citetsup{Gartner12_regPathComplexity} show that, in the worst case, the regularization path for binary SVM contains an exponential number of break points.
Although the exact path following methods for SVM are still developed~\citepsup{Sentelle15_regPathSVM}, approximate methods might be more suited for practical use cases.
\citetsup{Karasuyama11_regPathSvm} proposed a method providing a trade-off between accuracy of the path and its computational cost.
Recently, \citetsup{Giesen12_regPathTheory} have developed a general framework to construct a piecewise-constant $\epsilon$-approximate path with at most~$\bigO(1/\sqrt{\epsilon})$ break points and applied it, e.g., to binary SVM. 

In contrast to binary SVM, regularization paths for multi-class SVMs are more complex and less studied.
\citetsup{Lee06_regPathMultiSVM} proposed a path following method for multi-class SVM in the MSVM formulation of~\citetsup{Lee04_MSVM}. \citetsup{Wang06_regPathMultiSVM} analyzed the regularization path for the L1 version of MSVM.
Finally, \citetsup{Jun-Tao10_regPathMultiSVM} constructed the regularization path for the multi-class SVM with huberized loss.
We are not aware of any work computing the regularization path for SSVM or, for its predecessor multi-class SVM in the formulation of \citetsup{Crammer01_multiClassSVM}.

The induction step of our method is similar to Alg.~1 of~\citepsup{Giesen12_regPathTheory} applied to the case of binary SVM.
They also construct a piecewise linear $\epsilon$-approximate path by alternating the SVM solver and a procedure to identify the region where the output of the solver is accurate enough. 

In contrast to our method, \citetsup{Giesen12_regPathTheory} construct the path only for the predefined segment of the values of~$\regularizerweight$. We do not require such a segment as input and are able to find the largest and smallest value automatically.
Another difference to~\citepsup{Giesen12_regPathTheory} consists in using the $\regularizerweight$-formulation of SVM instead of the $C$-formulation. In the two formulations, the accuracy parameter~$\epsilon$ is scaled differently for the different values of the regularization parameters. The $\regularizerweight$-formulation requests higher accuracy for the small values of~$\regularizerweight$ and, thus, creates more break points in that region.

\section{Block descent lemma for BCFW \label{app:descent_lemma}}

\begin{definition}[Block curvature constant]
    \label{def:curvature_const}
    Consider a convex function $f$ defined on a separable domain~$\domain=\domain^{(1)} \times\dots\times \domain^{(n)}$.
    The curvature constant~$\Cf^{(i)}$ of the function $f$ w.r.t.\ the individual block of coordinates $\domain^{(i)}$ is defined by
    \begin{align}
    \notag \Cf^{(i)} := \sup\; &\frac{2}{\stepsize^2}\left( f(\dualvarvtwo)-f(\dualvarv)-\langle \dualvarvtwo_{(i)}-\dualvarv_{(i)}, \nabla_{(i)} f(\dualvarv)\rangle \right) \\
    \notag \mbox{s.t.}\; & \dualvarv\in\domain,\,\sv_{(i)}\in \domain^{(i)}, \stepsize\in[0,1], \\
    \label{eqn:CfProduct} & \dualvarvtwo = \dualvarv+\stepsize(\sv_{[i]}-\dualvarv_{[i]}).
    \end{align}
\end{definition}
Here~$\sv_{[i]} \in \R^m$ and $\dualvarv_{[i]} \in \R^m$ are the zero-padded versions of~$\sv_{(i)}\in \domain^{(i)}$ and~$\dualvarv_{(i)}\in \domain^{(i)}$, respectively.
Note that, although $\sv_{[i]} \not\in \domain$ and $\dualvarv_{[i]} \not\in \domain$, we have that $\dualvarvtwo := \dualvarv+\stepsize(\sv_{[i]}-\dualvarv_{[i]}) \in \domain$.

In the case of SSVM, the curvature constant~$\Cf^{(i)}$ can be upper bounded (tightly in the worst case) with $\frac{4R_i^2}{\regularizerweight n^2}$ where
$R_i:= \max_{\outputvarv \in \outputdomain_i}\norm{\featuremapdiffv_i(\outputvarv)}_2$~\citep[Appendix~A]{lacosteJulien13bcfw}.\footnote{More generally, let $\|\cdot \|_i$ be some norm defined on $\domain^{(i)}$. Then suppose that $L_i$ is the Lipschitz-continuity constant with respect to this norm for $\nabla_{(i)} f(\dualvarv)$ when only $\dualvarv_{(i)}$ varies, i.e., $\| \nabla_{(i)} f(\dualvarv) - \nabla_{(i)} f(\dualvarv + \sv_{[i]} - \dualvarv_{[i]}) \|_i^* \leq L_i \| \sv_{(i)} -  \dualvarv_{(i)} \|_i$ for all $\dualvarv \in \domain$, $\sv_{(i)} \in \domain^{(i)}$, where $\|\cdot\|_i^*$ is the dual norm of $\|\cdot\|_i$. Then similarly to Lemma~7 in~\citetsup{Jaggi:2013wg}, we have $\Cf^{(i)} \leq L_i \big(\diam_{\|\cdot\|_i} \domain^{(i)} \big)^2$.
\label{foot:Cfi}}

For reference, we restate below the key descent lemma used for the proof of convergence of BCFW and its variants. We note in passing that this is an affine invariant analog of the standard descent lemmas that use the Lipschitz continuity of the gradient function to show progress during first order optimization algorithms.


%
%
%

\begin{lemma}[Block descent lemma]\label{lem:block_step_improvement}
    For any~$\dualvarv \in \domain$ and block~$i$, let $\sv_{(i)}\in \domain^{(i)}$ be the Frank-Wolfe corner selected by the max oracle of block~$i$ at~$\dualvarv$. Let~$\dualvarv_{LS}$ be obtained by the line search between $\dualvarv_{(i)} \in \domain^{(i)}$ and $\sv_{(i)}$, i.e.,  $f(\dualvarv_{LS}) = \min_{\gamma\in[0,1]} f(\dualvarv_\stepsize)$ where $\dualvarv_\stepsize := \dualvarv+\stepsize(\sv_{[i]}-\dualvarv_{[i]})$. Then, it holds that for each $\stepsize\in[0,1]$: \\[-2mm]
    \begin{equation}
    \label{eq:block_descent_line_search_lemma}
    f(\dualvarv_{LS})
    \le f(\dualvarv) - \stepsize g_i(\dualvarv) + \frac{\stepsize^2}{2} \Cf^{(i)}\\[-1mm]
    \end{equation}
    where~$\Cf^{(i)}$ is the curvature constant of the function $f$ over the factor $\domain^{(i)}$ and $g_i(\dualvarv)$ is the block gap at the point~$\dualvarv$ w.r.t. the block~$i$.
\end{lemma}
\begin{proof}
    From Definition~\ref{def:curvature_const} of the curvature constant and the expression~\eqref{eq:block_gap} for the block gap, we have
    \begin{align*}
    f(\dualvarv_\stepsize) = & f(\dualvarv+\stepsize(\sv_{[i]}-\dualvarv_{[i]})) \\
    \le & f(\dualvarv) + \stepsize \langle \sv_{(i)}-\dualvarv_{(i)}, \nabla_{(i)} f(\dualvarv)\rangle + \frac{\stepsize^2}{2}\Cf^{(i)} \\
    = & f(\dualvarv) - \stepsize g_i(\dualvarv) + \frac{\stepsize^2}{2}\Cf^{(i)} \ .
    \end{align*}
    The inequality~$f(\dualvarv_{LS}) \leq f(\dualvarv_\stepsize)$ completes the proof.
\end{proof}




\section{Toy example for gap sampling \label{app:toy_example}}
%
%
%
%
%
%
%

%
%
In this section, we construct a toy example of the structured SVM problem where the adaptive gap-based sampling is $n$ times faster than non-adaptive sampling schemes such as uniform sampling or curvature-based sampling (the latter being the affine invariant analog of Lipschitz-based sampling).

\paragraph{General idea.} The main idea is to consider a training set where there are $n-1$ ``easy'' objects that need to be visited only once to learn to classify them, and one ``hard'' object that requires at least $K \gg 1$ visits in order to get the optimal parameter. We can design the example in such a way that the curvature or Lipschitz constants are non-informative about which example is hard, and which is easy. The non-adaptive sampling schemes will thus have to visit the easy objects as often as the hard object, whereas the gap sampling technique can adapt to focus only on the single hard object after having visited the easy objects once, thus yielding an overall $\min\{n,K\}$-times speedup.
%

Note that large-scale learning datasets could have analogous features as this toy example: a subgroup of objects might be easier to learn than another, and moreover, they might share similar information, so that after visiting a subset, we do not need to linger on the other ones from the same subset as all the information has already been extracted. We cannot know in advance which subgroups are these subsets, and thus an adaptive scheme is needed. 

\subsection{Explicit construction}

For simplicity, we set the weight of the regularizer $\regularizerweight$ to $\nicefrac{1}{n}$ so that the scaling factor defining~$A$ in Problem~\eqref{eq:svmstruct_nslack_dual} is $\nicefrac{1}{\regularizerweight n} = 1$. The matrix~$A$ thus consists of the difference feature maps, i.e., $A := \SetOf{\featuremapdiffv_i(\outputvarv):= \featuremapv(\inputvarv_i,\outputvarv_i) - \featuremapv(\inputvarv_i,\outputvarv) \in\R^d}{i\in[n],\outputvarv \in \outputdomain_i}$.
In our example, we use feature maps of dimensionality $d:=K+1 := |\outputdomain_i|$.
Let $\outputdomain_i:=\lbrace 0, 1, \dots, K \rbrace$ be the set of labels for the object~$i$ and the label~$0$ be the correct label.
We consider the zero-one loss, i.e., $\errorterm_i(0)=0$ and  $\errorterm_i(k)=1$ for $k\geq 1$. In the following, let $\{\basis_j\}_{j=1}^d$ be the standard basis vectors for $\R^d$.

\paragraph{Hard and easy objects.}
We construct the feature map~$\featuremapv(\inputvarv,\outputvarv)$ so that only the last coordinate of the parameter vector is needed to classify correctly the easy object, whereas all the other coordinates are needed to classify correctly the hard object. By using a different set of coordinates between the easy and the hard objects, we simplify the analysis as the optimization for both block types decouples (become independent).
Specifically, we set the feature map for the correct label to be $\featuremapv(\inputvarv_i,0):=\bm{0}$ for all objects~$i$. We let $i=1$ be the hard object and we set $\featuremapv(\inputvarv_i,k):={-\frac1{\sqrt{2}} \basis_{k}}$ for $k = 1,\dots,K$. For the easy object, $i\in \{2,\dots, n\}$, we use the constant $\featuremapv(\inputvarv_i,k) := {-\basis_{K+1}}$ for all $k\geq 1$.
The normalization of the feature maps is made so that the curvature constants for all the objects are equal (see below). Note also here that $\featuremapdiffv_1(k) \perp \featuremapdiffv_i(l)$ for any labels~$k, l$, and thus the optimization over block~$1$ decouples with the one for the other blocks $i = 2, \ldots, n$. The SSVM dual~\eqref{eq:svmstruct_nslack_dual} takes here the following simple form:
\begin{align}
    \label{eq:SSVMdualSimple}
    \min_{\substack{ \dualvarv\in\R^{m} \\  \dualvarv \succcurlyeq 0}} \quad  & \;\;
    \frac{1}{n} \sum_{k=1}^K \big(\frac{1}{4}{\dualvar_1(k)}^2 - \dualvar_1(k) \big) 
 	+ \frac{1}{n} (\frac{1}{2} u^2 - u ) \\
    \text{s.t.} ~~ & \;\; \sum_{k=0}^K  \dualvar_i(k) = 1 ~~~\forall i\in[n] \, , ~~  u = \sum_{i=2}^n \sum_{k=1}^K \dualvar_i(k)\, , \notag
\end{align}
where we have introduced the auxiliary variable~$u$ to highlight the simple structure for the optimization over the easy blocks. The unique\footnote{Uniqueness can be proved by noticing that the objective is strongly convex in $\dualvarv_1$ after removing $\dualvar_1(0)$ and replacing the equality constraint with an inequality.} solution for the first (hard) block is easily seen to be $\dualvar_1^*(k) = \frac{1}{K}$ for $k \geq 1$ and $\dualvar^*_1(0) = 0$. For the easy blocks, any feasible combination of dual variables that gives $u^* = 1$ is a solution. This gives the optimal parameter $\weightv^* = A \dualvarv^* = \basis_{K+1}+\frac{1}{K} \sum_{k=1}^K \basis_k$.


\paragraph{Optimization on the hard object.}
The objective for the hard object (block~1) in~\eqref{eq:SSVMdualSimple} is similar to the one used to show a lower bound of $\Omega(1/t)$ suboptimality error after~$t$ iterations for the Frank-Wolfe algorithm for $t$ smaller than the dimensionality (e.g., see Lemma~3 and~4 in~\citetsup{Jaggi:2013wg}), hence showing that the optimization is difficult on this block.
The BCFW algorithm is initialized with $\weightv = \bm{0}$, which corresponds to putting all the mass on the correct label, i.e., $\dualvar_i(0)=1$ and $\dualvar_i(k)=0$, $k\geq 1$. 
At each iteration of BCFW, the mass can be moved only towards one corner, and all the corners (of the simplex) have exactly one non-zero coordinate.
This means that after~$t$ iterations of BCFW on the first block, at most~$t$ non-ground truth dual variables can be non-zero. Minimizing the objective~\eqref{eq:SSVMdualSimple} over the first block with the constraint that at most~$t$ of these variables are non-zero give the similar solution~$\dualvar_1(k)=1/t$ for $k=1, \ldots, t$, which gives a suboptimality of~$\frac{1}{4n}(\frac{1}{t} - \frac{1}{K})$ for $t \leq K$. 
Similarly, this also yields the smallest FW gap\footnote{Recall that the FW gap here is the same as the Lagrangian duality gap (see Section~\ref{sec:duality_gap}), and so if one cares about the SSVM primal suboptimality, one needs a small FW gap.} 
possible for this block after~$t$ iterations, which is~$\frac{1}{n}\frac{1}{2t}$. This means that in order to get a suboptimality error smaller than~$\epsilon$, one needs at least
\begin{equation} \label{eq:HardBlockComplexity}
t \geq \Omega(\min\{K, \frac{1}{n\epsilon}\} ) 
\end{equation} 
BCFW iterations on the first 
block.\footnote{In fact, BCFW also has a $\mathcal{O}(\frac{1}{nt})$ gap after~$t$ iterations on the first block by the standard FW convergence theorem, as $\Cf^{(1)} = \frac{1}{n}$ as we show in~\eqref{eq:Cf1HardBlock}.}

%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
 
\paragraph{Optimization on easy objects.}
Finally, we now show that after one iteration on \emph{any} easy object, the gaps $g_i$ on all easy objects become zero (i.e., they are all optimal and then stay optimal as the optimization is decoupled with the first block). After this iteration, BCFW with gap sampling visits all the easy objects exactly once and sets their gap estimates to zero, thus never revisiting them again.

Note that before visiting any easy object~$i$, we have $\langle \weightv, \featuremapdiffv_i (k) \rangle = 0$ for all~$k$ as the features for the hard object are orthogonal and $\weightv$ is initialized to zero. Thus, at the first visit of an easy object $i \in \{2,\dots,n\}$, we have $H_i(0;\weightv) = 0$ and $H_i(k;\weightv) = 1$, $k\geq 1$, and the max oracle returns some (any) label~$k \in \{1,\dots,K\}$.
Following the steps of Algorithm~\ref{alg:FW_product_SVM}, we have $\weightv_{\sv} := \frac{1}{\regularizerweight n} \featuremapdiffv (k) = \basis_{K+1}$ and $\ell_s=\frac{1}{n}\errorterm_i(k)=\frac{1}{n}$.
Then $g_i = \frac{1}{n}$ and $\stepsize = 1$ as $\weightv_i = \0$. 
The assignment ${\weightv_i} =  \basis_{K+1}$ implies the update $\weightv \leftarrow \weightv + \basis_{K+1}$ of the parameter vector.
After such an update, at all iterations, for all easy objects $i\in \{2,\dots,n\}$ and for all labels $k \in \{1,\dots,K\}$, we have 
\begin{equation}
\label{eq:easyblockHiszero}
H_i(k;\weightv) = \errorterm_i(k)-\langle \weightv, \basis_{K+1} \rangle=0 
\end{equation}
because the coordinate $\weight_{K+1}$ is never updated again. According to~\eqref{eq:duality_gap_withH}, the equalities~\eqref{eq:easyblockHiszero} imply that the block gaps~$g_i$ equal zero for all the easy objects.

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


\paragraph{Curvature constants.}
The simple structure of the matrix~$A$ allows us to explicitly compute the curvature constants~$\Cf^{(i)}$ corresponding to both easy and hard objects. 

The SSVM dual~\eqref{eq:svmstruct_nslack_dual} is a quadratic function with a constant Hessian~$H:=  \regularizerweight A^\transpose A$, so the second-order Taylor expansion of $f$ at a point $\dualvarv$ allows us to rewrite the definition~\ref{eqn:CfProduct} as
\begin{align*}
\notag
\Cf^{(i)} &= \sup_{\substack{\dualvarv \in \domain\\ \sv_{(i)} \in \domain^{(i)}}}
(\sv_{[i]}-\dualvarv_{[i]})^\transpose   H (\sv_{[i]}-\dualvarv_{[i]}) \\ %
 &= \regularizerweight\!\!\!\! \sup_{\substack{\dualvarv \in \domain\\ \sv_{(i)} \in \domain^{(i)}}}
\!\!\!\! \Vert A (\sv_{[i]}-\dualvarv_{[i]}) \Vert_2^2 \\
 &= \regularizerweight  \max_{k \, , \, l}  \| \featuremapv(\inputvarv_i,k)-\featuremapv(\inputvarv_i,l) \|_2^2 .
\end{align*}
The last line uses the property that the maximum of a convex function over a convex set is obtained at a vertex.

In the case of the hard object, we can get
\begin{equation} \label{eq:Cf1HardBlock}
\Cf^{(1)} = \regularizerweight  \Vert \featuremapv(\inputvarv_1,1)-\featuremapv(\inputvarv_1,2)\Vert_2^2 = \frac{1}{n}.
\end{equation}
In the case of an easy object, we can get 
\begin{equation}  \label{eq:CfiEasyBlock}
\Cf^{(i)} = \regularizerweight  \Vert \featuremapv(\inputvarv_i,1)-\featuremapv(\inputvarv_i,0)\Vert_2^2 = \frac{1}{n}.
\end{equation}

%
%
%
%
%
%
%
%
%
%
%



%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%




\paragraph{Adaptive and non-adaptive sampling.}
Let~$t$ be the number of steps needed on the hard block. By~\eqref{eq:HardBlockComplexity}, we need $t \geq \Omega(\min\{K, \frac{1}{n\epsilon}\} )$ to get a suboptimality smaller than~$\epsilon$. 
The uniform sampling scheme visits all the objects with the same probability.
In the setting constructed above, it makes, on average, $t$ visits to each easy object prior to visiting the hard object $t$ times.
Thus, the overall scheme will call the max-oracle $O(nt)$ times.
All the curvature constants~$\Cf^{(i)}$ are equal, so the sampling proportional to the curvature constants is equivalent to uniform sampling.

The adaptive sampling scheme visits each easy object only once after the first visit to any of them.
After such a visit to any easy object, its local gap estimate equals zero and this object is never visited again.
The gap sampling scheme thus makes an overall $O(n + t)$ oracle calls.
The adaptive scheme is thus approximately $\min\{n,t\} = \min\{n, K, \frac{1}{n \epsilon} \}$ times faster than the non-adaptive ones.
The speed-up can be made arbitrary large by setting both $n$ and $K$ large enough, and $\epsilon$ small enough.

\paragraph{Lipschitz and curvature constants.}
The non-uniform sampling techniques used in the work of~\citet{Nesterov:2012fa,needell2014,ZhaoImportanceSampling_ICML15} use Lipschitz constants of partial derivatives to obtain the sampling probabilities.
In our discussion above, we use the curvature constants.
\citet[Appendix C]{LacosteJulien2015linearFW} note that the curvature constants are affine invariant quantities and, thus, are more suited for the analysis of Frank-Wolfe methods compared to Lipschitz constants (which depend on a choice of norm).
We illustrate this point on our toy example by explicitly computing the Lipschitz constants over blocks for the $\ell_2$ and $\ell_1$ norm.
For both easy and hard blocks, the Lipschitz constant of the gradient with respect to the $\ell_2$ norm equals the largest eigenvalues of the corresponding block Hessians.
For an easy object, the block Hessian is a rank one matrix with the only non-zero eigenvalue equal to $\regularizerweight(|\outputdomain_i|-1) = \frac{K}{n}$.
For the hard object, the block Hessian is a diagonal matrix with non-zero entries equal to~$\frac{\regularizerweight}{2} = \frac{1}{2n}$. 
Here the Lipschitz constant for the easy block is about $K$ times bigger than the one for the hard block, and thus for a large number of labels~$K$, sampling according to Lipschitz constants can be much slower than sampling according to the curvature constants, which was itself slower than the adaptive sampling scheme.

This poor scaling of the Lipschitz constants is partly due to the bad choice of norm in relationship to the optimization domain. \citetsup{aspremont2013optimalFW} suggests to use the atomic norm of the domain~$\domain$ for the analysis. In the case of the simplex, we get the $\ell_1$ norm to measure the diameter of the domain, and its dual norm ($\ell_\infty$) to measure the Lipschitz constant of the gradient. With this norm, the Lipschitz constant stays as $\frac{1}{2n}$ for the hard block, but decreases to the more reasonable $\frac{1}{n}$ for the easy blocks. As explained in footnote~\ref{foot:Cfi}, we can use the bound \mbox{$\Cf^{(i)} \leq L_i \big(\diam_{\|\cdot\|_i} \domain^{(i)} \big)^2$} for the curvature constant. As the diameter for the simplex measured with the $\ell_1$-norm is~$2$, we get the bound~$\Cf^{(1)} \leq \frac{2}{n}$ for the hard block, very close to its exact value of~$\frac{1}{n}$ as derived in~\eqref{eq:Cf1HardBlock}. The $\ell_1$ norm thus appears as a more appropriate choice for this problem.

\begin{algorithm}[t]
    \caption{Block-coordinate Frank-Wolfe (BCFW) algorithm with gap sampling for structured SVM}%
    \label{alg:FW_product_SVM_gapsampling}
\begin{algorithmic}[1]
        %
        %
        \STATE Let $\weightv^{(0)}\!\!:=\!{\weightv_i}^{(0)}\!\!:=\!\0$; \: $\ell^{(0)}\!\!:=\!{\ell_i}^{(0)}\!\!:=\!0$; \: $g_i^{(0)}\!\!:=\!+\infty$;
        \STATE $k_i\!:=\!0$
        \COMMENT{the last time $g_i$ was computed}
        %
       \FOR{$k:=0,\dots,\infty$}
                \STATE Pick $i$ at random with probability $\propto~g_i^{(k_i)}$
                \STATE Solve $\outputvarv_i^* := \displaystyle\argmax_{\outputvarv\in\outputdomain_i} \ H_i(\outputvarv;\weightv^{(k)})$  \label{alg:FW_product_SVM_gap_sampling:max_oracle} %
                %
                \STATE Let $k_i := k$
                \STATE Let $\weightv_{\sv} := \frac1{\regularizerweight n} \featuremapdiffv_i(\outputvarv_i^*)$ \;
                and \; $\ell_{\sv} := \frac1n \errorterm_i(\outputvarv_i^*)$
                %
                %
               \STATE Let $g_i^{(k_i)} :=  \regularizerweight (\weightv_i^{(k)}-\weightv_{\sv})^\transpose \weightv^{(k)} - \ell_i^{(k)} + \ell_{\sv}$\label{alg:FW_product_SVM_gap_sampling:block_gap}
               \STATE {\small Let $\stepsize := \frac{ g_i^{(k_i)}}{ \regularizerweight \|\weightv_i^{(k)}-\weightv_{\sv}\|^2}$~and clip to $[0,1]$}\label{alg:FW_product_SVM_gap_sampling:line_search}
               %
               %
                \STATE Update ${\weightv_i}^{(k+1)}:= (1-\stepsize){\weightv_i}^{(k)}+\stepsize \,\weightv_{\sv}$
                \STATE {\small~~~~~~~and~ ${\ell_i}^{(k+1)}:= (1-\stepsize){\ell_i}^{(k)}+\stepsize\, \ell_{\sv}$}
              %
                \STATE Update $\weightv^{(k+1)}\;:= \weightv^{(k)} + {\weightv_i}^{(k+1)} - {\weightv_i}^{(k)}$
                \STATE {\small~~~~~~~and~~ $\ell^{(k+1)}:= ~\ell^{(k)}+{\ell_i}^{(k+1)} \ \  - {\ell_i}^{(k)}$}
              %
              %
              \IF {update global gap}
                \FOR{$i:=1,\dots,n$} 
                    \STATE Let $k_i:= k+1$
                    \STATE Solve $\outputvarv_i^* := \displaystyle\argmax_{\outputvarv\in\outputdomain_i} \ H_i(\outputvarv;\weightv^{(k_i)})$ 
                    \STATE Let $\weightv_{\sv} := \frac1{\regularizerweight n} \featuremapdiffv_i(\outputvarv_i^*)$ \; 
                 and \; $\ell_{\sv} := \frac1n \errorterm_i(\outputvarv_i^*)$
                    \STATE $g_i^{(k_i)} := \regularizerweight (\weightv_i^{(k_i)}-\weightv_{\sv})^\transpose \weightv^{(k_i)} - \ell_i^{(k_i)} + \ell_{\sv}$
                \ENDFOR
              \ENDIF
        \ENDFOR
%
%
\end{algorithmic}
\end{algorithm}


\section{Detailed algorithms. \label{app:algorithms}}
In this section, we give the detailed versions of our BCFW variants applied to the SSVM objective presented in the main paper. Algorithm~\ref{alg:FW_product_SVM_gapsampling} describes BCFW with adaptive gap sampling. We give the block-coordinate version of pairwise FW (BCPFW) in Algorithm~\ref{alg:pFW_product_SVM_pairwise}, and of away-step FW (BCAFW) in Algorithm~\ref{alg:aFW_product_SVM_away_steps}. We note that these two algorithms are simply the \emph{blockwise} application of the PFW and AFW algorithms as described in~\citet{LacosteJulien2015linearFW}, but in the context of SSVM which complicates the notation.  Algorithm~\ref{alg:FW_product_SVM_caching} presents the BCFW algorithm with caching. Algorithm~\ref{alg:rpAlgorithmSSVM} presents our method for computing the regularization path and Algorithm~\ref{alg:rpInitSSVM} presents the initialization of the regularization path.

Note that the three modifications proposed in our paper (gap sampling, caching, pairwise/away steps) can be straightforwardly put together in any combination. In our experimental study, we evaluate all the possibilities.

When using gap sampling or caching and to guarantee convergence, we have to do a full pass over the data every so often to refresh the global gap estimates and to compensate for the staleness effect.
In the experiments, we perform this computation every 10 passes over the data (this is the ``update global gap'' condition in the algorithms). This global gap can also be used as a certificate (upper bound) on the current suboptimality. We thus use the same frequency of global gap computation (every 10 passes) when we run a SSVM solver with a specific convergence tolerance threshold. This is used in our regularization path algorithm which runs a SSVM solver up to a fixed convergence tolerance at each breakpoint.

In our description of the regularization path algorithms (Algorithm~\ref{alg:rpInitSSVM} and Algorithm~\ref{alg:rpAlgorithmSSVM}), we explicitly describe how to update the active sets over the dual variables when the regularization parameter is updated. This is needed when using a SSVM solver that requires the active set over the dual variables (such as BCPFW or BCAFW). When using the simpler BCFW solver, then lines~\ref{alg:rpInitSSVM:activeSetBegin}--\ref{alg:rpInitSSVM:activeSetEnd} of Algorithm~\ref{alg:rpInitSSVM}
and lines~\ref{alg:rpAlgorithmSSVM:activeSetBegin}--\ref{alg:rpAlgorithmSSVM:activeSetEnd} of Algorithm~\ref{alg:rpAlgorithmSSVM} can simply be omitted.

\begin{algorithm}[t]
    \caption{Block-coordinate pairwise Frank-Wolfe (BCPFW) algorithm for structured SVM}%
    \label{alg:pFW_product_SVM_pairwise}
\begin{algorithmic}[1]
        %
        %
        \STATE Let $\weightv^{(0)}:= {\weightv_i}^{(0)}:= \0$; \; $\ell^{(0)}:={\ell_i}^{(0)}:=0$; \;  
        \STATE ${\activeS_i}^{(0)}:=\{\outputvarv_i\}$;
        \COMMENT{active sets}
        \STATE $\dualvar_i^{(0)}(\outputvarv):=0$, $\outputvarv\neq \outputvarv_i$; \; $\dualvar_i^{(0)}(\outputvarv_i):=1$
        %
       \FOR{$k:=0,\dots,\infty$}
                \STATE Pick $i$ at random in $\{1,\ldots,n\}$ 
                \STATE Solve $\outputvarv_i^* := \displaystyle\argmax_{\outputvarv\in\outputdomain_i} \ H_i(\outputvarv;\weightv^{(k)})$ %
                %
                     \COMMENT{FW corner}
                \STATE Let $\weightv_{\sv} := \frac1{\regularizerweight n} \featuremapdiffv_i(\outputvarv_i^*)$ \;
                and \; $\ell_{\sv} := \frac1n \errorterm_i(\outputvarv_i^*)$
                \STATE Solve $\outputvarv_i^a := \displaystyle\argmin_{\outputvarv\in\activeS_i^{(k)}} \ H_i(\outputvarv;\weightv^{(k)})$ \COMMENT{away corner}
                \STATE Let $\weightv_{\awayv} := \frac1{\regularizerweight n} \featuremapdiffv_i(\outputvarv_i^a)$ \;
                and  $\ell_{\awayv} := \frac1n \errorterm_i(\outputvarv_i^a)$  
                \STATE Let $\weightv_{\bm{d}} :=  \weightv_{\sv}-\weightv_{\awayv}$ \;
                 and \; $\ell_{\bm{d}}=\ell_{\sv}-\ell_{\awayv}$
				%
%
%
%
%
%
                %
                %
               \STATE Let $\stepsize := \frac{-\regularizerweight {\weightv_{\bm{d}}}^\transpose \weightv^{(k)} + \ell_{\bm{d}}}{ \regularizerweight \|\weightv_{\bm{d}}\|^2}$~and clip to $[0,\dualvar_i^{(k)}(\outputvarv_i^a)]$
               %
               %
                \STATE Update ${\weightv_i}^{(k+1)}:= {\weightv_i}^{(k)}+\stepsize \,\weightv_{\bm{d}}$
                \STATE {\small~~~~~~~and~ ${\ell_i}^{(k+1)}:= {\ell_i}^{(k)}+\stepsize\, \ell_{\bm{d}}$}
              %
                \STATE Update $\weightv^{(k+1)}\;:= \weightv^{(k)} + {\weightv_i}^{(k+1)} - {\weightv_i}^{(k)}$
                \STATE {\small~~~~~~~and~~ $\ell^{(k+1)}:= ~\ell^{(k)}+{\ell_i}^{(k+1)} \ \  - {\ell_i}^{(k)}$}
                \STATE Update ${\activeS_i}^{(k+1)}\;:= {\activeS_i}^{(k)} \cup \{\outputvarv_i^* \}$ 
                %
                \STATE {\small~~~~~~~and~~ $\dualvar_i^{(k+1)}(\outputvarv_i^a):= \dualvar_i^{(k)}(\outputvarv_i^a)-\stepsize$}
                 \STATE {\small~~~~~~~and~~ $\dualvar_i^{(k+1)}(\outputvarv_i^*):= \dualvar_i^{(k)}(\outputvarv_i^*)+\stepsize$}
               \IF{$\stepsize=\dualvar_i^{(k)}(\outputvarv_i^a)$}
                 \STATE Set ${\activeS_i}^{(k+1)}:= {\activeS_i}^{(k+1)} \setminus \{\outputvarv_i^a \}$ \COMMENT{drop step} 
                \ENDIF
              %
              %
        \ENDFOR
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[t]
    \caption{Block-coordinate away-step Frank-Wolfe (BCAFW) algorithm for structured SVM}%
    \label{alg:aFW_product_SVM_away_steps}
\begin{algorithmic}[1]
        %
        %
        \STATE Let $\weightv^{(0)}:= {\weightv_i}^{(0)}:= \0$; \; $\ell^{(0)}:={\ell_i}^{(0)}:=0$;
        \STATE $\activeS_i^{(0)}:=\{\outputvarv_i\}$;
        \COMMENT{active sets} 
        \STATE $\dualvar_i^{(0)}(\outputvarv):=0$, $\outputvarv\neq \outputvarv_i$; \; $\dualvar_i^{(0)}(\outputvarv_i):=1$
        %
       \FOR{$k:=0,\dots,\infty$}
                \STATE Pick $i$ at random in $\{1,\ldots,n\}$ 
                \STATE Solve $\outputvarv_i^* := \displaystyle\argmax_{\outputvarv\in\outputdomain_i} \ H_i(\outputvarv;\weightv^{(k)})$ %
                %
                     \COMMENT{FW corner}
                \STATE Let $\weightv_{\sv} := \frac1{\regularizerweight n} \featuremapdiffv_i(\outputvarv_i^*)$ \;
                and \; $\ell_{\sv} := \frac1n \errorterm_i(\outputvarv_i^*)$
                 \STATE Solve $\outputvarv_i^a := \displaystyle\argmin_{\outputvarv\in\activeS_i^{(k)}} \ H_i(\outputvarv;\weightv^{(k)})$ \COMMENT{away corner}
                \STATE Let $\weightv_{\awayv} := \frac1{\regularizerweight n} \featuremapdiffv_i(\outputvarv_i^a)$ \;
                and \; $\ell_{\awayv} := \frac1n \errorterm_i(\outputvarv_i^a)$
				\STATE Let $g_i^{FW} :=  \regularizerweight (\weightv_i^{(k)}-\weightv_{\sv})^\transpose \weightv^{(k)} - \ell_i^{(k)} + \ell_{\sv}$ \label{alg:FW_product_SVM_away_steps:block_gap}
                \STATE Let $g_i^{A} :=  \regularizerweight (\weightv_{\awayv}-\weightv_i^{(k)})^\transpose \weightv^{(k)} + \ell_i^{(k)} - \ell_{\awayv}$\label{alg:FW_product_SVM_away_steps:away_gap}\vspace{1.5mm}
				\IF{$g_i^{FW} > g_i^{A}$
%
} \label{alg:aFW_product_SVM_startA}  \COMMENT{FW step}
		  			%
		  			%
                \STATE Let $\stepsize := \frac{g_i^{FW}}{ \regularizerweight \|\weightv_i^{(k)}-\weightv_{\sv}\|^2}$~and clip to $[0,1]$
               %
		  			\STATE Update ${\activeS_i}^{(k+1)}\;:= {\activeS_i}^{(k)} \cup \{\outputvarv_i^* \}$
		  			\STATE {\small~~~~~~~and~~ $\dualvar_i^{(k+1)}(\outputvarv):= (1-\stepsize)\dualvar_i^{(k)}(\outputvarv)$}
                     \STATE {\small~~~~~~~and~~ $\dualvar_i^{(k+1)}(\outputvarv_i^*):= \dualvar_i^{(k+1)}(\outputvarv_i^*)+\stepsize$}
                     \STATE Set ${\activeS_i}^{(k+1)}\;:= \{\outputvarv_i^* \}$ if $\stepsize=1$
		  		\ELSE \COMMENT{away step}
		  			%
		  			%
                    \STATE Let $\stepsize := \frac{g_i^{A}}{ \regularizerweight \|\weightv_i^{(k)}-\weightv_{\awayv}\|^2}$~and clip to $[0,\frac{\dualvar_i(\outputvarv_i^a)}{1-\dualvar_i(\outputvarv_i^a)}]$
		  			\STATE Update {\small~ $\dualvar_i^{(k+1)}(\outputvarv):= (1+\stepsize)\dualvar_i^{(k)}(\outputvarv)$}
                     \STATE {\small~~~~~~~and~~ $\dualvar_i^{(k+1)}(\outputvarv_i^a):= \dualvar_i^{(k+1)}(\outputvarv_i^a)-\stepsize$}		  							 \STATE {\small~~~~~~~~and~~ ${\activeS_i}^{(k+1)}\;:= {\activeS_i}^{(k)} \setminus \{\outputvarv_i^a \}$ if $\dualvar_i^{(k+1)}(\outputvarv_i^a)=0$}	
		  			
		  		\ENDIF \label{alg:aFW_product_SVM_endA}
                %
                %
               %
               %
               %
                \STATE Update ${\weightv_i}^{(k+1)}:= {\weightv_i}^{(k)}+\stepsize \,\weightv_{\bm{d}}$
                \STATE {\small~~~~~~~and~ ${\ell_i}^{(k+1)}:= {\ell_i}^{(k)}+\stepsize\, \ell_{\bm{d}}$}
              %
                \STATE Update $\weightv^{(k+1)}\;:= \weightv^{(k)} + {\weightv_i}^{(k+1)} - {\weightv_i}^{(k)}$
                \STATE {\small~~~~~~~and~~ $\ell^{(k+1)}:= ~\ell^{(k)}+{\ell_i}^{(k+1)} \ \  - {\ell_i}^{(k)}$}
              %
              %
        \ENDFOR
\end{algorithmic}
\end{algorithm}


\section{Proof of Theorem~\ref{thm:convTheoremGapSampling} (convergence of BCFW with gap sampling) \label{app:proof_gap_sampling}}

\begin{lemma}[Expected block descent lemma] \label{lemma:convGapSampling:expectedDescent}
	Let $g_j(\dualvarv^{(k)})$ be the block gap for block~$j$ for the iterate $\dualvarv^{(k)}$. Let $\dualvarv^{(k+1)}$ be obtained by sampling a block~$i$ with probability $p_i$ and then doing a (block) FW step with line-search on this block, starting from $\dualvarv^{(k)}$. Consider \emph{any} set of scalars $\stepsize_j \in [0,1]$, $j = 1, \ldots, n$, which do not depend on the chosen block $i$.
    Then in conditional expectation over the random choice of block $i$ with probabilities~$p_i$, it holds:
    \begin{align}  
    \notag\E\big[ f(\dualvarv^{(k+1)})\,|\, \dualvarv^{(k)}\big]
    &\le f(\dualvarv^{(k)}) -  \sum_{i=1}^n{\gamma_i p_i g_i(\dualvarv^{(k)})} \\[-0.1cm]
    \label{mineq} &\quad + \frac{1}{2}\sum_{i=1}^n{\gamma_i^2 p_i \Cf^{(i)}}.
    \end{align}\\[-1.0cm]
\end{lemma}
\begin{proof}
    The proof is analogous to the proof of Lemma~\ref{lem:block_step_improvement}, but being careful with the expectation. Let block~$i$ be the chosen one that defined $\dualvarv^{(k+1)}$ and let $\dualvarv_\stepsize := \dualvarv+\stepsize(\sv_{[i]}-\dualvarv_{[i]})$, where $\mathbf{s}_{(i)}\in \domain^{(i)}$ is the FW corner on block~$i$ and~$\mathbf{s}_{[i]} \in \R^m$ is its zero-padded version. By the line-search, we have $f(\dualvarv^{(k+1)}) \leq f(\dualvarv_\stepsize)$ for any $\stepsize \in [0,1]$. By using $\stepsize = \stepsize_i$ in the bound~\eqref{eqn:CfProduct} provided in the curvature Definition~\ref{def:curvature_const}, and by the definition of the Frank-Wolfe gap, we get:
    \begin{align*}
    f(\dualvarv^{(k+1)}) \leq  f(\dualvarv_{\stepsize_i})
    &= 
    f(\dualvarv^{(k)} + \gamma_i (\mathbf{s}_{[i]} - \dualvarv^{(k)}_{[i]})) \\
    & \leq f(\dualvarv^{(k)}) + \gamma_i g_i(\dualvarv^{(k)}) + \frac{\gamma_i^2}{2}\Cf^{(i)}.
    \end{align*}
    Taking the expectation of the bound with respect to $i$, conditioned on $\dualvarv^{(k)}$, proves the lemma.
\end{proof}

\begin{definition}
    \label{def:nonUnifMeasure}
    The \emph{nonuniformity measure} $\nonuniformity(\bm{x})$ of a vector~$\bm{x}\in\R^n_+$ is defined as:
    \begin{equation*}
    \nonuniformity(\bm{x}) 
    :=  
    \sqrt{1+n^2\Var \big[\bm{p}\big]}
    %
    \end{equation*}
    where~$\bm{p} := \frac{\bm{x}}{\Vert \bm{x} \Vert_1}$ is the probability vector obtained by normalizing~$\bm{x}$.
\end{definition}

\begin{lemma}
    \label{lemma:L1L2}
    Let $\bm{x}\in\R^n_+$.
    The following relation between its $\ell_1$-norm and $\ell_2$-norm holds:
    %
    %
    %
    \begin{equation*}
    \Vert \bm{x} \Vert_2 =  \frac{\nonuniformity(\bm{x})}{\sqrt{n}}\Vert \bm{x} \Vert_1 \, .
    \end{equation*}
\end{lemma}
\begin{proof}
    We have that\\[-0.2cm]
    \begin{equation}
    \label{eq:lemma_L1L2_proof1}
    \Var \big[\bm{p}\big] = \E \big[ \bm{p}^2 \big] - \E \big[ \bm{p} \big]^2 = \frac{1}{n} \| \bm{p} \|_2^2 - \frac{1}{n^2}.
    \end{equation}
    Combining~\eqref{eq:lemma_L1L2_proof1} and Definition~\ref{def:nonUnifMeasure} we prove the lemma.
\end{proof}


\textbf{Remark.} For any~$\bm{x} \in \R^n_+$, the quantity $\nonuniformity(\bm{x})$ always belongs to the segment~$[1,\sqrt{n}]$. 
We have $\nonuniformity(\bm{x})=1$ when all the elements of~$\bm{x}$ are equal and~$\nonuniformity(\bm{x})=\sqrt{n}$ when all the elements, except one, equal zero.
%

\begin{reptheorem}{thm:convTheoremGapSampling}
    Assume that at each iterate~$\dualvarv^{(k)}$, $k\ge 0$, BCFW with gap sampling (Algorithm~\ref{alg:FW_product_SVM_gapsampling}) has access to the exact values of the block gaps. 
    Then, at each iteration, it holds that
    $
    \E \big[f(\dualvarv^{(k)})\big] - f(\dualvarv^*) \le \frac{2n}{k+2n}\big(\CfTotal \nonuniformityTotal + h_0\big)
    $
    where $\dualvarv^*\in \domain$ is a solution of problem~(\ref{eq:svmstruct_nslack_dual}), $h_0 := f(\dualvarv^{(0)}) - f(\dualvarv^*)$ is the suboptimality at the starting point of the algorithm, the constant~$\CfTotal := \sum_{i=1}^n \Cf^{(i)}$ is the sum of the curvature constants, and the constant~$\nonuniformityTotal$ is an upper bound on $\E \Big[ \frac{\nonuniformity(\Cf^{(:)})}{\nonuniformity(\bm{g}_{:}(\dualvarv^{(k)}))^3} \Big]$, which quantities the amount of non-uniformity of the $\Cf^{(i)}$'s in relationship to the non-uniformity of the gaps obtained during the algorithm.
    The expectations are taken over the random choice of the sampled block at iterations $1,\dots,k$ of the algorithm.
\end{reptheorem}

\begin{proof}
    Starting from Lemma~\ref{lemma:convGapSampling:expectedDescent} with $\stepsize_i:=\stepsize$ for some $\stepsize$ to be determined later and $p_i := \frac{g_i}{g}$ where~$g_i:=g_i(\dualvarv^{(k)})$ and~$g:=g(\dualvarv^{(k)})$, we get\\[-0.4cm]
    \begin{align}  
    \notag
    \E\big[ f(\dualvarv^{(k+1)})\,|\, \dualvarv^{(k)}\big]
    &\le f(\dualvarv^{(k)}) -  \gamma \sum_{i=1}^n \frac{g_i^2}{ g } \\[-0.1cm]
    \label{eq:thrmGapSampling_proof_descent1}
    & + \frac{\gamma^2}{2}\sum_{i=1}^n  \Cf^{(i)} \frac{g_i}{ g }.
    \end{align}\\[-0.3cm]
    The Cauchy-Schwarz inequality bounds the dot product between the vectors of curvature constants~$\bm{c} := \Cf^{(:)} := (\Cf^{(i)})_{i=1}^n$ and block gaps~$\bm{g}:= \bm{g}_{:}(\dualvarv^{(k)}) :=(g_i)_{i=1}^n$\\[-0.3cm]
    \begin{equation}
    \label{eq:thrmGapSampling_proof_CS}
    \sum_{i=1}^n  \Cf^{(i)} g_i
    \leq 
    \| \bm{c} \|_2 \;  \| \bm{g} \|_2 \,.
    \end{equation}\\[-0.2cm]
    Combining~\eqref{eq:thrmGapSampling_proof_CS} and the result of Lemma~\ref{lemma:L1L2} for the vectors of curvature constants and block gaps (with $\| \bm{g} \|_1 = g$ and $\| \bm{c} \|_1 = \CfTotal$), we can further bound~\eqref{eq:thrmGapSampling_proof_descent1}: 
    \begin{align}  
    \notag
    \E\big[ f(\dualvarv^{(k+1)})\,|\, \dualvarv^{(k)}\big]
    &\le f(\dualvarv^{(k)}) -  \frac{\gamma g}{n} \nonuniformity( \bm{g} )^2 \\[-0.1cm]
    \label{eq:thrmGapSampling_proof_descent2}
    & + \frac{\gamma^2}{2n} \nonuniformity( \bm{g} ) \, \nonuniformity( \bm{c} ) \, \CfTotal.
    \end{align}
    Subtracting the minimal function value $f(\dualvarv^*)$ from both sides of~\eqref{eq:thrmGapSampling_proof_descent2} and by using 
    $
    h(\dualvarv^{(k)}) := f(\dualvarv^{(k)}) - f(\dualvarv^*) \leq g
    $,
    we bound the conditional expectation of the suboptimality $h$ with\\[-0.4cm]
    \begin{align}
    \notag \E [ h(\dualvarv^{(k+1)}) \mid \dualvarv^{(k)}] 
    \le\;& 
    h(\dualvarv^{(k)}) - \frac{\stepsize}{n} \nonuniformity( \bm{g} )^2\, h(\dualvarv^{(k)}) 
    \\
    \label{eq:thrmGapSampling_proof_descent3}
    +\;&
    \frac{\stepsize^2}{2n} \nonuniformity( \bm{g} ) \, \nonuniformity( \bm{c} ) \,\CfTotal 
    \end{align}\\[-0.3cm]
    which is analogous to~\citep[Eq.~(20)]{lacosteJulien13bcfw}.
    In what follows, we use the modified induction technique of~\citep[Proof of Theorem~C.1]{lacosteJulien13bcfw}.
    
    By induction, we are going to prove the following upper bound on the unconditional expectation of the suboptimality $h$:\\[-0.2cm]
    \begin{equation}
    \label{eq:thrmGapSampling_proof_inductionAssumption}
    \E \big[ h(\dualvarv^{(k)}) \big] \leq \frac{2n C}{k+2n} , \quad \text{for\;$k\geq0$,}
    \end{equation}
    that corresponds to the statement of the theorem with~$C:= \CfTotal \nonuniformityTotal + h_0$.
    
    The basis of the induction $k=0$ follows immediately from the definition of~$C$, given that~$\CfTotal \geq 0$ and $\nonuniformityTotal > 0$.
    
    Consider the induction step.
    Assume that~\eqref{eq:thrmGapSampling_proof_inductionAssumption} is satisfied for~$k \geq 0$.  
    With a particular choice of step size~$\gamma:= \frac{2n}{\nonuniformity( \bm{g} )^2 (k+2n)} \in[0,1]$ (which does not depend on the picked~$i$), we rewrite the bound~\eqref{eq:thrmGapSampling_proof_descent3} on the conditional expectation as\\[-0.4cm]
    \begin{align}
    \notag \E [ h(\dualvarv^{(k+1)}) \mid \dualvarv^{(k)}] 
    \le\;& 
    \Bigl( 1 - \frac{2}{k+2n} \Bigr) \, h(\dualvarv^{(k)}) 
    \\
    \label{eq:thrmGapSampling_proof_inductionStep1}
    +\;&
    \frac{2n}{(k+2n)^2} \frac{ \nonuniformity( \bm{c} ) \,\CfTotal }{\nonuniformity( \bm{g} )^3} \, .
    \end{align}\\[-0.3cm]
    Taking the unconditional expectation of~\eqref{eq:thrmGapSampling_proof_inductionStep1}, then the induction assumption~\eqref{eq:thrmGapSampling_proof_inductionAssumption} and the definition of~$\nonuniformityTotal$ give us the deterministic inequality\\[-0.6cm]
    \begin{align}
    \notag \E [ h(\dualvarv^{(k+1)}) ] 
    \le\;& 
    \Bigl( 1 - \frac{2}{k+2n} \Bigr) \, \frac{2nC}{k+2n} 
    \\
    \label{eq:thrmGapSampling_proof_inductionStep2}
    +\;&
    \frac{2n}{(k+2n)^2} \nonuniformityTotal \,\CfTotal.
    \end{align}\\[-0.3cm]
    Bounding~$\nonuniformityTotal \,\CfTotal$ by~$C$ and rearranging the terms gives\\[-0.4cm]
    \begin{align}
    \notag \E [ h(\dualvarv^{(k+1)}) ]  
    \le&
    \frac{2nC}{k+2n} \left(1 - \frac{2}{k+2n} + \frac{1}{k+2n}\right)
    \\
    \notag=&
    \frac{2nC}{k+2n} \frac{k+2n-1}{k+2n}
    \\
    \notag\le&
    \frac{2nC}{k+2n} \frac{k+2n}{k+2n+1}
    \\
    \notag=&
    \frac{2nC}{(k+1)+2n} \ ,
    \end{align}\\[-0.3cm]
    which completes the induction proof.  
\end{proof}

\paragraph{Comparison with uniform sampling.} 
We now compare the rates obtained by Theorem~\ref{thm:convTheoremGapSampling} for BCFW with gap sampling and by Theorem~\ref{thm:convergence_FW_product} for BCFW with uniform sampling. The only difference is in the constants: Theorem~\ref{thm:convTheoremGapSampling} has~$\CfTotal \nonuniformityTotal$ and Theorem~\ref{thm:convergence_FW_product} has~$\CfTotal$. 

Recall that by definition\\[-0.4cm]
\begin{equation*}
\nonuniformityTotal = \max_k \;\E \Big[ \frac{\nonuniformity(\Cf^{(:)})}{\nonuniformity(\bm{g}_{:}(\dualvarv^{(k)}))^3} \Big]
\end{equation*}\\[-0.3cm]
In the best case for gap sampling, the curvature constants are uniform, $\nonuniformity(\Cf^{(:)})=1$, and the gaps are nonuniform~$\nonuniformity(\bm{g}_{:}(\dualvarv^{(k)})) \approx \sqrt{n}$. Thus, $\nonuniformityTotal \approx \frac{1}{n\sqrt{n}}$.

In the worst case for gap sampling, the curvature constants are very non-uniform, $\nonuniformity(\Cf^{(:)}) \approx \sqrt{n}$. The constant for gap sampling is still better if the gaps are non-uniform enough, i.e., $\nonuniformity(\bm{g}_{:}(\dualvarv^{(k)})) \geq n^{\frac{1}{6}}$. 

We note that to design a sampling scheme that always dominates uniform sampling (in terms of bounds at least), we would need to include the $\Cf^{(i)}$'s in the sampling scheme (as was essentially done by~\citet{Csiba15adaSDCA} for SDCA). Unfortunately, computing good estimates for $\Cf^{(i)}$'s is too expensive for structured SVM, thus motivating our simpler yet practically efficient scheme. See also the discussion after~\eqref{eq:expectedImprovement}.

\section{Proof of Theorem~\ref{thm:cacheTheorem} (convergence of BCFW with caching)\label{app:caching_theorem}}
\begin{reptheorem}{thm:cacheTheorem} \label{thm:convergence_cache_supp}
Let~$\tilde{\cacheN} := \frac1n\cacheN \leq 1$. 
Then, for each $k\ge 0$, the iterate $\dualvarv^{(k)}$ of %
Algorithm~\ref{alg:FW_product_SVM_caching} satisfies
$
\E\big[f(\dualvarv^{(k)})\big] - f(\dualvarv^*) \le \frac{2n}{\tilde{\cacheN}k+2n}\big(\frac{1}{\tilde{\cacheN}}\CfTotal+ h_0\big)
$
where $\dualvarv^*\in \domain$ is a solution of problem~(\ref{eq:svmstruct_nslack_dual}), $h_0 := f(\dualvarv^{(0)}) - f(\dualvarv^*)$ is the suboptimality at the starting point of the algorithm, $\CfTotal := \sum_{i=1}^n \Cf^{(i)}$ is the sum of the curvature constants (see Definition~\ref{def:curvature_const}) of $f$ with respect to the domains~$\domain^{(i)}$ of individual blocks.
The expectation is taken over the random choice of the sampled blocks at iterations $1,\dots,k$ of the algorithm.
\end{reptheorem}
\begin{proof}
The key observation of the proof consists in the fact that the combined oracle (the cache oracle in the case of a cache hit and the max oracle in the case of a cache miss) closely resembles an oracle with multiplicative approximation error~\citep[Eq.~(12) of Appendix~C]{lacosteJulien13bcfw}.

In the case of a cache hit, Definition~\ref{def:curvature_const} of curvature constant for any step size $\gamma \in [0,1]$ gives us
\begin{align*}
 &f(\dualvarv^{(k+1)}_\stepsize) := f(\dualvarv^{(k)}+\stepsize(\cachev_{[i]}-\dualvarv^{(k)}_{[i]})) \\
    \leq \; & f(\dualvarv^{(k)}) + \stepsize \langle \cachev_{(i)}\!-\!\dualvarv^{(k)}_{(i)}, \nabla_{(i)} f(\dualvarv^{(k)})\rangle + \frac{\stepsize^2}{2}\Cf^{(i)} \\
     = \;& f(\dualvarv^{(k)})- \stepsize \cachegap_i^{(k)} + \frac{\stepsize^2}{2}\Cf^{(i)} \\
     \leq \;& f(\dualvarv^{(k)}) - \gamma \tilde{\cacheN} g^{(k_0)} + \frac{\stepsize^2}{2}\Cf^{(i)}
\end{align*}
where the corner~$\cachev_{(i)} \in \domain^{(i)}$ and its zero-padded version~$\cachev_{[i]} \in \R^m$ are provided by the cache oracle, and $\tilde{\cacheN} = \frac1n \cacheN$ is the constant controlling the global part of the cache-hit criterion.
In the case of a cache miss, similarly to Lemma~\ref{lem:block_step_improvement}, we get
\begin{equation*}
    f(\dualvarv^{(k+1)}_\stepsize) \leq f(\dualvarv^{(k)}) - \stepsize g_i^{(k)} + \frac{\stepsize^2}{2}\Cf^{(i)} \ .
\end{equation*}

Combining the two cases we get 
\begin{equation}
\label{eq:cache_proof:block_descent}
    f(\dualvarv^{(k+1)}_\stepsize) \leq f(\dualvarv^{(k)}) - \stepsize \tilde{g}_i^{(k)} + \frac{\stepsize^2}{2}\Cf^{(i)}
\end{equation}
where
\[
\tilde{g}_i^{(k)}\! := [i \text{ is a cache miss}] g_i^{(k)}  +   [i \text{ is a cache hit}] \tilde{\cacheN} g^{(k_0)}.
\]
%
%
%


\begin{algorithm}[t!]
    \caption{Block-coordinate Frank-Wolfe (BCFW) algorithm with cache for structured SVM \label{alg:FW_product_SVM_caching}}%
    \begin{algorithmic}[1]
        %
        %
        \STATE Let $\weightv^{(0)}\!:=\!{\weightv_i}^{(0)}\!:=\!\0$; \; $\ell^{(0)}\!:=\!{\ell_i}^{(0)}\!:=\!0$; \; $\cache_i\!:=\!\{\outputvarv_i\}$;
        \STATE $g^{(0)}\!:=\!g_i^{(0)}\!=\!+\infty$
        \STATE $k_0\!:=\!k_i\!:=\!0$ ; \; \COMMENT{the last time $g$ / $g_i$ was computed}
        %
        \FOR{$k:=0,\dots,\infty$}
        \STATE Pick $i$ at random in $\{1,\ldots,n\}$ \label{alg:FW_product_SVM_caching:randomSample}
        \COMMENT{either uniform or \\ \hfill with probability $\propto g_i^{(k_i)}$ for gap sampling}
        \STATE Solve $\outputvarv_i^c := \argmax_{\outputvarv\in\cache_i} \ H_i(\outputvarv;\weightv)$ 
        	\COMMENT{cache corner}
        \STATE Let $\weightv_{\cachev} := \frac{ \featuremapdiffv_i(\outputvarv_i^c)}{\regularizerweight n}$ \; 
        and \; $\ell_{\cachev}:=\frac1n L_i(\outputvarv_i^c)$
        \STATE Let $\cachegap_i^{(k)} := \regularizerweight(\weightv_i^{(k)}-\weightv_{\cachev})^\transpose \weightv^{(k)}-{\ell_i}^{(k)}+\ell_{\cachev}$
        \IF{$\cachegap_i^{(k)} \geq \max(\cacheF g_i^{(k_i)},\frac{\cacheN}{n} g^{(k_0)})$} \COMMENT{cache hit}
        \STATE $\weightv_{\sv}:= \weightv_{\cachev}$, $\ell_{\sv} := \ell_{\cachev}$, $\cachegap_i := \cachegap_i^{(k)}$
        \ELSE \COMMENT{cache miss}
        \STATE Solve $\outputvarv_i^* := \displaystyle\argmax_{\outputvarv\in\outputdomain_i} \ H_i(\outputvarv;\weightv^{(k)})$  \COMMENT{FW corner}
        \STATE Let $\weightv_{\sv} := \frac1{\regularizerweight n} \featuremapdiffv_i(\outputvarv_i^*)$ \;
        and\; $\ell_{\sv} := \frac1n \errorterm_i(\outputvarv_i^*)$
        \STATE Let $g_i^{(k)} :=  \regularizerweight (\weightv_i^{(k)}-\weightv_{\sv})^\transpose \weightv^{(k)} - \ell_i^{(k)} + \ell_{\sv}$\label{alg:FW_product_SVM_caching:block_gap}
        \STATE Set $k_i := k$, $\cachegap_i := g_i^{(k)}$
        \STATE Update ${\cache_i}\;:= {\cache_i} \cup \{\outputvarv_i^* \}$  
        \ENDIF 
        \STATE Let $\stepsize := \frac{ \cachegap_i }{ \regularizerweight \|\weightv_i^{(k)}-\weightv_{\sv}\|^2}$~and clip to $[0,1]$ \label{alg:FW_product_SVM_caching:line_search}
        %
        %
        \STATE Update ${\weightv_i}^{(k+1)}:= (1-\stepsize){\weightv_i}^{(k)}+\stepsize \,\weightv_{\sv}$
        \STATE {\small~~~~~~~and~ ${\ell_i}^{(k+1)}:= (1-\stepsize){\ell_i}^{(k)}+\stepsize\, \ell_{\sv}$}
        %
        \STATE Update $\weightv^{(k+1)}\;:= \weightv^{(k)} + {\weightv_i}^{(k+1)} - {\weightv_i}^{(k)}$
        \STATE {\small~~~~~~~and~~ $\ell^{(k+1)}:= ~\ell^{(k)}+{\ell_i}^{(k+1)} \ \  - {\ell_i}^{(k)}$}
        %
        %
        \IF {update global gap}
        \STATE Let $g^{(k_0)} := 0$, $k_0 := k+1$
        \FOR{$i:=1,\dots,n$} 
        \STATE Solve $\outputvarv_i^* := \displaystyle\argmax_{\outputvarv\in\outputdomain_i} \ H_i(\outputvarv;\weightv^{(k_0)})$ 
        \STATE Let $\weightv_{\sv} := \frac1{\regularizerweight n} \featuremapdiffv_i(\outputvarv_i^*)$ \;
        and \; $\ell_{\sv} := \frac1n \errorterm_i(\outputvarv_i^*)$
        \STATE $g^{(k_0)} +\!\!= \regularizerweight (\weightv_i^{(k_0)}-\weightv_{\sv})^\transpose \weightv^{(k_0)} - \ell_i^{(k_0)} + \ell_{\sv}$
        \STATE Set $k_i:=k_0$
        \ENDFOR
        \ENDIF
        \ENDFOR
    \end{algorithmic}
\end{algorithm}




Subtracting~$f(\dualvarv^*)$ from both sides of~\eqref{eq:cache_proof:block_descent} and taking the expectation of~\eqref{eq:cache_proof:block_descent} w.r.t.\ the block index~$i$ we get
\begin{equation}
\label{eq:cache_proof:block_descent_expected_h}
    \E\big[ h(\dualvarv^{(k+1)}_\stepsize)\,|\, \dualvarv^{(k)}\big] \leq h(\dualvarv^{(k)}) - \frac{\stepsize}{n} \tilde{g}^{(k)} + \frac{\stepsize^2}{2n}\CfTotal
\end{equation}
where~$h(\dualvarv):=f(\dualvarv)-f(\dualvarv^*)$ is the suboptimality of the function~$f$ and $\tilde{g}^{(k)} := \sum_{i=1}^n\tilde{g}_i^{(k)}$.
We know that the duality gap upper-bounds the suboptimality, i.e., $g(\dualvarv)\geq h(\dualvarv)$, and that cache miss steps, as well as cache hit steps, always decrease suboptimality, i.e., $h(\dualvarv^{(k)}) \leq h(\dualvarv^{(k_0)})$. 

If at iteration~$k$ there is at least one cache hit, then we can bound the quantity~$\tilde{g}^{(k)}$ from below:\\[-2mm]
\begin{equation}
\label{eq:cache_proof:case_one_cache_hit}
\tilde{g}^{(k)} \geq \tilde{\cacheN} g(\dualvarv^{(k_0)}) \geq \tilde{\cacheN} h(\dualvarv^{(k_0)}) \geq \tilde{\cacheN} h(\dualvarv^{(k)}).
\end{equation}
In the case of no cache hits, we have\\[-2mm]
\[
\tilde{g}^{(k)} = g(\dualvarv^{(k)}) \geq h(\dualvarv^{(k)}) \geq \tilde{\cacheN} h(\dualvarv^{(k)})
\]
where the last inequality holds because~$\tilde{\cacheN} \leq 1$. Applying the lower bound on~$\tilde{g}^{(k)}$ to~\eqref{eq:cache_proof:block_descent_expected_h}, we get
\begin{equation}
\begin{aligned}
\label{eq:cache_proof:block_descent_expected_h_bounded}
    \E\big[ h({ \dualvarv^{(k+1)}_\stepsize})\,|\, { \dualvarv^{(k)}}\big] &\leq h(\dualvarv^{(k)}) - \frac{\stepsize\tilde{\cacheN}}{n} h(\dualvarv^{(k)}) \\ &\quad+ \frac{\stepsize^2}{2n}\CfTotal.
\end{aligned}
\end{equation}



\begin{algorithm}[t!]
    \caption{{\sc init-reg-path}: Initialization of the regularization path for structured SVM}
    \label{alg:rpInitSSVM}
    \begin{algorithmic}[1]
        \INPUT $\factorRegPath$, tolerance $\epsilon$ %
        \STATE  $\weightv := \weightv_i := 0$; \; $\ell := \ell_i := 0$; \; $\tilde{\featuremapdiffv} := 0$
        %
        \FOR{$i := 1, \dots , n$}
        \STATE $\tilde{\outputvarv}_i := \argmax_{\outputvarv \in \outputdomain_i} H_i (\outputvarv; \0)$
        \STATE $\ell_i := \frac1n \errorterm(\outputvarv_i, \tilde{\outputvarv}_i)$
        \STATE $\ell := \ell + \ell_i$
        \STATE $\tilde{\featuremapdiffv} := \tilde{\featuremapdiffv} + \frac1n \featuremapdiffv(\tilde{\outputvarv}_i)$
        \ENDFOR
        \FOR{$i := 1, \dots, n$}
        \STATE $\theta_i := \max_{\outputvarv \in \mathcal{Y}_i}  \big( - \tilde{\featuremapdiffv}^{\transpose} \featuremapdiffv(\outputvarv) \big)$
        \ENDFOR
        \STATE Let $\regularizerweight^\infty := \frac{1}{\factorRegPath\epsilon}\bigg( \|\tilde{\featuremapdiffv}\|^2 + \frac1n \sum_{i=1}^n \theta_i \bigg)$
        \STATE Let $\weightv := \frac{1}{\regularizerweight^\infty} \tilde{\featuremapdiffv}$; \; $\weightv_i := \frac{1}{n \regularizerweight^\infty} \featuremapdiffv(\tilde{\outputvarv}_i)$
        \STATE \textbf{for} $i:=1,\dots,n$ \;\textbf{do}\; $g_i := \frac{1}{\numsamples \regularizerweight^\infty}\theta_i + \regularizerweight^\infty \weightv_i^{\transpose} \weightv$
        \FOR{$i:=1,\dots,n$} \COMMENT{optional} \label{alg:rpInitSSVM:activeSetBegin}
        \STATE $\activeS_i := \{\tilde{\outputvarv}_i\}$ 
        \STATE $\dualvar_i(\tilde{\outputvarv}_i):=1$ and $\dualvar_i(\outputvarv):=0$ for $\outputvarv \neq \tilde{\outputvarv}_i$
        \ENDFOR \label{alg:rpInitSSVM:activeSetEnd}
        \STATE \textbf{return} $\weightv$, $\weightv_i$, $\ell$, $\ell_i$, $g_i$, $\regularizerweight^\infty$, $\activeS_i$, $\dualvarv$
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}[t!]
    \caption{Regularization path for structured SVM}
    \label{alg:rpAlgorithmSSVM}
    \begin{algorithmic}[1]
        \INPUT $\factorRegPath$, tolerance $\epsilon$, $\regularizerweight_{min}$
        \STATE Initialize regularization path using Algorithm~\ref{alg:rpInitSSVM}. $\{\weightv^0\!\!, \weightv_i^0, \ell^0\!\!, \ell_i^0, g_i^0, \regularizerweight^0\!\!, \activeS_i^0, \dualvarv^0 \}$ \!:=\! {\sc init-reg-path} $(\factorRegPath, \epsilon)$
        \STATE $J := 0$
        \REPEAT
        \STATE \textbf{For}\; $i := 1,\dots,n$ \;\textbf{do}\; $\delta_i := \ell_i^J -\regularizerweight^J \langle\weightv^J, \weightv_i^J\rangle $
        \STATE Compute excess gap\; $\tau := \epsilon - \sum_{i=1}^n g_i^J$
        \STATE Let\; $\Delta := \sum_i^{n} \delta_i$
        \IF{$\Delta \leq  \tau$}
        \STATE Let $\regPathStep := 1 - \frac{\tau}{\Delta}$
        \ELSE
        \STATE $\weightv^J$ is $\epsilon$-approximate for any $\regularizerweight < \regularizerweight^J$
        \STATE \textbf{return}\quad$\{\regularizerweight^j\}_{j=0}^J$, $\{\weightv^j\}_{j=0}^J$
        \ENDIF
        \STATE Let\; $\regularizerweight^{J+1} := \regPathStep\regularizerweight^{J}$,\; $\ell^{J+1} := \regPathStep\ell^{J}$,\; 
        \FOR{$i:=1,\dots,n$} \COMMENT{update gaps using~\eqref{eq:ssvm_rp_gapnew}}
        \STATE Let\; $g^{J+1}_i := g^{J}_i + (1 - \regPathStep ) \delta_i$ \; and \; $\ell_i^{J+1} := \regPathStep\ell_i^{J}$
        \ENDFOR
        \FOR{$i:=1,\dots,n$} \COMMENT{optional: update duals} \label{alg:rpAlgorithmSSVM:activeSetBegin}
            \STATE $\activeS_i^{J+1} := \activeS_i^{J} \cup \{\outputvarv_i\}$
            \STATE $\dualvar_i^{J+1}(\outputvarv) := \regPathStep \dualvar_i^{J}(\outputvarv)$ for  $\outputvarv \in \activeS_i^{J+1} \setminus \{\outputvarv_i\}$
            \STATE $\dualvar_i^{J+1}(\outputvarv_i):=1-\sum_{\outputvarv \in \activeS_i^{J+1}\setminus \{\outputvarv_i\}} \dualvar_i^{J+1}(\outputvarv)$
        \ENDFOR \label{alg:rpAlgorithmSSVM:activeSetEnd}
        \STATE Run SSVM-optimizer with tolerance $\factorRegPath\,\epsilon$ \label{alg:rpAlgorithmSSVM:SSVM} \\ \quad\; to update $\weightv^{J+1}$\!\!, $\weightv_i^{J+1}$\!\!, $\ell^{J+1}$\!\!, $\ell_i^{J+1}$\!\!, $g_i^{J+1}$\!\!, $\activeS_i^{J+1}$\!\!, $\dualvarv^{J+1}$\!\! 
        \COMMENT{to have $\epsilon$-appr. path, gaps $g_i^{J+1}$ have to be exact}
        \STATE $J := J+1$
        \UNTIL{$\regularizerweight^{J+1} < \regularizerweight_{min}$}
        \STATE \textbf{return}\quad$\{\regularizerweight^j\}_{j=0}^J$, $\{\weightv^j\}_{j=0}^J$
    \end{algorithmic}
\end{algorithm}

Inequality~\eqref{eq:cache_proof:block_descent_expected_h_bounded} is identical to the inequality~\citep[Eq.~(20)]{lacosteJulien13bcfw} in the proof of convergence of BCFW with a multiplicative approximation error in the oracle.
We recopy their argument below for reference to finish the proof.
First, we take the expectation of~\eqref{eq:cache_proof:block_descent_expected_h_bounded} w.r.t.\ the choice of previous blocks:
\begin{equation}
\label{eq:cache_proof:block_descent_expected_h_bounded2}
    \E\big[ h(\dualvarv^{(k+1)}_\stepsize)] \leq (1-\frac{\stepsize\tilde{\cacheN}}{n})\E\big[h(\dualvarv^{(k)})]+ \frac{\stepsize^2}{2n}\CfTotal.
\end{equation}

Following the proof of Theorem~C.1 in~\citet{lacosteJulien13bcfw}, we prove the bound of Theorem~\ref{thm:cacheTheorem} by induction. The induction hypothesis consists in inequality
\begin{equation*}
\E\big[h(\dualvarv^{(k)})] \le \frac{2nC}{\tilde{\cacheN}k+2n} \, \text{ for } k\geq 0
\end{equation*}
where $C:=\big(\frac{1}{\tilde{\cacheN}}\CfTotal+ h_0\big)$. 

The base-case $k=0$ follows directly from $C\geq h_0$.
We now prove the induction step. Assume that the hypothesis is true for a given $k\geq 0$. 
Let us now prove that the hypothesis is true for $k+1$.
We use inequality~\eqref{eq:cache_proof:block_descent_expected_h_bounded} with the step size $\stepsize_k:=\frac{2n}{\tilde{\mapprox} k+2n} \in[0,1]$:
\begin{align*}
\E\big[ h(\dualvarv^{(k+1)}_{\stepsize_k})] \le&\: (1- \frac{\stepsize_k\tilde{\mapprox}}{n}) \E\big[h(\dualvarv^{(k)})] + (\stepsize_k)^2 \frac{C\tilde{\mapprox}}{2n} \\[3pt]
=& \: (1-\frac{2\tilde{\mapprox}}{\tilde{\mapprox} k+2n}) \E\big[h(\dualvarv^{(k)})] + (\frac{2n}{\tilde{\mapprox} k+2n})^2 \frac{C\tilde{\mapprox}}{2n}  \\[3pt]
\le& \: (1-\frac{2\tilde{\mapprox}}{\tilde{\mapprox} k+2n}) \frac{2n C}{\tilde{\mapprox} k+2n} + (\frac{1}{\tilde{\mapprox} k+2n})^2 2nC\tilde{\mapprox}
\end{align*}
where, in the first line, we use inequality $\CfTotal \le C \tilde{\mapprox}$, and, in the last line, we use the induction hypothesis for $\E\big[h(\dualvarv^{(k)})]$.

By rearranging the terms, we have
\begin{align*}
    \E\big[ h(\dualvarv^{(k+1)})] \le& \frac{2nC}{\tilde{\mapprox} k+2n} \left(1 - \frac{2\tilde{\mapprox}}{\tilde{\mapprox} k+2n} + \frac{\tilde{\mapprox}}{\tilde{\mapprox} k+2n}\right) \\[3pt]
                      =& \frac{2nC}{\tilde{\mapprox} k+2n} \frac{\tilde{\mapprox} k+2n-\tilde{\mapprox}}{\tilde{\mapprox} k+2n} \\[3pt]
                     \le& \frac{2nC}{\tilde{\mapprox} k+2n} \frac{\tilde{\mapprox} k+2n}{\tilde{\mapprox} k+2n+\tilde{\mapprox}} \\[3pt]
                      =& \frac{2nC}{\tilde{\mapprox} (k+1)+2n} \ ,
\end{align*}  
which finishes the proof.
%
%
%
%
%
%
%
%
\end{proof}








%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%



\section{Convergence of BCPFW and BCAFW} \label{app:BCPFWconvergence}

In this section, we prove Theorem~\ref{thm:BCPFW} that states that the suboptimality error on~\eqref{eq:svmstruct_nslack_dual} decreases geometrically in expectation for BCPFW and BCAFW for the iterates at which \emph{no} block would have a drop step, i.e., when no atom would be removed from the active sets. We follow closely the notation and the results from~\citet{LacosteJulien2015linearFW} where the global linear convergence of the (batch) pairwise FW (PFW) and away-step FW (AFW) algorithms was shown. The main insight to get our result is that the ``pairwise FW gap'' decomposes also as a sum of block gaps.
We give our result for the following more general setting (the block-separable analog of the setup in Appendix~F of~\citet{LacosteJulien2015linearFW}):
\begin{equation}
\begin{aligned} \label{eq:blockPFWproblem}
\min_{\bx \in \domain} \, f(\bx) \quad 
& \text{with} \quad f(\bx) := q(A \bx) + \bv^\top \bx \\
& \text{and} \quad \domain=\domain^{(1)} \times\dots\times \domain^{(n)},
\end{aligned}
\end{equation}
where $q$ is a strongly convex function, and $\domain^{(i)} := \conv(\atoms^{(i)})$ for each $i$, where $\atoms^{(i)} \subseteq \R^{m_i}$ is a \emph{finite} set of vectors (called \emph{atoms}). In other words, each $\domain^{(i)}$ is a polytope. For the example of the dual SSVM objective~\eqref{eq:svmstruct_nslack_dual}, $q(\cdot) := \frac{\regularizerweight}{2} \| \cdot \|^2$ and $\atoms^{(i)}$ are the corners of a probability simplex in $m_i := |\outputdomain_i|$ dimensions.

Suppose that we maintain an active set $\activeS_i$ for each block (as in the BCPFW algorithm). We first relate the batch PFW direction with the block PFW directions, as well as their respective batch and blockwise \emph{PFW gaps} (the PFW gap is replacing the FW gap~\eqref{eq:duality_gap} in the analysis of PFW).

\begin{definition}[Block PFW gaps] \label{def:blockPFW}
Consider the problem~\eqref{eq:blockPFWproblem} and suppose that the point $\bx$ has each
of its block $\bx_{(i)}$ with current active set $\activeS_i \subseteq \atoms^{(i)}$.\footnote{That is, $\bx_{(i)}$ is a convex combination of \emph{all} the elements of $\activeS^{(i)}$ with non-zero coefficients.}
We define the corresponding \emph{batch PFW gap} at~$\bx$ with active set $\activeS := \activeS_1 \times \cdots \times \activeS_n$ as:
\begin{align}
g^{\PFW}(\bx ; \activeS) &:= \max_{\sv \in \domain, \vv \in \activeS}  \,\langle -\nabla f(\bx) \,  , \, \sv - \vv  \rangle \label{eq:batchPFW} \\
&= \max_{\sv \in \domain, \vv \in \activeS}  \sum_i \langle  -\nabla_{(i)} f(\bx) \, , \, \sv_{(i)} - \vv_{(i)} \rangle \notag \\
&= \sum_i  \underbrace{\max_{\substack{ \sv_{(i)} \in \domain^{(i)} \\ \vv_{(i)} \in \activeS_i } } \langle   -\nabla_{(i)} f(\bx) \, , \, \sv_{(i)} - \vv_{(i)}\rangle} \notag \\
&=: \qquad   \sum_i \qquad g_i^\PFW(\bx \, ; \, \activeS_i) , \label{eq:blockPFW} 
\end{align}
where $g_i^\PFW$ is the PFW gap for block~$i$. We recognize that the maximizing arguments for $g_i^\PFW$ are the FW corner $\sv_{(i)}$ and the away corner $\vv_{(i)}$ for block $i$ that one would obtain when running BCPFW on this block.
\end{definition}

We note that by maintaining independent active sets $\activeS_i$ for each block, the number of potential away corner combinations is exponential in the number of blocks, yielding many more possible directions of movement than in the batch PFW algorithm where the number of away corners is bounded by the number of iterations. Moreover, suppose that we have an explicit expansion for each block $\bx_{(i)}$ as a convex combination of atoms in the active set: $\bx_{(i)} = \sum_{\vv_{(i)} \in \activeS_i} \beta_i(\vv_{(i)}) \, \vv_{(i)}$, where $\beta_i(\vv_{(i)}) > 0$ is the convex combination coefficient associated with atom~$\vv_{(i)}$. Then we can also express $\bx$ as an explicit convex combination of the (exponential size) active set $\activeS$ as follows:
$\bx = \sum_{\vv \in \activeS} \beta(\vv) \, \vv$, where $\beta(\vv) := \prod_{i=1}^n \beta_i(\vv_{(i)})$.

We can now prove an analog of the expected block descent lemma (Lemma~\ref{lemma:convGapSampling:expectedDescent} for BCFW) in the case of BCPFW and BCAFW\@. %
For technical reasons, we need a slightly different block curvature constant~$\CfAi$ (cf.~Eq.~(26) in~\citet{LacosteJulien2015linearFW}).

\begin{lemma}[Expected BCPFW descent lemma] \label{lem:BCPFWdescent}
Consider running the BCPFW algorithm on problem~\eqref{eq:blockPFWproblem}. Let~$\bx^{(k)}$ be the current iterate, and suppose that $\activeS_j$ is the current active set for each block $\bx_{(j)}^{(k)}$. Let $\activeS^{(k)} := \activeS_1 \times \cdots \times \activeS_n$ be the current (implicit) active set for $\bx^{(k)}$. Suppose that \emph{there is no drop set at~$\bx^{(k)}$}, that is, that for each possible block~$i$ that could be picked at this stage, the PFW step with line-search on block~$i$ will not have its step size truncated (we say that the line-search \emph{will succeed}). Then, conditioned on the current state,
in expectation over the random choice of block~$i$ with uniform probability and for any $\stepsize \in [0,1]$, it holds for the next iterate 
$\bx^{(k+1)}$ of BCPFW:
    \begin{multline}   \label{eq:BCPFWlemma}
    \E\big[ f(\bx^{(k+1)})\,|\, \bx^{(k)}, \activeS^{(k)} \big]
    \le \\ 
    f(\bx^{(k)}) -  \frac{\stepsize}{n} g^\PFW(\bx^{(k)} \,;\, \activeS^{(k)})
      + \frac{\stepsize^2}{2n} \CfATotal,
    \end{multline}
where $\CfATotal := \sum_{i=1}^n \CfAi$ is the total (away) curvature constant, and where $\CfAi$ is defined as in Definition~\ref{def:curvature_const}, but allowing the reference point~$\dualvarv_{(i)}$ in~\eqref{eqn:CfProduct} to be any point $\vv_{(i)} \in \domain^{(i)}$ instead, thus allowing a pairwise FW direction $\sv_{[i]} - \vv_{[i]}$ to be used in its definition.

Moreover,~\eqref{eq:BCPFWlemma} also holds for BCAFW (again under the assumption of \emph{no drop step}), but with an extra $\nicefrac{1}{2}$ factor in front of $g^\PFW(\bx^{(k)} \,;\, \activeS^{(k)})$ in the bound.
\end{lemma}
\begin{proof}
Let block~$i$ be the chosen one that defined $\bx^{(k+1)}$ and let $\bx_\stepsize := \bx^{(k)}+\stepsize(\sv_{[i]}-\vv_{[i]})$, where $\mathbf{s}_{(i)} \in \domain^{(i)}$ is the FW corner on block~$i$ with~$\mathbf{s}_{[i]} \in \R^m$ its zero-padded version, and similarly $\vv_{(i)} \in \activeS_i$ is the chosen away corner on block~$i$. By assumption, we have that the line-search succeeds, i.e., the minimum of $\min_{\stepsize \in [0,\stepmax]} f(\bx_\stepsize)$ is achieved for $\stepsize^* < \stepmax$, where $\stepmax$ is the maximum step size for this block for the PFW direction (this is because the optimal step size for the line-search cannot be truncated at $\stepmax$, as otherwise it would be a drop step). As $f$ is a convex function, this means that $f(\bx^{(k+1)}) = \min_{\stepsize \in [0,\stepmax]} f(\bx_\stepsize) = \min_{\stepsize \geq 0}  f(\bx_\stepsize)$ (removing inactive constraints does not change its minimum). By definition of~$\CfAi$, we thus have for any $\stepsize \in [0,1]$:
\begin{align}
f({\scriptstyle \bx^{(k+1)}}) &\leq  f(\bx_\stepsize) \notag \\
&= f({\scriptstyle \bx^{(k)}} + \gamma (\sv_{[i]} - \vv_{[i]})) \notag \\
& \leq f({\scriptstyle \bx^{(k)}})\! +\! \gamma \langle  \nabla_{(i)} f(\bx) , \, \sv_{(i)} \!\! -\! \vv_{(i)}\rangle \!+\! \frac{\gamma^2}{2}\CfAi \notag \\
&=  f({\scriptstyle \bx^{(k)}}) - \gamma \, g_i^\PFW({\scriptstyle \bx^{(k)}} ;  \activeS_i) + \frac{\gamma^2}{2}\CfAi \!\!\!\!\!\!. \label{eq:blockDescentPFW}
\end{align}
Taking the expectation of the bound with respect to~$i$, conditioned on~$\bx^{(k)}$ and~$\activeS^{(k)}$, yields~\eqref{eq:BCPFWlemma} by using the block-decomposition relationship~\eqref{eq:blockPFW} in the definition of~$g^\PFW(\bx^{(k)} \,;\, \activeS^{(k)})$. This completes the proof for BCPFW.

In the case of BCAFW, let $\dd_i$ be the chosen direction for block~$i$ (either a FW direction or an away direction). Then since $\dd_i$ is chosen to maximize the inner product with $-\nabla_{(i)} f(\bx^{(k)})$, we have
$\langle -\nabla_{(i)} f(\bx^{(k)}), \dd_i \rangle \geq \frac{1}{2} g_i^\PFW(\bx^{(k)} \, ; \, \activeS_i)$ (with a similar argument as used to get Eq.~(6) in~\citet{LacosteJulien2015linearFW} for~AFW). We then follow the same argument to derive~\eqref{eq:blockDescentPFW}, but using $\dd_i$ instead of $(\sv_{(i)} \!\! -\! \vv_{(i)})$, which gives an extra $\nicefrac{1}{2}$ factor as $\langle -\nabla_{(i)} f(\bx^{(k)}), \dd_i \rangle$ is potentially only half of $g_i^\PFW(\bx^{(k)} \, ; \, \activeS_i)$. Taking again the expectation of~\eqref{eq:blockDescentPFW} completes the proof.
\end{proof}

\begin{remark}
The important condition that there is \emph{no drop step} at $\bx^{(k)}$ in the BCPFW descent lemma~\ref{lem:BCPFWdescent} is to allow the bound~\eqref{eq:blockDescentPFW} to hold for \emph{any} $\stepsize \in [0,1]$. Otherwise, let $I$ be the (non-empty) set of blocks for which there would be a drop step at $\bx^{(k)}$ and let $\stepsize_I := \min_{i \in I}  \stepmax^{(i)}$, where $\stepmax^{(i)}$ is the maximum step size for block~$i$. Then in this case we could only show the bound~\eqref{eq:blockDescentPFW} for $\stepsize \leq \stepsize_I$. But $\stepsize_I$ could be arbitrarily small,\footnote{Small maximum step sizes happen when the current coordinate value for an away corner is small (perhaps because a small step size was used by the line-search when they were added as a FW corner previously).} and so no significant progress is guaranteed in expectation in this case. 

We also note that $\CfAi$ is used instead of $\Cf^{(i)}$ in the lemma because $\Cf^{(i)}$ can only be used with a feasible step from $\bx^{(k)}$, and thus again, the bound would only be valid for $\stepsize \leq \stepmax$ (as bigger step sizes can take you outside of $\domain^{(i)}$). If the gradient of $f$ is Lipschitz continuous, one can bound $\CfAi \leq \tilde{L}_i \big(\diam_{\|\cdot\|_i} \domain^{(i)} \big)^2$, which is almost the same bound as for~$\Cf^{(i)}$ explained in footnote~\ref{foot:Cfi}, but with $\tilde{L}_i$ being the Lipschitz constant of $\nabla_{(i)} f$ for variations in the slightly extended domain $\domain^{(i)} + (\domain^{(i)} - \domain^{(i)})$ (with set addition in the Minkowski sense).
\end{remark}

\begin{theorem}[Geometric convergence of BCPFW] \label{thm:BCPFW}
Consider running BCPFW (or BCAFW) on problem~\eqref{eq:blockPFWproblem} where $q$ is a strongly convex function and $\domain$ is a block-separable polytope. Let $h_k := f(\bx^{(k)}) - f(\bx^*)$ be the suboptimality of the iterate~$k$, where $\bx^*$ is any optimal solution to~\eqref{eq:blockPFWproblem}. Conditioned on any iterate $\bx^{(k)}$ with active set $\activeS^{(k)}$ such that \emph{no block could give a drop set} (as defined in the conditions for Lemma~\ref{lem:BCPFWdescent}), then the expected new suboptimality decreases geometrically, that is:
\begin{equation} \label{eq:BCPFWgeometric}
\E\big[ h_{k+1} \,|\, \bx^{(k)}, \activeS^{(k)} \big] \leq (1-\rho) h_k , 
\end{equation}
with rate:
\begin{align}
\rho &:= \frac{1}{2n} \min \{1, 2 \frac{\strongConvGeneralized}{\CfATotal} \} \,\, \text{for the BCPFW algorithm}, \label{eq:BCPFWrate} \\ 
\rho &:= \frac{1}{4n} \min \{1, \frac{\strongConvGeneralized}{\CfATotal} \} \,\,\,\, \text{for the BCAFW algorithm}, \label{eq:BCAFWrate} 
\end{align}
where $\CfATotal := \sum_{i=1}^n \CfAi$ is the total (away) curvature constant for problem~\eqref{eq:blockPFWproblem} as defined in Lemma~\ref{lem:BCPFWdescent}, and $\strongConvGeneralized$ is the generalized strong convexity constant for problem~\eqref{eq:blockPFWproblem} as defined in~Eq.~(39) of~\citet{LacosteJulien2015linearFW} ($\strongConvGeneralized$ is strictly greater than zero when $q$ is strongly convex and $\domain$ is a polytope).
\end{theorem}
\begin{proof}
We first do the argument for BCPFW. Let $g_k := g^\PFW(\bx^{(k)} \,;\, \activeS^{(k)})$, and notice that $g_k \geq h_k$ always. Because we assume that there is no drop step at $\bx^{(k)}$, we can use the expected BCPFW descent lemma~\ref{lem:BCPFWdescent}. By subtracting $f(\bx^*)$ on both side of the descent inequality~\eqref{eq:BCPFWlemma}, we get (for any $\stepsize \in [0,1]$):
\begin{equation} \label{eq:hkNextBCPFW}
\E\big[h_{k+1} \,|\, \bx^{(k)}, \activeS^{(k)} \big]
\le h_k -  \frac{\stepsize}{n} g_k
  + \frac{\stepsize^2}{2n} \CfATotal . 
\end{equation}
We can minimize the RHS of~\eqref{eq:hkNextBCPFW} with $\stepsize^* = \frac{g_k}{\CfATotal}$. If $g_k > \CfATotal$ (i.e. $\stepsize^* > 1$), then use $\stepsize = 1$ in~\eqref{eq:hkNextBCPFW} to get:
\begin{equation} \label{eq:rhoBigBCPFW}
\E\big[h_{k+1} \,|\, \bx^{(k)}, \activeS^{(k)} \big]
\le h_k -  \frac{1}{2n} g_k \leq (1-\frac{1}{2n}) h_k .
\end{equation}
This gives a geometric rate of $\rho = \frac{1}{2n}$. So now suppose that $g_k \leq \CfATotal$ (so that $\stepsize^* \leq 1$); putting $\stepsize = \stepsize^*$ in~\eqref{eq:hkNextBCPFW}, we get:
\begin{equation} \label{eq:hkBCPFWprogress}
\E\big[h_{k+1} \,|\, \bx^{(k)}, \activeS^{(k)} \big]
\le h_k -  \frac{1}{2n \CfATotal} \, {g_k}^2 .
\end{equation}
We now use the key relationship between the suboptimality~$h_k$ and the PFW gap~$g_k$ derived in inequality~(43) of~\citet{LacosteJulien2015linearFW} (which is true for any function~$f$ by definition of~$\strongConvGeneralized$ if we allow it to be zero):
\begin{equation} \label{eq:hkPFWgap}
h_k \leq \frac{{g_k}^2}{2 \strongConvGeneralized} .\textsl{}
\end{equation}
Substituting~\eqref{eq:hkPFWgap} into~\eqref{eq:hkBCPFWprogress}, we get:
\begin{equation} \label{eq:rhoBCPFW}
\E\big[h_{k+1} \,|\, \bx^{(k)}, \activeS^{(k)} \big] \leq (1-\frac{\strongConvGeneralized}{n  \CfATotal}) h_k ,
\end{equation}
which gives the $\rho = \nicefrac{\strongConvGeneralized}{n  \CfATotal}$ rate. Taking the worst rate of~\eqref{eq:rhoBigBCPFW} and~\eqref{eq:rhoBCPFW} gives the rate~\eqref{eq:BCPFWrate}, completing 
the proof for BCPFW.

In the case of BCAFW, Lemma~\ref{lem:BCPFWdescent} yields the inequality~\eqref{eq:hkNextBCPFW} but with an extra $\nicefrac{1}{2}$ factor in front of $g_k$. Re-using the same argument as above, we get a rate of $\rho = \nicefrac{1}{4n}$ when $\stepsize^* > 1$, and $\rho = \nicefrac{\strongConvGeneralized}{4n  \CfATotal}$ when $\stepsize^* \leq 1$, showing~\eqref{eq:BCAFWrate} as required.
%

Finally, the fact that $\strongConvGeneralized > 0$ when $q$ is $\mu$-strongly convex and $\domain$ is a polytope comes from the lower bound given in Theorem~10 of~\citet{LacosteJulien2015linearFW} in terms of the \emph{pyramidal width} of~$\domain$ (a strictly positive geometric quantity for polytopes), and the \emph{generalized strong convexity} of $f$ as defined in Lemma~9 of~\citet{LacosteJulien2015linearFW}. The generalized strong convexity of $f$ is simply $\mu$ if $f$ is $\mu$-strongly convex. In the more general case of problem~\eqref{eq:blockPFWproblem} where only~$q$ is $\mu$-strongly convex, the generalized strong convexity depends both on~$\mu$ and the \emph{Hoffman constant}~\citepsup{hoffman1952constant} associated with the linear system of problem~\eqref{eq:blockPFWproblem}. See~\citet{LacosteJulien2015linearFW} for more details, as well as Lemma~2.2 of~\citetsup{beck2015AFW}.
\end{proof}

\paragraph{Interpretation.} Theorem~\ref{thm:BCPFW} only guarantees progress of BCPFW or BCAFW when there would not be any drop step for \emph{any} block~$i$ for the current iterate. For the batch AFW algorithm, one can easily lower bound the number of times that these ``good steps'' can happen as a drop step reduces the size of the active set and thus cannot happen more than half of the time. On the other hand, in the block coordinate setting, we can be unlucky and always have one block that could give a drop step (while we pick other blocks during the algorithm, this bad block affects the expectation). This means that without a refined analysis of the drop step possibility, we cannot guarantee any progress in the worst case for BCPFW or BCAFW. As a safeguard, one can modify BCPFW or BCAFW so that it also has the option to do a standard BCFW step on a block if it yields better progress on $f$ -- this way, the algorithm inherits at least the (sublinear) convergence guarantees of BCFW.

\paragraph{Empirical linear convergence.} In our experiments, we note that BCPFW always converged empirically, and had an empirical linear convergence rate for the SSVM objective when $\lambda$ was big enough ($q(\cdot) = \frac{\lambda}{2} \| \cdot \|^2$ for the SSVM objective~\eqref{eq:svmstruct_nslack_dual}). See Figure~\ref{fig:app:exp1_dataset1} for OCR-large~(c) for example. We also tried the modified BCPFW algorithm where a choice is made between a FW step, a pairwise FW step or an away step on a block by picking the one which gives the biggest progress. We did not notice any significant speed-up for this modified method.

\paragraph{On the dimension of SSVM.} Finally, we note that the rate constant~$\rho$ in Theorem~\ref{thm:BCPFW} has an implicit dependence on the dimensionality (in particular, through the pyramidal width of~$\domain$). \citet{LacosteJulien2015linearFW} showed that the largest possible pyramidal width of a polytope in dimension~$m$ (for a fixed diameter) is achieved by the probability simplex and is $\Theta(1/\sqrt{m})$. For the SSVM in the general form~\eqref{eq:svmstruct_nslack_dual}, the dimensionality of~$\domain^{(i)}$ is the number of possible structured outputs for input~$i$, which is typically an exponentially large number, and thus the pyramidal width lower bound would be useless in this case. Fortunately, the matrix~$A$ (feature map) and vector~$\bv$ (loss function) are highly structured, and thus many~$\dualvarv$'s are mapped to the same objective value. For a feature mapping~$\featuremapdiffv_i(\outputvarv)$ representing the sufficient statistics for an energy function associated with a graphical model (as for a conditional random field~\citepsup{laffery2001CRF}), then the SSVM objective is implicitly optimizing over the \emph{marginal polytope} for the graphical model~\citepsup{wainwright2008graphical}. More specifically, let $A_i$ be the $d \times m_i$ submatrix of~$A$ associated with example~$i$. Then we can write $A_i = B_i M_i$ where $M_i$ is a $p \times m_i$ \emph{marginalization} matrix, that is, $\bm{\mu} = M_i \dualvarv$ is an element of the marginal polytope for the graphical model, where $p$ is the dimensionality of the marginal polytope -- which is a polynomial number in the size of the graph, rather than exponential. By the affine invariance property of the FW-type algorithms, we can thus instead use the pyramidal width of the marginal polytope for the convergence analysis (and similarly for the Hoffman constant). \citet{LacosteJulien2015linearFW} conjectured that the pyramidal width of a marginal polytope in dimension $p$ was also~$\Theta(1/\sqrt{p})$, thus giving a more reasonable bound for the convergence rate of BCPFW for SSVM.


\section{BCFW for SSVM with box constraints}
\label{app:pos_const}

\subsection{Problem with box constraints}
Problem~\eqref{eq:svmstruct_nslack_primal_nonsmooth} can be equivalently rewritten as a quadratic program (QP) with an exponential number of constraints:
\begin{align}
    \label{eq:svmstruct_nslack_primal}
    \min_{\weightv,\, \bm{\xi}} \quad &   \frac{\regularizerweight}{2}\norm{\weightv}^2 +
    \frac1n \sum_{i=1}^n \xi_i\\
    \text{s.t.} \quad &
    \langle \weightv, \featuremapdiffv_i(\outputvarv) \rangle
    \geq \errorterm(\outputvarv_i,\outputvarv) - \xi_i \quad  \forall i, \, \forall \outputvarv \in \outputdomain_i
      \notag
\end{align}
where the slack variable~$\xi_i$ measures the surrogate loss for the $i$-th datapoint.
Problem~\eqref{eq:svmstruct_nslack_primal} is often referred to as the $n$-slack structured SVM with margin-rescaling~\citep[Optimization Problem 2]{Joachims:2009ex}.

In this section, we consider the problem~(\ref{eq:svmstruct_nslack_primal}) with additional box constraints on the parameter vector~$\weightv$:
\begin{align}
    \label{eq:ssvn_nslack_primal_bounded}
    \min_{\weightv,\, \bm{\xi}} \quad &   \frac{\regularizerweight}{2}\norm{\weightv}^2 +
    \frac1n \sum_{i=1}^n \xi_i\\
    \text{s.t.} \quad &
    \langle \weightv, \featuremapdiffv_i(\outputvarv) \rangle
    \geq \errorterm(\outputvarv_i,\outputvarv) - \xi_i \quad  \forall i, \, \forall \outputvarv \in  \outputdomain_i, \notag \\
    \quad & \lb \preccurlyeq \weightv \preccurlyeq \ub
      \notag,
\end{align}

where $\lb \in \R^d$ and $\ub \in \R^d$ denote the lower and upper bounds, respectively, and the symbol ``$\preccurlyeq$'' is the element-wise ``less or equal to'' sign.
In the following, we assume that the box constraints are feasible, i.e., $\lb \preccurlyeq \ub$.
Note that the following discussion can be directly extended to the case where only some dimension of the weight vector have to respect the box constraints.
The Lagrangian of problem~\eqref{eq:ssvn_nslack_primal_bounded} can be written as
\begin{multline}
 \label{eq:ssvn_nslack_lagrang_bounded}
L(\weightv,\bm{\xi},\dualvarv, \duallv, \dualuv)
  = \frac\regularizerweight2 \langle\weightv,\weightv\rangle+\frac1n\sum_{i=1}^n \xi_i   \\
  + \sum_{i\in[n],\,\outputvarv \in \outputdomain_i}
  \frac1n\dualvar_i(\outputvarv)
  \left( -\xi_i +
     \langle \weightv, -\featuremapdiffv_i(\outputvarv) \rangle
     + \errorterm_i(\outputvarv)
  \right)  \\
  +\regularizerweight \langle \dualuv, \weightv - \ub \rangle + \regularizerweight \langle \duallv, -\weightv + \lb \rangle
\end{multline}
where $\duallv \in \R^d$ and  $\dualuv \in \R^d$ are the dual variables associated with the lower and upper bound constraints, respectively.
From the KKT conditions, we obtain
\begin{align}
\label{eq:kkt_box_w}
\weightv =& \ A\dualvarv-(\dualuv-\duallv), \\
\label{eq:kkt_box_alp}
\sum_{\outputvarv \in \outputdomain_i}
  \dualvar_i(\outputvarv) = & \ 1 ~~~\forall i\in[n].
\end{align}
Finally, the dual of problem~\eqref{eq:ssvn_nslack_primal_bounded} (here written in a minimization form) can be written as follows:
\begin{align}
\label{eq:ssvn_nslack_dual_bounded}
    \min_{\substack{ \dualvarv\in\R^{m} \\  \dualvarv \succcurlyeq 0}}  f(\dualvarv,\duallv,\dualuv) :=&  \;
    \frac{\regularizerweight}{2}
    \big\|A \dualvarv - (\dualuv - \duallv)\big\|^2 - \bv^\transpose \dualvarv \notag \\[-3mm]
    & + \regularizerweight ( \dualuv^{\transpose} \ub - \duallv^{\transpose} \lb )
    \notag \\[1mm]
    \text{s.t. } & \sum_{\outputvarv \in \outputdomain}  \dualvar_i(\outputvarv) = 1 ~~~ \forall i\in[n], \notag \\
      \text{and } & \dualuv \succcurlyeq 0 , \duallv \succcurlyeq 0.
\end{align}

%
%
%
%
%
%
%
%
%
%
%
%

\paragraph{A modified block optimization method.}
Ideally, we should optimize $f(\dualvarv,\duallv,\dualuv)$ jointly w.r.t.\ all the dual variables.
This task is not directly suitable for the Frank-Wolfe approach as the domain for $\duallv$ and $\dualuv$ is unbounded.
However, joint optimization w.r.t.~$\duallv$ and $\dualuv$ with $\dualvarv$ kept fixed can be done in closed form.
After that, optimization w.r.t.~$\dualvarv$ can be performed using the Frank-Wolfe blockwise approach.
Therefore, we resort to optimizing in a blockwise fashion: we iterate either a batch FW or a BCFW step on $\dualvarv$ with an exact block-update on $(\dualuv,\duallv)$. As we will see below, this principled approach is similar to a commonly used heuristic of truncating the value of $\weightv$ to make it feasible during an algorithm which works on the dual. In fact, our approach will be equivalent to run FW or BCFW with a truncation making $\weightv(\dualvarv)$ feasible after each FW step, but with a \emph{change in the optimal step-size computation} (line~\ref{alg:lineStepsizeFWpositive} in Algorithm~\ref{alg:FW_SVM_box} for FW; line~\ref{alg:lineStepsizeBCFWpositive} in Algorithm~\ref{alg:FW_product_SVM_box} for BCFW) due to the different nature of the optimization problem.


\begin{algorithm}[t!]
    \caption{Batch Frank-Wolfe algorithm for structured SVM with box constraints} %
    \label{alg:FW_SVM_box}
    \begin{algorithmic}[1]
        %
        %
        \STATE Let $\notruncv^{(0)}:= \0$; $\ell^{(0)}:=0$
        \STATE $\weightv^{(0)}:= [ \notruncv^{(0)} ]_{\lb}^{\ub}$
        \COMMENT{truncation to the feasible set}
        %
        \FOR{$k:=0,\dots,\infty$}
        \FOR{$i:=1,\dots,n$}
        \STATE Solve $\outputvarv_i^* := \displaystyle\argmax_{\outputvarv\in\outputdomain_i} \ H_i(\outputvarv;\weightv^{(k)})$ \vspace{-2mm}%
        %
        \ENDFOR %
        %
        %
        \STATE Let $\notruncv_{\sv} := {\displaystyle\sum_{i=1}^n} \frac1{\regularizerweight n} \featuremapdiffv_i(\outputvarv_i^*)$ \;
        and \; $\ell_{\sv} := \frac1n \displaystyle\sum_{i=1}^n \errorterm_i(\outputvarv_i^*)$
        %
        %
        %
        %
        %
        \STATE Let $\stepsize := \frac{\regularizerweight (\notruncv^{(k)}-\notruncv_{\sv})^\transpose \weightv^{(k)} - \ell^{(k)} + \ell_{\sv} }{ \regularizerweight \|\notruncv^{(k)}-\notruncv_{\sv}\|^2 }$ ~and clip  to $[0,1]$ \label{alg:lineStepsizeFWpositive}
        %
        %
        %
        \STATE Update $\notruncv^{(k+1)}:= (1-\stepsize)\notruncv^{(k)}+\stepsize\, \notruncv_{\sv}$
        \STATE {\small~~~~~~~and~ $\ell^{(k+1)}:= (1-\stepsize)\ell^{(k)}+\stepsize\, \ell_{\sv}$}
        \STATE {\small~~~~~~~and~ $\weightv^{(k+1)}:= [ \notruncv^{(k+1)} ]_{\lb}^{\ub}$}
        \ENDFOR
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}[t]
    \caption{Block-coordinate Frank-Wolfe algorithm for structured SVM with box constraints}%
    \label{alg:FW_product_SVM_box}
    \begin{algorithmic}[1]
        %
        %
        \STATE Let $\notruncv^{(0)}:= {\notruncv_i}^{(0)} := \0$; \; $\ell^{(0)}:={\ell_i}^{(0)}:=0$;
        \STATE $\weightv^{(0)}:= [ \notruncv^{(0)} ]_{\lb}^{\ub}$
        \COMMENT{truncation to the feasible set}
        %
        \FOR{$k:=0,\dots,\infty$}
        \STATE Pick $i$ at random in $\{1,\ldots,n\}$
        \STATE Solve $\outputvarv_i^* := \displaystyle\argmax_{\outputvarv\in\outputdomain_i} \ H_i(\outputvarv;\weightv^{(k)})$ %
        %
        \STATE Let $\notruncv_{\sv} := \frac1{\regularizerweight n} \featuremapdiffv_i(\outputvarv_i^*)$
        \; and \;  $\ell_{\sv} := \frac1n \errorterm_i(\outputvarv_i^*)$
        %
        %
        \STATE Let $\stepsize := \frac{ \regularizerweight (\notruncv_i^{(k)}-\notruncv_{\sv})^\transpose \weightv^{(k)} - \ell_i^{(k)} + \ell_{\sv} }{ \regularizerweight \|\notruncv_i^{(k)}-\notruncv_{\sv}\|^2}$~and clip to $[0,1]$ \label{alg:lineStepsizeBCFWpositive}
        %
        %
        \STATE Update ${\notruncv_i}^{(k+1)}:= (1-\stepsize){\notruncv_i}^{(k)}+\stepsize \,\notruncv_{\sv}$
        \STATE {\small~~~~~~~and~ ${\ell_i}^{(k+1)}:= (1-\stepsize){\ell_i}^{(k)}+\stepsize\, \ell_{\sv}$}
        %
        \STATE Update $\notruncv^{(k+1)}\;:= \notruncv^{(k)} + {\notruncv_i}^{(k+1)} - {\notruncv_i}^{(k)}$
        \STATE {\small~~~~~~~and~~ $\ell^{(k+1)}:= ~\ell^{(k)}+{\ell_i}^{(k+1)} \ \  - {\ell_i}^{(k)}$}
        \STATE Let $\weightv^{(k+1)}:= [ \notruncv^{(k+1)} ]_{\lb}^{\ub}$
        %
        \ENDFOR
    \end{algorithmic}
\end{algorithm}



\subsection{Optimizing w.r.t \texorpdfstring{$\bm{\beta}_u$}{betaU} and \texorpdfstring{$\duallv$}{betaL} while fixing \texorpdfstring{$\dualvarv$}{alpha}}

The optimization w.r.t.~$\dualuv$ with $\dualvarv$ and $\duallv$ fixed can be easily solved in closed form via a simple thresholding operation:
\begin{equation}
\label{eq:optimalBetaU}
\bm{\beta}_u^* = [A\dualvarv + \duallv - \ub ]_+ \ .
\end{equation}
The optimization w.r.t.~$\duallv$ with $\dualvarv$ and $\dualuv$ fixed is analogous:
\begin{equation}
\label{eq:optimalBetaL}
\duallv^* = [ - A\dualvarv + \dualuv + \lb ]_+ \ .
\end{equation}

Denote the $p$-th variable of $\dualuv$ and $\duallv$ with $\dualuv(p)$ and $\duallv(p)$, respectively.
For any index $p$, both $\dualuv(p)$ and $\duallv(p)$ cannot be nonzero simultaneously, because if one of the constraints is violated (either the upper or the lower bound), then the other constraint must be satisfied.
Hence, $\dualuv(p) \neq 0$ implies $\duallv(p) = 0$ and vice versa. %
Therefore, the final update equations can equivalently be written as
\begin{align}
\label{eq:optimalBetaULfinal}
\dualuv^*(\dualvarv) &= [A\dualvarv - \ub ]_+ \ , \\
\duallv^*(\dualvarv) &= [-A\dualvarv + \lb ]_+ \ .
\end{align}

Introducing $\notruncv(\dualvarv):=A\dualvarv$, we get $\bm{\beta}_u^* = [\notruncv - \ub ]_+$ and $\bm{\beta}^*_l = [-\notruncv + \lb ]_+$.
Hence, the operation $\weightv = \notruncv - (\bm{\beta}^*_u - \bm{\beta}^*_l)$ is simply the projection (truncation) of $\notruncv$ on the feasible set defined by the upper and lower bounds.
%
%
In the final algorithm, we maintain $\notruncv(\dualvarv)$ and directly update the primal variables~$\weightv$ without updating $\dualuv$ and $\duallv$.

\subsection{Batch setting: optimizing w.r.t \texorpdfstring{$\dualvarv$}{alpha} with  \texorpdfstring{$\bm{\beta}_u$}{betaU} and \texorpdfstring{$\duallv$}{betaL} fixed}
When the variables $\dualuv$ and $\duallv$ are fixed, the convex problem~\eqref{eq:ssvn_nslack_dual_bounded}
has a compact domain and so we can use the Frank-Wolfe algorithm on it.
In the following, we highlight the differences with the setting without box constraints.
We denote by $\weightv(\dualvarv)$ the truncation of $\notruncv(\dualvarv)$ on the box constraints, i.e.,
\begin{equation} \label{eq:wTruncation}
\weightv(\dualvarv) := \notruncv(\dualvarv) - \left(\bm{\beta}^*_u(\dualvarv) - \bm{\beta}^*_l (\dualvarv)\right).
\end{equation}
The derivations below assume that $\dualuv$ and $\duallv$ are fixed to their optimal values $\bm{\beta}^*_u(\dualvarv)$ and $\bm{\beta}^*_l (\dualvarv)$ for a specific $\dualvarv$.

\paragraph{Linear subproblem.}
The Frank-Wolfe linear subproblem can be written as
\begin{align}
\label{eq:linearizationProblemFWbox}
\bm{s} = \argmin_{\bm{s}' \in \mathcal{M}} \langle \bm{s}', \nabla_{\dualvarv} f(\dualvarv, \bm{\beta}_l, \bm{\beta}_u) \rangle
\end{align}
where $\nabla_{\dualvarv} f(\dualvarv, \bm{\beta}_l, \bm{\beta}_u)$ can be easily computed:
\begin{align}
\label{eq:gradient_ssvmdual_bounded}
\nabla_{\dualvarv} f(\dualvarv, \bm{\beta}_l, \bm{\beta}_u) &= \regularizerweight A^{\transpose} (A\dualvarv - (\bm{\beta}_u - \bm{\beta}_l)) - \bm{b} \notag \\
&= \regularizerweight A^{\transpose} \weightv - \bm{b}.
\end{align}
Analogously to the problem without box constraints, the linear subproblem used by the Frank-Wolfe algorithm is equivalent to the loss-augmented decoding subproblem~\eqref{eq:subproblem_loss_augm}.
The update of $\dualvarv$ can be made using the corner $\bm{s}$.
In what follows, we show that this update can be performed without explicitly keeping the dual variables at the cost of storing the extra vector~$\notruncv$.

\paragraph{The duality gap.} The Frank-Wolfe gap for problem~\eqref{eq:ssvn_nslack_dual_bounded} can be written as
\[
\begin{array}{rl}
\label{eq:FWdualgap_box}
  g(\dualvarv) :=& \displaystyle\max_{\sv' \in \domain} \,\langle \dualvarv - \sv',\nabla_{\dualvarv} f(\dualvarv, \bm{\beta}_l, \bm{\beta}_u) \rangle \\
    =& (\dualvarv - \sv)^\transpose (\regularizerweight A^{\transpose} (A\dualvarv - (\bm{\beta}_u - \bm{\beta}_l)) - \bv) \\
    =& \regularizerweight (\notruncv-\notruncv_{\sv})^\transpose \weightv - \bv^\transpose \dualvarv + \bv^\transpose \sv
   \vspace{-1mm}
\end{array}
\]
where $\notruncv_{\sv}:=A\sv$.
Below, we prove that the Frank-Wolfe duality gap~$g(\dualvarv)$ for the problem~\eqref{eq:ssvn_nslack_dual_bounded} when $\dualuv$ and $\duallv$ are fixed at their current optimal value for the current $\dualvarv$ equals to a Lagrange duality gap, analogously to the case without box constraints~\citep[Appendix~B.2]{lacosteJulien13bcfw}.\footnote{We stress that this relationship is only valid for the pair $\weightv = \weightv(\dualvarv)$ in the primal, and $\dualuv = \dualuv^*(\dualvarv), \duallv = \duallv^*(\dualvarv)$ in the dual.}


\begin{proof}
Consider the difference between the primal objective of~\eqref{eq:ssvn_nslack_primal_bounded} at $\weightv := A\dualvarv-(\dualuv-\duallv)$  with the optimal slack variables~$\bm{\xi}$ and the dual objective of~\eqref{eq:ssvn_nslack_dual_bounded} at $\dualvarv$ (in the maximization form).
We get
\begin{align*}
g_{\text{\tiny Lag.}}(\weightv,\dualvarv)&=\frac\regularizerweight2 \weightv^\transpose \weightv + \frac1n \sum_{i=1}^n \tilde{H}_i(\weightv) \\
& \qquad - \left(\bv^\transpose \dualvarv - \frac\regularizerweight2 \weightv^\transpose \weightv - \regularizerweight ( \dualuv^{\transpose} \ub - \duallv^{\transpose} \lb ) \right) \\
&= \regularizerweight \weightv^\transpose \weightv - \bv^\transpose \dualvarv + \frac1n \sum_{i=1}^n \max_{\outputvarv\in\outputdomain_i} H_i(\outputvarv;\weightv) \\
&\qquad + \regularizerweight ( \dualuv^{\transpose} \ub - \duallv^{\transpose} \lb ) \ .
\end{align*}

Recalling
\begin{align*}
\frac1n \sum_{i=1}^n \max_{\outputvarv\in\outputdomain_i} H_i(\outputvarv;\weightv) &=  \max_{\sv'\in \domain} -\sv'^\transpose \nabla_{\dualvarv} f(\dualvarv, \bm{\beta}_l, \bm{\beta}_u) \\ &= -\sv^\transpose \nabla_{\dualvarv} f(\dualvarv, \bm{\beta}_l, \bm{\beta}_u),
\end{align*}
we can write
\begin{align*}
g_{\text{\tiny Lag.}}(\weightv,\dualvarv)  &=  \regularizerweight \weightv^\transpose (A\dualvarv-(\dualuv-\duallv)) - \bv^\transpose \dualvarv  \\
&\qquad - \sv^{\transpose}\nabla_{\dualvarv} f(\dualvarv, \bm{\beta}_l, \bm{\beta}_u) + \regularizerweight ( \dualuv^{\transpose} \ub - \duallv^{\transpose} \lb ) \\
&= (\regularizerweight \weightv^\transpose  A - \bv^\transpose ) \dualvarv - \sv^\transpose\nabla_{\dualvarv} f(\dualvarv, \bm{\beta}_l, \bm{\beta}_u)  \\
&\qquad -\regularizerweight ( \weightv^\transpose  \dualuv - \weightv^\transpose \duallv)
+ \regularizerweight ( \dualuv^{\transpose} \ub - \duallv^{\transpose} \lb )  \\
	&= (\dualvarv - \sv)^\transpose \nabla_{\dualvarv} f(\dualvarv, \bm{\beta}_l, \bm{\beta}_u)
	\\&\qquad +  \regularizerweight(\dualuv^{\transpose}(\ub-\weightv)
	-\duallv^\transpose (\lb-\weightv)).
\end{align*}

As we assumed that $\dualuv = \dualuv^{*}(\dualvarv)$ and $\duallv = \duallv^{*}(\dualvarv)$, we have $\dualuv^{*\transpose}(\ub-\weightv)=0$ and $\duallv^{*\transpose}(\lb-\weightv)=0$, and thus
\begin{align*}
g_{\text{\tiny Lag}}(\weightv,\dualvarv,\dualuv^*,\duallv^*) = g(\dualvarv).
\end{align*}
\end{proof}
%


\paragraph{Line-Search.} Line search can be performed efficiently using 
$\stepsize_{\text{\tiny opt}s} := \frac{\langle \dualvarv - \sv, \nabla f(\dualvarv) \rangle}{\regularizerweight\norm{A(\dualvarv-\sv)}^2}
= \frac{g(\dualvarv)}{\regularizerweight\norm{\notruncv-\notruncv_{\sv}}^2}$.

\paragraph{Algorithm.}
Algorithm~\ref{alg:FW_SVM_box} contains the batch Frank-Wolfe algorithm with box constraints.
The main idea consists in maintaining the vector $\notruncv := A\dualvarv$ in order to perform all the updates of $\dualvarv$ using only the primal variables.
Optimization w.r.t.\ $\duallv$ and $\dualuv$ corresponds to the truncation of $\notruncv$.
Given these variables, the gap and the optimal step size are easy to compute.

\subsection{The block-coordinate setting}
Algorithm~\ref{alg:FW_product_SVM_box} describes the block-coordinate version of the Frank-Wolfe algorithm with box constraints.
The algorithm is obtained from the batch version (Algorithm~\ref{alg:FW_SVM_box}) in exactly the same way as BCFW (Algorithm~\ref{alg:FW_product_SVM}) is obtained from the batch Frank-Wolfe method~\citep[Algorithm~2]{lacosteJulien13bcfw}.



%
%
%

%
%
%
%

%
%
\newlength{\citationLengthTable}
\setlength{\citationLengthTable}{\widthof{Citation Citation More}} %

\begin{table*}[t]
    \centering
    \caption{ Statistics of the datasets used in the experimental evaluation. For the oracle time, we report the sum of the average running times for both the max oracle and the joint feature map computation (the starred numbers indicate that the input features were stored on disk instead of RAM, thus slowing down the computation). \vspace{3mm}\label{tab:dataset}}
    \resizebox{\textwidth}{!}{ %
        \begin{tabular}{@{}lllccrrccl@{}}
        \toprule
            Dataset                   & Task & Sructure (Oracle)                             & Citation          & Version              & $n$     & $d$                           & Sparsity & 
            \parbox{\widthof{constraints}}{\centering Box \\ constraints} & 
            \parbox{\widthof{Oracle time}}{\centering Oracle time \\ (in s)} \\ \midrule
            \multirow{2}{*}{OCR}      & \multirow{2}{*}{character recognition}  & \multirow{2}{*}{chain (Viterbi)}  & \multirow{2}{\citationLengthTable}{\centering \citep{Taskar2003}} & small                & $626$   & $4,082$                        & No       & No              & $5\times 10^{-4}$ 
            %
            \\
            &                   &                   &                   & large                & $6,251$  & $4,082$                        & No       & No              & $5\times 10^{-4}$ 
            %
            \\ \midrule
            CoNLL                     & text chunking      & \multirow{1}{*}{chain (Viterbi)}             & \parbox{\citationLengthTable}{\centering \citep{Sang2000}}                  & \multicolumn{1}{l}{} & $8,936$  & \multicolumn{1}{l}{$1,643,026$} & Yes      & No              &         $2\times 10^{-2}$                            \\   %
            %
            \midrule
            \multirow{3}{*}{HorseSeg} & \multirow{3}{*}{binary segmentation} & \multirow{3}{*}{grid (graph cut)} & \multirow{3}{\citationLengthTable}{\centering \citep{kolesnikov2014closed}} & small                & $147$   & $1,969$                        & No       & Yes             & $1\times 10^{-3} $
            %
            \\
            &                   &                   &                   & medium               & $6,121$  & $1,969$                        & No       & Yes             & $1\times 10^{-3} $
            %
            \\
            &                   &                   &                   & large                & $25,438$ & $1,969$                        & No       & Yes             & $2 \times 10^{-2}$ (*)
            %
            \\ \midrule
            LSP                       & pose estimation          & \multirow{1}{*}{tree (max sum)}            &   \parbox{\citationLengthTable}{\centering \citep{Johnson10}}                & small                & $100$   & $2,676$                        & No       & Yes             &  $2$ (*) \\ %
            %
            \bottomrule
        \end{tabular}
    }
\end{table*}


\section{Dataset description \label{app:dataset_table}}
In our experiments, we use four structured prediction datasets: OCR~\cite{Taskar2003} for character recognition; CoNLL~\cite{Sang2000} for text chunking; HorseSeg~\cite{kolesnikov2014closed} for binary image segmentation; LSP~\cite{Johnson10} for pose estimation.
In this section, we provide the description of the datasets and the corresponding models.
Table~\ref{tab:dataset} summarizes quantitative statistics for all the datasets.
For the OCR and CoNLL datasets, the features and models described below are exactly the same as used by~\citet{lacosteJulien13bcfw}; we give a detailed description for reference. For HorseSeg and LSP, we had to build the models ourselves from previous work referenced in the relevant section.


\subsection{OCR}
The Optical Character Recognition (OCR) dataset collected by~\citet{Taskar2003} poses the task of recognizing English words from sequences of handwritten symbols represented by binary images.
The average length of sequences equals~$7.6$ symbols.
For a sequence of length~$T$,  the input feature representation~$\inputvarv$ consists of~$T$ binary images of size~$16 \times 8$.
The output object~$\outputvarv$ is a sequence of length~$T$ with each symbol taking 26 possible values.

The OCR dataset contains $6,877$ words.
In the small version, $626$ words are used for training and the rest for testing.
In the large version, $6,251$ words are used for training and the rest for testing.

The prediction model is a chain. The feature map~$\featuremapv(\inputvarv, \outputvarv)$ contains features of three types: emission, transition and bias.
The $16 \times 8 \times 26$ emission features count the number of times along the chain a specific position of the $16\times8$ binary image equals~$1$ when associated with a specific output symbol.
The $26 \times 26$ transition features count the number of times one symbol follows another.
The $26 \times 3$ bias features represent three biases for each element of the output alphabet: one model bias, and a bias for when the letter appears at the beginning or at the end of the sequence.

As the structured error between output vectors $\errorterm(\outputvarv_i,\outputvarv)$, the OCR dataset uses the Hamming distance normalized by the length of the sequences.
The loss-augmented structured score $H_i(\outputvarv;\weightv)$ is a function with unary and pairwise potentials defined on a chain and is exactly maximized with the dynamic programming algorithm of~\citet{Viterbi67}.

\subsection{CoNLL}
The CoNLL dataset released by~\citet{Sang2000} poses the task of text chunking.
Text chunking, also known as shallow parsing~\citesup{sha2003shallow}, consists in dividing the input text into syntactically related non-overlapping groups of words, called phrase or chunks.
The task of text chunking can be cast as a sequence labeling where a sequence of labels $\outputvarv$ is predicted from an input sequence of tokens $\inputvarv$.
For a given token $\inputvar_t$ (a word with its corresponding part-of-speech tag), the associated label $\outputvar_t$ gives the type of phrase the token belongs to, i.e., says whether or not it corresponds to the beginning of a chunk, or encodes the fact that the token does not belong to a chunk.

%
%
%

The CoNLL dataset contains $8,936$ training English sentences extracted from the Wall Street Journal part of the Penn Treebank II~\citesup{marcus1993pennTreebank}. Each output label $\outputvar_t$ can take up to $22$ different values.
%

We use the feature map~$\featuremapv(\inputvarv, \outputvarv)$ proposed by~\citetsup{sha2003shallow}.
First, for each position~$t$ of the input sequence~$\inputvarv$, we construct a unary feature representation, containing the local information.
We start with extracting several attributes representing the words and the part-of-speech tags at the positions neighboring to~$t$.\footnote{We extract the attributes with the CRFsuite library~\citesup{CRFsuite} and refer to its documentation for the exact list of attributes: \url{http://www.chokkan.org/software/crfsuite/tutorial.html}.}
Each attribute is encoded with an indicator vector of length equal to either the dictionary size or the number of part-of-speech tags.
We concatenate these vectors to get a unary feature representation, which is a sparse binary vector of dimensionality~$74,658$.
Note that these representation can be precomputed outside of the training process.

%
%
%
%
%

%
%
%
%

Given a labeling $\outputvarv$ and the unary representations, the feature map $\featuremapv$ is constructed by concatenating features of three types (as in the chain model for OCR): emission, transition and bias.
The $74,658 \times 22$ emission features count the number of times each coordinate of the unary representation of token~$x_t$ is nonzero and the corresponding output variable~$y_t$ is assigned a particular value.
The transition map of size $22 \times 22$ encodes the number of times one label follows another in the output~$\outputvarv$.
The $22 \times 3$ bias features encode biases for all the possible values of the output variables and, specifically, biases for the first and the last variables.

As in the OCR task, the structured error~$\errorterm(\outputvarv_i,\outputvarv)$ is the normalized Hamming distance and thus the max-oracle can be efficiently implemented using the dynamic programming algorithm of~\citet{Viterbi67}.

\subsection{HorseSeg}
The HorseSeg dataset\footnote{\url{https://pub.ist.ac.at/~akolesnikov/HDSeg/HDSeg.tar}} released by~\citet{kolesnikov2014closed} poses the task of object/background segmentation of images containing horses, i.e., assigning a label ``horse'' or ``background'' to each pixel of the image.
HorseSeg contains $25,438$ training images, $147$ of which are manually annotated, $5,974$~annotations are constructed from object bounding boxes by the automatic method of~\citetsup{guillaumin2014autoAnnotation}, while the remaining $19,317$~annotations were constructed by the same method but without any human supervision.
The test set of HorseSeg consists of $241$~images with manual annotations.
In our experiments, we use training sets of three different sizes: $147$ images for HorseSeg-small, $6,121$~images for HorseSeg-medium and $25,438$ for HorseSeg-large.
%
%

In addition to images and their pixel-level annotations, \citet{kolesnikov2014closed} released\footnote{\url{https://pub.ist.ac.at/~akolesnikov/HDSeg/data.tar}} oversegmentations (superpixels) of the images precomputed with the SLIC algorithm~\citesup{achanta2012slicSuperpixels} and the unary features of each superpixel computed similarly to the work of~\citetsup{lempitsky2011pylon}.
On average, each image contains $147$~superpixels. The $1,969$ unary features include $1$~constant feature, $512$-bin histograms of densely sampled visual SIFT words~\citesup{lowe2004sift}, $128$-bin histograms of RGB colors, $16$-bin histograms of locations (each pixel of a region of interest is matched to a cell of the $4 \times 4$ uniform grid).
The three aforementioned histograms are computed on the superpixels themselves, on the superpixels together with their neighboring superpixels, and on the second-order neighborhoods.

%
%
%

For each pair of adjacent superpixels, we construct $100$ pairwise features: a constant feature; and
quantities $\exp{(-\eta d_{pq})}$ where~$d_{pq}$ is a $\chi^2$-distance between $9$ pairs of the corresponding histograms of each type for neighbors $p$ and $q$, and $\eta$ is a parameter taking $11$~values from the set $2^{-5}, 2^{-4}, \dots, 2^5$.

The structured feature map is defined in such a way that the corresponding structured score function contains unary and pairwise Potts potentials\\[-4mm]
\begin{align*}
\langle \weightv, \featuremapv(\inputvarv_i,\outputvarv) \rangle
&=
\sum_{p \in \mathcal{V}_i} \langle \weightv_U, \inputvarv_{i, p}^U \rangle ([y_p = 1] - [y_p=0]) \\
&+
\sum_{\{p,q\} \in \mathcal{E}_i} \langle \weightv_P,\inputvarv_{i, pq}^P \rangle [y_p \neq y_p]
\end{align*}
where the vector $\inputvarv_i = ( (\inputvarv_{i, p}^U)_{p\in\mathcal{V}_i}, (\inputvarv_{i, pq}^P)_{\{p,q\} \in \mathcal{E}_i})$ denotes all the features of image~$i$, the vector~$\outputvarv = (\outputvar_p)_{p\in \mathcal{V}_i} \in \{0,1\}^{\mathcal{V}_i}$ is a feasible labeling, the set~$\mathcal{V}_i$ is the set of the superpixels of the image~$i$, and the set $\mathcal{E}_i$ represents the adjacency graph.

The structured error is measured with a Hamming loss with class-normalized penalties 
\begin{equation*}
\errorterm(\outputvarv_i,\outputvarv)
=
\sum_{p \in \mathcal{V}_i} \omega_{y_{i,p}} [y_{i,p} \neq y_{p}]
\end{equation*}
where $\outputvarv_i = (\outputvar_{i,p})_{p\in \mathcal{V}_i}$ is the labeling of superpixels closest to the ground-truth annotation and the weights~$\omega_{0}$ and $\omega_{1}$ are proportional to the ground-truth area of each class.

The loss-augmented score function
$$
\errorterm(\outputvarv_i,\outputvarv) - \langle \weightv, \featuremapv(\inputvarv_i,\outputvarv)\rangle
$$
is a discrete function defined w.r.t.\ a cyclic graph and can be maximized in polynomial time when it is supermodular. By construction, all our pairwise features are nonnegative, so we can ensure supermodularity by adding positivity constraints on the weights corresponding to the pairwise features~$\weightv_P \succcurlyeq 0$. 
The version of BCFW with positivity constraints is described in Appendix~\ref{app:pos_const}.

The discrete optimization problem arising in the max-oracle is solved by the min-cut/max-flow algorithm of~\citet{boykov2004graphcut}.
The running time or the max-flow is small compared to the operations with the features required to compute the potentials.

\subsection{LSP}
The Leeds Sports Pose (LSP) dataset introduced by~\cite{Johnson10} poses the tasks of full body pose estimation from still images containing sports activities.
Based on the input image with a centered prominent person, the task is to predict the locations of $14$ body-parts (joints), e.g., ``left knee'' or ``right ankle''.

We cast the task of pose estimation as a structured prediction problem and build our model based on the work of~\citet{Chen_NIPS14}, which is one of the state-of-the-art methods for pose estimation.
First, we construct an acyclic graph  where the nodes $p\in\mathcal{V}$ correspond to the different body-parts. The set of body parts is extended from the original $14$ parts of interest by the midway points to get the $26$ nodes of the graph. Second, the graph is converted into the directed one by utilizing the arcs of both orientations for each original edge. We denote the resulting graph by $\mathcal{G}=(\mathcal{V},\vec{\mathcal{E}})$.

In the model of~\citet{Chen_NIPS14}, each node~$p\in\mathcal{V}$ has a variable $\bm{l}_p \in \mathcal{P} \subset \R^2$ denoting the spatial position of the corresponding joint that belongs to a finite set of possibilities~$\mathcal{P}$; each arc~$(p,q) \in \vec{\mathcal{E}}$ has a variable $t_{pq} \in \mathcal{T} = \{1,\dots,13\}$ representing the type of spacial relationship between the two nodes. The output variable~$\outputvarv$ is constructed by concatenating the unary and pairwise variables $\outputvarv = \bigl( (\bm{l}_p)_{p \in \mathcal{V}}, \: (t_{pq})_{(p,q) \in \vec{\mathcal{E}}} \bigr)$.

The structure score function is a function of discrete variables $\bm{l}_p$ and $t_{pq}$ that is defined w.r.t.\ the graph~$\mathcal{G}$:
\begin{align*}
\langle \weightv, \featuremapv(\inputvarv_i,\outputvarv) \rangle
&=
\sum_{p \in \mathcal{V}} \weight_{U,p} \phi^U_p( \bm{I}_i, \bm{l}_p ) \\
&+
\sum_{(p,q) \in \mathcal{E}} 
\weight_{T,pq}
\phi^P_{pq}(  \bm{I}_i, \bm{l}_p, t_{pq} )
\\
&+
\sum_{(p,q) \in \mathcal{E}} 
\langle \weightv_{P,pq, t_{pq}},
\Delta( \bm{l}_p - \bm{l}_q - \bm{r}_{pq}^{t_{pq}})\rangle.
\end{align*}
Here, the input
$
\inputvarv_i = \bigl(  \bm{I}_i, (\bm{r}_{pq}^{t})_{(p,q) \in \vec{\mathcal{E}}}^{t\in \mathcal{T}} \bigr)
$
consists of the original image~$\bm{I}_i$ and the mean relative positions $\bm{r}_{pq}^{t} \in \R^2$ of each type of spatial relationship corresponding to each arc~$(p,q) \in \vec{\mathcal{E}}$.
Functions~$\phi^U_p(\bm{I}_i, \cdot)$ and $\phi^P_{pq}(\bm{I}_i, \cdot, \cdot)$ compute the scores for each possible value of the discrete variables $\bm{l}_p$ and $(\bm{l}_p,t_{pq})$, respectively.
The vector-valued function $\Delta(\bm{l}) = (l_1, l_1^2, l_2, l_2^2)$, $\bm{l} \in \R^2$, measures different types of mismatch between the preferred relative displacement~$\bm{r}_{pq}^{t_{pq}}$ and the displacement $\bm{l}_p - \bm{l}_q$ coming from the labeling~$\outputvarv$.
The vector~$\weightv = \bigl(  (\weight_{U,p})_{p \in \mathcal{V}}, (\weight_{T,pq})_{(p,q)\in \vec{\mathcal{E}}}, (\weightv_{P,pq, t})_{(p,q)\in \vec{\mathcal{E}}}^{t \in \mathcal{T}} \bigr)$ is the joint vector of parameters learned by structured SVM. Overall, this setup has $2,676$ parameters.

Displacements $\bm{r}_{pq}^{t}$ and functions~$\phi^U_p$, $\phi^P$ are computed at the preprocessing stage of the structured SVM.
We follow~\citet{Chen_NIPS14} and obtain the displacements $\bm{r}_{pq}^{t}$ with the K-means clustering of the displacements of the training set. Functions~$\phi^U_p$, $\phi^P$ consists in a Convolutional Neural Network (CNN) and are also learned from the training set. We refer the reader to the work of~\citet{Chen_NIPS14} and their project page\footnote{\url{http://www.stat.ucla.edu/~xianjie.chen/projects/pose_estimation/pose_estimation.html}} for further details.
Note that the last training stage of~\citet{Chen_NIPS14} is different from ours, i.e., it consists in binary SVM on the carefully sampled sets of positive and negative examples. 

To run SSVM, we define the structured error $\errorterm(\outputvarv_i,\outputvarv)$ as a decomposable function w.r.t. the positions of the joints
\begin{equation*}
\errorterm(\outputvarv_i,\outputvarv) = \frac{1}{|\mathcal{V}|}\sum_{p \in \mathcal{V}} \max \left(1,\frac{\Vert \bm{l}_{i,p}-\bm{l}_{p}\Vert_2^2}{s_i^2}\right)
\end{equation*}
where $\bm{l}_{i,p}$ belongs to the ground-truth labeling~$\outputvarv_i$ and $\bm{l}_{p}$ belongs to the labeling~$\outputvarv$.
The quantity $s_i\in\R$ is the scaling factor and is defined by the distance between the left shoulder and the right hip in the ground-truth labeling~$\outputvarv_i$.
Similarly to~\citetsup{osokin14_ECCV}, the complex dependence of the loss on the ground-truth labeling does not influence the complexity of the max oracle.

For the defined structured score and loss, the optimization problem of the max oracle can be exactly solved by the max-sum belief propagation algorithm on an acyclic graph with messages computed with the generalized distance transform (GDT)~\cite{felzenszwalb2005distTrans}.
The usage of GDTs allows to significantly reduce the oracle running time, but requires positivity constraints on the connection weights~$\weightv_{P,pq, t}$.
BCFW with positivity constraints is described in Appendix~\ref{app:pos_const}.

The resulting max oracle is quite slow (2 seconds per image) even when the belief propagation algorithm is optimized with GDTs.
Slow running time made it intractable for us to run the experiments on the full original training set consisting of $1,000$ images.
We use only the first $100$ images and refer to this dataset as LSP-small. However, both the CNN training and clustering of the displacements were done on the original training set.



%
%

\newpage

%
\section{Full experimental evaluation: comparing BCFW variants}
\label{app:exp1}
In Figures~\ref{fig:app:exp1_dataset1} and~\ref{fig:app:exp1_dataset2}, we give the detailed results of the experiments described in section~\ref{subsec:exp1}.
As a reminder, we are comparing different methods (caching vs no caching, gap sampling vs uniform sampling, pairwise FW steps vs regular FW steps) on different datasets in three main regimes w.r.t.~$\regularizerweight$.
For each dataset, we use the good value of~$\regularizerweight$ (``good" meaning the smallest possible test error) together with its smaller and larger values.
The three regimes are displayed in the middle~(b), top~(a) and bottom~(c) of each subfigure, respectively.

\section{Full experimental evaluation: regularization path}
\label{app:exp2}
In this section, we evaluate the regularization path method proposed in Section~\ref{sec:reg_path}.
Our experiments are organized in three stages.
First, we choose the $\factorRegPath$ parameter for Algorithm~\ref{alg:rpAlgorithmSSVM} computing the $\epsilon$-approximate regularization path.
Second, we define and evaluate the heuristic regularization path.
Finally, we compare both $\epsilon$-approximate and heuristic paths against the grid search on multiple datasets.

%
%
%

%
%
%
%


%
%
%
%
%
%

\paragraph{$\epsilon$-approximate path.}
Algorithm~\ref{alg:rpAlgorithmSSVM} for computing the $\epsilon$-approximate regularization path has one parameter~$\factorRegPath$ controlling how large are the induction steps in terms of~$\regularizerweight$.
This parameter provides the trade-off between the number of breakpoints and the accuracy of optimization for each breakpoint.
We explore this trade-off in Figure~\ref{fig:regPath_epsApprox_ocrSmall}.
For several values of $\factorRegPath$, we report the cumulative number of effective passes (bottom plots) and cumulative time (top plots) required to get  $\epsilon$-approximate solution for each $\regularizerweight$. 
We report both plots for the two methods used as the SSVM solver: BCPFW with gap sampling and caching; BCFW with gap sampling and without caching.
We conclude that both too small ($< 0.5$) and too large ($\approx 1$) values of parameter~$\factorRegPath$ result in slower methods, but overall the method is not too sensitive to~$\factorRegPath$. In all remaining experiments, we use $\factorRegPath=0.9$ when computing the $\epsilon$-approximate path.

%
%
%
%
%
%
%

\paragraph{Heuristic path.}
When computing an $\epsilon$-approximate regularization path, Algorithm~\ref{alg:rpAlgorithmSSVM} needs to perform at least one pass over the dataset at each breakpoint to check the convergence criterion, i.e., that the gap is not larger than~$\epsilon$.
When having many breakpoints, this extra pass can be of significant cost, especially for large values of~$\regularizerweight$ where the SSVM solver converges very quickly.
However, in practice, we observe that the stale gap estimates are often good enough to determine convergence and actually checking the convergence criterion is not necessary.
At the cost of loosing the guarantees, we can expect computational speed-up.
We refer to the result of Algorithm~\ref{alg:rpAlgorithmSSVM} when the SSVM solver (at line~\ref{alg:rpAlgorithmSSVM:SSVM}) uses stale gap estimates instead of the exact gaps to check the convergence as a \emph{heuristic path}.
In the case of heuristic path, the parameter~$\factorRegPath$ provides a trade-off between the running time and accuracy of the path.
In Figure~\ref{fig:regPath_heuristic_ocrSmall}, we illustrate this trade-off on the OCR-small dataset.
For several values of~$\factorRegPath$, we report the true value of the duality gap at each breakpoint (the SSVM solver terminates when the stale gap estimate is below~$\factorRegPath\epsilon$, $\epsilon=0.1$) and the cumulative time.
We use BCFW + gap sampling and BCPFW + gap sampling + cache as the SSVM solvers.
For large $\factorRegPath$, we observe that the solutions for small $\regularizerweight$ are not $\epsilon$-approximate and that the method in this regime requires less number of passes over the data, i.e., runs faster.
In what follows we always use $\factorRegPath =0.7$ for the heuristic path as it is the largest value providing accurate enough results.



%
%
%
%
%
%
%
%

\paragraph{Comparison of paths against grid search.}
Finally, we compare the regularization path methods to the standard grid search approach.
We define a grid of 31 values chosen to cover all the values of $\regularizerweight$ used in the experiments of Section~\ref{app:exp1} ($2^{15},2^{14},\dots,2^{-15}$).
For each value of the grid, we optimize the SSVM objective optimize either independently or by warm-starting from the nearest larger value.
We have considered two variants of warm start: keeping the primal variables and rescaling the dual variables, or keeping the dual variables and rescaling the primal variables.
In our experiments, we do not notice any consistent difference between the two approaches, so we only use the first type of warm start.

Figure~\ref{fig:regPath_4datasets} presents the results on the four datasets: HorseSeg-small (Figure~\ref{fig:regPath_horseSmall}),
OCR-small (Figure~\ref{fig:regPath_ocrSmall}),
HorseSeg-medium (Figure~\ref{fig:regPath_horseMedium}),
OCR-large (Figure~\ref{fig:regPath_ocrLarge}).

For both the OCR-large and HorseSeg-medium datasets, neither heuristic, nor $\epsilon$-approximate path methods did not reach their stopping criterion and were terminated at the time limit of 24 hours.
In the case of OCR-large, the regularization path reached a value of $\regularizerweight$ smaller than the lower limit of the predefined grid, i.e., $2^{-15}$. 
In the case of HorseSeg-medium, the grid search methods did not reach the lower bound of the grid and were terminated at the 24-hour time limit.


\newpage
\def \tableHeight {3cm}
%
\begin{figure*}
\resizebox{0.98\textwidth}{!}{
\begin{tabular}{c@{\; \; }c@{\qquad \quad}c@{\; \; }c}
\multicolumn{4}{c}{\includegraphics[width=1.1\textwidth]{figures/legend_long_frame_white_new_BCPFW.pdf}}  \\[0.1cm]
\includegraphics[height=\tableHeight]{figures/gapVsPass_ocrLarge_lambda_1e-04.pdf} &
\includegraphics[height=\tableHeight, trim=0.6cm 0cm 0cm 0cm , clip]{figures/gapVsTime_ocrLarge_lambda_1e-04.pdf}
&
\includegraphics[height=\tableHeight]{figures/gapVsPass_conll_lambda_1e-04.pdf} &
\includegraphics[height=\tableHeight, trim=0.6cm 0cm 0cm 0cm, clip]{figures/gapVsTime_conll_lambda_1e-04.pdf}
\\
\multicolumn{2}{c}{(a) OCR-large, $\regularizerweight=0.0001$} &  \multicolumn{2}{c}{(a) CoNLL, $\regularizerweight=0.0001$}
\\
\includegraphics[height=\tableHeight]{figures/gapVsPass_ocrLarge_lambda_1e-03.pdf} &
\includegraphics[height=\tableHeight, trim=0.6cm 0cm 0cm 0cm, clip]{figures/gapVsTime_ocrLarge_lambda_1e-03.pdf}
&
\includegraphics[height=\tableHeight]{figures/gapVsPass_conll_lambda_1e-02.pdf} &
\includegraphics[height=\tableHeight, trim=0.6cm 0cm 0cm 0cm, clip]{figures/gapVsTime_conll_lambda_1e-02.pdf}
\\
\multicolumn{2}{c}{(b) OCR-large, $\regularizerweight=0.001$} &  \multicolumn{2}{c}{(b) CoNLL, $\regularizerweight=0.01$}
\\
\includegraphics[height=\tableHeight]{figures/gapVsPass_ocrLarge_lambda_1e-01.pdf} &
\includegraphics[height=\tableHeight, trim=0.6cm 0cm 0cm 0cm, clip]{figures/gapVsTime_ocrLarge_lambda_1e-01.pdf}
&
\includegraphics[height=\tableHeight]{figures/gapVsPass_conll_lambda_1e-01.pdf} &
\includegraphics[height=\tableHeight, trim=0.6cm 0cm 0cm 0cm, clip]{figures/gapVsTime_conll_lambda_1e-01.pdf}
\\
\multicolumn{2}{c}{(c) OCR-large, $\regularizerweight=0.1$} &  \multicolumn{2}{c}{(c) CoNLL, $\regularizerweight=0.1$}
\\[2mm]\hline \\[-1mm]

%

\includegraphics[height=\tableHeight]{figures/gapVsPass_horseSmall_lambda_1e-01.pdf} &
\includegraphics[height=\tableHeight, trim=0.6cm 0cm 0cm 0cm, clip]{figures/gapVsTime_horseSmall_lambda_1e-01.pdf}
&
\includegraphics[height=\tableHeight]{figures/gapVsPass_horseMedium_lambda_1e-01.pdf} &
\includegraphics[height=\tableHeight, trim=0.6cm 0cm 0cm 0cm, clip]{figures/gapVsTime_horseMedium_lambda_1e-01.pdf}
\\
\multicolumn{2}{c}{(a) HorseSeg-small, $\regularizerweight=0.1$} &  \multicolumn{2}{c}{(a) HorseSeg-medium, $\regularizerweight=0.1$}
\\
\includegraphics[height=\tableHeight]{figures/gapVsPass_horseSmall_lambda_1e+02.pdf} &
\includegraphics[height=\tableHeight, trim=0.6cm 0cm 0cm 0cm, clip]{figures/gapVsTime_horseSmall_lambda_1e+02.pdf}
&
\includegraphics[height=\tableHeight]{figures/gapVsPass_horseMedium_lambda_1e+00.pdf} &
\includegraphics[height=\tableHeight, trim=0.6cm 0cm 0cm 0cm, clip]{figures/gapVsTime_horseMedium_lambda_1e+00.pdf}
\\
\multicolumn{2}{c}{(b) HorseSeg-small, $\regularizerweight=100$} &  \multicolumn{2}{c}{(b) HorseSeg-medium, $\regularizerweight=1$}
\\
\includegraphics[height=\tableHeight]{figures/gapVsPass_horseSmall_lambda_1e+03.pdf} &
\includegraphics[height=\tableHeight, trim=0.6cm 0cm 0cm 0cm, clip]{figures/gapVsTime_horseSmall_lambda_1e+03.pdf}
&
\includegraphics[height=\tableHeight]{figures/gapVsPass_horseMedium_lambda_1e+01.pdf} &
\includegraphics[height=\tableHeight, trim=0.6cm 0cm 0cm 0cm, clip]{figures/gapVsTime_horseMedium_lambda_1e+01.pdf}
\\
\multicolumn{2}{c}{(c) HorseSeg-small, $\regularizerweight=1000$} &  \multicolumn{2}{c}{(c) HorseSeg-medium, $\regularizerweight=10$}
\\
\end{tabular}
}
\caption{Comparison of the variants of BCFW.
    We compare 8 different methods that can be represented by 3 binary dimensions: object sampling, caching, type of FW steps.
    We represent these dimensions in different ways:
    the dimension of caching (in blue) versus no caching (in orange) is represented through colors,
    the dimension of gap sampling (solid lines) versus uniform sampling (dashed lines) is represented through line style,
    the dimension of pairwise FW steps (circle markers) versus regular FW steps (square markers) is represented through markers.
    For each method, we report both the number of effective passes over data ($n$ oracle calls) and the running time against obtained duality gap (computed offline). \textit{The figure is continued in Figure~\ref{fig:app:exp1_dataset2}.}
  }
  \label{fig:app:exp1_dataset1}
\end{figure*}

\clearpage

%
%

\begin{figure*}[t!]
\resizebox{0.98\textwidth}{!}{
\begin{tabular}{c@{\; \; }c@{\qquad \quad}c@{\; \; }c}
\multicolumn{4}{c}{\includegraphics[width=1.1\textwidth]{figures/legend_long_frame_white_new_BCPFW.pdf}}  \\[0.1cm]
\includegraphics[height=\tableHeight]{figures/gapVsPass_horseLarge_lambda_1e-01.pdf} &
\includegraphics[height=\tableHeight, trim=0.6cm 0cm 0cm 0cm , clip]{figures/gapVsTime_horseLarge_lambda_1e-01.pdf}
&
\includegraphics[height=\tableHeight]{figures/gapVsPass_lspSmall_lambda_1e+01.pdf} &
\includegraphics[height=\tableHeight, trim=0.6cm 0cm 0cm 0cm, clip]{figures/gapVsTime_lspSmall_lambda_1e+01.pdf}
\\
\multicolumn{2}{c}{(a) HorseSeg-large, $\regularizerweight=0.1$} &  \multicolumn{2}{c}{(a) LSP-small, $\regularizerweight=10$}
\\
\includegraphics[height=\tableHeight]{figures/gapVsPass_horseLarge_lambda_1e+00.pdf} &
\includegraphics[height=\tableHeight, trim=0.6cm 0cm 0cm 0cm, clip]{figures/gapVsTime_horseLarge_lambda_1e+00.pdf}
&
\includegraphics[height=\tableHeight]{figures/gapVsPass_lspSmall_lambda_1e+02.pdf} &
\includegraphics[height=\tableHeight, trim=0.6cm 0cm 0cm 0cm, clip]{figures/gapVsTime_lspSmall_lambda_1e+02.pdf}
\\
\multicolumn{2}{c}{(b) HorseSeg-large, $\regularizerweight=1$} &  \multicolumn{2}{c}{(b) LSP-small, $\regularizerweight=100$}
\\
\includegraphics[height=\tableHeight]{figures/gapVsPass_horseLarge_lambda_1e+01.pdf} &
\includegraphics[height=\tableHeight, trim=0.6cm 0cm 0cm 0cm, clip]{figures/gapVsTime_horseLarge_lambda_1e+01.pdf}
&
\includegraphics[height=\tableHeight]{figures/gapVsPass_lspSmall_lambda_1e+03.pdf} &
\includegraphics[height=\tableHeight, trim=0.6cm 0cm 0cm 0cm, clip]{figures/gapVsTime_lspSmall_lambda_1e+03.pdf}
\\
\multicolumn{2}{c}{(c) HorseSeg-large, $\regularizerweight=10$} &  \multicolumn{2}{c}{(c) LSP-small, $\regularizerweight=1000$}
\\[-0cm]
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
\end{tabular}
}
\caption{\label{fig:app:exp1_dataset2} \textit{Continuation of Figure~\ref{fig:app:exp1_dataset1}.}
    Comparison of the variants of BCFW on HorseSeg-large and LSP-small.
    }
%
%
%
%
%
%
\end{figure*}

\begin{figure*}[h!]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
	\resizebox{\textwidth}{!}{
    \begin{tabular}{c@{$\:$}c}
        \includegraphics[width=0.5\textwidth]{figures/fig7_ocrSmall_bcfwGap_time.pdf} &
        \includegraphics[width=0.5\textwidth]{figures/fig7_ocrSmall_bcPfwGapCache_time.pdf}\\
        \includegraphics[width=0.5\textwidth]{figures/fig7_ocrSmall_bcfwGap_numpass.pdf} &
        \includegraphics[width=0.5\textwidth]{figures/fig7_ocrSmall_bcPfwGapCache_numpass.pdf}\\
        {\scriptsize BCFW + gap sampling}
        &
        {\scriptsize BCPFW + gap sampling + caching}
        \\[-0.0cm]
    \end{tabular}
    }
    \caption{
        \label{fig:regPath_epsApprox_ocrSmall} $\epsilon$-approximate regularization paths. 
        For different values of $\factorRegPath$, we report the cumulative number of effective passes (bottom) and the cumulative time (top) required to get an $\epsilon$-approximate solution for each $\regularizerweight$. We analyze the two methods: BCFW + gap sampling (left) and BCPFW + gap sampling + caching (right).
    }
    \end{subfigure}
     \hfill
    \begin{subfigure}[b]{0.48\textwidth}
\resizebox{\textwidth}{!}{
    \begin{tabular}{c@{$\:$}c}
        \includegraphics[width=0.5\textwidth]{figures/fig8_ocrSmall_bcfwGap_time.pdf} &
        \includegraphics[width=0.5\textwidth]{figures/fig8_ocrSmall_bcPfwGapCache_time.pdf}\\
        \includegraphics[width=0.5\textwidth]{figures/fig8_ocrSmall_bcfwGap_dual.pdf} &
        \includegraphics[width=0.5\textwidth]{figures/fig8_ocrSmall_bcPfwGapCache_dual.pdf}\\
        {\scriptsize BCFW + gap sampling}
        &
        {\scriptsize BCPFW + gap sampling + caching}
        \\[-0.0cm]
    \end{tabular}
    }
    \caption{
        \label{fig:regPath_heuristic_ocrSmall} Heuristic regularization paths.
        For different values of $\factorRegPath$, we report the cumulative time required to compute a path until $\regularizerweight$ (top) along with the true value of the duality gap obtained for each $\regularizerweight$ (bottom).We analyze the two methods: BCFW + gap sampling (left) and BCPFW + gap sampling + caching (right).
    }
    \end{subfigure}
	\caption{Experiment exploring the effect of $\factorRegPath$ for regularization paths computed on the OCR-small dataset.}
	\label{fig:bigFigure}
\end{figure*}



%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\clearpage
\begin{figure*}
\centering
\begin{subfigure}[b]{0.48\textwidth}
    \begin{tabular}{c@{$\:$}c}
        \includegraphics[width=0.49\textwidth]{figures/horseSmall_bcfwGap_time.pdf} &
        \includegraphics[width=0.49\textwidth]{figures/horseSmall_bcPfwGapCache_time.pdf}\\
        \includegraphics[width=0.49\textwidth]{figures/horseSmall_bcfwGap_numpass.pdf} &
        \includegraphics[width=0.49\textwidth]{figures/horseSmall_bcPfwGapCache_numpass.pdf}\\
        {\scriptsize BCFW + gap sampling}
        &
        {\scriptsize BCPFW + gap sampling + caching}
        \\[-0.0cm]
    \end{tabular}
    \caption{
        \label{fig:regPath_horseSmall} HorseSeg-small
        }
\end{subfigure}
\quad
\begin{subfigure}[b]{0.48\textwidth}
    \begin{tabular}{c@{$\:$}c}
        \includegraphics[width=0.49\textwidth]{figures/ocrSmall_bcfwGap_time.pdf} &
        \includegraphics[width=0.49\textwidth]{figures/ocrSmall_bcPfwGapCache_time.pdf}\\
        \includegraphics[width=0.49\textwidth]{figures/ocrSmall_bcfwGap_numpass.pdf} &
        \includegraphics[width=0.49\textwidth]{figures/ocrSmall_bcPfwGapCache_numpass.pdf}\\
        {\scriptsize BCFW + gap sampling}
        &
        {\scriptsize BCPFW + gap sampling + caching}
        \\[-0.0cm]
    \end{tabular}
    \caption{
        \label{fig:regPath_ocrSmall} OCR-small
        }
\end{subfigure}
\\[5mm]
\begin{subfigure}[b]{0.48\textwidth}
    \begin{tabular}{c@{$\:$}c}
        \includegraphics[width=0.49\textwidth]{figures/horseMedium_bcfwGap_time.pdf} &
        \includegraphics[width=0.49\textwidth]{figures/horseMedium_bcPfwGapCache_time.pdf}\\
        \includegraphics[width=0.49\textwidth]{figures/horseMedium_bcfwGap_numpass.pdf} &
        \includegraphics[width=0.49\textwidth]{figures/horseMedium_bcPfwGapCache_numpass.pdf}\\
        {\scriptsize BCFW + gap sampling}
        &
        {\scriptsize BCPFW + gap sampling + caching}
        \\[-0.0cm]
    \end{tabular}
    \caption{
        \label{fig:regPath_horseMedium} HorseSeg-medium
        }
\end{subfigure}
\quad
\begin{subfigure}[b]{0.48\textwidth}
    \begin{tabular}{c@{$\:$}c}
        \includegraphics[width=0.49\textwidth]{figures/ocrLarge_bcfwGap_time.pdf} &
        \includegraphics[width=0.49\textwidth]{figures/ocrLarge_bcPfwGapCache_time.pdf}\\
        \includegraphics[width=0.49\textwidth]{figures/ocrLarge_bcfwGap_numpass.pdf} &
        \includegraphics[width=0.49\textwidth]{figures/ocrLarge_bcPfwGapCache_numpass.pdf}\\
        {\scriptsize BCFW + gap sampling}
        &
        {\scriptsize BCPFW + gap sampling + caching}
        \\[-0.0cm]
    \end{tabular}
    \caption{
        \label{fig:regPath_ocrLarge} OCR-large
        }
\end{subfigure}
\caption{Comparison of regularization path methods. In each subfigure, we compare the $\epsilon$-approximate regularization path against the heuristic path and the grid search with/without warm start for a specific dataset. 
In each subfigure, we report the cumulative running time (top) and the cumulative effective number of passes (bottom) required to get to each value of the regularization parameter~$\regularizerweight$. We report results using two different methods as the SSVM solver: BCFW + gap sampling (left) and BCPFW + gap sampling + caching (right).
    Note that for OCR-large, the time limit of 24 hours was reached for the regularization path methods.
    However, the reached value of $\regularizerweight$ is smaller than the lower boundary of the grid $2^{-15}$.
    For HorseSeg-medium, the time limit of 24 hours was reached for both the regularization path and grid search methods.
    \label{fig:regPath_4datasets}}
\end{figure*}

\clearpage
\bibliographysup{icml2016_bcfw}
\bibliographystylesup{icml2016}

\end{document}




%
%
%
%
%
%
%
%
%
%
\documentclass{article}
\newcommand{\SAGAsection}{\textsc{Saga}}
%\usepackage[para]{footmisc}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{empheq}
\usepackage{xcolor, color, colortbl}
\usepackage{mdframed}
\usepackage{wrapfig}
\usepackage{dsfont}
\usepackage{nccmath}
%
\usepackage{graphicx} %
\usepackage{subfigure}
%
\usepackage{natbib}

%
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{stmaryrd}
\usepackage{nicefrac}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{rotating}
\usepackage{float}



\definecolor{mydarkblue}{rgb}{0,0.08,0.45}
\usepackage[colorlinks=true,
    linkcolor=mydarkblue,
    citecolor=mydarkblue,
    filecolor=mydarkblue,
    urlcolor=mydarkblue,
    pdfview=FitH,
    breaklinks]{hyperref}
\urlstyle{same}
%
%
\newcommand{\theHalgorithm}{\arabic{algorithm}}

%
%
%
%
\usepackage[final]{nips_2017}

\newcommand{\blue}{\color{blue}}
\def\RR{{\mathbb R}}
\def\EE{{\mathbb E}}
\newcommand{\Econd}{\mathbf{E}}
\def\prox{{\mathbf{prox}}}
\def\eprox{{\emph{\text{prox}}}}


\newcommand*\mybluebox[1]{\colorbox{myblue}{\hspace{1em}#1\hspace{1em}}}

\newcommand{\CYCLADES}{\textsc{Cyclades}}
\newcommand{\PASAGA}{\textsc{ProxAsaga}}
\newcommand{\SAGA}{\textsc{Saga}}
\newcommand{\ASAGA}{\textsc{Asaga}}
\newcommand{\SAG}{\textsc{Sag}}
\newcommand{\MISO}{\textsc{Miso}}
\newcommand{\ADMM}{\textsc{Admm}}
\newcommand{\PSSAGA}{\textsc{Ps-Saga}}
\newcommand{\SVRG}{\textsc{Svrg}}
\newcommand{\ProxSVRG}{Prox\textsc{Svrg}}
\newcommand{\SDCA}{\textsc{Sdca}}
\newcommand{\SGD}{\textsc{Sgd}}
\newcommand{\HOGWILD}{\textsc{Hogwild}}
\newcommand{\AsySPCD}{\textsc{AsySpcd}}

\newcommand{\BlockSet}{\mathcal{B}}


\DeclareMathOperator*{\argmin}{arg\,min}
\def\xx{{\boldsymbol x}}
\def\ww{{\boldsymbol w}}
\def\yy{{\boldsymbol y}}
\def\vv{{\boldsymbol v}}
\def\uu{{\boldsymbol u}}
\def\ww{{\boldsymbol w}}
\def\zz{{\boldsymbol z}}
\def\DD{{\boldsymbol D}}
\def\balpha{{\boldsymbol \alpha}}


\newcommand{\tablefont}[1] {{\fontsize{8}{10}\sffamily{#1}}}

\definecolor{myblue}{HTML}{D2E4FC}
\definecolor{Gray}{gray}{0.92}

\newmdtheoremenv{theo}{Theorem}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{innercustomtheorem}{Theorem}
\newenvironment{customtheorem}[1]
  {\renewcommand\theinnercustomtheorem{#1}\innercustomtheorem}
  {\endinnercustomtheorem}

\newtheorem{innercustomcorollary}{Corollary}
\newenvironment{customcorollary}[1]
  {\renewcommand\theinnercustomcorollary{#1}\innercustomcorollary}
  {\endinnercustomcorollary}

\graphicspath{{./figures/}}
\renewcommand{\llbracket}{[}
\renewcommand{\rrbracket}{]}

\renewcommand{\algorithmicloop}{\textbf{keep doing in parallel}} %
\renewcommand{\algorithmicendloop}{\algorithmicend\ \textbf{parallel loop}}



\title{Breaking the Nonsmooth Barrier: A Scalable Parallel Method for Composite Optimization}

%\author[1, 2]{Fabian Pedregosa}
%\author[1, 2]{R\'emi Leblond}
%\author[3]{Simon Lacoste-Julien}
%\affil[1]{\textsc{Inria}, \textsc{Sierra} Project team, Paris, France}
%\affil[2]{\'Ecole normale sup\'erieure, CNRS, PSL Research University }
%\affil[3]{MILA \& DIRO, Universit\'e de Montr\'eal, Montr\'eal, Canada}
%
%
\author{
    Fabian Pedregosa \\
    INRIA/ENS\thanks{DI \'{E}cole normale sup\'{e}rieure, CNRS, PSL Research University} \\ Paris, France\\
    \And
    R\'emi Leblond\\
    INRIA/ENS\footnotemark[1] \\
    Paris, France\\
    \And
    Simon Lacoste-Julien\\
    MILA and DIRO \\
    Universit\'{e} de Montr\'{e}al, Canada
}


\begin{document}
\maketitle


\begin{abstract}
  Due to their simplicity and excellent performance, parallel asynchronous variants of stochastic gradient descent have become popular methods to solve a wide range of large-scale optimization problems on multi-core architectures. Yet, despite their practical success, support for nonsmooth objectives is still lacking, making them unsuitable for many problems of interest in machine learning, such as the Lasso, group Lasso or empirical risk minimization with convex constraints.
  In this work, we propose and analyze \PASAGA, a fully asynchronous sparse method inspired by \SAGA, a variance reduced incremental gradient algorithm. The proposed method is easy to implement and significantly outperforms the state of the art on several nonsmooth, large-scale problems. We prove that our method achieves a theoretical linear speedup with respect to the sequential version under assumptions on the sparsity of gradients and block-separability of the proximal term. Empirical benchmarks on a multi-core architecture illustrate practical speedups of up to 12x on a 20-core machine.
\end{abstract}


\section{Introduction}
The widespread availability of multi-core computers motivates the development of parallel methods adapted for these architectures.
One of the most popular approaches is \HOGWILD~\citep{hogwild2011}, an asynchronous variant of stochastic gradient descent (\SGD).
In this algorithm, multiple threads run the update rule of \SGD\ asynchronously in parallel.
As \SGD, it only requires visiting a small batch of random examples per iteration, which makes it ideally suited for large scale machine learning problems.
Due to its simplicity and excellent performance, this parallelization approach has recently been extended to other variants of \SGD\ with better convergence properties, such as \SVRG~\citep{johnson2013accelerating} and \SAGA~\citep{defazio2014saga}.

Despite their practical success, existing parallel asynchronous variants of \SGD\ are limited to smooth objectives, making them inapplicable to many problems in machine learning and signal processing.
In this work, we develop a sparse variant of the \SAGA\ algorithm and consider its parallel asynchronous variants for general \emph{composite} optimization problems of the form:
\begin{empheq}[box=\mybluebox]{equation}\label{eq:opt}\tag{OPT}
\argmin^{\vphantom{i}}_{\xx \in \RR^p} f(\xx) \,+~ h(\xx) \quad,\quad \text{ with } f(\xx) := \textstyle\frac{1}{n}\sum_{i=1}^n f_i(\xx) \quad,
\end{empheq}
where each $f_i$ is convex with $L$-Lipschitz gradient, the average function $f$ is $\mu$-strongly convex and $h$ is convex but potentially nonsmooth.
We further assume that $h$ is ``simple'' in the sense that we have access to its proximal operator, and that it is
block-separable, that is, it can be decomposed block coordinate-wise as $h(\xx) = \textstyle{\sum_{B \in \mathcal{B}}} h_B(\llbracket\xx\rrbracket_B)$, where $\mathcal{B}$ is a partition of the coefficients into subsets which will call \emph{blocks} and $h_B$ only depends on coordinates in block $B$.  Note that there is no loss of generality in this last assumption as a unique block covering all coordinates is a valid partition, though in this case, our sparse variant of the \SAGA\ algorithm reduces to the original \SAGA\ algorithm and no gain from sparsity is obtained.



This template models a broad range of problems arising in machine learning and signal processing: the finite-sum structure of $f$ includes the least squares or logistic loss functions;
the proximal term $h$ includes penalties such as the $\ell_1$ or group lasso penalty.
Furthermore, this term can be extended-valued, thus allowing for convex constraints through the indicator function.

\paragraph{Contributions.} This work presents two main contributions. First, in \S\ref{scs:sparse_prox_saga} we describe Sparse Proximal \SAGA, a novel variant of the \SAGA\ algorithm which features a reduced cost per iteration in the presence of sparse gradients and a block-separable penalty.
Like other variance reduced methods, it enjoys a linear convergence rate under strong convexity.
Second, in \S\ref{sec:pasaga} we present \PASAGA, a lock-free asynchronous parallel version of the aforementioned algorithm that does not require consistent reads.
Our main results states that \PASAGA\ obtains (under assumptions) a theoretical linear speedup with respect to its sequential version.
Empirical benchmarks reported in \S\ref{scs:experiments} show that this method dramatically outperforms state-of-the-art alternatives on large sparse datasets, while the empirical speedup analysis illustrates the practical gains as well as its limitations.

\subsection{Related work}


\paragraph{Asynchronous coordinate-descent.} For composite objective functions of the form~\eqref{eq:opt}, most of the existing literature on asynchronous optimization has focused on variants of coordinate descent.
\citet{liu2015asynchronous2} proposed an asynchronous variant of (proximal) coordinate descent and proved a near-linear speedup in the number of cores used, given a suitable step size.
This approach has been recently extended to general block-coordinate schemes by \citet{peng2016arock}, to greedy coordinate-descent schemes by~\citet{you2016asynchronous} and to non-convex problems by~\citet{apalm_davis}. However, as illustrated by our experiments, in the large sample regime coordinate descent compares poorly against incremental gradient methods like \SAGA.


\paragraph{Variance reduced incremental gradient and their asynchronous variants.} Initially proposed in the context of smooth optimization by \citet{roux2012stochastic}, variance reduced incremental gradient methods have since been extended to minimize composite problems of the form~\eqref{eq:opt} (see table below). Smooth variants of these methods have also recently been extended to the asynchronous setting, where multiple threads run the update rule asynchronously and in parallel.
Interestingly,
none of these methods achieve both simultaneously, i.e. asynchronous optimization of composite problems.
Since variance reduced incremental gradient methods have shown state of the art performance in both settings, this generalization is of key practical interest.
\begin{table}[H]
\centering\footnotesize
\hskip-0.5cm\begin{tabular}{c c | l l|}
\cline{2-4}
\multicolumn{1}{c|}{} & {\tablefont{Objective}} & {\hskip 1.5cm \tablefont{Sequential Algorithm}} & {\hskip 1.5cm \tablefont{Asynchronous Algorithm}}\\
\cline{2-4}
\multicolumn{1}{c|}{\multirow{0}{*}{}}
& \cellcolor{Gray} &\cellcolor{Gray}{\SVRG~\citep{johnson2013accelerating}}&
 \cellcolor{Gray}{\SVRG~\citep{reddi2015variance}}\\
 %
\multicolumn{1}{c|}{} &\multirow{-1}{*}{\tablefont{Smooth}}\cellcolor{Gray}&\SDCA~\citep{shalev2013stochastic}
 \cellcolor{Gray} &
\cellcolor{Gray}\textsc{Passcode}~\citep[\SDCA\ variant]{hsieh2015passcode}
\\
%
\multicolumn{1}{c|}{} &\cellcolor{Gray} &\SAGA~\citep{defazio2014saga}
 \cellcolor{Gray} &\cellcolor{Gray}\ASAGA~\citep[\SAGA\ variant]{leblond2016Asaga}\\
\multicolumn{1}{c|}{} &\multirow{0}{*}{}  &\textsc{ProxSdca}~\citep{shalev2012proximal} &  \\
\multicolumn{1}{c|}{}& \multirow{-1}{*}{\tablefont{Composite}} & 
\SAGA~\citep{defazio2014saga}  & {\qquad\qquad {This work: \PASAGA}}\\
\multicolumn{1}{c|}{}& \multirow{-2}{*}{} &
\ProxSVRG~\citep{xiao2014proximal} &  \\
\cline{2-4}\\
\end{tabular}
\end{table}

%\vspace{-1em}
\paragraph{On the difficulty of a composite extension.}
%
%
%
Two key issues explain the paucity in the development of asynchronous incremental gradient methods for composite optimization.
%
The first issue is related to the design of such algorithms.
Asynchronous variants of \SGD\ are most competitive when the updates are sparse and have a small overlap, that is, when each update modifies a small and different subset of the coefficients. This is typically achieved by updating only coefficients for which the partial gradient at a given iteration is nonzero,\footnote{Although some regularizers are sparsity inducing, large scale datasets are often extremely sparse and leveraging this property is crucial for the efficiency of the method.} but existing schemes such as the lagged updates technique~\citep{schmidt2016minimizing} are not applicable in the asynchronous setting.
The second difficulty is related to the analysis of such algorithms.
All convergence proofs crucially use the Lipschitz condition on the gradient to bound the noise terms derived from asynchrony.
However, in the composite case, the gradient mapping term~\citep{beck2009gradient}, which replaces the gradient in proximal-gradient methods, does not have a bounded Lipschitz constant.
Hence, the traditional proof technique breaks down in this scenario.




\paragraph{Other approaches.}
Recently, \citet{meng2017aaai, gu2016asynchronous} independently proposed a doubly stochastic method to solve the problem at hand. Following \citet{meng2017aaai} we refer to it as Async-\textsc{ProxSvrcd}.
This method performs coordinate descent-like updates in which the true gradient is replaced by its \SVRG\ approximation.
It hence features a doubly-stochastic loop: at each iteration we select a random coordinate \emph{and} a random sample.
Because the selected coordinate block is uncorrelated with the chosen sample, the algorithm can be orders of magnitude slower than \SAGA\ in the presence of sparse gradients. \ref{apx:experiments} contains a comparison of these methods.
%


\subsection{Definitions and notations}
By convention, we denote vectors and vector-valued functions in lowercase boldface (e.g. $\xx$) and matrices in uppercase boldface  (e.g. $\boldsymbol D$).
%
The { proximal operator } of a convex lower semicontinuous function $h$ is defined as
$  \prox_{ h}(\xx) := \argmin_{\zz \in \RR^p} \{ h(\zz) + \frac{1}{2}\|\xx - \zz\|^2\}$.
A function $f$ is said to be {$L$-smooth} if it is differentiable and its gradient is $L$-Lipschitz continuous.
A function $f$ is said to be {$\mu$-strongly convex} if $f - \frac{\mu}{2}\|\cdot\|^2$ is convex.
We use the notation $\kappa := L/\mu$ to denote the condition number for an $L$-smooth and $\mu$-strongly convex function.\footnote{Since we have assumed that each individual $f_i$ is $L$-smooth, $f$ itself is $L$-smooth -- but it could have a smaller smoothness constant. Our rates are in terms of this bigger $L/\mu$, as is standard in the \SAGA\ literature.}

$\boldsymbol{I}_{p}$ denotes the $p$-dimensional identity matrix, $\mathds{1}\{\text{cond}\}$ the characteristic function, which is $1$ if cond evaluates to true and $0$ otherwise.
The average of a vector or matrix is denoted $\overline{\boldsymbol\alpha}$ $:= \frac{1}{n}\sum_{i=1}^n {\boldsymbol\alpha}_i$.
We use $\| \cdot \|$ for the Euclidean norm.
For a positive semi-definite matrix~$\DD$, we define its associated distance as $\|\xx\|^2_{\boldsymbol{D}} := \langle \xx, \boldsymbol{D} \xx\rangle$. We denote by $[\,\xx\,]_b$ the $b$-th coordinate in $\xx$. This notation is overloaded so that for a collection of blocks $T = \{B_1, B_2, \ldots\}$, $\llbracket \xx \rrbracket_T$ denotes the vector $\xx$ restricted to the coordinates in the blocks of $T$. For convenience, when $T$ consists of a single block $B$ we use $\llbracket \xx \rrbracket_B$  as a shortcut of $\llbracket \xx \rrbracket_{\{B\}}$.
Finally, we distinguish $\EE$, the full expectation taken with respect to all the randomness in the system, from $\Econd$, the conditional expectation of a random $i_t$ (the random feature sampled at each iteration by \SGD-like algorithms) conditioned on all the ``past'', which the context will clarify.


\section{Sparse Proximal SAGA }\label{scs:sparse_prox_saga}

\paragraph{Original \SAGAsection\ algorithm.} The original \SAGA\ algorithm~\citep{defazio2014saga} maintains two moving quantities: the current iterate~$\xx$ and a table (memory) of historical gradients $({\boldsymbol\alpha}_i)_{i=1}^n$.
At every iteration, it samples an index $i \in \{1,\ldots, n\}$ uniformly at random, and computes the next iterate $(\xx^+, \balpha^+)$ according to the following recursion:
\begin{equation}\label{eq:original_saga}
  \uu_i = \nabla f_i(\xx) - {\boldsymbol\alpha}_i + \overline{\boldsymbol\alpha} \,;\quad\xx^+ = \prox_{\gamma h}\big(\xx - \gamma \uu_i\big);\quad {\boldsymbol\alpha}_i^+ = \nabla f_i(\xx)~.
\end{equation}
%
On each iteration, this update rule requires to visit all coefficients even if the partial gradients $\nabla f_i$ are sparse.
Sparse partial gradients arise in a variety of practical scenarios: for example, in generalized linear models the partial gradients inherit the sparsity pattern of the dataset.
Given that large-scale datasets are often sparse,\footnote{For example, in the \texttt{LibSVM} datasets suite, 8 out of the 11 datasets (as of May 2017) with more than a million samples have a density between $10^{-4}$ and $10^{-6}$.} leveraging this sparsity is crucial for the success of the optimizer.



\paragraph{Sparse Proximal \SAGAsection\ algorithm.} We will now describe an algorithm that leverages sparsity in the partial gradients by only updating those blocks that intersect with the support of the partial gradients. 
Since in this update scheme some blocks might appear more frequently than others, we will need to counterbalance this undersirable effect with a well-chosen block-wise reweighting of the average gradient and the proximal term. 

In order to make precise this block-wise reweighting, we define the following quantities.
We denote by $T_i$ the \emph{extended support} of $\nabla f_i$, which is the set of blocks that intersect the support of $\nabla f_i$, formally defined as $T_i := \{B: \text{supp}(\nabla f_i) \cap B \neq \varnothing, \,B\in\mathcal{B} \}$.
For totally separable penalties such as the $\ell_1$ norm, the blocks are individual coordinates and so the extended support covers the same coordinates as the support.
Let ${d}_B := n / n_B$, where $n_B:=\sum_i\mathds{1}\{B \in T_i\}$ is the number of times that $B \in T_i$. For simplicity we assume $n_B > 0$, as otherwise the problem can be reformulated without block $B$.
%
The update rule in \eqref{eq:original_saga} requires computing the proximal operator of $h$, which involves a full pass on the coordinates.
In our proposed algorithm, we replace~$h$ in~\eqref{eq:original_saga} with the function $\varphi_i(\xx) := \textstyle\sum_{B \in T_{i}} d_B h_B(\xx)$, whose form is justified by the following
 three properties. First, this function is zero outside $T_i$, allowing for sparse updates. Second, because of the block-wise reweighting~$d_B$, the function $\varphi_i$ is an unbiased estimator of $h$ (i.e.,  $\Econd\, \varphi_i = h$), property which will be crucial to prove the convergence of the method.
Third, $\varphi_i$ inherits the block-wise structure of $h$ and its proximal operator can be computed from that of $h$ as 
$
\llbracket\prox_{\gamma \varphi_i}(\xx)\rrbracket_{B} = \llbracket\prox_{(d_B \gamma) h_B}(\xx)\rrbracket_B$ if $B \in T_i$ and $
\llbracket\prox_{\gamma \varphi_i}(\xx)\rrbracket_{B} = \llbracket\xx\rrbracket_B$ otherwise.
%
Following~\citet{leblond2016Asaga}, we will also replace the dense gradient estimate $\uu_i$ by the sparse estimate $\vv_i := \nabla f_i(\xx) - \balpha_i + \DD_i \overline{\balpha}$, where $\DD_i$ is the diagonal matrix defined block-wise as $\llbracket\DD_i\rrbracket_{B, B} = d_B \mathds{1}\{B \in T_i\}\boldsymbol{I}_{|B|}$. 
It is easy to verify that the vector $\DD_i \overline{\balpha}$ is a weighted projection onto the support of $T_i$ and
$\Econd\,\DD_i \overline{\balpha} =\overline{\balpha}$,
making $\vv_i$ an unbiased estimate of the gradient.
%See \ref{apx:sparse_saga} for further details.

We now have all necessary elements to describe the Sparse Proximal \SAGA\ algorithm. As the original \SAGA\ algorithm, it maintains two moving quantities: the current iterate~$\xx \in \RR^p$ and a table of historical gradients $({\boldsymbol\alpha}_i)_{i=1}^n,\, \balpha_i \in \RR^p$.
At each iteration, the algorithm samples an index $i \in \{1, \ldots, n\}$ and computes the next iterate $(\xx^+, \balpha^+)$ as:
\begin{empheq}[box=\mybluebox]{equation}\label{eq:SPS}\tag{SPS}
    \vphantom{\sum_i^n}\vv_i = \nabla f_i(\xx) - \balpha_i + \DD_i \overline{\balpha}\,; ~\xx^+ = \prox_{\gamma \varphi_i}\big(\xx - \gamma \vv_i \big)\,;~\balpha_i^+ = \nabla f_i(\xx)~,
\end{empheq}
where in a practical implementation the vector $\overline{\boldsymbol\alpha}$ is updated incrementally at each iteration.

The above algorithm is sparse in the sense that it only requires to visit and update blocks in the extended support: if $B \notin T_i$, by the sparsity of $\vv_i$ and $\prox_{\varphi_i}$, we have $\llbracket\xx^+\rrbracket_B = \llbracket\xx\rrbracket_B$.
Hence, when the extended support $T_i$ is sparse, this algorithm can be orders of magnitude faster than the naive \SAGA\ algorithm. The extended support is sparse for example when the partial gradients are sparse and the penalty is separable, as is the case of the $\ell_1$ norm or the indicator function over a hypercube, or when the the penalty is block-separable in a way such that only a small subset of the blocks overlap with the support of the partial gradients.
Initialization of variables and a reduced storage scheme for the memory are discussed in the implementation details section of \ref{apx:implementation_details}.


{\bfseries Relationship with existing methods}.
This algorithm can be seen as a generalization of both the Standard \SAGA\ algorithm and the Sparse \SAGA\ algorithm of \citet{leblond2016Asaga}.
When the proximal term is not block-separable, then $d_B=1$ (for a unique block $B$) and the algorithm defaults to the Standard (dense) \SAGA\ algorithm.
In the smooth case (i.e., $h=0$), the algorithm defaults to the Sparse \SAGA\ method.
Hence we note that the sparse gradient estimate $\vv_i$ in our algorithm is the same as the one proposed in~\citet{leblond2016Asaga}.
However, we emphasize that a straightforward combination of this sparse update rule with the proximal update from the Standard \SAGA\ algorithm results in a nonconvergent algorithm: the block-wise reweighting of $h$ is a surprisingly simple but crucial change.
We now give the convergence guarantees for this algorithm.

\begin{theorem}\label{theorem:rates_sparse_saga}
  \label{th1} Let $\gamma = \frac{a}{5L}$ for any $a\leq 1$ and $f$ be $\mu$-strongly convex ($\mu > 0$). Then Sparse Proximal \SAGA\ converges geometrically in expectation with a rate factor of at least $\rho = \frac{1}{5} \min\{\frac{1}{n}, a\frac{1}{\kappa}\}$. That is, for $\xx_t$ obtained after $t$ updates, we have the following bound:
  $$
  \EE \|\xx_t - \xx^*\|^2 \leq (1-\rho)^t C_0\,,~\text{ with }C_0 := \|\xx_0 - \xx^*\|^2 + \textstyle\frac{1}{5 L^2} \textstyle\sum_{i=1}^n\|{\boldsymbol\alpha}_i^0 - \nabla f_i(\xx^*)\|^2  \, \quad.
  $$
\end{theorem}
{\bfseries Remark}. For the step size $\gamma=\nicefrac{1}{5L}$, the convergence rate is $(1 - \nicefrac{1}{5}\min\{\nicefrac{1}{n}, \nicefrac{1}{\kappa}\})$.
We can thus identify two regimes: the ``big data'' regime, $n \geq \kappa$, in which the rate factor is bounded by $ \nicefrac{1}{5n}$, and the ``ill-conditioned'' regime, $\kappa \geq n$, in which the rate factor is bounded by $ \nicefrac{1}{5\kappa}$.
This rate roughly matches the rate obtained by \citet{defazio2014saga}.
While the step size bound of $\nicefrac{1}{5L}$ is slightly smaller than the $\nicefrac{1}{3L}$ one obtained in that work, this can be explained by their stronger assumptions: each $f_i$ is strongly convex whereas they are strongly convex only on average in this work. All proofs for this section can be found in \ref{apx:sparse_saga}.


\section{Asynchronous Sparse Proximal SAGA}\label{sec:pasaga}

We introduce \PASAGA\ -- the asynchronous parallel variant of Sparse Proximal \SAGA. In this algorithm, multiple cores update a central parameter vector using the Sparse Proximal \SAGA\ introduced in the previous section, and updates are performed asynchronously. The algorithm parameters are read and written without vector locks, i.e., the vector content of the shared memory can potentially change while a core is reading or writing to main memory coordinate by coordinate. These operations are typically called \emph{inconsistent} (at the vector level).



The full algorithm is described in
Algorithm~\ref{alg:theoretical} for its theoretical version (on which our analysis is built) and in Algorithm~\ref{alg:sagasync} for its practical implementation. The practical implementation differs from the analyzed agorithm in three points.
First, in the implemented algorithm, index $i$ is sampled before reading the coefficients to minimize memory access since only the extended support needs to be read. Second, since our implementation targets generalized linear models, the memory $\boldsymbol\alpha_i$ can be compressed into a single scalar in L\ref{line:alphaiupdate} (see \ref{apx:implementation_details}). Third, $\overline\balpha$ is stored in memory and updated incrementally instead of recomputed at each iteration. %


The rest of the section is structured as follows:
we start by describing our framework of analysis;
we then derive essential properties of \PASAGA\ along with a classical delay assumption.
Finally, we state our main convergence and speedup result.

\begin{figure*}[ttt!]
 \begin{minipage}[t]{0.5\textwidth}
   \begin{algorithm}[H]
     \caption{\PASAGA\ (analyzed)}
     \label{alg:theoretical}
     \label{theoreticalgo}
     \begin{algorithmic}[1]
    \STATE Initialize shared variables $\xx$ and $({\boldsymbol\alpha}_i)_{i=1}^n$
    \LOOP
    \STATE $\hat \xx = $ inconsistent read of $\xx$
 	  \STATE $\hat {\boldsymbol\alpha} = $ inconsistent read of ${\boldsymbol\alpha}$
    	  \STATE \emph{Sample} $i$  uniformly in $\{1,...,n\}$
        \STATE $S_i :=$ support of $\nabla f_i$
        \STATE $T_i :=$ extended support of $\nabla f_i$ in $\mathcal{B}$
        \STATE $\llbracket\,\overline{{\boldsymbol\alpha}}\,\rrbracket_{T_i} = \nicefrac{1}{n} \sum_{j=1}^n \llbracket\,\hat{{\boldsymbol\alpha}}_j\,\rrbracket_{T_i}$
        \STATE $\llbracket\,\delta\balpha\,\rrbracket_{S_i} = \llbracket\nabla f_i(\hat \xx)\rrbracket_{S_i} - \llbracket\hat{{\boldsymbol\alpha}}_i\rrbracket_{S_i}$
        \STATE $\llbracket\,\hat \vv\,\rrbracket_{T_i} = \llbracket\,\delta\balpha\,\rrbracket_{T_i} + \llbracket\DD_i \overline {\boldsymbol\alpha}\,\rrbracket_{T_i}$
        \STATE $\llbracket\,\delta\xx\,\rrbracket_{T_i} = \llbracket\prox_{\gamma \varphi_i}(\hat{\xx} - \gamma \hat \vv)\rrbracket_{T_i} - \llbracket\hat\xx\rrbracket_{T_i}$
        \FOR{$B$ \,{\bfseries in} $T_i$}
        \FOR{$b \in B$}
        \STATE $[\,\xx\,]_b \gets [\,\xx\,]_b + [\,\delta\xx\,]_b$\hfill$\triangleright$ atomic \label{alg1:atomicLine}
        \IF{$b \in S_i$}
        \STATE $[{\boldsymbol\alpha}_i]_b \gets [\nabla f_i(\hat{\xx})]_b$
        \ENDIF
        \ENDFOR
        \ENDFOR
        \STATE // {\footnotesize(`$\gets$' denotes shared memory update.)}
    \ENDLOOP
   \end{algorithmic}
    \end{algorithm}
 \end{minipage}
 \hfill
 \begin{minipage}[t]{0.5\textwidth}
    \begin{algorithm}[H]
      \caption{\PASAGA\ (implemented)}
      \label{alg:sagasync}
      \begin{algorithmic}[1]
     \STATE Initialize shared variables $\xx$, $({\boldsymbol\alpha}_i)_{i=1}^n$, $\overline{\balpha}$
     \LOOP
     \STATE \emph{Sample} $i$ uniformly in $\{1,...,n\}$
     \STATE $S_i :=$ support of $\nabla f_i$
     \STATE $T_i :=$ extended support of $\nabla f_i$ in $\mathcal{B}$
     \STATE $\llbracket\,\hat \xx\,\rrbracket_{T_i} = $ inconsistent read of $\xx$ on $T_i$
     \STATE $\hat {\boldsymbol\alpha}_i = $ inconsistent read of ${\boldsymbol\alpha}_i$
     \STATE $\llbracket\,\overline{\boldsymbol\alpha}\,\rrbracket_{T_i} = $ inconsistent read of $\overline{\boldsymbol\alpha}$ on $T_i$
     \STATE $\llbracket\,\delta\balpha\,\rrbracket_{S_i} = \llbracket\nabla f_i(\hat \xx)\rrbracket_{S_i} - \llbracket\hat{{\boldsymbol\alpha}}_i\rrbracket_{S_i}$
     \STATE $\llbracket\,\hat \vv\,\rrbracket_{T_i} = \llbracket\delta \balpha\,\rrbracket_{T_i} + \llbracket\,\DD_i \overline {\boldsymbol\alpha}\,\rrbracket_{T_i}$
     \STATE $\llbracket\,\delta\xx\,\rrbracket_{T_i} = \llbracket\prox_{\gamma \varphi_i}(\hat{\xx} - \gamma \hat \vv)\rrbracket_{T_i} - \llbracket\hat\xx\rrbracket_{T_i}$
     \FOR{$B$ \,{\bfseries in} $T_i$}
     \FOR{$b$ \,{\bfseries in} $B$}
     \STATE $[\,\xx\,]_b \gets [\,\xx\,]_b + [\,\delta\xx\,]_b$\hfill$\triangleright$ atomic
     \IF{$b \in S_i$}
      \STATE $[\,\overline {\boldsymbol\alpha}\,]_b \gets [\overline {\boldsymbol\alpha}]_b + \nicefrac{1}{n}[\delta {\boldsymbol\alpha}]_b$\hfill$\triangleright$ atomic
     \ENDIF
     \ENDFOR
     \ENDFOR
     \STATE ${\boldsymbol\alpha}_i \gets \nabla f_i(\hat\xx)$  \quad  (scalar update)\hfill$\triangleright$ atomic\label{line:alphaiupdate}
     \ENDLOOP
      \end{algorithmic}
    \end{algorithm}
 \end{minipage}
\end{figure*}

\subsection{Analysis framework}
As in most of the recent asynchronous optimization literature, we build on the hardware model introduced by~\citet{hogwild2011}, with multiple cores reading and writing to a shared memory parameter vector.
These operations are asynchronous (lock-free) and \textit{inconsistent}:\footnote{This is an extension of the framework of \citet{hogwild2011}, where consistent updates were assumed.} $\hat \xx_t$, the local copy of the parameters of a given core, does not necessarily correspond to a consistent iterate in memory.

\paragraph{``Perturbed'' iterates.}
To handle this additional difficulty, contrary to most contributions in this field, we choose the ``perturbed iterate framework'' proposed by~\citet{mania2015perturbed} and refined by~\citet{leblond2016Asaga}.
%
This framework can  analyze variants of \SGD\ which obey the update rule:
\begin{equation*}
\xx_{t+1} = \xx_t - \gamma \vv(\xx_t, i_t)\, ,~\text{ where $\vv$ verifies the unbiasedness condition $\Econd\, \vv(\xx, i_t) = \nabla f(\xx)$}
\end{equation*}
and the expectation is computed with respect to $i_t$.
In the asynchronous parallel setting, cores are reading inconsistent iterates from memory, which we denote $\hat{\xx}_t$.
As these inconsistent iterates are affected by various delays induced by asynchrony, they cannot easily be written as a function of their previous iterates.
To alleviate this issue,~\citet{mania2015perturbed} choose to introduce an additional quantity for the purpose of the analysis:
\begin{equation}
\xx_{t+1} := \xx_t - \gamma \vv(\hat \xx_t, i_t)\,, \quad\text{ the ``virtual iterate'' -- which is never actually computed}\, .
\end{equation}
Note that this equation is the \textit{definition} of this new quantity~$\xx_t$.
This virtual iterate is useful for the convergence analysis and makes for much easier proofs than in the related literature.

\paragraph{``After read'' labeling.}
How we choose to define the iteration counter $t$ to label an iterate $\xx_t$ matters in the analysis.
In this paper, we follow the ``after read'' labeling proposed in \citet{leblond2016Asaga}, in which
we update our iterate counter, $t$, as each core \emph{finishes reading} its copy of the parameters (in the specific case of \PASAGA, this includes both~$\hat \xx_t$ and~$\hat {\boldsymbol\alpha}^t$).
This means that $\hat \xx_t$ is the $(t+1)^{th}$ fully completed read.
One key advantage of this approach compared to the classical choice of~\citet{hogwild2011} -- where $t$ is increasing after each successful update -- is that it guarantees both that the $i_t$ are uniformly distributed and that~$i_t$ and~$\hat \xx_t$ are independent.
This property is not verified when using the ``after write'' labeling of~\citet{hogwild2011}, although it is still implicitly assumed in the papers using this approach, see \citet[Section 3.2]{leblond2016Asaga} for a discussion of issues related to the different labeling schemes.
%


\paragraph{Generalization to composite optimization.}
Although the perturbed iterate framework was designed for gradient-based updates, we can extend it to proximal methods by remarking that in the sequential setting, proximal stochastic gradient descent and its variants can be characterized by the following similar update rule:
\begin{equation}\label{eq:def_grad_mapping}
\xx_{t+1} = \xx_t - \gamma {\boldsymbol g}(\xx_t, \vv_{i_t}, {i_t}) \,,\quad \text{ with }\, {\boldsymbol g}(\xx, \vv, i) := \textstyle\frac{1}{\gamma}\big(\xx - \prox_{\gamma \varphi_i}(\xx - \gamma \vv)\big)\,,
\end{equation}
where as before $\vv$ verifies the unbiasedness condition $\Econd\, \vv = \nabla f(\xx)$.
The Proximal Sparse \SAGA\ iteration can be easily written within this template by using $\varphi_{i}$ and $\vv_i$ as defined in \S\ref{scs:sparse_prox_saga}. Using this definition of $\boldsymbol g$,
we can define \PASAGA\ virtual iterates as:
\begin{equation}\label{eq:definition}
\xx_{t+1} := \xx_t - \gamma \boldsymbol g(\hat{\xx}_t, \hat\vv^t_{i_t}, {i_t}) \,,\quad\text{ with } \hat{\vv}_{i_t}^t = \nabla f_{i_t}(\hat \xx_t) - \hat{{\boldsymbol\alpha}}^t_{i_t} + {\boldsymbol D}_{i_t} \overline{{\boldsymbol\alpha}}^t\quad,
\end{equation}
 where as in the sequential case, the memory terms are updated as
$\hat {\boldsymbol\alpha}^t_{i_t} = \nabla f_{i_t}(\hat \xx_t)$. Our theoretical analysis of \PASAGA\ will be based on this definition of the virtual iterate $\xx_{t+1}$.

\subsection{Properties and assumptions}
Now that we have introduced the ``after read'' labeling for proximal methods in Eq.~\eqref{eq:definition}, we can leverage the framework of~\citet[Section 3.3]{leblond2016Asaga} to derive essential properties for the analysis of \PASAGA.
We describe below three useful properties arising from the definition of Algorithm~\ref{alg:theoretical}, and then state a central (but standard) assumption that the delays induced by the asynchrony are uniformly bounded. 

{\bfseries Independence:} Due to the ``after read'' global ordering, $i_r$ is independent of $\hat{\xx}_t$ for all $r \geq t$. We enforce the independence for $r = t$ by having the cores read all the shared parameters before their iterations.

{\bfseries Unbiasedness:} The term $\hat \vv^t_{i_t}$ is an unbiased estimator of the gradient of $f$ at $\hat \xx_t$.
This property is a consequence of the independence between $i_t$ and $\hat{\xx}_t$.

{\bfseries Atomicity:} The shared parameter coordinate update of $[\xx]_b$ on Line~\ref{alg1:atomicLine} is atomic.
This means that there are no overwrites for a single coordinate even if several cores compete for the same resources. Most modern processors have support for atomic operations with minimal overhead.

{\bfseries Bounded overlap assumption.} We assume that there exists a uniform bound, $\tau$, on the maximum number of overlapping iterations.
This means that every coordinate update from iteration $t$ is successfully written to memory before iteration $t + \tau + 1$ starts.
Our result will give us conditions on $\tau$ to obtain linear speedups.
%\vspace{-2mm}

{\bfseries Bounding $\hat \xx_t - \xx_t$.}
The delay assumption of the previous paragraph allows to express the difference between real and virtual iterate using the gradient mapping  $\boldsymbol g_u := \boldsymbol g(\hat{\xx}_u, \hat\vv^u_{i_u}, {i_u})$ as:
\begin{equation}
\hat \xx_t - \xx_t = \gamma \textstyle\sum_{u=(t - \tau)_+}^{t-1}\boldsymbol G_{u}^t \boldsymbol g_u \,, \text{where $\boldsymbol G^t_{u}$ are $p\times p$ diagonal matrices with terms in $\{0, +1\}$.}
\end{equation}
$0$ represents instances where both $\hat \xx_u$ and $\xx_u$ have received the corresponding updates.
$+1$, on the contrary, represents instances where $\hat \xx_u$ has not yet received an update that is already in $\xx_u$ by definition. This bound will prove essential to our analysis.

\subsection{Analysis}\label{ssec:analysis}
In this section, we state our convergence and speedup results for \PASAGA.
The full details of the analysis can be found in \ref{apx:async}.
Following~\citet{hogwild2011}, we introduce a sparsity measure (generalized to the composite setting) that will appear in our results.
\begin{definition}
	Let $\Delta := \max_{B \in \mathcal{B}} |\{i: T_i \ni B\}| / n$.
	This is the normalized maximum number of times that a block appears in the extended support. For example, if a block is present in all $T_i$, then $\Delta=1$. If no two $T_i$ share the same block, then $\Delta = \nicefrac{1}{n}$. We always have $\nicefrac{1}{n} \leq \Delta \leq 1$.
\end{definition}

\begin{theorem}[Convergence guarantee of \PASAGA]\label{thm:convergence}
	Suppose $\tau \leq \frac{1}{10 \sqrt{\Delta}}$.
	For any step size $\gamma = \frac{a}{L}$ with $a \leq a^*(\tau) := \frac{1}{36} \min\{1, \frac{6 \kappa}{\tau}\}$, the inconsistent read iterates of Algorithm~\ref{alg:theoretical} converge in expectation at a geometric rate factor of at least: $\rho(a) = \frac{1}{5} \min \big\{\frac{1}{n},  a \frac{1}{\kappa}\big\},$
	i.e. $\EE \|\hat{\xx}_t-\xx^*\|^2 \leq (1-\rho)^t \,  \tilde C_0$, where $\tilde C_0$ is a constant independent of $t$ ($\approx \frac{n \kappa}{a}C_0$ with $C_0$ as defined in Theorem~\ref{th1}).
	%
\end{theorem}
%
%
This last result is similar to the original \SAGA\ convergence result and our own Theorem~\ref{th1}, with both an extra condition on $\tau$ and on the maximum allowable step size. In the best sparsity case, $\Delta = \nicefrac{1}{n}$ and we get the condition $\tau \leq \nicefrac{\sqrt{n}}{10}$.
We now compare the geometric rate above to the one of Sparse Proximal \SAGA\ to derive the necessary conditions under which \PASAGA\ is linearly faster.

\begin{corollary}[Speedup]\label{thm:corollary}
	Suppose $\tau \leq \frac{1}{10 \sqrt{\Delta}}$.
	If $\kappa \geq n$, then using the step size $\gamma = \nicefrac{1}{36 L}$, \PASAGA\ converges geometrically with rate factor $\Omega(\frac{1}{\kappa})$.
	If $\kappa < n$, then using the step size $\gamma = \nicefrac{1}{36 n \mu}$, \PASAGA\ converges geometrically with rate factor $\Omega(\frac{1}{n})$.
	In both cases, the convergence rate is the same as Sparse Proximal \SAGA. Thus \PASAGA\ is linearly faster than its sequential counterpart up to a constant factor. Note that in both cases \emph{the step size does not depend on~$\tau$}.

	Furthermore, if $\tau \leq 6 \kappa$, we can use a universal step size of $\Theta(\nicefrac{1}{L})$ to get a similar rate for \PASAGA\ than Sparse Proximal \SAGA, thus making it adaptive to local strong convexity since the knowledge of $\kappa$ is not required.
\end{corollary}

These speedup regimes are comparable with the best ones obtained in the smooth case, including~\citet{hogwild2011,reddi2015variance}, even though unlike these papers, we support inconsistent reads and nonsmooth objective functions.
The one exception is~\citet{leblond2016Asaga}, where the authors prove that their algorithm, \ASAGA, can obtain a linear speedup even without sparsity in the well-conditioned regime.
In contrast, \PASAGA\ always requires some sparsity.
Whether this property for smooth objective functions could be extended to the composite case remains an open problem.

Relative to \AsySPCD, in the best case scenario (where the components of the gradient are uncorrelated, a somewhat unrealistic setting), \AsySPCD\ can get a near-linear speedup for~$\tau$ as big as~$\sqrt[4]{p}$.
Our result states that $\tau = \mathcal{O}(\nicefrac{1}{\sqrt{\Delta}})$ is necessary for a linear speedup.
%
%
This means in case $\Delta \leq \nicefrac{1}{\sqrt{p}}$ our bound is better than the one obtained for \AsySPCD.
%
Recalling that $\nicefrac{1}{n} \leq \Delta \leq 1$, it appears that \PASAGA\ is favored when $n$ is bigger than $\sqrt{p}$ whereas \AsySPCD\ may have a better bound otherwise, though this comparison should be taken with a grain of salt given the assumptions we had to make to arrive at comparable quantities.
An extended comparison with the related work can be found in \ref{apx:related_work}.

%





\section{Experiments} \label{scs:experiments}

In this section, we compare \PASAGA\ with related methods on different datasets.
Although \PASAGA\ can be applied more broadly, we focus on $\ell_1\!+\!\ell_2$-regularized logistic regression, a model of particular practical importance.
The objective function takes the form
\begin{equation}\label{eq:logistic_loss}
  \frac{1}{n} \sum_{i=1}^n \log\big(1 + \exp(- b_i \boldsymbol a_i^\intercal \xx)\big) + \textstyle\frac{\lambda_1}{2} \|\xx\|_2^2 + \lambda_2 \|\xx\|_1\quad,
\end{equation}
where $\boldsymbol a_i \in \mathbb{R}^p$ and $b_i \in \{-1,+1\}$ are the data samples. Following~\citet{defazio2014saga}, we set $\lambda_1 = 1/n$. The amount of $\ell_1$ regularization ($\lambda_2$) is selected to give an approximate $\nicefrac{1}{10}$ nonzero coefficients. Implementation details are available in~\ref{apx:implementation_details}. We chose the 3 datasets described in Table~\ref{tab:datasets}
%\vspace{-2mm}
\begin{table}
\caption{Description of datasets.} \label{tab:datasets}
\centering
%\resizebox{\linewidth}{!}{
\begin{tabular}{lrrrrr}
\toprule
{\bfseries\sffamily Dataset} & \multicolumn{1}{c}{$n$} & \multicolumn{1}{c}{$p$} & {\tablefont{density}} & \multicolumn{1}{c}{$L$} & $\Delta$\\
\midrule
{\bfseries\sffamily KDD 2010}~\citep{yu2010feature} & \hfill 19,264,097 & \hfill 1,163,024 & \hfill $10^{-6}$ & \hfill 28.12 & 0.15\\
{\bfseries\sffamily KDD 2012}~\citep{juan2016field} & \hfill 149,639,105 & \hfill 54,686,452 & \hfill $2 \times 10^{-7}$ & \hfill $1.25$ & 0.85\\
{\bfseries\sffamily Criteo}~\citep{juan2016field} & \hfill 45,840,617 & \hfill 1,000,000 & \hfill $4 \times 10^{-5}$ & \hfill $1.25$ & 0.89\\
\bottomrule
\end{tabular}
%}
%\vspace{-5mm}
\end{table}


\begin{figure*}
\makebox[\textwidth][c]{
%\hspace{-0.8cm}
\includegraphics[width=1.0\linewidth]{prox_asaga_1}
}
\makebox[\textwidth][c]{
%\hspace{-0.9cm}
\includegraphics[width=1.0\linewidth]{prox_asaga_2}
}
\caption{{\bfseries Convergence for asynchronous stochastic methods for $\ell_1 + \ell_2$-regularized logistic regression}. {Top}: Suboptimality as a function of time for different asynchronous methods using 1 and 10 cores.
%
{Bottom}: Running time speedup as function of the number of cores. \PASAGA\ achieves significant speedups over its sequential version while being orders of magnitude faster than competing methods.
\AsySPCD\ achieves the highest speedups but it also the slowest overall method.
}\label{fig:suboptimality}
%\vspace{-4mm}
\end{figure*}


\paragraph{Results.} We compare three parallel asynchronous methods on the aforementioned datasets: \PASAGA\ (this work),\footnote{A reference C++/Python implementation of is available at \url{https://github.com/fabianp/ProxASAGA}} \AsySPCD, the asynchronous proximal coordinate descent method of~\citet{liu2015asynchronous2} and the (synchronous) \textsc{Fista} algorithm~\citep{beck2009gradient}, in which the gradient computation is parallelized by splitting the dataset into equal batches.
We aim to benchmark these methods in the most realistic scenario possible; to this end we use the following step size: $1 / 2L$ for \PASAGA, $1/L_c$ for \AsySPCD, where $L_c$ is the coordinate-wise Lipschitz constant of the gradient, while \textsc{Fista} uses backtracking line-search.
The results can be seen in Figure~\ref{fig:suboptimality} (top) with both one (thus sequential) and ten processors.
Two main observations can be made from this figure. First, \PASAGA\ is significantly faster on these problems. Second, its asynchronous version offers a significant speedup over its sequential counterpart.

In Figure~\ref{fig:suboptimality} (bottom) we present speedup with respect to the number of cores, where speedup is computed as the time to achieve a suboptimality of $10^{-10}$ with one core divided by the time to achieve the same suboptimality using several cores.
%
While our \emph{theoretical speedups} (with respect to the number of iterations) are almost linear as our theory predicts (see~\ref{apx:experiments}),  we observe a different story for our \emph{running time} speedups.
This can be attributed to memory access overhead, which our model does not take into account.
As predicted by our theoretical results, we observe a high correlation between the $\Delta$ dataset sparsity measure and the empirical speedup: KDD 2010 ($\Delta=0.15$) achieves a 11x speedup, while in Criteo ($\Delta=0.89$) the speedup is never above 6x.


Note that although competitor methods exhibit similar or sometimes better speedups, they remain orders of magnitude slower than \PASAGA\ in running time for large sparse problems. In fact, our method is between 5x and 80x times faster (in time to reach $10^{-10}$ suboptimality) than \textsc{Fista} and between 13x and 290x times faster  than \AsySPCD\ (see~\ref{apx:timing_benchmarks}).


\section{Conclusion and future work}
In this work, we have described \PASAGA, an asynchronous variance reduced algorithm with support for composite objective functions.
This method builds upon a novel sparse variant of the (proximal) \SAGA\ algorithm that takes advantage of sparsity in the individual gradients.
We have proven that this algorithm is linearly convergent under a condition on the step size and that it is linearly faster than its sequential counterpart given a bound on the delay.
%
Empirical benchmarks show that \PASAGA\ is orders of magnitude faster than existing state-of-the-art methods. %

This work can be extended in several ways.
First, we have focused on the \SAGA\ method as the basic iteration loop, but this approach can likely be extended to other proximal incremental schemes such as \SGD\ or \ProxSVRG.
Second, as mentioned in \S\ref{ssec:analysis}, it is an open question whether it is possible to obtain convergence guarantees without any sparsity assumption, as was done for \ASAGA.

\section*{Acknowledgements}

The authors would like to thank our colleagues Damien Garreau, Robert Gower, Thomas Kerdreux, Geoffrey Negiar and Konstantin Mishchenko for their feedback on this manuscript, and Jean-Baptiste Alayrac for support managing the computational resources.

This work was partially supported by a Google Research Award.
FP acknowledges support from the chaire \emph{\'Economie des nouvelles donn\'ees} with the \emph{data science} joint research initiative with the \emph{ fonds AXA pour la recherche}.



\bibliography{index}{}
\bibliographystyle{icml2017}

%
\clearpage
\appendix
\gdef\thesection{Appendix \Alph{section}}

%
%
%

%
%
%
%
%

\section*{\centering\LARGE Breaking the Nonsmooth Barrier: A Scalable Parallel Method for Composite Optimization\\
\hfill\\
\centering{Supplementary material}}

\vspace{2em}

\paragraph{Notations.} Throughout the supplementary material we use the following extra notation. We denote by $\langle \cdot, \cdot \rangle_{(i)}$ (resp. $\|\cdot\|_{(i)}$) the scalar product (resp. norm) restricted to blocks in $T_i$, i.e., $\langle \xx, \yy \rangle_{(i)} := \sum_{B \in T_i}\langle \llbracket\xx\rrbracket_B, \llbracket\yy\rrbracket_B \rangle$ and $\|\xx\|_{(i)} := \sqrt{\langle \xx, \xx\rangle_{(i)}}$.
We will also use the following definitions: $\varphi := \sum_{B \in \mathcal{B}} d_B h_B(\xx)$ and $\DD$ is the diagonal matrix defined block-wise as $\llbracket\DD\rrbracket_{B,B} = d_B \boldsymbol I_{|B|}$.


The {\bfseries Bregman divergence} associated with a convex function $f$ for points $\xx, \yy$ in its domain is defined as:
\begin{equation}
  B_f(\xx, \yy) := f(\xx) - f(\yy) - \langle \nabla f(\yy), \xx - \yy \rangle \,.
\end{equation}
  Note that this is always positive due to the convexity of $f$.

\section{Basic properties}



\begin{lemma}\label{lemma:neseterov_inequality}For any $\mu$-strongly convex function $f$ we have the following inequality:
\begin{equation}
\langle \nabla f(\yy) - \nabla f(\xx), \yy - \xx\rangle \geq \frac{\mu }{2}\|\yy - \xx\|^2  + B_f(\xx, \yy) \, .
\end{equation}

\end{lemma}
\begin{proof} By strong convexity, $f$ verifies the inequality:
  \begin{equation}\label{eq:strongconvex}
	f(\yy) \leq f(\xx) + \langle \nabla f(\yy), \yy - \xx \rangle - \frac{\mu}{2}\|\yy - \xx\|^2 \, ,
  \end{equation}

  for any $\xx, \yy$ in the domain (see e.g.~\citep{nesterov2004introductory}). We then have the equivalences:
\begin{align}
  &\qquad     f(\xx) \leq f(\yy) + \langle \nabla f(\xx), \xx - \yy \rangle - \frac{\mu}{2}\|\xx - \yy\|^2 \nonumber \\
  &\iff \frac{\mu}{2}\|\xx - \yy\|^2 + f(\xx) - f(\yy) \leq \langle \nabla f(\xx), \xx - \yy \rangle \nonumber \\
  &\iff \frac{\mu}{2}\|\xx - \yy\|^2 + \underbrace{f(\xx) - f(\yy) - \langle \nabla f(\yy), \xx - \yy\rangle}_{B_f(\xx, \yy)} \leq \langle \nabla f(\xx) - \nabla f(\yy), \xx - \yy \rangle \,,
\end{align}
  where in the last line we have subtracted $\langle \nabla f(\yy), \xx - \yy\rangle$ from both sides of the inequality.
\end{proof}

\hfill

\begin{lemma}\label{lemma:l_smooth_ineq}
  Let the $f_i$ be $L$-smooth and convex functions.
  Then it is verified that:
\begin{equation}
\frac{1}{n}\sum_{i=1}^n\|\nabla f_i(\xx) - \nabla f_i(\yy)\|^2 \leq 2 L B_f(\xx, \yy)\,.
\end{equation}
\end{lemma}

\begin{proof}
Since each $f_i$ is $L$-smooth, it is verified (see e.g.~\citet[Theorem 2.1.5]{nesterov2004introductory}) that
\begin{equation}
\|\nabla f_i(\xx) - \nabla f_i(\yy)\|^2 \leq 2 L \big(f_i(\xx) - f_i(\yy) - \langle \nabla f_i(\yy), \xx - \yy\rangle\big)\,.
\end{equation}
The result is obtained by averaging over $i$.
\end{proof}

\hfill

\begin{lemma}[Characterization of the proximal operator]\label{lemma:charac_prox}
  Let $h$ be convex lower semicontinuous. Then we have the following characterization of the proximal operator:
  \begin{equation}\label{eq:prox_char}
    \zz = \prox_{\gamma h}(\xx)  \iff  \frac{1}{\gamma}(\xx - \zz) \in \partial h(\zz)\,.
  \end{equation}
\end{lemma}
\begin{proof}
  This is a direct consequence of the first order optimality conditions on the definition of proximal operator, see e.g.~\citep{beck2009gradient, nesterov20013gradient}.
\end{proof}

\hfill

\begin{lemma}[Firm non-expansiveness]\label{lemmma:block_nonexpansive}
  Let $\xx, \tilde{\xx}$ be two arbitrary elements in the domain of $\varphi_i$ and $\zz, \tilde{\zz}$ be defined as $\zz := \eprox_{\varphi_i}(\xx)$, $\tilde{\zz} := \eprox_{\varphi_i}(\tilde{\xx})$. Then it is verified that:
\begin{equation}
\langle \zz - \tilde{\zz}, \xx - \tilde{\xx} \rangle_{(i)} \geq \|\zz - \tilde{\zz}\|_{(i)}^2\,.
\end{equation}
\end{lemma}
\begin{proof}
  By the block-separability of $\varphi_i$, the proximal operator is the concatenation of the proximal operators of the blocks. In other words, for any block $B \in T_i$ we have:
\begin{equation}
  \llbracket\zz\rrbracket_B = \prox_{\gamma \varphi_B}(\llbracket\xx\rrbracket_B)~,\quad \llbracket\tilde{\zz}\rrbracket_B = \prox_{\gamma \varphi_B}(\llbracket\tilde{\xx}\rrbracket_B)\,,
\end{equation}
where $\varphi_B$ is the restriction of $\varphi_i$ to $B$.
  By firm non-expansiveness of the proximal operator (see e.g.~\citet[Proposition 4.2]{bauschke2011convex}) we have that:
  $$
  \langle \llbracket\zz\rrbracket_B - \llbracket\tilde{\zz}\rrbracket_B, \llbracket\xx\rrbracket_B - \llbracket\tilde{\xx}\rrbracket_B\rangle \geq \|\llbracket\zz\rrbracket_B - \llbracket\tilde{\zz}\rrbracket_B\|^2 \,.
  $$
Summing over the blocks in $T_i$ yields the desired result.
\end{proof}

\clearpage
\section{Sparse Proximal SAGA}\label{apx:sparse_saga}

\vspace{0.5em}

This Appendix contains all proofs for Section~\ref{scs:sparse_prox_saga}. The main result of this section is Theorem~\ref{theorem:rates_sparse_saga}, whose proof is structured as follows:
\begin{itemize}
\item We start by proving four auxiliary results that will be used later on in the proofs of both  synchronous and asynchronous variants. 
The first is the unbiasedness of key quantities used in the algorithm. 
The second is a characterization of the solutions of \eqref{eq:opt} in terms of $f$ and $\varphi$ (defined below) in Lemma~\ref{lemma:fixed_point}.
The third is a key inequality in Lemma~\ref{lemma:gradient_mapping_1} that relates the gradient mapping to other terms that arise in the optimization.
The fourth is an upper bound on the variance terms of the gradient estimator, relating it to the Bregman divergence of $f$ and the past gradient estimator terms.
\item In Lemma~\ref{lemma:lyapunov_inequality}, we define an upper bound on the iterates $\|\xx_t - \xx^*\|^2$, called a Lyapunov function, and prove an inequality that relates this Lyapunov function value at the current iterate with its value at the previous iterate.
\item Finally, in the proof of Theorem~\ref{theorem:rates_sparse_saga} we use the previous inequality in terms of the Lyapunov function to prove a geometric convergence of the iterates.
\end{itemize}

% 
% \vspace{0.5em}

We start by proving the following unbiasedness result, mentioned in \S\ref{scs:sparse_prox_saga}.
\begin{lemma}\label{lemma:unbiasedness}
Let $\DD_i$ and $\varphi_i$ be defined as in \S\ref{scs:sparse_prox_saga}. Then it is verified that 
$\Econd \DD_i = \boldsymbol{I}_p$ and $\Econd\, \varphi_i = h$.
\end{lemma}
\begin{proof}
Let $B \in \mathcal{B}$ an arbitrary block. We have the following sequence of equalities:
\begin{align}
\Econd \llbracket\DD_i\rrbracket_{B, B} &= \frac{1}{n}\sum_{i=1}^n \llbracket\DD_i\rrbracket_{B, B} = \frac{1}{n}\sum_{i=1}^n d_B \mathds{1}\{B \in T_i\}\boldsymbol{I}_{|B|}\\
&=  \frac{1}{n}\sum_{i=1}^n \frac{n}{n_B} \mathds{1}\{B \in T_i\}\boldsymbol{I}_{|B|}\\
&= \left(\frac{1}{n_B} \sum_{i=1}^n \mathds{1}\{B \in T_i\}\right) \boldsymbol{I}_{|B|}= \boldsymbol{I}_{|B|}~,
\end{align}
where the last equality comes from the definition of $n_B$. $\Econd \DD_i = \boldsymbol{I}_p$ then follows from the arbitrariness of $B$.

Similarly, for $\varphi_i$ we have:
\begin{align}
\Econd \varphi_i(\llbracket\xx\rrbracket_B) &= \frac{1}{n}\sum_{i=1}^n d_B \mathds{1}\{B \in T_i\}h_B(\llbracket\xx\rrbracket_B)\\
&=  \frac{1}{n}\sum_{i=1}^n \frac{n}{n_B} \mathds{1}\{B \in T_i\}h_B(\llbracket\xx\rrbracket_B)\\
&= \left(\frac{1}{n_B} \sum_{i=1}^n \mathds{1}\{B \in T_i\}\right) h_B(\llbracket\xx\rrbracket_B)= h_B(\llbracket\xx\rrbracket_B)~,
\end{align}
Finally, the result $\Econd\, \varphi_i = h$ comes from adding over all blocks.
\end{proof}

\begin{lemma}\label{lemma:fixed_point}
  $\xx^*$ is a solution to (OPT) if and only if the following condition is verified:
\begin{equation}
 \xx^* = \prox_{\gamma \varphi}\big(\xx^* - \gamma \boldsymbol \DD \nabla f(\xx^*)\big)\,.
\end{equation}
\end{lemma}
\begin{proof}By the first order optimality conditions, the solutions to \eqref{eq:opt} are characterized by the subdifferential inclusion ${-\nabla f(\xx^*) \in \partial h(\xx^*)}$.
  We can then write the following sequence of equivalences:
\begin{align}
    - \nabla f(\xx^*) \in \partial h(\xx^*) &\iff - \DD \nabla f(\xx^*) \in \DD \partial h(\xx^*)\nonumber\\
    &\qquad \text{ (multiplying by $\DD$, equivalence since diagonals are nonzero)}\nonumber\\
    &\iff - \DD \nabla f(\xx^*) \in \partial \varphi(\xx^*)\nonumber\\
    &\qquad \text{ (by definition of $\varphi$)}\nonumber\\
    &\iff \mfrac{1}{\gamma}(\xx^* - \gamma \DD \nabla f(\xx^*) - \xx^*)  \in \partial \varphi(\xx^*) \nonumber\\
    &\qquad \text{ (adding and subtracting $\xx^*$)}\nonumber\\
    &\iff \xx^* = \prox_{\gamma \varphi}(\xx^* - \gamma \DD \nabla f(\xx^*))\,.\\
    &\qquad \text{ (by Lemma \ref{lemma:charac_prox})} \nonumber
\end{align}
  Since all steps are equivalences, we have the desired result.
\end{proof}

\hfill

The following lemma will be key in the proof of convergence for both the sequential and the parallel versions of the algorithm.
With this result, we will be able to bound the product between the gradient mapping and the iterate suboptimality by:
\begin{itemize}
\item First, the negative norm of the gradient mapping, which will be key in the parallel setting to cancel out the terms arising from the asynchrony.
\item  Second, variance terms in $\|\vv_i - \DD_i \nabla f(\xx^*)\|^2$ that we will be able to bound by the Bregman divergence using Lemma~\ref{lemma:l_smooth_ineq}.
\item Third and last, a product with terms in $\langle \vv_i - \DD_i\nabla f(\xx^*) , \xx - \xx^* \rangle$, which taken in expectation gives $\langle \nabla f(\xx) - \nabla f(\xx^*) , \xx - \xx^* \rangle$ and will allow us to apply Lemma~\ref{lemma:neseterov_inequality} to obtain the contraction terms needed to obtain a geometric rate of convergence.
\end{itemize}

\hfill


\begin{lemma}[Gradient mapping inequality]\label{lemma:gradient_mapping_1}
  Let $\xx$ be an arbitrary vector, $\xx^*$ a solution to \eqref{eq:opt},
  $\vv_i$ as defined in \eqref{eq:SPS} and $\boldsymbol g = \boldsymbol g(\xx, \vv_i, i)$ the gradient mapping defined in~\eqref{eq:def_grad_mapping}.  Then the following inequality is verified for any $\beta > 0$:
  \begin{equation}
  \langle \boldsymbol g, \xx - \xx^*\rangle\geq -\frac{\gamma}{2}(\beta - 2)\| \boldsymbol g\|^2 -  \frac{\gamma}{2\beta}\| \vv_i -  \DD_i \nabla f(\xx^*) \|^2 + \langle  \vv_i - \DD_i \nabla f(\xx^*) , \xx - \xx^*\rangle \,.
  \end{equation}
\end{lemma}
\begin{proof}
%   {\blue replace $\zz=\xx^+, \xx^* = \xx^*?$}
%   Let $\zz, \xx^*$ be defined as
% \begin{equation}
%   \zz = \prox_{\gamma \varphi_i}(\xx - \gamma \vv_i)~, \quad \zz^* = \prox_{\gamma \varphi}(\xx^* - \gamma \DD \nabla f(\xx^*))\,.
% \end{equation}
%   We note that with this notation the gradient mapping $\boldsymbol g$ defined in the statement of the lemma can be written as $\boldsymbol g = \frac{1}{\gamma}(\xx - \zz)$. Note also that Lemma~\ref{lemma:fixed_point} implies that $\xx^* = \zz^*$.

By firm non-expansiveness of the proximal operator (Lemma~\ref{lemmma:block_nonexpansive}) applied to $\zz = \prox_{\gamma \varphi_i}(\xx - \gamma \vv_i)$ and $\tilde{\zz} = \prox_{\gamma \varphi_i}(\xx^* - \gamma \DD  \nabla f(\xx^*))$ we have:
\begin{equation}
  \|\zz - \tilde\zz\|_{(i)}^2 - \langle \zz - \tilde\zz, \xx - \gamma \vv_i - \xx^* + \gamma \DD \nabla f(\xx^*) \rangle_{(i)} \leq 0~.
\end{equation}
By the \eqref{eq:SPS} iteration we have $\xx^+ = \zz$ and by Lemma~\ref{lemma:charac_prox} we have that $\llbracket\zz\rrbracket_{T_i} = \llbracket\xx^*\rrbracket_{T_i}$, hence the above can be rewritten as
  \begin{equation}\label{eq:nonexpansive}
    \|\xx^+ - \xx^*\|_{(i)}^2 - \langle \xx^+ - \xx^*, \xx - \gamma \vv_i - \xx^* + \gamma \DD \nabla f(\xx^*) \rangle_{(i)} \leq 0~.
  \end{equation}
  We can now write the following sequence of inequalities
\begin{align}
    &\langle \gamma \boldsymbol g, \xx - \xx^*\rangle = \langle \xx - \xx^+, \xx - \xx^* \rangle_{(i)} \qquad \text{ (by definition and sparsity of $g$)}\nonumber\\
    &\qquad=  \langle \xx - \xx^+ + \xx^* - \xx^*, \xx - \xx^* \rangle_{(i)} \nonumber\\
    &\qquad= \|\xx - \xx^*\|_{(i)}^2 -  \langle \xx^+ - \xx^*, \xx - \xx^* \rangle_{(i)} \nonumber\\
    &\qquad\geq \|\xx - \xx^*\|_{(i)}^2 - \langle \xx^+ - \xx^*, 2 \xx - \gamma \vv_i- 2 \xx^* + \gamma \DD \nabla f(\xx^*) \rangle_{(i)} + \|\xx^+ - \xx^*\|^2_{(i)} \\
    &\qquad\qquad \text{ (adding Eq.~\eqref{eq:nonexpansive})} \nonumber\\
    &\qquad= \|\xx - \xx^+\|_{(i)}^2 + \langle \xx^+ - \xx^*, \gamma \vv_i -  \gamma \DD \nabla f(\xx^*) \rangle_{(i)} \qquad \text{ (completing the square)} \nonumber\\
    &\qquad= \|\xx - \xx^+\|_{(i)}^2 + \langle \xx - \xx^*, \gamma \vv_i -  \gamma \DD \nabla f(\xx^*) \rangle_{(i)}- \langle \xx - \xx^+, \gamma \vv_i-  \gamma \DD \nabla f(\xx^*) \rangle_{(i)}  \nonumber\\
    &\qquad\qquad \text{ (adding and substracting $\xx$)} \nonumber\\
    &\qquad\geq \Big(1 - \frac{\beta}{2}\Big)\|\xx - \xx^+\|_{(i)}^2 -  \frac{\gamma^2}{2\beta}\| \vv_i -  \DD \nabla f(\xx^*) \|_{(i)}^2 + \gamma\langle  \vv_i -  \DD \nabla f(\xx^*) , \xx - \xx^*\rangle_{(i)} \nonumber\\
    &\qquad\qquad \text{(Young's inequality ${2 \langle a, b \rangle \leq \frac{\|a\|^2}{\beta} + \beta \|b\|^2}$, valid for arbitrary $\beta > 0$)} \nonumber\\
    &\qquad\geq \Big(1 - \frac{\beta}{2}\Big)\|\xx - \xx^+\|_{(i)}^2 -  \frac{\gamma^2}{2\beta}\| \vv_i -  \DD_i \nabla f(\xx^*) \|^2 + \gamma\langle  \vv_i -  \DD_i \nabla f(\xx^*) , \xx - \xx^*\rangle \nonumber\\
    &\qquad\qquad \text{ (by definition of $\DD_i$ and using the fact that $\vv_i$ is $T_i$-sparse)}\nonumber\\
    &\qquad= \Big(1 - \frac{\beta}{2}\Big)\|\gamma \boldsymbol g\|^2 -  \frac{\gamma^2}{2\beta}\| \vv_i -  \DD_i \nabla f(\xx^*) \|^2 + \gamma\langle  \vv_i -  \DD \nabla f(\xx^*) , \xx - \xx^*\rangle \,,
\end{align}
  where in the last inequality we have used the fact that $\boldsymbol g$ is $T_i$-sparse.
Finally, dividing by $\gamma$ both sides yields the desired result.
\end{proof}

\begin{lemma}[Upper bound on the gradient estimator variance]\label{lma:variance}
For arbitrary vectors $\xx$, $({\boldsymbol\alpha}_i)_{i=0}^n$, and $\vv_i$ as defined in \eqref{eq:SPS} we have:
\begin{equation}\label{eq:variance_terms}
    \Econd \|\vv_i - \DD_i \nabla f(\xx^*)\|^2  \leq 4 L B_f(\xx, \xx^*) + 2\Econd\|{\boldsymbol\alpha}_i - \nabla f_i(\xx^*) \|^2 \, .
\end{equation}

\end{lemma}
\begin{proof}
We will now bound the variance terms. For this we have:
\begin{align}
&\Econd\| \vv_i -  \DD \nabla f(\xx^*) \|_{(i)}^2 = \Econd\|\nabla f_i(\xx) - \nabla f_i(\xx^*)  + \nabla f_i(\xx^*) - {\boldsymbol\alpha}_i + \DD_i \overline{{\boldsymbol\alpha}} - \DD \nabla f(\xx^*)\|_{(i)}^2 \nonumber\\
&\qquad\leq 2 \Econd \|\nabla f_i(\xx) - \nabla f_i(\xx^*)\|^2 + 2 \Econd \|  \nabla f_i(\xx^*) - {\boldsymbol\alpha}_i - (\DD\nabla f(\xx^*) - \DD \overline{{\boldsymbol\alpha}} ) \|_{(i)}^2 \nonumber\\
&\qquad\qquad \text{ (by inequality $\|a + b\|^2 \leq 2 \|a\|^2 + 2 \|b\|^2$)}\nonumber\\
&\qquad= 2 \Econd \|\nabla f_i(\xx) - \nabla f_i(\xx^*)\|^2+ 2\Econd\|\nabla f_i(\xx^*) - {\boldsymbol\alpha}_i \|^2 \nonumber\\
&\qquad\qquad- 4 \Econd \langle \nabla f_i(\xx^*) - {\boldsymbol\alpha}_i, \DD \nabla f(\xx^*) - \DD \overline{{\boldsymbol\alpha}}  \rangle_{(i)} + 2\Econd \|\DD \nabla f(\xx^*) - \DD \overline{{\boldsymbol\alpha}} \|_{(i)}^2\,.\\
&\qquad\qquad \text{ (developing the square)}\nonumber
\end{align}
We will now simplify the last two terms in the above expression. For the first of the two last terms we have:
\begin{align}
  &- 4\Econd \langle \nabla f_i(\xx^*) - {\boldsymbol\alpha}_i, \DD \nabla f(\xx^*) - \DD \overline{{\boldsymbol\alpha}}  \rangle_{(i)} =- 4 \Econd \langle \nabla f_i(\xx^*) - {\boldsymbol\alpha}_i, \DD \nabla f(\xx^*) - \DD \overline{{\boldsymbol\alpha}}  \rangle\\
  &\qquad \quad \text{ (support of first term)} \nonumber\\
  &\qquad= - 4 \langle \nabla f(\xx^*) - \overline{{\boldsymbol\alpha}}, \DD\nabla f(\xx^*) - \DD\overline{{\boldsymbol\alpha}}  \rangle \nonumber\\
  &\qquad= - 4 \|\nabla f(\xx^*) - \overline{{\boldsymbol\alpha}} \|_{\DD}^2 \,.
\end{align}
Similarly, for the last term we have:
\begin{align}
  2 \Econd \|\DD\nabla f(\xx^*) - \DD \overline{{\boldsymbol\alpha}} \|_{(i)}^2  &= 2\Econd \langle \DD_i \nabla f(\xx^*) - \DD_i \overline{{\boldsymbol\alpha}}, \DD \nabla f(\xx^*) - \DD \overline{{\boldsymbol\alpha}} \rangle \nonumber\\
  &= 2 \langle \nabla f(\xx^*) - \overline{{\boldsymbol\alpha}}, \DD \nabla f(\xx^*) - \DD \overline{{\boldsymbol\alpha}} \rangle \nonumber\\
  &\qquad \text{ (using Lemma~\ref{lemma:unbiasedness})}\nonumber\\
  &= 2\|\nabla f(\xx^*) - \overline{{\boldsymbol\alpha}} \|_{\DD}^2 \,.
\end{align}
and so the addition of these terms is negative and can be dropped. In all, for the variance terms we have
\begin{align}
    \Econd \|\vv_i - \DD \nabla f(\xx^*)\|_{(i)}^2 &\leq 2 \Econd \|\nabla f_i(\xx) - \nabla f_i(\xx^*)\|^2 + 2\Econd\|{\boldsymbol\alpha}_i - \nabla f_i(\xx^*) \|^2 \nonumber\\
    &\leq 4 L B_f(\xx, \xx^*) + 2\Econd\|{\boldsymbol\alpha}_i - \nabla f_i(\xx^*) \|^2 \, . \qquad \text{ (by Lemma~\ref{lemma:l_smooth_ineq}) }
\end{align}
\end{proof}

We now define an upper bound on the quantity that we would like to bound, often called a Lyapunov function, and establish a recursive inequality on this Lyapunov function.

\begin{lemma}[Lyapunov inequality]\label{lemma:lyapunov_inequality}
  Let $\mathcal{L}$ be the following $c$-parametrized function:
\begin{equation}\label{eq:lyapunov}
  \mathcal{L}(\xx,{\boldsymbol\alpha}) := \|\xx - \xx^*\|^2 + \frac{c}{n} \sum_{i=1}^n \|{\boldsymbol\alpha}_i - \nabla f_i(\xx^*)\|^2 \, .
\end{equation}
  Let $\xx^+$ and ${\boldsymbol\alpha}^+$ be obtained from the Sparse Proximal \SAGA\ updates~\eqref{eq:SPS}.
  Then we have:
\begin{align}
  \Econd \mathcal{L}(\xx^+,{\boldsymbol\alpha}^+) - \mathcal{L}(\xx,{\boldsymbol\alpha}) \leq  &- \gamma \mu \|\xx - \xx^*\|^2 +  \left(4 L \gamma^2 - 2 \gamma + 2L\frac{c}{n}\right) B_f(\xx, \xx^*)
  \nonumber \\
  &+ \left(2 \gamma^2 - \frac{c}{n}\right) \Econd \|{\boldsymbol\alpha}_i - \nabla f_i(\xx)\|^2\,.
\end{align}
\end{lemma}
\begin{proof}
  For the first term of $\mathcal{L}$ we have:
  \begin{align}
    \|\xx^+ - \xx^*\|^2 &= \|\xx - \gamma \boldsymbol g - \xx^*\|^2 \qquad \text{ ($\boldsymbol g := \boldsymbol g(\xx, \vv_i, i)$)}   \nonumber\\
    &= \|\xx - \xx^*\|^2 - 2\gamma \langle \boldsymbol g, \xx - \xx^* \rangle + \|\gamma \boldsymbol g\|^2 \nonumber\\
    &\leq \|\xx - \xx^*\|^2 +  \gamma^2\| \vv_i -  \DD_i \nabla f(\xx^*) \|^2 - 2 \gamma\langle  \vv_i - \DD_i \nabla f(\xx^*) , \xx - \xx^*\rangle \nonumber\\
    &\qquad \text{ (by Lemma~\ref{lemma:gradient_mapping_1} with $\beta=1$)}\nonumber\\
    % &= \|\xx - \xx^*\|^2 +  \gamma^2\| \vv_i -  \DD_i \nabla f(\xx^*) \|^2 - 2 \gamma\langle  \vv_i - \DD_i \nabla f(\xx^*) , \xx - \xx^*\rangle \,.\\
    % &\qquad \text{ (by construction $T_i$
    % contains the support of $\vv_i$)} \nonumber
  \end{align}
Since $\vv_i$ is an unbiased estimator of the gradient and  $\Econd \DD_i =  \boldsymbol I_p$, taking expectations we have:
\begin{equation}\label{eq:proof_lyapunov_1}
  \begin{aligned}
    \Econd \|\xx^+ - \xx^*\|^2 &\leq \|\xx - \xx^*\|^2 +  \gamma^2\Econd\| \vv_i -  \DD_i \nabla f(\xx^*) \|^2 - 2 \gamma\langle  \nabla f(\xx) - \nabla f(\xx^*) , \xx - \xx^*\rangle\\
    &\leq (1 - \gamma \mu)\|\xx - \xx^*\|^2 +  \gamma^2\Econd\| \vv_i -  \DD_i \nabla f(\xx^*) \|^2 - 2 \gamma B_f(\xx, \xx^*)\,.\\
    &\qquad \text{ (by Lemma~\ref{lemma:neseterov_inequality})}
  \end{aligned}
\end{equation}

By using the variance terms bound (Lemma~\ref{lma:variance}) in the previous equation we have:
\begin{equation}\label{eq:lemma_lyapunov_3}
  \begin{aligned}
    \Econd \|\xx^+ - \xx^*\|^2
    &\leq (1 - \gamma \mu)\|\xx - \xx^*\|^2 + (4 L \gamma^2 - 2 \gamma) B_f(\xx, \xx^*)\\
    &\qquad + 2 \gamma^2 \Econd \|{\boldsymbol\alpha}_i - \nabla f_i(\xx^*)\|^2\,.\\
  \end{aligned}
\end{equation}

\hfill

We will now bound the second term of the Lyapunov function. We have:
  \begin{align}\label{eq:lemma_lyapunov_4}
    \frac{1}{n}\sum_{i=1}^n \|{\boldsymbol\alpha}_i^+ - \nabla f_i(\xx^*)\|^2 &= \left( 1 - \frac{1}{n}\right) \Econd \|{\boldsymbol\alpha}_i - \nabla f_i(\xx^*)\|^2 + \frac{1}{n}\Econd\|\nabla f_i(\xx) - \nabla f_i(\xx^*)\|^2  \nonumber\\
    &\qquad \text{ (by definition of $\balpha^+$)}\\
    &\leq \left( 1 - \frac{1}{n}\right) \Econd \|{\boldsymbol\alpha}_i - \nabla f_i(\xx^*)\|^2 + \frac{2}{n}L B_f(\xx, \xx^*)\,.\quad \text{ (by Lemma~\ref{lemma:l_smooth_ineq})}
  \end{align}
Combining Eq.~\eqref{eq:lemma_lyapunov_3} and \eqref{eq:lemma_lyapunov_4} we have:
\begin{align}
\Econd \mathcal{L}(\xx^+, {\boldsymbol\alpha}^+) &\leq (1 - \gamma \mu)\|\xx - \xx^*\|^2 +  (4 L \gamma^2 - 2 \gamma) B_f(\xx, \xx^*) + 2 \gamma^2 \Econd \|{\boldsymbol\alpha}_i - \nabla f_i(\xx^*)\|^2 \nonumber\\
 &\qquad + c\left[\left( 1 - \frac{1}{n}\right) \Econd \|{\boldsymbol\alpha}_i - \nabla f_i(\xx^*)\|^2 + \frac{1}{n}2 L B_f(\xx, \xx^*)\right] \nonumber\\
 &=  (1 - \gamma \mu)\|\xx - \xx^*\|^2 +  \left(4 L \gamma^2 - 2 \gamma + 2L\frac{c}{n}\right) B_f(\xx, \xx^*) \nonumber\\
  & \qquad + \left(2 \gamma^2 - \frac{c}{n}\right) \Econd \|{\boldsymbol\alpha}_i - \nabla f_i(\xx^*)\|^2 + c \Econd \|{\boldsymbol\alpha}_i - \nabla f_i(\xx^*)\|^2 \nonumber\\
  &= \mathcal{L}(\xx,{\boldsymbol\alpha})  - \gamma \mu \|\xx - \xx^*\|^2 + \left(4 L \gamma^2 - 2 \gamma + 2L\frac{c}{n}\right) B_f(\xx, \xx^*) \nonumber \\
  & \qquad+ \left(2 \gamma^2 - \frac{c}{n}\right) \Econd \|{\boldsymbol\alpha}_i - \nabla f_i(\xx^*)\|^2\,.
\end{align}
Finally, subtracting $\mathcal{L}(\xx,{\boldsymbol\alpha})$ from both sides yields the desired result.
\end{proof}

\begin{customtheorem}{1}
Let $\gamma = \frac{a}{5L}$ for any $a\leq 1$ and $f$ be $\mu$-strongly convex. Then Sparse Proximal \SAGA\ converges geometrically in expectation with a rate factor of at least $\rho = \frac{1}{5} \min\{\frac{1}{n}, a\frac{1}{\kappa}\}$. That is, for $\xx_t$ obtained after $t$ updates and $\xx^*$ the solution to \eqref{eq:opt}, we have the bound:
  $$
  \EE \|\xx_t - \xx^*\|^2 \leq (1-\rho)^t C_0\,,\quad \text{ with } C_0 :=  \|\xx_0 - \xx^*\|^2 + \textstyle\frac{1}{5 L^2} \sum_{i=1}^n\|{\boldsymbol\alpha}_i^0 - \nabla f_i(\xx^*)\|^2 \quad.
  $$
\end{customtheorem}
\begin{proof}
  Let $\overline{H} := \frac{1}{n}\sum_i \|{\boldsymbol\alpha}_i - \nabla f_i(\xx^*)\|^2$.
  By the Lyapunov inequality from Lemma~\ref{lemma:lyapunov_inequality}, we have:
\begin{align}
  &\Econd\mathcal{L}_{t+1} - (1 - \rho)\mathcal{L}_t \leq \rho \mathcal{L}_t - \gamma \mu\|\xx_t - \xx^*\|^2 +  \left(4 L \gamma^2 - 2 \gamma + 2L\frac{c}{n}\right)B_f(\xx_t, \xx^*) + \left(2\gamma^2 - \frac{c}{n} \right)\overline{H} \nonumber\\
  &\qquad=\left(\rho - \gamma \mu\right)\|\xx_t - \xx^*\|^2 +  \left(4 L \gamma^2 - 2 \gamma + 2L\frac{c}{n}\right)B_f(\xx_t, \xx^*) + \left[2\gamma^2 + c \left(\rho - \frac{1}{n}\right) \right]\overline{H} \nonumber\\
  &\qquad\qquad \text{ (by definition of $\mathcal{L}_t$)} \nonumber\\
  &\qquad\leq\left(\rho - \gamma \mu\right)\|\xx_t - \xx^*\|^2 +  \left(4 L \gamma^2 - 2 \gamma + 2L\frac{c}{n}\right)B_f(\xx_t, \xx^*) + \left(2\gamma^2 - \frac{2 c}{3 n} \right)\overline{H} \nonumber\\
  &\qquad\qquad\text{ (choosing $\rho \leq \frac{1}{3 n}$)}\nonumber\\
  &\qquad=\left(\rho - \gamma \mu\right)\|\xx_t - \xx^*\|^2 +  \left(10 L \gamma^2 - 2 \gamma \right)B_f(\xx_t, \xx^*) \nonumber\\
  &\qquad\qquad\text{ (choosing $\frac{c}{n} = 3 \gamma^2$)} \nonumber\\
  &\qquad\leq\left(\rho - \frac{a \mu}{5L} \right)\|\xx_t - \xx^*\|^2 \qquad\text{ (for all $\gamma = \frac{a}{5L}, a \leq 1$)} \nonumber\\
  &\qquad\leq0 \,.\qquad\text{ (for $\rho \leq \frac{a}{5} \cdot\frac{\mu}{L}$)}
\end{align}
And so we have the bound:
\begin{equation}
\Econd\mathcal{L}_{t+1} \leq \left(1 - \min\Big\{\frac{1}{3n}, \frac{a}{5}\cdot \frac{1}{\kappa}\Big\}\right)\mathcal{L}_t \leq \left(1 - \frac{1}{5}\min\Big\{\frac{1}{n}, a\cdot \frac{1}{\kappa}\Big\}\right)\mathcal{L}_t \,,
\end{equation}
where in the last inequality we have used the trivial bound $\frac{1}{3n} \leq \frac{1}{5n}$ merely for clarity of exposition.
Chaining expectations from $t$ to $0$ we have:
\begin{align}
  \EE \mathcal{L}_{t+1} &\leq \left(1 - \frac{1}{5}\min\Big\{\frac{1}{n}, a\cdot \frac{1}{\kappa}\Big\}\right)^{t+1}\mathcal{L}_0 \nonumber\\
  &= \left(1 - \frac{1}{5}\min\Big\{\frac{1}{n}, a\cdot \frac{1}{\kappa}\Big\}\right)^{t+1} \left(\|\xx_0 - \xx^*\|^2 + \frac{3 a^2}{5^2 L^2} \sum_{i=1}^n\|{\boldsymbol\alpha}_i^0 - \nabla f_i(\xx^*)\|^2 \right) \nonumber\\
  &\leq \left(1 - \frac{1}{5}\min\Big\{\frac{1}{n}, a\cdot \frac{1}{\kappa}\Big\}\right)^{t+1} \left(\|\xx_0 - \xx^*\|^2 + \frac{1}{5 L^2} \sum_{i=1}^n\|{\boldsymbol\alpha}_i^0 - \nabla f_i(\xx^*)\|^2 \right) \\
  &\qquad \text{ (since $a \leq 1$ and $\nicefrac{3}{5} \leq 1$)} \,.\nonumber
\end{align}
The fact that $\mathcal{L}_t$ is a majorizer of $\|\xx_{t} - \xx^*\|^2$ completes the proof.
\end{proof}

\clearpage


\section{ProxASAGA}\label{apx:async}

\vspace{0.5em}

In this Appendix we provide the proofs for results from Section~\ref{sec:pasaga}, that is Theorem~\ref{thm:convergence}  (the convergence theorem for \PASAGA) and Corollary~\ref{thm:corollary} (its speedup result).
\paragraph{Notation.} Through this section, we use the following shorthand for the gradient mapping: $\boldsymbol g_t := \boldsymbol g(\hat{\xx}_t, \hat{\vv}^t_{i_t}, i_t)$.

\subsection{Proof outline.}\label{ssec:proof}

As in the smooth case ($h=0$), we start by using the definition of $\xx_{t+1}$  in Eq.~\eqref{eq:definition} to relate the distance to the optimum in terms of its previous iterates:
\begin{equation}
\begin{aligned}
\|\xx_{t+1} - \xx^*\|^2 = &\|\xx_{t} - \xx^*\|^2  + 2 \gamma \langle \hat \xx_t - \xx_t, \boldsymbol g_t \rangle +\gamma^2 \|\boldsymbol g_t\|^2 - 2 \gamma \langle \hat \xx_t - \xx^*, \boldsymbol g_t\rangle \,  .
\end{aligned}
\end{equation}
However, in this case $\boldsymbol g_t$ is not a gradient estimator but a gradient mapping, so we cannot continue as is customary -- by using the unbiasedness of the gradient in the $\langle\hat \xx_t - \xx^*, \boldsymbol g_t\rangle$ term together with the strong convexity of~$f$ (see~\citet[Section 3.5]{leblond2016Asaga}).

To circumvent this difficulty, we derive a tailored inequality for the gradient mapping (Lemma~\ref{lemma:gradient_mapping_1} in~\ref{apx:sparse_saga}), which in turn allows us to use the classical unbiasedness and strong convexity arguments to get the following inequality:
\begin{align}
a_{t+1}
\leq (1 - \frac{\gamma \mu}{2}) a_t + \gamma^2 &\EE \|\boldsymbol g_t\|^2 - 2 \gamma \EE B_f(\hat{\xx}_t, \xx^*) +
\underbrace{ \gamma \mu \EE \|\hat{\xx}_t - \xx\|^2 +2 \gamma \EE \langle \boldsymbol g_t,  \hat{\xx}_t - \xx_t \rangle}_{\text{additional asynchrony terms}} \\
&\underbrace{+ \gamma^2(\beta - 2) \EE \|\boldsymbol g_t\|^2 + \frac{\gamma^2}{\beta} \EE \|\hat \vv^t_{i_t} - \DD_{i_t} \nabla f(\xx^*)\|^2}_{\text{additional proximal and variance terms}} \,, \nonumber
\end{align}
where $a_t := \EE \|\xx_t - \xx^*\|^2$.
Note that since $f$ is strongly convex, $B_f(\hat{\xx}_t, \xx^*) \geq \frac{\mu}{2}\|\hat \xx_t - \xx^*\|^2 $.

In the smooth setting, one first expresses the additional asynchrony terms as linear combinations of past gradient variance terms $(\EE \|\boldsymbol g_u\|^2)_{0 \leq u \leq t}$.
Then one crucially uses the negative Bregman divergence term to control the variance terms. However, in our current setting, we cannot relate the norm of the gradient mapping $\EE \|\boldsymbol g_t\|^2$ to the Bregman divergence (from which $h$ is absent).
Instead, we use the negative term $\gamma^2(\beta - 1)\EE\|\boldsymbol g_t\|^2$ to control all the $(\EE \|\boldsymbol g_u\|^2)_{0 \leq u \leq t}$ terms that arise from asynchrony.
%

The rest of the proof consists in:

$i)$ expressing the additional asynchrony terms as linear combinations of $(\EE \|\boldsymbol g_u\|^2)_{0 \leq u \leq t}$, following~\citet[Lemma 1]{leblond2016Asaga};


$ii)$ expressing the last variance term, $\|\hat \vv^t_{i_t} - D_{i_t} \nabla f(\xx^*)\|^2$, as a linear combination of past Bregman divergences (Lemma~\ref{lma:variance} in~\ref{apx:sparse_saga} and Lemma~2 from \citet{leblond2016Asaga});

$iii)$ defining a Lyapunov function, $\mathcal{L}_t := {\sum_{u=0}^t (1 - \rho)^{t-u} a_u}$, and proving that it is bounded by a contraction given conditions on the maximum step size and delay.


\vspace{0.5em}

\subsection{Detailed proof}

\begin{customtheorem}{2}[Convergence guarantee and rate of \PASAGA]
	Suppose $\tau \leq \frac{1}{10\sqrt{\Delta}}$.
	For any step size $\gamma = \frac{a}{L}$ with $a \leq \frac{1}{36} \min\{1, \frac{6 \kappa}{\tau}\}$, the inconsistent read iterates of Algorithm~\ref{alg:theoretical} converge in expectation at a geometric rate factor of at least: $\rho(a) = \frac{1}{5} \min \big\{\frac{1}{n},  a \frac{1}{\kappa}\big\},$
	i.e. $\EE \|\hat \xx_t-\xx^*\|^2 \leq (1-\rho)^t \,  \tilde C_0$, where $\tilde C_0$ is a constant independent of $t$ ($\approx \frac{n \kappa}{a}C_0$ with $C_0$ as defined in Theorem~\ref{th1}).
\end{customtheorem}
\begin{proof}
  In order to get an \textbf{initial recursive inequality}, we first unroll the (virtual) update:
  \begin{align}
    \|\xx_{t+1} - \xx^*\|^2 &= \|\xx_t - \gamma \boldsymbol g_t - \xx^*\|^2 = \|\xx_t - \xx^*\|^2 + \|\gamma \boldsymbol g_t\|^2 - 2 \gamma \langle \boldsymbol g_t, \xx_t - \xx^*\rangle \nonumber \\
    &= \|\xx_t - \xx^*\|^2 + \|\gamma \boldsymbol g_t\|^2 - 2 \gamma \langle \boldsymbol g_t, \hat{\xx}_t - \xx^*\rangle + 2 \gamma \langle \boldsymbol g_t, \hat{\xx}_t - \xx_t\rangle \, ,
  \end{align}
  and then apply Lemma~\ref{lemma:gradient_mapping_1} with $\xx = \hat \xx_t$ and $\vv = \hat{\vv}_{i_t}^t$.
  Note that in this case we have $\boldsymbol g = \boldsymbol g_t$ and $\langle \cdot\rangle_{(i)} = \langle \cdot \rangle_{(i_t)}$.

  \begin{align}\label{eq:initrec}
	\|\xx_{t+1} - \xx^*\|^2
	&\leq \|\xx_t - \xx^*\|^2 + 2 \gamma \langle \boldsymbol g_t,  \hat{\xx}_t - \xx_t \rangle + \gamma^2\|\boldsymbol g_t\|^2 + \gamma^2(\beta - 2)\|\boldsymbol g_t\|^2\nonumber\\
	&\qquad  + \frac{\gamma^2}{\beta} \|\hat{\vv}_{i_t}^t - \DD \nabla f(\xx^*)\|_{({i_t})}^2
	- 2 \gamma \langle \hat{\vv}_{i_t}^t - \DD \nabla f(\xx^*), \hat{\xx}_t - \xx^* \rangle_{({i_t})} \nonumber\\
	&=\|\xx_t - \xx^*\|^2 + 2 \gamma \langle \boldsymbol g_t,  \hat{\xx}_t - \xx_t \rangle + \gamma^2(\beta - 1)\|\boldsymbol g_t\|^2\nonumber\\
	&\qquad  + \frac{\gamma^2}{\beta} \|\hat{\vv}_{i_t}^t - \DD_{i_t} \nabla f(\xx^*)\|^2
	- 2 \gamma \langle \hat{\vv}_{i_t}^t - \DD_{i_t} \nabla f(\xx^*), \hat{\xx}_t - \xx^* \rangle  . \\
	&
	\qquad \qquad \text{(as  ${[\hat{\vv}_{i_t}^t]_{T_{i_t}}} = \hat{\vv}_{i_t}^t$)} \nonumber
  \end{align}

  We now use the property that $i_t$ is independent of $\hat \xx_t$ (which we enforce by reading $\hat \xx_t$ before picking $i_t$, see Section~\ref{sec:pasaga}), together with the unbiasedness of the gradient update $\hat{\vv}_{i_t}^t$ ($\Econd \hat{\vv}_{i_t}^t = \nabla f(\hat{\xx}_t)$) and the definition of $\DD$ to simplify the following expression as follows:
  \begin{align}
    \Econd \langle \hat{\vv}_{i_t}^t - \DD_{i_t} \nabla f(\xx^*), \hat{\xx}_t - \xx^* \rangle &=  \langle \nabla f(\hat{\xx}_t) - \nabla f(\xx^*), \hat{\xx}_t - \xx^* \rangle \nonumber\\
    &\geq \frac{\mu }{2}\|\hat{\xx}_t - \xx^*\|^2  + B_f(\hat{\xx}_t, \xx^*) \, ,
  \end{align}
  where the last inequality comes from Lemma~\ref{lemma:neseterov_inequality}.
  Taking conditional expectations on~\eqref{eq:initrec} we get:
  \begin{align}
    \Econd\|\xx_{t+1} - \xx^*\|^2
    &\leq \|\xx_t - \xx^*\|^2 + 2 \gamma \Econd \langle \boldsymbol g_t,  \hat{\xx}_t - \xx_t \rangle + \gamma^2(\beta - 1) \Econd \|\boldsymbol g_t\|^2 \\
    &\qquad+ \frac{\gamma^2}{\beta} \Econd \|\hat{\vv}_{i_t}^t - \DD_{i_t} \nabla f(\xx^*)\|^2 - \gamma \mu\|\hat{\xx}_t - \xx^*\|^2 - 2 \gamma B_f(\hat{\xx}_t, \xx^*) \notag \\
    &\leq (1 - \frac{\gamma \mu}{2})\|\xx_t - \xx^*\|^2 + 2 \gamma \Econd \langle \boldsymbol g_t,  \hat{\xx}_t - \xx_t \rangle + \gamma^2(\beta - 1) \Econd\|\boldsymbol g_t\|^2\notag\\
    &\qquad + \frac{\gamma^2}{\beta} \Econd \|\hat{\vv}_{i_t}^t - \DD_{i_t} \nabla f(\xx^*)\|^2 \notag  + \gamma \mu\|\hat{\xx}_t - \xx_t\|^2  - 2 \gamma B_f(\hat{\xx}_t, \xx^*) \notag \\
    &\qquad \text{ (using $\|a+b\|^2 \leq 2 \|a\|^2 + 2\|b\|^2$ on $\|\xx_t - \hat{\xx}_t + \hat{\xx}_t - \xx^*\|^2$)} \notag \\
    &\leq (1 - \frac{\gamma \mu}{2})\|\xx_t - \xx^*\|^2 + \gamma^2(\beta - 1) \Econd\|\boldsymbol g_t\|^2 + \gamma \mu\|\hat{\xx}_t - \xx_t\|^2  + 2 \gamma \Econd \langle \boldsymbol g_t,  \hat{\xx}_t - \xx_t \rangle \notag \\
    &\qquad  - 2 \gamma B_f(\hat{\xx}_t, \xx^*) + \frac{4\gamma^2 L}{\beta}  B_f(\hat{\xx}_t, \xx^*) + \frac{2\gamma^2}{\beta} \Econd \|\hat{{\boldsymbol\alpha}}^t_{i_t} - \nabla f_{i_t}(\xx^*)\|^2 \,.\label{eq:BeginMaster} \\
    &\qquad \text{ (using Lemma~\ref{lma:variance} on the variance terms)} \notag
  \end{align}

  Since we also have:
  \begin{equation}
  \hat \xx_t - \xx_t = \gamma \sum_{u=(t - \tau)_+}^{t-1}\boldsymbol G_{u}^t \boldsymbol g(\hat \xx_{u}, \hat {\boldsymbol\alpha}^u, i_{u}),
  \end{equation}
  the effect of asynchrony for the perturbed iterate updates was already derived in a very similar setup in~\citet{leblond2016Asaga}.
  We re-use the following bounds from their Appendix C.4:\footnote{The appearance of the sparsity constant $\Delta$ is coming from the crucial property that $\Econd \|\xx\|_{(i)}^2 \leq \Delta \|\xx\|^2$ $\forall x \in \RR^p$ (see Eq.~(39) in~\citet{leblond2016Asaga}, where they use the notation $\|\cdot\|_i$ for our $\|\cdot\|_{(i)}$).}
  \begin{align}
    &\EE \|\hat{\xx}_t - \xx_t\|^2 \leq \gamma^2(1 + \sqrt{\Delta}\tau) \sum_{u=(t-\tau)_+}^{t-1} \EE \|\boldsymbol g_u\|^2 \,, & &\text{\citet[Eq.~(48)]{leblond2016Asaga}} \label{eq:XhatMinusXt}\\
    &\EE\langle \boldsymbol g_t, \hat{\xx}_t - \xx_t\rangle \leq \frac{\gamma \sqrt{\Delta}}{2} \sum_{u=(t-\tau)_+}^{t-1} \EE \|\boldsymbol g_u\|^2 + \frac{\gamma \sqrt{\Delta}\tau}{2}\EE \|\boldsymbol g_t\|^2 \, . & &\text{\citet[Eq.~(46)]{leblond2016Asaga}}   \label{eq:gtVsXhat}.
  \end{align}
  Because the updates on ${\boldsymbol\alpha}$ are the same for $\PASAGA$ as for $\ASAGA$, we can
  re-use the same argument arising in the proof of~\citet[Lemma 2]{leblond2016Asaga} to get the following bound on $\EE \|\hat{{\boldsymbol\alpha}}^t_{i_t} - \nabla f_{i_t}(\xx^*)\|^2$:
  \begin{equation} \label{eq:alphaitBound}
      \EE \|\hat{{\boldsymbol\alpha}}^t_{i_t} - \nabla f_{i_t}(\xx^*)\|^2 \leq \frac{2L}{n}\underbrace{\sum_{u=1}^{t - 1}(1 - \frac{1}{n})^{(t - 2 \tau - u  - 1)_+} \EE B_f(\hat{\xx}_u, \xx^*)}_{\text{\small Henceforth denoted $H_t$}} + 2 L (1 - \frac{1}{n})^{(t - \tau)_+}\tilde{e}_0 \,,
  \end{equation}
  where $\tilde e_0 := \frac{1}{2L} \EE\|{\boldsymbol\alpha}_i^0 - f'_i(\xx^*)\|^2$.
  %
  This bound is obtained by analyzing which gradient could be the source of ${\boldsymbol\alpha}_{i_t}$ in the past (taking in consideration the inconsistent writes), and then applying Lemma~\ref{lemma:l_smooth_ineq} on the $\Econd \|\nabla f(\hat{\xx}_u) - \nabla f(\xx^*)\|^2$ terms, explaining the presence of $B_f(\hat{\xx}_u,\xx^*)$ terms.\footnote{Note that~\citet{leblond2016Asaga} analyzed the unconstrained scenario, and so
  $B_f(\hat{\xx}_u,\xx^*)$ is replaced by the simpler $f(\hat{x}_u) - f(\xx^*)$ in their bound.}
  The inequality~\eqref{eq:alphaitBound} corresponds to Eq.~(56) and~(57) in~\citet{leblond2016Asaga}.

  By taking the full expectation of~\eqref{eq:BeginMaster} and plugging the above inequalities back, we obtain an inequality similar to~\citet[Master inequality (28)]{leblond2016Asaga} which describes how the error terms $a_t := \EE \|\xx_t - \xx^*\|^2$ of the virtual iterates are related:
  \begin{equation}\label{eq:masterineq}
  \begin{aligned}
    a_{t+1} \leq
    &(1 - \frac{\gamma \mu}{2})a_t
       + \frac{4 \gamma^2 L}{\beta} (1 - \frac{1}{n})^{(t - \tau)_+}\tilde{e}_0 \\
    &+ \gamma^2\left[\beta - 1 + \sqrt{\Delta}\tau \right] \EE \|\boldsymbol g_t\|^2
       + \left[\gamma^2 \sqrt{\Delta} + \gamma^3 \mu (1 + \sqrt{\Delta}\tau) \right]\sum_{u=(t-\tau)_+}^t \EE \|\boldsymbol g_u\|^2\\
    &- 2 \gamma \EE B_f(\hat{\xx}_t, \xx^*) + \frac{4\gamma^2 L}{\beta} \EE B_f(\hat{\xx}_t, \xx^*)  + \frac{4 \gamma^2 L}{\beta n} H_t \, .
  \end{aligned}
  \end{equation}
  We now have a promising inequality with a contractive term and several quantities that we need to bound.
  In order to achieve our final result, we introduce the same Lyapunov function as in~\citet{leblond2016Asaga}:
  $$
  \begin{aligned}
    \mathcal{L}_t := \sum_{u=0}^t (1 - \rho)^{t-u} a_u \,,
  \end{aligned}
  $$
  where $\rho$ is a target rate factor for which we will provide a value later on.
  Proving that this Lyapunov function is bounded by a contraction will finish our proof.
  We have:
  \begin{align}
    \mathcal{L}_{t+1} = \sum_{u=0}^{t+1} (1 - \rho)^{t+1-u} a_u
    &= (1 - \rho)^{t+1}a_0
    + \sum_{u=1}^{t+1}(1 - \rho)^{t+1-u}a_u
    \nonumber \\
    &= (1 - \rho)^{t+1}a_0
    + \sum_{u=0}^t(1 - \rho)^{t-u}a_{u+1} \,  .
  \end{align}

  We now plug our new bound on $a_{t+1}$,~\eqref{eq:masterineq}:
  \begin{equation}
  \begin{aligned}
  \mathcal{L}_{t+1}
  \leq (1 - \rho)^{t+1} a_0
  + \sum_{u = 0}^t(1 - \rho)^{t-u} \Big[
  &(1 - \frac{\gamma\mu}{2}) a_u
  + \frac{4 \gamma^2 L}{\beta} (1 - \frac{1}{n})^{(u - \tau)_+}\tilde{e}_0 \\
  &+ \gamma^2 \big(\beta - 1 + \sqrt{\Delta}\tau \big) \EE \|\boldsymbol g_u\|^2 \\
  &+ \big(\gamma^2 \sqrt{\Delta} + \gamma^3 \mu (1 + \sqrt{\Delta}\tau) \big)\sum_{v=(u-\tau)_+}^u \EE \|\boldsymbol g_v\|^2\\
  &- 2 \gamma \EE B_f(\hat{\xx}_u, \xx^*) + \frac{4\gamma^2 L}{\beta} \EE B_f(\hat{\xx}_u, \xx^*)  + \frac{4 \gamma^2 L}{\beta n} H_u \Big] \,.
  \end{aligned}
  \end{equation}
  After regrouping similar terms, we get:
  \begin{equation}\label{eq:recLyap}
  \begin{aligned}
    \mathcal{L}_{t+1}
      \leq (1 - \rho)^{t+1} (a_0 + A \tilde{e}_0)
      + (1 - \frac{\gamma \mu}{2}) \mathcal{L}_t
      + \sum_{u=0}^t s_u^t \EE \|\boldsymbol g_u\|^2
      + \sum_{u=1}^t r_u^t  \EE B_f(\hat{\xx}_u, \xx^*) \, .
  \end{aligned}
  \end{equation}
  Now, provided that we can prove that under certain conditions the $s_u^t$ and $r_u^t$ terms are all negative (and that the $A$ term is not too big), we can drop them from the right-hand side of~\eqref{eq:recLyap} which will allow us to finish the proof.

  Let us compute these terms.
  Let $q := \frac{1-\nicefrac{1}{n}}{1-\rho}$ and we assume in the rest that $\rho < \nicefrac{1}{n}$.

  \textbf{Computing $A$.}
  We have:
  \begin{align}
    \frac{4 \gamma^2 L}{\beta} \sum_{u=0}^t (1 - \rho)^{t-u} (1 - \frac{1}{n})^{(u - \tau)_+}
    &\leq \frac{4 \gamma^2 L}{\beta} (1 - \rho)^t (1 - \rho)^{-\tau} (\tau +1+\frac{1}{1 - q}) \nonumber\\
    &\qquad \qquad \text{from~\citet[Eq (75)]{leblond2016Asaga}} \nonumber\\
    &= (1 - \rho)^{t+1} \underbrace{\frac{4 \gamma^2 L}{\beta} (1 - \rho)^{-\tau -1}(\tau +1+\frac{1}{1 - q})}_{:=A} \,.
  \end{align}

  \textbf{Computing $s_u^t$.}
  Since we have:
  \begin{equation}
    \sum_{u=0}^t (1 - \rho)^{t-u} \sum_{v = (u - \tau)_+}^{u-1}\EE \|\boldsymbol g_u\|^2
    \leq \tau (1 - \rho)^{-\tau} \sum_{u=0}^t (1 - \rho)^{t - u}  \EE\|\boldsymbol g_u\|^2 \,,
  \end{equation}
  we have for all $0 \leq u \leq t$:
  \begin{equation} \label{eq:sut}
  \begin{aligned}
  s_u^t \leq (1 - \rho)^{t - u} \Big[\gamma^2 \big(\beta -1 + \sqrt{\Delta} \tau) + \tau (1 - \rho)^{-\tau} \big(\gamma^2 \sqrt{\Delta} + \gamma^3 \mu (1 + \sqrt{\Delta}\tau) \big)\Big] \, .
  \end{aligned}
  \end{equation}

  \textbf{Computing $r_u^t$.}
  To analyze these quantities, we need to compute: $\sum_{u = 0}^t(1 - \rho)^{t-u} \sum_{v = 1}^{u-1} (1 - \frac{1}{n})^{(u - 2 \tau - v  - 1)_+}$.
  Fortunately, this is already done in~\citet[Eq (66)]{leblond2016Asaga}, and thus we know that for all $1\leq u \leq t$:
  \begin{equation} \label{eq:rut}
  \begin{aligned}
  r_u^t \leq (1 - \rho)^{t - u} \left[-2 \gamma + \frac{4\gamma^2 L}{\beta} + \frac{4L \gamma^2}{n \beta} (1 - \rho)^{-2 \tau -1} \Big( 2\tau+  \frac{1}{1 - q}\Big)\right]\,,
  \end{aligned}
  \end{equation}
  recalling that $q := \frac{1 - \nicefrac{1}{n}}{1 - \rho}$ and that we assumed $\rho < \frac{1}{n}$.

  We now need some assumptions to further analyze these quantities.
  We make simple choices for simplicity, though a tighter analysis is possible.
  To get manageable (and simple) constants, we follow~\citet[Eq.~(82) and (83)]{leblond2016Asaga} and assume:
  \begin{equation} \label{eq:initialRhoTauConditions}
  \begin{aligned}
  \rho \leq \frac{1}{4 n}  ;\quad \tau \leq \frac{n}{10} \,.
  \end{aligned}
  \end{equation}
  This tells us:
  \begin{equation*}
  \begin{aligned}
  \frac{1}{1 - q} &\leq \frac{4 n}{3}
  \\
  (1 - \rho)^{- k \tau - 1} &\leq \frac{4}{3} \qquad \qquad \text{for} \, \, 0 \leq k \leq 2 \,.
  \qquad \text{(using Bernouilli's inequality)}
  \end{aligned}
  \end{equation*}
  Additionally, we set $\beta = \frac{1}{2}$.
  Equation~\eqref{eq:sut} thus becomes:
  \begin{equation}
  s_u^t \leq \gamma^2 (1 - \rho)^{t - u} \left[-\frac{1}{2} + \sqrt{\Delta} \tau + \frac{4}{3} \big( \sqrt{\Delta} \tau + \gamma \mu \tau (1 + \sqrt{\Delta} \tau) \big)\right] \,.
  \end{equation}

  We see that for $s_u^t$ to be negative, we need $\tau = \mathcal{O}(\frac{1}{\sqrt{\Delta}})$.
  Let us assume that $\tau \leq \frac{1}{10 \sqrt{\Delta}}$.
  We then get:
  \begin{equation}
  \begin{aligned}
  s_u^t \leq \gamma^2 (1 - \rho)^{t - u} \left[ - \frac{1}{2} + \frac{1}{10} + \frac{4}{30} + \gamma \mu \tau \frac{4}{3} \frac{11}{10} \right]\,.
  \end{aligned}
  \end{equation}
  Thus, the condition under which all $s_u^t$ are negative boils down to:
  \begin{equation} \label{eq:stepTauCondition}
  \gamma \mu \tau \leq \frac{2}{11}\,.
  \end{equation}

  Now looking at the $r_u^t$ terms given our assumptions, the inequality~\eqref{eq:rut} becomes:
  \begin{align}
  r_u^t &\leq (1 - \rho)^{t - u} \left[ -2 \gamma + 8 \gamma^2 L + \frac{8 \gamma^2 L}{n} \frac{4}{3}\big(\frac{n}{5} + \frac{4n}{3}\big)\right] \nonumber\\
  &\leq (1 - \rho)^{t - u} \big(-2 \gamma + 36 \gamma^2 L\big)\,.
  \end{align}

  The condition for all $r_u^t$ to be negative then can be simplified down to:
  \begin{equation}
  \gamma \leq \frac{1}{18L}\,.
  \end{equation}

  We now have a promising inequality for proving that our Lyapunov function is bounded by a contraction.
  However we have defined $\mathcal{L}_t$ in terms of the virtual iterate $\xx_t$, which means that our result would only hold for a given $T$ fixed in advance, as is the case in~\citet{mania2015perturbed}.
  Fortunately, we can use the same trick as in~\citet[Eq.~(97)]{leblond2016Asaga}: we simply add $\gamma B_f(\hat \xx_t, \xx^*)$ to both sides in~\eqref{eq:recLyap}.
  $r_t^t$ is replaced by $r_t^t + \gamma$, which makes for a slightly worse bound on $\gamma$ to ensure linear convergence:
  \begin{equation}  \label{eq:stepCondition}
  \gamma \leq \frac{1}{36L} \, .
  \end{equation}
  For this small cost, we get a contraction bound on $B_f(\hat \xx_t, \xx^*)$, and thus by the strong convexity of $f$ (see~\eqref{eq:strongconvex}) we get a contraction bound for $\EE \|\hat \xx_t - \xx^*\|^2$.

  \textbf{Recap.}
  Let us use $\rho = \frac{1}{4 n}$ and $\gamma := \frac{a}{L}$. Then the conditions~\eqref{eq:stepTauCondition} and~\eqref{eq:stepCondition} on the step size~$\gamma$ reduce to:
  \begin{equation} \label{eq:FinalStepsizeCondition}
  a \leq \frac{1}{36} \min \{1, \frac{72}{11} \frac{\kappa}{\tau} \} .
  \end{equation}
  Moreover, the condition:
  \begin{equation} \label{eq:FinalTauCondition}
  \tau \leq \frac{1}{10 \sqrt{\Delta}}
  \end{equation}
  is sufficient to also ensure that~\eqref{eq:initialRhoTauConditions} is satisfied as $\Delta \in [\frac{1}{n}, 1]$, and thus $\frac{1}{\sqrt{\Delta}} \leq \sqrt{n} \leq n$.

  Thus under the conditions~\eqref{eq:FinalStepsizeCondition} and~\eqref{eq:FinalTauCondition},
  we have that all $s_u^t$ and $r_u^t$ terms are negative and we can rewrite the recurrent step of our Lyapunov function as:
  \begin{equation}\label{eq:finish}
  \mathcal{L}_{t+1} \leq \gamma \EE B_f(\hat{\xx}_t) + \mathcal{L}_{t+1}
  \leq (1 - \rho)^{t+1} (a_0 + A \tilde{e}_0)
  + (1 - \frac{\gamma \mu}{2}) \mathcal{L}_{t}\,.
  \end{equation}

  By unrolling the recursion~\eqref{eq:finish}, we can carefully combine the effect of the geometric term~$(1-\rho)$ with the one of~$(1-\frac{\gamma \mu}{2})$. This was already done in~\citet[Apx C.9, Eq. (101) to (103)]{leblond2016Asaga}, with a trick to handle various boundary cases, yielding the overall rate:
   \begin{equation}
	  \EE B_f(\hat \xx_t, \xx^*) \leq (1 - \rho^*)^{t+1} \hat{C}_0,
  \end{equation}
  where $\rho^* = \min\{\frac{1}{5n}, a \frac{2}{5\kappa}\}$ (that we simplified to $\rho^* = \frac{1}{5} \min\{\frac{1}{n}, a \frac{1}{\kappa}\}$ in the theorem statement). To get the final constant, we need to bound $A$.
  We have:
  \begin{align}
    A &= \frac{4 \gamma^2 L}{\beta} (1 - \rho)^{-\tau -1}(\tau +1+\frac{1}{1 - q})
    \nonumber\\
    &\leq 8 \gamma^2 L \frac{4}{3} (\frac{n}{10} + 1 + \frac{4n}{3})
	\nonumber    \\
    &\leq 26 \gamma^2 L n
    \nonumber\\
    &\leq \gamma n \, .
  \end{align}
  This is the same bound on $A$ that was used by~\citet{{leblond2016Asaga}} and so we obtain the same constant as their Eq.~(104):
  \begin{equation}
    \hat C_0 := \frac{21n}{\gamma}(\|\xx_0 - \xx^*\|^2 + \gamma \frac{n}{2L} \EE \|{\boldsymbol\alpha}_i^0 - \nabla f_i(\xx^*)\|^2) .
  \end{equation}
  Note that $\hat C_0 = \mathcal{O}(\frac{n}{\gamma} C_0)$ with $C_0$ defined as in Theorem~\ref{th1}.

  Now, using the strong convexity of~$f$ via~\eqref{eq:strongconvex}, we get:
  \begin{equation}
    \EE \|\hat \xx_t - \xx^*\|^2 \leq \frac{2}{\mu} \EE B_f(\hat \xx_t, \xx^*) \leq (1 - \rho^*)^{t+1} \tilde{C}_0,
  \end{equation}
  where $\tilde{C_0} = \mathcal{O}(\frac{n \kappa}{a} C_0)$.

  This finishes the proof for Theorem~\ref{thm:convergence}.
\end{proof}


\begin{customcorollary}{3}[Speedup]
	Suppose $\tau \leq \frac{1}{10 \sqrt{\Delta}}$.
	If $\kappa \geq n$, then using the step size $\gamma = \nicefrac{1}{36 L}$, \PASAGA\ converges geometrically with rate factor $\Omega(\frac{1}{\kappa})$.
	If $\kappa < n$, then using the step size $\gamma = \nicefrac{1}{36 n \mu}$, \PASAGA\ converges geometrically with rate factor $\Omega(\frac{1}{n})$.
	In both cases, the convergence rate is the same as Sparse Proximal \SAGA\ and \PASAGA\ is thus linearly faster than its sequential counterpart up to a constant factor. Note that in both cases \emph{the step size does not depend on~$\tau$}.

	Furthermore, if $\tau \leq 6 \kappa$, we can use a universal step size of $\Theta(\nicefrac{1}{L})$ to get a similar rate for \PASAGA\ than Sparse Proximal \SAGA, thus making it adaptive to local strong convexity since the knowledge of $\kappa$ is not required.
\end{customcorollary}
\begin{proof}
	If $\kappa \geq n$, the rate factor of Sparse Proximal \SAGA\ is $\nicefrac{1}{\kappa}$.
	To get the same rate factor, we need to choose $a = \Omega(1)$, which we can fortunately do since $\kappa \geq n \geq \sqrt{n} \geq 10 \frac{1}{10\sqrt{\Delta}} \geq 10 \tau$.

	If $\kappa < n$, then the rate factor of Sparse Proximal \SAGA\ is $\nicefrac{1}{n}$.
	Any choice of $a$ bigger than $\Omega(\nicefrac{\kappa}{n})$ gives us the same rate factor for \PASAGA.
	Since $\tau \leq \nicefrac{\sqrt{n}}{10}$ we can pick such an $a$ without violating the condition of Theorem~\ref{thm:convergence}.
\end{proof}


\clearpage


\section{Comparison with related work}\label{apx:related_work}

In this section, we relate our theoretical results and proof technique with the related literature.

\paragraph{Speedups.}
Our speedup regimes are comparable with the best ones obtained in the smooth case, including~\citet{hogwild2011,reddi2015variance}, even though unlike these papers, we support inconsistent reads and nonsmooth objective functions.
The one exception is~\citet{leblond2016Asaga}, where the authors prove that their algorithm, \ASAGA, can obtain a linear speedup even without sparsity in the well-conditioned regime.
In contrast, \PASAGA\ always requires some sparsity.
Whether this property for smooth objective functions could be extended to the composite case remains an open problem.

\paragraph{Coordinate Descent.}
We compare our approach for composite objective functions to its most natural competitor: \AsySPCD~\citep{liu2015asynchronous2}, an asynchronous stochastic coordinate descent algorithm.
While \AsySPCD\ also exhibits linear speedups, subject to a condition on $\tau$, one has to be especially careful when trying to compare these conditions.

First, while in theory the iterations of both algorithms have the same cost, in practice various tricks are introduced to save on computation, yielding different costs per updates.\footnote{For \PASAGA\ the relevant quantity becomes the average number of features per data point. For \AsySPCD\ it is rather the average number of data points per feature. In both cases the tricks involved are not covered by the theory.}
Second, the bound on $\tau$ for the coordinate descent algorithm depends on $p$, the dimensionality of the problem, whereas ours involves $n$, the number of data points.
Third, a more subtle issue is that $\tau$ is not affected by the same quantities for both algorithms.\footnote{To make sure $\tau$ is the same quantity for both algorithms, we have to assume that the iteration costs are homogeneous.}
%
See~\ref{apx:liu} for a more detailed explanation of the differences between the bounds.

In the best case scenario (where the components of the gradient are uncorrelated, a somewhat unrealistic setting), \AsySPCD\ can get a near-linear speedup for~$\tau$ as big as~$\sqrt[4]{p}$.
Our result states that $\tau = \mathcal{O}(\nicefrac{1}{\sqrt{\Delta}})$ is necessary for a linear speedup.
%
%
This means in case $\Delta \leq \nicefrac{1}{\sqrt{p}}$ our bound is better than the one obtained for \AsySPCD.
%
Recalling that $\nicefrac{1}{n} \leq \Delta \leq 1$, it appears that \PASAGA\ is favored when $n$ is bigger than $\sqrt{p}$ whereas \AsySPCD\ may have a better bound otherwise, though this comparison should be taken with a grain of salt given the assumptions we had to make to arrive at comparable quantities.

Furthermore, one has to note that while~\citet{liu2015asynchronous2} use the classical labeling scheme inherited from~\citet{hogwild2011}, they still assume in their proof that the $i_t$ are uniformly distributed and that their gradient estimators are conditionally unbiased -- though neither property is verified in the general asynchronous setting. Finally, we note that \AsySPCD\ (as well as its incremental variant Async-\textsc{ProxSvrcd}) assumes that the computation and assignment of the proximal operator is an atomic step, while we do not make such assumption.

%
\paragraph{SVRG.}
The Async-\ProxSVRG\ algorithm of~\citet{meng2017aaai} also exhibits theoretical linear speedups subject to the same condition as ours.
However, the analyzed algorithm uses dense updates and consistent read and writes.
Although they make the analysis easier, these two factors introduce costly bottlenecks and prevent linear speedups in running time.
Furthermore, here again the classical labeling scheme is used together with the unverified conditional unbiasedness condition.


\paragraph{Doubly stochastic algorithms.}
The Async-\textsc{ProxSvrcd} algorithm from~\citet{meng2017aaai,gu2016asynchronous} has a maximum allowable stepsize\footnote{To the best of our understanding, noting that extracting an interpretable bound from the given theoretical results was difficult.
	Furthermore, it appears that the proof technique may still have significant issues: for example, the ``fully lock-free'' assumption of~\citet{gu2016asynchronous} allows for overwrites, and is thus incompatible with their framework of analysis, in particular their Eq. (8).}
 that is in $\mathcal{O}(\nicefrac{1}{pL})$, whereas the maximum step size for \PASAGA\ is in $\Omega(\nicefrac{1}{L})$, so can be up to $p$ times bigger.
Consequently, \PASAGA\ enjoys much faster theoretical convergence rates.
Unfortunately, we could not find a condition for linear speedups to compare to.
We also note that their algorithm is not appropriate in a sparse features setting.
This is illustrated in an empirical comparison in~\ref{apx:experiments} where we see that their convergence in number of iterations is orders of magnitude slower than appropriate algorithms like \SAGA\ or \PASAGA.
%

%
%
%
%



\subsection{Comparison of bounds with~\citet{liu2015asynchronous2}}\label{apx:liu}
\paragraph{Iteration costs.}
For both \PASAGA\ and \AsySPCD, the average cost of an iteration is $\mathcal{O}(n \overline S)$ (where $\overline S$ is the average support size).
In the case of \PASAGA\ (see Algorithm~\ref{alg:theoretical}), at each iteration the most costly operation is the computation of $\overline {\boldsymbol\alpha}$, while in the general case we need to compute a full gradient for \AsySPCD.

In order to reduce these prohibitive computation costs, several tricks are introduced.
Although they lead to much improved empirical performance, it should be noted that in both cases these tricks are not covered by the theory.
In particular, the unbiasedness condition can be violated.

In the case of \PASAGA, we store the average gradient term $\overline {\boldsymbol\alpha}$ in shared memory.
The cost of each iteration then becomes the size of the extended support of the partial gradient selected at random at this iteration, hence it is in $\mathcal{O}(\Delta_l)$, where $\Delta_l := \max_{i=1..n} |T_i|$.

For \AsySPCD, following~\citet{peng2016arock} we can store intermediary quantities for specific losses (e.g. $\ell_1$-regularized logistic regression).
The cost of an iteration then becomes the number of data points whose extended support includes the coordinate selected at random at this iteration, hence it is in $\mathcal{O}(n \Delta)$.

The relative difference in update cost of both algorithms then depends heavily on the data matrix: if the partial gradients usually have a extended support but coordinates belong to few of them (this can be the case if $n \ll p$ for example), then the iterations of \AsySPCD\ can be cheaper than those of \PASAGA.
Conversely, if data points usually have small extended support but coordinates belong to many of them (which can happen when $p \ll n$ for example), then the updates of \PASAGA\ are the cheaper ones.

\paragraph{Dependency of $\tau$ on the data matrix.}
In the case of \PASAGA\ the sizes of the extended support of each data point are important -- they are directly linked to the cost of each iteration.
Identical iteration costs for each data point do not influence $\tau$, whereas heterogeneous costs may cause $\tau$ to increase substantially.
In contrast, in the case of \AsySPCD, the relevant parts of the data matrix are the number of data points each dimension touches -- for much the same reason.
In the bipartite graph between data points and dimensions, either the left or the right degrees matter for $\tau$, depending on which algorithm you choose.

In order to compare their respective bounds, we have to make the assumption that the iteration costs are homogeneous, which means that each data point has the same support size and each dimension is active in the same number of data points. This implies that $\tau$ is the same quantity for both algorithms.

\paragraph{Best case scenario bound for AsySPCD.}
The result obtained in~\citet{liu2015asynchronous2} states that if $\tau^2 \Lambda = \mathcal{O}(\sqrt{p})$, \AsySPCD\ can get a near-linear speedup (where $\Lambda$ is a measure of the interactions between the components of the gradient, with $1 \leq \Lambda \leq \sqrt{p}$).
In the best possible scenario where $\Lambda = 1$ (which means that the coordinates of the gradients are completely uncorrelated), $\tau$ can be as big as $\sqrt[4]{p}$.



\clearpage

\section{Implementation details}\label{apx:implementation_details}


\paragraph{Initialization.} In the Sparse Proximal \SAGA\ algorithm and its asynchronous variant, \PASAGA, the vector $\xx$ can be initialized arbitrarily. The memory terms $\balpha_i$ can be initialized to any vector that verifies $\text{supp}(\balpha_i) = \text{supp}(\nabla f_i)$. In practice we found that the initialization $\balpha_i = \boldsymbol 0$ is very fast to set up and often outperforms more costly initializations.

With this initialization, the gradient approximation before the first update of the memory terms becomes $\nabla f_i(\xx) + \DD_i \overline{\balpha}$. Since most of the values in $\balpha$ are zero, $\overline{\balpha}$ will tend to be small compared to $\nabla f_i(\xx)$, and so the gradient estimate is very close to the \SGD\ estimate $\nabla f_i(\xx)$. The \SGD\ approximation is known to have a very fast initial convergence (which, in light of Figure~\ref{fig:suboptimality}, our method inherits) and has even been used as a heuristic to use during the first epoch of variance reduced methods~\citep{schmidt2016minimizing}.

The initialization of coefficients $\xx_0$ was always set to zero.

\paragraph{Exact regularization.} Computing the gradient of a smooth regularization such as the squared $\ell_2$ penalty of Eq.~\eqref{eq:logistic_loss} is independent of $n$ and so we can use the exact regularizer in the update of the coefficients instead of storing it in $\balpha$, which would also destroy the compressed storage of the memory terms described below. In practice we use this ``exact regularization'', multiplied by $\DD_i$ to preserve the sparsity pattern. 

Assuming a squared $\ell_2$ regularization term of the form $\frac{\lambda}{2}$, the gradient estimate in~\eqref{eq:SPS} becomes (note the extra $\lambda \xx$)
\begin{equation}
\vv_i = \nabla f_i(\xx) - \balpha_i + \DD_i (\overline\balpha + \lambda \xx)~.
\end{equation}


\paragraph{Storage of memory terms.}
The storage requirements for this method is in the worst case a table of size $n \times p$.
However, as for \SAG\ and \SAGA, for linearly parametrized loss functions of the form $f_i(\xx) = \ell(\boldsymbol a_i^T \xx)$, where $\ell$ is some real-valued function and $(\boldsymbol a_i)_{i=1}^n$ are samples associated with the learning problem, 
this can be reduced to a table of size $n$~\citep[\S 4.1]{schmidt2016minimizing}. 
This includes popular linear models such as least squares or logistic regression with $\ell$ the squared or logistic function, respectively.

The reduce storage comes from the fact that in this case the partial gradients have the structure
\begin{equation}
\nabla f_i(\xx) = \boldsymbol a_i \underbrace{\ell'(\boldsymbol a_i^T \xx)}_{\text{ scalar }} \quad.
\end{equation}
Since $\boldsymbol a_i$ is independent of $\xx$, we only need to store the scalar $\ell'(\boldsymbol a_i^T \xx)$. This decomposition also explains why $\nabla f_i$ inherits the sparsity pattern of $\boldsymbol a_i$.

\paragraph{Atomic updates.} Most modern processors have support for atomic operations with minimal overhead. In our case, we implemented a double-precision atomic type using the {\verb!C++11!} atomic features (\verb!std::atomic<double>!). This type implements atomic operations through the compare and swap semantics.

Empirically, we have found it necessary to implement atomic operations at least in the vector $\balpha$ and $\overline\balpha$ to reach arbitrary precision. If non-atomic operations are used, the method converges only to a limited precision (around normalized function suboptimality of $10^{-3}$), which might be sufficient for some machine learning applications but which we found not satisfying from an optimization point of view.

\paragraph{AsySPCD.} Following~\citep{peng2016arock} we keep the vector $(\boldsymbol a_i^T \xx)_{i=1}^n$ in memory and update it at each iteration using atomic updates.

\paragraph{Hardware and software.}
All experiments were run on a Dell PowerEdge 920 machine with 4 Intel Xeon E7-4830v2
processors with 10 2.2GHz cores each and 384GB 1600 Mhz RAM.
The \PASAGA and \AsySPCD\ code was implemented on C++ and binded in Python. The \textsc{Fista} code is implemented in pure Python using NumPY and SciPy for matrix computations (in this case the bottleneck is in large sparse matrix-vector operations for which efficient BLAS routines were used). Our \PASAGA\ implementation can be downloaded from \url{http://github.com/fabianp/ProxASAGA}.

\clearpage

\section{Experiments}\label{apx:experiments}

All datasets used for the experiments were downloaded from the LibSVM dataset suite.\footnote{\url{https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/}}

\subsection{\large Comparison of ProxASAGA with other sequential methods}


We provide a comparison between the Sparse Proximal \SAGA\ and related methods in the sequential case. We compare against two methods: the \textsc{Mrbcd} method of \citet{zhao2014accelerated} (which forms the basis of Async-\textsc{ProxSvrcd}) and the vanilla implementation of \SAGA~\citep{defazio2014saga}, which does not have the ability to perform sparse updates. 
We compare in terms of both passes through the data (epochs) and time.
We use the same step size for all methods $(1/ 3 L)$. Due to the slow convergence of some methods, we use a smaller dataset than the ones used in \S\ref{scs:experiments}. Dataset RCV1 has $n=697,641, d=47,236$ and  a density of $0.15$,
while Covtype is a dense dataset with $n=581,012, d=54$.

%

\begin{figure}[h]
  \centering\includegraphics[width=0.7\linewidth]{prox_asaga_dense_rcv1}
  \centering\includegraphics[width=0.7\linewidth]{prox_asaga_dense_covtype}
  \caption{Suboptimality of different sequential algorithms. Each marker represents one pass through the dataset.}\label{fig:comparison_dense}
\end{figure}

We observe that for the convergence behavior in terms of number of passes, Sparse Proximal \SAGA\ performs as well as vanilla \SAGA, though the latter requires dense updates at every iteration (Fig.~\ref{fig:comparison_dense} top left). On the other hand, in terms of running time, our implementation of Sparse Proximal \SAGA\ is much more efficient than the other methods for sparse input (Fig.~\ref{fig:comparison_dense} top right). In the case of dense input (Fig.~\ref{fig:comparison_dense} bottom), the three methods perform similarly.

\vspace{-3mm}
\paragraph{A note on the performance of MRBCD.}
It may appear surprising that Sparse Proximal \SAGA\ outperforms \textsc{Mrbcd} so dramatically on sparse datasets.
However, one should note that \textsc{Mrbcd} is a doubly stochastic algorithm where both a random data point and a random coordinate are sampled for each iteration.
If the data matrix is very sparse, then the probability that the sampled coordinate is in the support of the sampled data point becomes very low.
This means that the gradient estimator term only contains the reference gradient term of \SVRG, which only changes once per epoch.
As a result, this estimator becomes very coarse and produces a slower empirical convergence.

This is reflected in the theoretical results given in~\citet{zhao2014accelerated}, where the epoch size needed to get linear convergence are $k$ times bigger than the ones required by plain \SVRG, where $k$ is the size of the set of blocks of coordinates.
\clearpage

\subsection{Theoretical speedups.}
In the experimental section, we have shown experimental speedup results where suboptimality was a function of the running time.
This measure encompasses both theoretical algorithmic optimization properties and hardware overheads (such as contention of shared memory) which are not taken into account in our analysis.

In order to isolate these two effects, we now plot our speedup results in Figure~\ref{fig:ideal} where suboptimality is a function of the number of iterations; thus, we abstract away any potential hardware overhead.
	To do so, we implement a global counter which is sparsely updated (every $100$ iterations for example) in order not to modify the asynchrony of the system.
	This counter is used only for plotting purposes and is not needed otherwise.
Specifically, we define the theoretical speedup as:
\begin{equation*}
\text{theoretical speedup} := \text{(number of cores)} \, \frac{\text{number of iterations for sequential algorithm}}{\text{total number of iterations for parallel algorithm}} \, .
\end{equation*}

\begin{figure}[h]
	\includegraphics[width=\linewidth]{prox_asaga_ideal_speedup.pdf}	
	\caption{{\bfseries Theoretical optimization speedups for $\ell_1 \!+\! \ell_2$-regularized logistic regression}. Speedup as measured by the number of iterations required to reach $10^{-5}$ suboptimality for \PASAGA\ and \AsySPCD. In \textsc{Fista} the iterates are the same with different cores and so matches the ``ideal'' speedup.
}\label{fig:ideal}
\end{figure}

We see clearly that the theoretical speedups obtained by both \PASAGA and \AsySPCD\ are linear (i.e. ideal).
As we observe worse results in running time, this means that the hardware overheads of asynchronous methods are quite significant.

\subsection{Timing benchmarks}\label{apx:timing_benchmarks}
We now provide the time it takes for the different methods with 10 cores to reach a suboptimality of $10^{-10}$. All results are in hours.
\begin{table}[h]
\centering
\begin{tabular}{lrrr}
\toprule
{\bfseries\sffamily Dataset} & \multicolumn{1}{c}{\PASAGA} & \multicolumn{1}{c}{\AsySPCD} & \textsc{Fista} \\
\midrule
{\bfseries\sffamily KDD 2010} & \hfill 1.01 & \hfill 13.3 & \hfill 5.2 \\
{\bfseries\sffamily KDD 2012} & \hfill 0.09 & \hfill 26.6 & \hfill 8.3 \\
{\bfseries\sffamily Criteo} &   \hfill 0.14 & \hfill 33.3 & \hfill 6.6 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Hyperparameters}
The $\ell_1$-regularization parameter $\lambda_2$ was chosen as to give around 10\% of non-zero features. The exact chosen values are the following: $\lambda_2 = 10^{-11}$ for KDD 2010, $\lambda_2 = 10^{-16}$ for KDD 2012 and $\lambda_2 = 4 \times 10^{-12}$ for Criteo.


%
%
%


\clearpage


\end{document}
\documentclass{article}
\pdfoutput=1
\usepackage{nips15submit_e, times}
\usepackage{graphicx} %
\usepackage[center]{subfigure} 
\usepackage[round]{natbib}
\usepackage{multibib}
%
%
\usepackage{algorithmic}
\usepackage[ruled]{algorithm2e}
\SetAlFnt{\small}
\SetAlCapFnt{\small}
\SetAlCapNameFnt{\small}
\algsetup{linenosize=\small}
\usepackage{enumitem}
\usepackage{url}
\usepackage{cancel}
\usepackage{amsmath,amsfonts} %
\usepackage[bookmarks=false]{hyperref}
\usepackage{amsthm}
\usepackage{comment}
\usepackage{tikz-qtree}
\usepackage{listings}
\usepackage{bbm}
\usepackage{mathtools}
\usepackage{breqn}
\usepackage{todonotes}
\usepackage{rotating}
\usepackage{ amssymb }
\usepackage{booktabs}
\usepackage{mathtools}
\usepackage{bm}
\usepackage{mathdots}
\newcites{sup}{Supplementary References}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\newcommand{\Hc}{\bar{H}}
\newcommand{\etal}{et. al}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\brangle{\langle}{\rangle}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{properties}[theorem]{Property}
\newtheorem{problem}[theorem]{Problem}
%
%
%
%
%
%
%
%

%
\hypersetup{
  plainpages=false,
  %
  colorlinks=true,   %
  linkcolor=blue,    %
  anchorcolor=blue,  %
  citecolor=blue,    %
  filecolor=blue,    %
  pagecolor=blue,    %
  urlcolor=blue,     %
  pdfview=FitH,               %
  pdfstartview=FitH,          %
  pdfpagelayout=SinglePage    %
}


%

%
\let\oldnl\nl%
\newcommand{\nonl}{\renewcommand{\nl}{\let\nl\oldnl}}%


\theoremstyle{remark}
\newtheorem{remark}{Remark}

\newcommand{\shrinkAmount}{\delta}
%
%
\DeclareMathOperator{\st}{\textrm{s.t.}}
\DeclareMathOperator*{\conv}{conv}
\DeclareMathOperator{\diam}{diam}
\DeclareMathOperator*{\E}{\rm E}
\DeclareMathOperator*{\argmin}{\arg\min}
\DeclareMathOperator*{\argmax}{\arg\max}
\DeclareMathOperator{\lmax}{\lambda_{\max}}
\DeclareMathOperator{\lmin}{\lambda_{\min}}
\DeclareMathOperator*{\supp}{supp}
\DeclareMathOperator*{\diag}{diag}
\DeclareMathOperator*{\sign}{sign}
\DeclareMathOperator*{\tr}{Tr}
\DeclareMathOperator*{\rk}{Rk}
\DeclareMathOperator*{\card}{card}
\DeclareMathOperator*{\relint}{\mathop{relint}}

%
\providecommand{\abs}[1]{\left\lvert#1\right\rvert}
\providecommand{\norm}[1]{\left\lVert#1\right\rVert}
\providecommand{\dualnorm}[1]{\norm{#1}_*}
\providecommand{\frobnorm}[1]{\norm{#1}_{Fro}}
\providecommand{\tracenorm}[1]{\norm{#1}_{tr}}
\providecommand{\opnorm}[1]{\norm{#1}_{op}}
\providecommand{\maxnorm}[1]{\norm{#1}_{\max}}
\providecommand{\latGrNorm}[1]{\norm{#1}_{\groups}}

\newcommand{\ball}{\mathcal{B}}
%

\newcommand{\bigO}{O}
\newcommand{\R}{\mathbb{R}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Sym}{\mathbb{S}}

%
\newcommand{\domain}{\mathcal{D}}
\newcommand{\stepsize}{\gamma}
%\newcommand{\stepmax}{\gamma\textnormal{\scriptsize max}} %
\newcommand{\stepmax}{{\stepsize}_{\mathrm{max}}}
\newcommand{\stepbound}{\gamma^\textnormal{\scriptsize B}} %
\newcommand{\FW}{{\hspace{0.05em}\textnormal{FW}}}
\newcommand{\pFW}{{\hspace{0.05em}\textnormal{pFW}}}
\newcommand{\away}{{\hspace{0.06em}\textnormal{\scriptsize A}}}
\newcommand{\Cf}{C_{\hspace{-0.08em}f}}
\newcommand{\CfA}{C_{\hspace{-0.08em}f}^-}
\newcommand{\CfMFW}{C_{\hspace{-0.08em}f}^\away}
\newcommand{\strongConvFW}{\mu_{\hspace{-0.08em}f}^\FW}
\newcommand{\strongConvMFW}{\mu_{\hspace{-0.08em}f}^\away}
\newcommand{\distToBoundary}{{\Delta_{\x^*\!,\domain}}}
\newcommand{\dirW}{\mathop{dirW}}
\newcommand{\x}{\bm{x}}
\newcommand{\y}{\bm{y}}
\newcommand{\z}{\bm{z}}
\newcommand{\s}{\bm{s}}
\newcommand{\dd}{\bm{d}}
\newcommand{\uu}{\bm{u}}
\newcommand{\vv}{\bm{v}} %
\newcommand{\atoms}{\mathcal A}
\newcommand{\groups}{\mathcal G}
\newcommand{\row}{\text{row}}
\newcommand{\col}{\text{col}}
\newcommand{\lft}{\text{left}}
\newcommand{\rgt}{\text{right}}
\newcommand{\mapprox}{\nu} %
\newcommand{\CfTotal}{C_f^{\text{\tiny{prod}}}}

%
\newcommand{\Vertices}{\mathcal{V}}
\newcommand{\Coreset}{\mathcal{S}}
\renewcommand{\S}{\mathcal{S}}
\renewcommand{\aa}{\bm{\alpha}}
\renewcommand{\r}{\bm{r}}
\newcommand{\PdirW}{\mathop{PdirW}}
\newcommand{\innerProd}[2]{\left\langle #1 , #2 \right\rangle}
\newcommand{\C}{\mathcal{C}}
\newcommand{\proj}{\bm{P}}
\newcommand{\K}{\bm{K}}
\newcommand{\Kface}{\mathcal{K}}

\newcommand{\LOCAL}{\mathbb{L}} %
\newcommand{\MARG}{\mathcal{M}} %
\newcommand{\VAL}{\textrm{VAL}}
\newcommand{\Rregion}{\mathcal{R}}
\newcommand{\Bregion}{\mathcal{B}}

\newcommand{\nn}{\bm{n}} %
\newcommand{\smallMax}{{\textnormal{\scriptsize max}}}

%


%
\newcommand{\xk}{\bm{x}^{(k)}}
\newcommand{\xnext}{\bm{x}^{(k+1)}}

\newcommand{\dk}{\bm{d}^{(k)}}
\newcommand{\sk}{\bm{s}^{(k)}}
\newcommand{\snext}{\bm{s}^{(k+1)}}
\newcommand{\aak}{\bm{\alpha}^{(k)}}

\newcommand{\Cconst}{\widetilde{C}}
\newcommand{\Cconstk}{\widetilde{C}_{\hspace{-0.08em}(k)}}

\newcommand{\fk}{f(\bm{x}^{(k)})}
\newcommand{\fnext}{f(\bm{x}^{(k+1)})}

\newcommand{\hk}{h_k}
\newcommand{\hkarg}[1]{h_{#1}} %
\newcommand{\hnext}{h_{k+1}}
\newcommand{\hprev}{h_{k-1}}
\newcommand{\gk}{g(\bm{x}^{(k)})}
\newcommand{\raiseto}[3]{\left(#1\right)^{\frac{#2}{#3}}}
\newcommand{\gam}[1]{\gamma_{(#1)}}
\newcommand{\tgam}[1]{\tilde{\gamma}_{(#1)}}
\newcommand{\textprf}[1]{\left[\text{#1}\right]}
\newcommand{\DOM}{\mathcal{D}}
\newcommand{\xopt}{\bm{x}^{*}}
\newcommand{\TREEPOL}{\mathbb{T}} %
\newcommand{\errlogz}{\zeta_{\log Z}}
\newcommand{\errmu}{\zeta_{\mu}}

\newcommand{\modContinuity}{\omega} %

%
\newcommand{\unif}{\bm{u}_0} %
\newcommand{\gunif}{g_u(\bm{x}^{(k)})} %
\newcommand{\gunifNox}{g_u} %
\newcommand{\geps}{g_{(\shrinkAmount)}(\bm{x}^{(k)})}
\newcommand{\gepsk}[1]{g_{(\shrinkAmount^{(#1)})}(\bm{x}^{(k)})}
\newcommand{\Meps}{\mathcal{M}_{\shrinkAmount}}
\newcommand{\DOMeps}{\mathcal{D}_{\shrinkAmount}}
\newcommand{\dkeps}{\bm{d}^{(k)}_{(\shrinkAmount)}}
\newcommand{\skeps}{\bm{s}^{(k)}_{(\shrinkAmount)}}
\newcommand{\hkeps}{h^{\shrinkAmount}(\bm{x}^{(k)})}
\newcommand{\xkfinal}{\bm{x}^{(final)}_{\shrinkAmount}}
\newcommand{\xopteps}{\bm{x}^{*}_{(\shrinkAmount)}}
\newcommand{\xoptshift}{\tilde{\bm{x}}_{(\shrinkAmount)}}
\newcommand{\vveps}{\bm{v}_{(\shrinkAmount)}}
\newcommand{\epsk}[1]{\shrinkAmount^{(#1)}}
\newcommand{\del}{\mathrm{\bm{d}}}
\newcommand{\h}{h}
\newcommand{\vrho}{\bm{\rho}}
\newcommand{\vmu}{\vec{\bm{\mu}}}
\newcommand{\bmu}{\bm{\mu}}
\newcommand{\edges}{\{i,j\}\in E}
\newcommand{\nodes}{i\in V}
\newcommand{\vtheta}{\vec{\bm{\theta}}}
\newcommand{\btheta}{\bm{\theta}}
\newcommand{\valpha}{\vec{\bm{\alpha}}}
\newcommand{\trwent}{H(\vec{\bm{\mu}};\vrho)}
\newcommand{\LOCALSEARCH}{\textbf{\textrm{LOCALSEARCH}}}
\newcommand{\CORRECTION}{\textbf{\textrm{CORRECTION}}}
\newcommand{\MIfunction}{\textbf{\textrm{edgesMI}}}
\newcommand{\MAXITS}{\textbf{\textrm{MAXITS}}}
\newcommand{\MAXRHOITS}{\textbf{\textrm{MAX$\_$RHO$\_$ITS}}}
\newcommand{\TRW}{\text{TRW}(\vmu;\vtheta,\vrho)}
\newcommand{\algComment}[1]{\quad\emph{\small{(#1)}}}
\newcommand{\nbrs}[1]{\mathcal{N}(#1)}
\newcommand{\gunifBound}{B}
\newcommand{\stopCrit}{\epsilon}


\title{Barrier Frank-Wolfe for Marginal Inference}

\author{
Rahul G. Krishnan \\
Courant Institute\\
New York University\\
%
\And
Simon Lacoste-Julien \\ 
INRIA - Sierra Project-Team\\
\'{E}cole Normale Sup\'{e}rieure, Paris
%
\And
David Sontag\\
Courant Institute\\
New York University\\
%
}

%
%
%
%
%
%
%

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\note}[1]{{\textbf{\color{red}#1}}}

\nipsfinalcopy %

\begin{document}
\maketitle

\begin{abstract} 
We introduce a globally-convergent algorithm for optimizing the
tree-reweighted (TRW) variational objective over the marginal
polytope.
The algorithm is based on the conditional gradient method (Frank-Wolfe) 
and moves pseudomarginals within the marginal
polytope through repeated maximum a posteriori (MAP) calls.
%
This modular structure enables us 
to leverage black-box MAP solvers (both exact and approximate) for
variational inference, and obtains more accurate results than
tree-reweighted algorithms that optimize over the local consistency
relaxation.
%
Theoretically, we bound the sub-optimality for the proposed algorithm
despite the TRW objective having unbounded gradients at the boundary of the marginal polytope. 
%
Empirically, we demonstrate the increased quality of results found  
by tightening the relaxation over the marginal polytope as well 
as the spanning tree polytope on synthetic and real-world instances. 
\end{abstract} 
\input{introduction.tex}
\input{background.tex}
\input{theory.tex}
\input{algorithm.tex}
\input{experiments.tex}
\input{discussion.tex}
\vspace{-2mm}
\section*{Acknowledgements}
\vspace{-2mm}
%
%
%
%
%
%
%
%
RK and DS gratefully acknowledge the support of the Defense Advanced
Research Projects Agency (DARPA) Probabilistic Programming for
Advancing Machine Learning (PPAML) Program under Air Force Research
Laboratory (AFRL) prime contract no. FA8750-14-C-0005. Any opinions,
findings, and conclusions or recommendations expressed in this
material are those of the author(s) and do not necessarily reflect the
view of DARPA, AFRL, or the US government.
%
\bibliographystyle{abbrvnat}
%
\small{
\bibliography{ref}
}

%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
\clearpage %
\input{supplement}
\end{document}
\section{Introduction}
\vspace{-3mm}

Markov random fields (MRFs) are used in many areas of computer science such as vision and speech. 
Inference in these undirected graphical models is generally intractable. 
Our work focuses on performing approximate marginal inference 
%
by optimizing the Tree Re-Weighted (TRW)
objective \citep{wainwright2005new}. 
The TRW objective is concave, is exact for tree-structured MRFs, 
and provides an upper bound on the log-partition function. 

Fast combinatorial solvers for the TRW objective exist, including Tree-Reweighted 
Belief Propagation (TRBP) \citep{wainwright2005new}, convergent
message-passing based on geometric programming
\citep{globerson2012convergent}, and dual decomposition
\citep{Jancsary11}. These methods optimize over the set
of pairwise consistency constraints, also called the local polytope. \citet{sontag2007new} showed that significantly better 
results could be obtained by optimizing over tighter relaxations of the marginal 
polytope. However, deriving a message-passing algorithm for the TRW objective over tighter relaxations of the marginal polytope
is challenging. Instead, \citet{sontag2007new} use the
conditional gradient method (also called Frank-Wolfe) and
off-the-shelf linear programming solvers to optimize TRW over the cycle
consistency relaxation.
Rather than optimizing over the cycle relaxation,
\citet{belangerWorkshop2013} optimize the TRW objective over
the exact marginal polytope. Then,
using Frank-Wolfe, the linear minimization performed in
the inner loop can be shown to correspond to MAP inference. 

%
%
%
%

%
%
%
%
%
%
%
%
%

The Frank-Wolfe optimization algorithm has seen increasing use in machine learning,
thanks in part to its efficient handling of complex constraint sets appearing with structured data~\citep{jaggi2013revisiting,lacoste2015MFW}.
However, applying Frank-Wolfe to
variational inference presents challenges that were never
resolved in previous work. 
First, the linear minimization performed in the
inner loop is computationally expensive, either requiring repeatedly solving a
large linear program, as in \citet{sontag2007new}, or performing
MAP inference, as in \citet{belangerWorkshop2013}.
Second, the TRW objective involves entropy terms whose gradients go to
infinity near the boundary of the feasible set, therefore
existing convergence guarantees for Frank-Wolfe do not apply.
Third, variational inference using TRW involves
both an outer and inner loop of Frank-Wolfe, where the outer loop
optimizes the edge appearance probabilities in the TRW entropy bound to tighten it. 
Neither \citet{sontag2007new} nor \citet{belangerWorkshop2013} explore the effect of
optimizing over the edge appearance probabilities. %
%
%

%
Although MAP inference is in general NP hard \citep{MAP_NP_Hard},
it is often possible to find exact solutions to large real-world
instances within reasonable running times 
\citep{SontagEtAl_uai08, allouche2010toulbar2,kappes2013comparative}. 
Moreover, as we show in our experiments, even approximate MAP solvers
can be successfully used within our variational inference algorithm.
As MAP solvers improve in their
runtime and performance, their iterative use could become feasible and as a byproduct enable more efficient and accurate
marginal inference. Our work provides a fast deterministic alternative to
recently proposed Perturb-and-MAP algorithms
\citep{papandreou2011perturb, hazan2012partition, ermonWISH}.

\textbf{Contributions.} 
This paper makes several theoretical and practical innovations. 
%
%
We propose a modification to the Frank-Wolfe algorithm that 
optimizes over adaptively chosen contractions of the domain and prove its 
rate of convergence for functions
whose gradients can be unbounded at the boundary.  
Our algorithm does not require a different oracle than standard Frank-Wolfe and could be 
%
useful for other convex optimization problems where the gradient is ill-behaved at the boundary.

We instantiate the algorithm for approximate marginal inference over the marginal polytope 
with the TRW objective.
With an exact MAP oracle, we obtain the first provably convergent algorithm 
for the optimization of the TRW objective over the marginal polytope, 
which had remained an open problem to the best of our knowledge.
%
%
%
%
Traditional proof techniques of convergence for first order methods fail as the gradient of the TRW objective is not Lipschitz continuous. 

We develop several heuristics to
make the algorithm practical: a fully-corrective
variant of Frank-Wolfe that reuses previously found integer
assignments thereby reducing the need for new (approximate) MAP
calls, the use of local search between MAP calls, 
and significant re-use of computations between
subsequent steps of optimizing over the spanning tree
polytope.
We perform an extensive experimental evaluation on both synthetic and real-world inference tasks. 
\documentclass{article}
\usepackage{nips15submit_e,times}
\usepackage{hyperref}
\usepackage{url}


\title{Rethinking LDA: Moment Matching for Discrete ICA}


\author{
Anastasia Podosinnikova\qquad Francis Bach\qquad Simon Lacoste-Julien   \\
INRIA - \'Ecole normale sup\'erieure Paris\\
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\nipsfinalcopy 

\usepackage[parfill]{parskip} 

\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage[font={small}]{caption} 
\usepackage{caption}
\usepackage{subcaption}

\usepackage{color}
\definecolor{mygreen}{RGB}{50, 130, 50}
\hypersetup{
    colorlinks=true, 
    linkcolor=mygreen,  
    citecolor=mygreen,  
    filecolor=mygreen,  
    urlcolor=mygreen,   
}

\makeatletter

\newenvironment{ma}
  {\start@align\@ne\st@rredtrue\m@ne}
  {\endalign}
\makeatother

\newenvironment{mi}{%
  \begin{list}{-}{}
  \let\olditem\item
}{%
  \end{list}
}

\makeatother


\newcommand\nameeq[2]{\phantom{\text{#2}}&&\begin{gathered}#1\end{gathered}&&\text{#2}}

\newcommand{\emp}[1]{\textbf{#1}}

\usepackage{amssymb,amsmath,amsthm}
\usepackage[makeroom]{cancel} 
\usepackage{dsfont} 

\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}{Lemma}[subsection]

\def\max{\mathop{\rm max}\limits}
\def\sup{\mathop{\rm sup}\limits}

\newcommand{\norm}[1]{\left\|#1\right\|}
\newcommand{\normp}[1]{\|#1\|}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\inner}[1]{\left\langle#1\right\rangle}
\newcommand{\innerp}[1]{\langle#1\rangle}

\newcommand{\rbra}[1]{\left(#1\right)}
\newcommand{\sbra}[1]{\left[#1\right]}
\newcommand{\cbra}[1]{\left\{#1\right\}}
\newcommand{\cbras}[1]{\{#1\}}

\newcommand{\wt}[1]{\widetilde{#1}}
\newcommand{\wh}[1]{\widehat{#1}}

\newcommand{\indicator}[1]{\mathds{1}_{\{#1\}}} 
\newcommand{\ones}{\vec{1}}

\newcommand{\diag}{\mathrm{diag}}

\newcommand{\R}{\mathbb{R}}
\newcommand{\rr}[1]{\mathbb{R}^{#1}}

\newcommand{\inv}{^{-1}}                      
\newcommand{\pinv}{^{\dagger}}                
\newcommand{\tp}{\otimes}                     

\newcommand{\sumk}{\sum_{k=1}^K}
\newcommand{\summ}{\sum_{m=1}^M}
\newcommand{\sumn}{\sum_{n=1}^N}

\newcommand{\ga}{\alpha}
\newcommand{\gb}{\beta}
\newcommand{\gl}{\lambda}
\newcommand{\gt}{\theta}
\newcommand{\gd}{\delta}
\newcommand{\Gt}{\Theta}
\newcommand{\Gd}{\Delta}

\newcommand{\tcal}{\mathcal{T}}
\newcommand{\acal}{\mathcal{A}}
\newcommand{\bcal}{\mathcal{B}}
\newcommand{\ccal}{\mathcal{C}}
\newcommand{\ecal}{\mathcal{E}}
\newcommand{\fcal}{\mathcal{F}}

\newcommand{\ebb}{\mathbb{E}}      
\newcommand{\webb}{\wh{\ebb}}       
\newcommand{\var}{\mathrm{var}}     
\newcommand{\wvar}{\wh{\var}}      
\newcommand{\cov}{\mathrm{cov}}     
\newcommand{\wcov}{\wh{\cov}}       
\newcommand{\cum}{\mathrm{cum}}    
\newcommand{\wcum}{\wh{\cum}}      
\def\ci{\perp\!\!\!\perp}         

\newcommand{\poi}{\mathrm{Poisson}}
\newcommand{\dir}{\mathrm{Dirichlet}}
\newcommand{\gam}{\mathrm{Gamma}}
\newcommand{\mul}{\mathrm{Multinomial}}
\newcommand{\nor}{\mathrm{Normal}}

\usepackage{algorithm}
\usepackage{algcompatible}

\usepackage[numbers]{natbib}
\setlength{\bibsep}{4.5pt plus 0.3ex}
\usepackage{multibib}
\newcites{sup}{Supplementary References} 

\usepackage{tikz}
\usetikzlibrary{arrows,decorations.pathmorphing,fit,positioning}


\makeatletter
%
\DeclareRobustCommand*\textsubscript[1]{%
  \@textsubscript{\selectfont#1}}
\def\@textsubscript#1{%
  {\m@th\ensuremath{_{\mbox{\fontsize\sf@size\z@#1}}}}}
\makeatother

\newcommand{\rpm}{\raisebox{.2ex}{$\scriptstyle\pm$}}

\renewcommand{\baselinestretch}{.98} 


\begin{document}


\maketitle


\begin{abstract}
\noindent
We consider moment matching techniques for estimation in latent Dirichlet allocation (LDA). By drawing explicit links between LDA and discrete versions of independent component analysis (ICA), we first derive a new set of cumulant-based tensors, with an improved sample complexity. Moreover, we reuse standard ICA techniques such as joint diagonalization of tensors to improve over existing methods based on the tensor power method. In an extensive set of experiments on both synthetic and real datasets, we show that our new combination of tensors and orthogonal joint diagonalization techniques outperforms existing moment matching methods.
\end{abstract}


\section{Introduction}
Topic models have emerged as flexible and important tools for the modelisation of text corpora. While early work has focused on graphical-model approximate inference techniques such as variational inference~\cite{BleEtAl2003} or Gibbs sampling~\cite{Gri2002}, tensor-based moment matching techniques have recently emerged as strong competitors due to their computational speed and theoretical guarantees~\cite{AnaEtAl2012,AnaEtAl2014}. In this paper, we draw explicit links with the independent component analysis (ICA) literature (e.g.,~\cite{ComJut2010} and references therein) by showing a strong relationship between latent Dirichlet allocation (LDA)~\cite{BleEtAl2003} and ICA~\cite{Jut1987,JutHer1991,Com1994}. We can then reuse standard ICA techniques and results, and derive new tensors with better sample complexity and new algorithms based on joint diagonalization.




\section{Is LDA discrete PCA or discrete ICA?}\label{sec:2}
\emp{Notation.} Following the text modeling terminology, we define a corpus $X=\cbra{x_1,\dots,x_N}$ as a collection of $N$ documents. Each document is a collection $\cbra{w_{n1},\dots,w_{nL_n}}$ of $L_n$ tokens. It is convenient to represent the $\ell$-th token of the $n$-th document  as a $1$-of-$M$ encoding with an indicator vector $w_{n\ell}\in\{0,1\}^{M}$ with only one non-zero, where $M$ is the vocabulary size, and each document as the count vector $x_n := \sum_{\ell} w_{n\ell}\in\rr{M}$. In such representation, the length $L_n$ of the $n$-th document is $L_n = \sum_m x_{nm}$. We will always use the index $k\in\cbra{1,\dots,K}$ to refer to topics,the  index $n\in\cbra{1,\dots,N}$ to refer to documents, the index $m\in\cbra{1,\dots,M}$ to refer to words from the vocabulary, and the  index $\ell\in\{1,\dots,L_n\}$ to refer to tokens of the $n$-th document. The plate diagrams of the models from this section are presented in Appendix~\ref{sec:pds}.

\emp{Latent Dirichlet allocation} \cite{BleEtAl2003} is a generative probabilistic model for discrete data such as text corpora.  
In accordance to this model, the $n$-th document is modeled as an \emph{admixture} over the vocabulary of $M$ words with $K$ latent topics. Specifically, the latent variable $\theta_n$, which is sampled from the Dirichlet distribution, represents the topic mixture proportion over $K$ topics for the $n$-th document. Given $\theta_n$,
the topic choice $z_{n\ell} | \theta_n$ for the $\ell$-th token is sampled from the multinomial distribution with the probability vector $\gt_n$. The token $w_{n\ell} | z_{n\ell}, \theta_n$ is then sampled from the multinomial distribution with the probability vector $d_{z_{n\ell}}$, or $d_k$ if $k$ is the index of the non-zero element in $z_{n\ell}$. This vector $d_k$ is the $k$-th topic, that is a vector of probabilities over the words from the vocabulary subject to the simplex constraint, i.e., $d_k\in\Delta_M$, where $\Delta_M := \cbras{d\in\rr{M}\;:\;d\succeq 0,\;\sum_m d_m = 1}$.
This generative process of a document (the index $n$ is omitted for simplicity) can be summarized as
\begin{equation}\label{lda-tokens}
\begin{aligned}
\theta &\sim \dir(c), \\
z_{\ell}|\theta &\sim \mul(1,\theta), \\
w_{\ell} | z_{\ell},\theta &\sim \mul(1,d_{z_{\ell}}).
\end{aligned}
\end{equation}
One can think of the latent variables $z_{\ell}$ as auxiliary variables which were introduced for  convenience of inference, but can in fact be marginalized out \cite{Bun2002}, which leads to the following model
\begin{flalign} \label{lda}
\nameeq{
	\begin{aligned}
	\theta &\sim \dir(c), \\
	x|\theta &\sim \mul(L,D\theta),
	\end{aligned}
}{LDA model}
\end{flalign}
where $D\in\rr{M\times K}$ is the topic matrix with the $k$-th column equal to the $k$-th topic $d_k$, and $c \in \rr{K}_{++}$ is the vector of parameters for the Dirichlet distribution. 
While a document is represented as a set of tokens $w_{\ell}$ in the formulation~\eqref{lda-tokens}, the formulation~\eqref{lda} instead compactly represents a document as the count vector $x$.
Although the two representations are equivalent, we focus on the second one in this paper and therefore refer to it as the LDA model.

Importantly, the LDA model does not model the length of documents. Indeed, although the original paper~\cite{BleEtAl2003} proposes to model the document length as $L|\gl\sim\poi(\gl)$, this is never used in practice and, in particular, the parameter $\gl$ is not learned. 
Therefore, in the way that the LDA model is typically used, it 
does not provide a complete generative process of a document as there is no rule to sample $L|\gl$. 
In this paper, this fact is important, as we need to model the document length in order to make the link with discrete ICA.




\emp{Discrete PCA.} The LDA model~\eqref{lda} can be seen as a discretization of principal component analysis (PCA) via replacement of the normal likelihood with the multinomial one and adjusting the prior~\cite{Bun2002} in the following  probabilistic PCA model~\cite{TipBis1999,Row1998}: $\theta \sim \nor(0,I_K)$ and $x|\theta \sim \nor(D\theta, \sigma^2 I_M)$,
where $D\in\rr{M\times K}$ is a transformation matrix and $\sigma$ is a parameter. 




\emp{Discrete ICA (DICA).} Interestingly, a small extension of the LDA model allows its interpretation as a discrete independent component analysis model. 
The extension naturally arises when the document length for the LDA model is modeled as a random variable from the gamma-Poisson mixture (which is equivalent to a negative binomial random variable), i.e., $L|\gl \sim \poi(\gl)$ and $\gl \sim \gam(c_0,b)$, where $c_0 := \sum_k c_k$ is the shape parameter and $b>0$ is the rate parameter. The LDA model~\eqref{lda} with such document length
is equivalent (see Appendix~\ref{sec:ldaproof2}) to 
\begin{flalign}\label{gp}
\nameeq{
	\begin{aligned}
	\ga_k &\sim \gam(c_k,b), \\
	x_m|\ga &\sim \poi([D\ga]_m),
	\end{aligned}
}{GP model}
\end{flalign}
where all $\ga_1,\ga_2,\dots,\ga_K$ are mutually independent, the parameters $c_k$ coincide with the ones of the LDA model in~\eqref{lda}, and the free parameter $b$ can be seen (see Appendix~\ref{sec:L:gp}) as a scaling parameter for the document length when $c_0$ is already prescribed.

This model was introduced by Canny~\cite{Can2004} and later named as a discrete ICA model~\cite{BunJak2004}. 
It is more natural, however, to name model~\eqref{gp} as the gamma-Poisson (GP) model and the model
\begin{flalign}\label{dica}
\nameeq{
	\begin{aligned}
	\ga_1,\dots,\ga_K &\sim \text{mutually independent}, \\
	x_m|\ga & \sim \poi([D\ga]_m)
	\end{aligned}
}{DICA model}
\end{flalign}
 as the discrete ICA (DICA) model. The only difference between~\eqref{dica} and the standard ICA model~\cite{Jut1987,JutHer1991,Com1994} (without additive noise) is the presence of the Poisson noise
which enforces discrete, instead of continuous, values of $x_m$. 
Note also that (a) the discrete ICA model is a \emph{semi-parametric} model that can adapt to any distribution on the topic intensities $\alpha_k$ and that (b) the GP model~\eqref{gp} is a particular case of both the LDA model~\eqref{lda} and the DICA model~\eqref{dica}.

Thanks to this close connection between LDA and ICA, we can reuse standard ICA techniques to derive new efficient algorithms for topic modeling.










\section{Moment matching for topic modeling}
The method of moments estimates latent parameters of a probabilistic model by matching theoretical expressions of its moments with their sample estimates. Recently~\cite{AnaEtAl2012,AnaEtAl2014}, the method of moments was applied to different latent variable models including LDA, resulting in computationally fast learning algorithms with theoretical guarantees. For LDA, they (a) construct {\it LDA moments} with a particular diagonal structure and (b) develop algorithms for estimating the parameters of the model by exploiting this diagonal structure. In this paper, we introduce the novel {\it GP/DICA cumulants} with a similar to the LDA moments structure.
This structure allows to reapply the algorithms of~\cite{AnaEtAl2012,AnaEtAl2014} for the estimation of the model parameters, with the same theoretical guarantees. We also consider another algorithm applicable to both the LDA moments and the GP/DICA cumulants. 


\subsection{Cumulants of the GP and DICA models} \label{sec:cum}

In this section, we derive and analyze the novel cumulants of the DICA model. As the GP model is a particular case of the DICA model, all results of this section extend to the GP model.

The first three \emph{cumulant tensors} for the random vector $x$ can be defined as follows
\begin{align}
\label{exp}
\cum(x) & := \ebb(x), \\
\label{covx}
\cum(x,x) & := \cov(x,x) = \ebb\sbra{ (x - \ebb(x)) ( x - \ebb(x))^{\top} }, \\
\label{cumx}
\cum(x,x,x) & := \ebb\sbra{ (x - \ebb(x)) \tp (x - \ebb(x)) \tp (x - \ebb(x))},
\end{align}
where $\tp$ denotes the tensor product (see some properties of  cumulants in Appendix~\ref{sec:cumulants}). The essential property of the cumulants (which does not hold for the moments) that we use in this paper is that the cumulant tensor for a random vector with \emph{independent} components is \emph{diagonal}.

Let $y = D\ga$; then for the Poisson random variable $x_m|y_m \sim \poi (y_m)$, the expectation is $\ebb(x_m|y_m) = y_m$. Hence, by the law of total expectation and the linearity of expectation,  the expectation in~\eqref{exp} has the following form
\begin{equation}
\ebb(x) = \ebb(\ebb(x|y)) = \ebb(y) = D\ebb(\ga).
\end{equation}
Further, the variance of the Poisson random variable $x_m$ is $\var(x_m|y_m) = y_m$ and, as $x_1$, $x_2$,~$\dots$,~$x_M$ are conditionally independent given $y$, then their covariance matrix is diagonal, i.e., $\cov(x,x|y) = \diag(y)$. Therefore, by the law of total covariance, the covariance in~\eqref{covx} has the form
\begin{equation}\label{cum2}
\begin{aligned}
\cov(x,x) & = \ebb\sbra{\cov(x,x|y)} + \cov\sbra{\ebb(x|y),\ebb(x|y)} \\
       & = \diag\sbra{\ebb(y)} + \cov(y,y) 
        = \diag\sbra{\ebb(x)} + D\cov(\ga,\ga)D^{\top},
\end{aligned}
\end{equation}
where the last equality follows by the multilinearity property of cumulants (see Appendix~\ref{sec:cumulants}). Moving the first term from the RHS of~\eqref{cum2} to the LHS, we define
\begin{flalign}\label{S}
\nameeq{
S := \cov(x,x)-\diag\sbra{\ebb(x)}.
}{DICA S-cum.}
\end{flalign}
From~\eqref{cum2} and by the independence of $\ga_1$, $\dots$, $\ga_K$ (see Appendix~\ref{sec:app:dicacum2}),  $S$ has the following diagonal structure
\begin{equation}
\label{diagS}
S = \sum\mathop{}_{k} \var(\ga_k) d_k d_k^{\top} = D\diag\sbra{\var(\ga)}D^{\top}.
\end{equation}


By analogy with the second order case, 
using the law of total cumulance, the multilinearity property of cumulants, and the independence of $\ga_1$, $\dots$, $\ga_K$, we derive in Appendix~\ref{sec:app:3dicacum} the expression~\eqref{cum3}, similar to~\eqref{cum2}, for the third cumulant~\eqref{cumx}. Moving the terms in this expression, we  define a tensor $T$ with the following element
\begin{align}
\phantom{\text{DICA}}& \!\!\!\! \!\!\! \sbra{T}_{m_1m_2m_3} := \cum(x_{m_1},x_{m_2},x_{m_3}) + 2\gd(m_1,m_2,m_3) \ebb(x_{m_1}) \qquad\qquad\, \text{DICA T-cum.} \label{T} \\
& - \gd(m_2,m_3) \cov(x_{m_1},x_{m_2}) - \gd(m_1,m_3) \cov(x_{m_1},x_{m_2}) - \gd(m_1,m_2) \cov(x_{m_1},x_{m_3}), \notag
\end{align}
where $\delta$ is the Kronecker delta.
By analogy with~\eqref{diagS} (Appendix~\ref{sec:app:dicacum2}), the diagonal structure of the tensor $T$:
\begin{equation} 
\label{diagT}
T = \sum\mathop{}_{k} \cum(\ga_k,\ga_k,\ga_k)d_k\tp d_k \tp d_k.
\end{equation}


In Appendix~\ref{sec:lda:moms:notation}, we recall (in our notation) the matrix $S$~\eqref{S:lda} and the tensor $T$~\eqref{T:lda} for the LDA model~\cite{AnaEtAl2012}, which are analogues of the matrix $S$~\eqref{S} and the tensor $T$~\eqref{T} for the GP/DICA models. Slightly abusing terminology, we refer to the matrix $S$~\eqref{S:lda} and the tensor $T$~\eqref{T:lda} as the {\it LDA moments} and to the matrix $S$~\eqref{S} and the tensor $T$~\eqref{T} as the {\it GP/DICA cumulants}. The diagonal structure~\eqref{diagS:lda}~\&~\eqref{diagT:lda} of the LDA moments is similar to the diagonal structure~\eqref{diagS}~\&~\eqref{diagT} of the GP/DICA cumulants, though arising through a slightly different argument, as discussed at the end of Appendix~\ref{sec:lda:moms:notation}. Importantly, due to this similarity, the algorithmic frameworks for both the GP/DICA cumulants and the LDA moments coincide. 









The following sample complexity results apply to the sample estimates of the GP cumulants:\footnote{Note that the expected squared error for the DICA cumulants is similar, but the expressions are less compact and, in general, depend on the prior on $\alpha_k$.}
\begin{proposition}\label{sample-complexity}
Under the GP model, the expected error for the sample estimator $\wh{S}$~\eqref{appSgp:fs} for the GP cumulant $S$~\eqref{S} is:
\begin{equation} \label{eq:S_complexity}
\ebb\sbra{ \normp{ \wh{S} - S }_F }  \leq 
\sqrt{\ebb\sbra{ \normp{ \wh{S} - S }_F^2 }}  \le O\rbra{ \frac{1}{\sqrt{N}}  \max\sbra{ \Delta \bar{L}^2,\,\bar{c}_0 \bar{L}  } } ,
\end{equation}
where $\Delta  := \max\mathop{}_k \normp{d_k}_2^2$, $\bar{c}_0 := \min(1,c_0)$ and $\bar{L} := \ebb(L)$.
\end{proposition}
A high probability bound could be derived using  concentration inequalities for  Poisson random variables~\cite{BouEtAl2013}; but the expectation already gives the right order of magnitude for the error (for example via Markov's inequality). 
The expression~\eqref{appSgp:fs} for an unbiased finite sample estimate $\wh{S}$ of $S$ and the expression~\eqref{appTgp:fs} for an unbiased finite sample estimate $\wh{T}$ of $T$ are defined\footnote{For completeness, we also present the finite sample estimates $\wh{S}$~\eqref{S:lda:fs} and $\wh{T}$~\eqref{T:lda:fs} of $S$~\eqref{S:lda} and $T$~\eqref{T:lda} for the LDA moments (which are consistent with the ones suggested in~\cite{AnaEtAl2014}) in Appendix~\ref{sec:lda:empirical}.
} 
in Appendix~\ref{sec:dica:fs}.
A sketch of a proof for Proposition~\ref{sample-complexity} can be found in Appendix~\ref{sec:sample-complexity}.

By following a similar analysis as in~\cite{AnaEtAl2013}, we can rephrase the topic recovery error in term of the error on the GP cumulant. Importantly, the whitening transformation (introduced in Section~\ref{sec:diag}) redivides the error on $S$~\eqref{eq:S_complexity} by $\bar{L}^2$, which is the scale of $S$ (see Appendix~\ref{section-analysis-of-whitening} for details). This means that the contribution from $\hat{S}$ to the recovery error will scale as $O({1}/{\sqrt{N}} \max\{\Delta, {\bar{c}_0}/{\bar{L}}\})$, where both $\Delta$ and ${\bar{c}_0}/{\bar{L}}$ are smaller than $1$ and can be very small. We do not present the exact expression for the expected squared error for the estimator of $T$, but due to a similar structure in the derivation, we expect the analogous bound of $\ebb[ \normp{ \wh{T} - T }_F ] \leq  {1}/{\sqrt{N}}\max\{ \Delta^{3/2} \bar{L}^3,\,\bar{c}_0^{3/2} \bar{L}^{3/2}  \}$.

Current sample complexity results of the LDA moments~\cite{AnaEtAl2012} can be summarized as $O(1/\sqrt{N})$. However, the proof (which can be found in the supplementary material~\cite{AnaEtAl2013}) analyzes only the case when finite sample estimates of the LDA moments are constructed from \emph{one} triple per document, i.e., $w_{1}\tp w_{2} \tp w_{3}$ only, and not from the U-statistics that average multiple (dependent) triples per document as in the practical expressions~\eqref{S:lda:fs} and~\eqref{T:lda:fs} (Appendix~\ref{sec:lda:empirical}). Moreover, one has to be careful when comparing upper bounds. Nevertheless, comparing the bound~\eqref{eq:S_complexity} with the current theoretical results for the LDA moments, we see that the GP/DICA cumulants sample complexity contains the $\ell_2$-norm of the columns of the topic matrix $D$ in the numerator, as opposed to the $O(1)$ coefficient for the LDA moments. This norm can be significantly smaller than $1$ for vectors in the simplex (e.g., $\Delta = O(1/\normp{d_k}_0)$ for sparse topics). This suggests that the GP/DICA cumulants may have better finite sample convergence properties than the LDA moments and our experimental results in Section~\ref{sec:exp:mom} are indeed consistent with this statement.

The GP/DICA cumulants have a somewhat more intuitive derivation than the LDA moments as they are expressed via the count vectors $x$ (which are the sufficient statistics for the model) and not the tokens $w_\ell$'s.
Note also that the construction of the LDA moments depend on the unknown parameter~$c_0$. Given that we are in an unsupervised setting and that moreover the evaluation of LDA is a difficult task~\cite{WalEtAl2009}, setting this parameter is non-trivial. In Appendix~\ref{sec:app:c0lda}, we observe experimentally that the LDA moments are somewhat sensitive to the choice of $c_0$.












\section{Diagonalization algorithms} \label{sec:diag} 

How is the diagonal structure~\eqref{diagS} of $S$ and~\eqref{diagT} of $T$ going to be helpful for the estimation of the model parameters? This question has already been thoroughly investigated in the signal processing (see, e.g.,~\cite{Car1989,Car1990,CarCom1996,Hyv1999,CarSou1993,ComJut2010} and references therein) and machine learning  (see~\cite{AnaEtAl2012,AnaEtAl2014} and references therein) literature. We review the approach in this section. Due to similar diagonal structure, the algorithms of this section apply to both the LDA moments and the GP/DICA cumulants.


For simplicity, let us rewrite the expressions~\eqref{diagS} and~\eqref{diagT} for $S$ and $T$ as follows
\begin{equation}  \label{ST}
\begin{aligned}
S  = \sum\mathop{}_{k} s_k d_k d_k^{\top}, \qquad
T  = \sum\mathop{}_{k} t_k d_k \tp d_k \tp d_k,
\end{aligned}
\end{equation}
where $s_k := \var(\ga_k)$ and $t_k := \cum(\ga_k,\ga_k,\ga_k)$.
Introducing the rescaled topics $\wt{d}_k :=  \sqrt{s_k} d_k$, we can also rewrite
$S = \wt{D} \wt{D}^{\top}$. Following the same assumption from~\cite{AnaEtAl2012} that the topic vectors are linearly independent ($\wt{D}$ is full rank), we can compute a whitening matrix $W\in\rr{K\times M}$ of $S$, i.e., a matrix such that $W S W^{\top} = I_{K}$ where $I_{K}$ is the $K$-by-$K$ identity matrix (see Appendix ~\ref{sec:whitening} for more details). As a result, the vectors $z_k := W\wt{d}_k$ form an orthonormal set of vectors.


Further, let us define a projection
 $\tcal(v)\in\rr{K\times K}$ of a tensor $\tcal\in\rr{K\times K\times K}$  onto a vector $u\in\rr{K}$:
\begin{equation}\label{projT}
\tcal(u)_{k_1 k_2} := \sum\mathop{}_{k_3} \tcal_{k_1k_2k_3} u_{k_3}.
\end{equation}
Applying the multilinear transformation (see, e.g.,~\cite{AnaEtAl2014} for the definition) with $W^{\top}$ to the tensor~$T$ from~\eqref{ST} and projecting the resulting tensor $\tcal :=T(W^{\top},W^{\top},W^{\top})$ onto some vector $u\in\rr{K}$, we obtain
\begin{equation}\label{orthT}
\tcal(u) = \sum\mathop{}_{k} \wt{t}_k \innerp{z_k,u} z_k z_k^{\top},
\end{equation}
where $\wt{t}_k := t_k / s_k^{3/2}$ is due to the rescaling of topics and $\innerp{\cdot,\cdot}$ stands for the inner product.  As the vectors $z_k$ are orthonormal, the pairs $z_k$ and $\gl_k :=\wt{t}_k\innerp{z_k,u}$ are the eigenpairs of the matrix $\tcal(u)$, which are uniquely defined if the eigenvalues $\gl_k$ are all different. If they are unique, we can recover the GP/DICA (as well as LDA) model parameters via $\wt{d}_k = W\pinv z_k$ and $\wt{t}_k = \gl_k / \innerp{z_k,u}$.


This procedure was referred to as
 the spectral algorithm for LDA~\cite{AnaEtAl2012} and the fourth-order\footnote{See Appendix~\ref{sec:on-the-orders-of-cumulants} for a discussion on the orders.}
blind identification algorithm for ICA~\cite{Car1989,Car1990}. Indeed, one can expect that the finite sample estimates
$\wh{S}$~\eqref{appSgp:fs} and $\wh{T}$~\eqref{appTgp:fs}
 possess approximately the diagonal structure~\eqref{diagS} and~\eqref{diagT} and, therefore, the reasoning from above can be applied, assuming that the effect of the sampling error is controlled.



This spectral algorithm, however, is known to be quite unstable in practice (see, e.g.,~\cite{Car1999}).
To overcome this problem, other algorithms were proposed. For ICA, the most notable ones are probably the FastICA algorithm~\cite{Hyv1999} and the JADE algorithm~\cite{CarSou1993}. The FastICA algorithm, with appropriate choice of a contrast function, estimates iteratively the topics, making use of the orthonormal structure~\eqref{orthT}, and performs the deflation procedure at every step. The recently introduced tensor power method (TPM) for the LDA model~\cite{AnaEtAl2014} is close  to the FastICA algorithm. Alternatively, the JADE algorithm modifies the spectral algorithm by performing \emph{multiple} projections  for~\eqref{orthT} and then jointly diagonalizing the resulting matrices with an orthogonal matrix. The spectral algorithm is a special case of this orthogonal joint diagonalization algorithm when only one projection is chosen. Importantly, a fast implementation~\cite{CarSou1996} of the orthogonal joint diagonalization algorithm from~\cite{BunEtAl1993} was proposed, which is based on closed-form iterative Jacobi updates (see, e.g.,~\cite{NocWri2006} for the later).

In practice, the orthogonal joint diagonalization (JD) algorithm is more robust than FastICA (see, e.g.,~\cite[p.~30]{BacJor2002}) or the spectral algorithm. Moreover, although the application of the JD algorithm for the learning of topic models was mentioned in the literature~\cite{AnaEtAl2014,KulEtAl2015}, it was never implemented in practice. In this paper, we apply 
the JD algorithm for the diagonalization of the GP/DICA cumulants as well as the LDA moments, which is described in Algorithm~\ref{alg:jd}. Note that the choice of a projection vector $v_p\in\rr{M}$ obtained as $v_p = \wh{W}^{\top} u_p$ for some vector $u_p \in \rr{K}$ is important and corresponds to the multilinear transformation of $\wh{T}$ with $\wh{W}^{\top}$ along the third mode. Importantly, in Algorithm~\ref{alg:jd}, the joint diagonalization routine is performed over $(P+1)$ matrices of size $K\!\times\!K$, where the number of topics $K$ is usually not too big. This makes the algorithm computationally fast (see Appendix~\ref{sec:code-and-compleixty}). The same is true for the spectral algorithm, but not for TPM.

\begin{algorithm}[th]
   \caption{Joint diagonalization (JD) algorithm for GP/DICA cumulants (or LDA moments)}
   \label{alg:jd}
\begin{algorithmic}[1]
         \STATE \emph{Input:} $X\in{\rr{M\times N}}$, $K$, $P$ (number of random projections); (and $c_0$ for LDA moments)
         \STATE Compute sample estimate $\wh{S}\in\rr{M\times M}$  (\eqref{appSgp:fs} for GP/DICA /~\eqref{S:lda:fs} for LDA in Appendix~\ref{sec:app:implementation})
	\STATE Estimate whitening matrix $\wh{W}\in\rr{K\times M}$ of $\wh{S}$  (see Appendix~\ref{sec:whitening})
	\STATEx 
	\emph{option (a):} Choose vectors $\cbra{u_1,u_2,\dots,u_P}\subseteq\rr{K}$ uniformly at random from the unit $\ell_2$-sphere and set $v_p = \wh{W}^{\top} u_p\in\rr{M}$ for all $p=1,\dots,P$ \quad \quad {\small($P=1$ yields the spectral algorithm)}
	\STATEx \emph{option (b):} Choose vectors $\cbra{u_1,u_2,\dots,u_P}\subseteq\rr{K}$ as the canonical basis $e_1,e_2,\dots,e_K$ of $\rr{K}$ and set $v_p = \wh{W}^{\top} u_p\in\rr{M}$ for all $p=1,\dots,K$
	\STATE For $\forall p$, compute $B_p = \wh{W}\wh{T}(v_p)\wh{W}^{\top}\in\rr{K\times K}$ (\eqref{est:T:dica} for GP/DICA /~\eqref{est:T:lda} for LDA; Appendix~\ref{sec:app:implementation})
	\STATE Perform orthogonal joint diagonalization  of  matrices
	$\cbras{\wh{W}\wh{S}\wh{W}^{\top}=I_K,\; B_p, \;p=1,\dots,P}$ (see~\cite{BunEtAl1993} and~\cite{CarSou1996})
	to find an orthogonal matrix $V\in\rr{K\times K}$ and vectors $\cbra{a_1,a_2,\dots,a_P}\subset\rr{K}$ such that
	$$
	V\wh{W}\wh{S}\wh{W}^{\top} V^{\top} = I_K, \;\mbox{and}\; VB_pV^{\top} \approx\diag(a_p), \; p =1,\dots,P
	$$
	\STATE Estimate joint diagonalization matrix $A=V \wh{W}$ and values $a_p$, $p=1,\dots,P$
	\STATE \emph{Output:} Estimate of $D$ and $c$ as described in Appendix~\ref{estimation-model-parameters} 
\end{algorithmic}
\end{algorithm}


In Section~\ref{sec:diagcmp}, we compare experimentally the performance of the spectral, JD, and TPM algorithms for the estimation of the parameters of the GP/DICA as well as LDA models. We are not aware of any experimental comparison of these algorithms in the LDA context. 
While already working on this manuscript, the JD algorithm was also independently analyzed by~\cite{KulEtAl2015} in the context of tensor factorization for general latent variable models.
However,~\cite{KulEtAl2015} focused mostly on the comparison of approaches for tensor factorization and their stability properties, with brief experiments using a latent variable model related but not equivalent to LDA for community detection. 
In contrast, we provide a detailed experimental comparison in the context of LDA in this paper, as well as propose a novel cumulant-based estimator.
Due to the space restriction 
the estimation of the topic matrix $D$ and the (gamma/Dirichlet) parameter $c$ are moved to  Appendix~\ref{estimation-model-parameters}.




















































\section{Experiments}\label{sec:exps}
In this section, (a) we compare experimentally the GP/DICA cumulants with the LDA moments and (b)
the spectral algorithm~\cite{AnaEtAl2012}, the tensor power method~\cite{AnaEtAl2014} (TPM), the joint diagonalization (JD) algorithm from Algorithm~\ref{alg:jd},
 and variational inference for LDA~\cite{BleEtAl2003}.


\emp{Real data:} the associated press (AP) dataset, from D.~Blei's web page,\footnote{\url{http://www.cs.columbia.edu/~blei/lda-c}} with $N = 2,243$ documents and $M = 10,473$ vocabulary words and the average document length $\wh{L}=194$; the NIPS papers dataset\footnote{\url{http://ai.stanford.edu/~gal/data}}~\cite{nips_data} of $2,483$ NIPS papers and $14,036$ words, and $\wh{L}=1,321$; the KOS dataset,\footnote{\url{https://archive.ics.uci.edu/ml/datasets/Bag+of+Words}} from the UCI Repository, with $3,430$ documents and $6,906$ words, and $\wh{L} = 136$. 


\emp{Semi-synthetic data} are constructed 
by analogy with~\cite{AroEtAl2013}: (1) the LDA parameters $D$ and~$c$ are learned from the real datasets with variational inference and (2) toy data are sampled from a model of interest with the given parameters $D$ and~$c$. This provides the ground truth parameters $D$ and~$c$. For each setting, data are sampled 5 times and the results are averaged. We plot error bars that are the minimum and maximum values. For the AP data, $K\in\{10,50\}$ topics are learned and, for the NIPS data, $K\in\{10,90\}$ topics are learned. 
For larger $K$, the obtained topic matrix is ill-conditioned, which violates the identifiability condition for topic recovery using moment matching techniques~\cite{AnaEtAl2012}. All the documents with less than $3$ tokens are resampled.


\emp{Sampling techniques.} All the sampling models have the parameter $c$ which is set to $c = c_0 \bar{c}/\norm{\bar{c}}_1$, where $\bar{c}$ is the learned $c$ from the real dataset with 
variational LDA, and $c_0$ is a parameter that we can vary.
The {\textit{GP}} data are sampled from the gamma-Poisson model~\eqref{gp} with $b=c_0/\wh{L}$ so that the expected document length is $\wh{L}$ (see Appendix~\ref{sec:L:gp}).
The {\textit{LDA-fix($L$)}} data are sampled from the LDA model~\eqref{lda} with the document length being fixed to a given $L$. The {\textit{LDA-fix2($\gamma$,$L_1$,$L_2$)}} data are sampled as follows: 
 $(1-\gamma)$-portion  of the documents are sampled from the \textit{LDA-fix($L_1$)} model with a given document length $L_1$ and $\gamma$-portion of the documents are sampled from the \textit{LDA-fix($L_2$)} model with a given document length $L_2$.


\emp{Evaluation.} The evaluation of topic recovery for semi-synthetic data is performed with the $\ell_1$-error between the recovered $\wh{D}$ and true $D$ topic matrices with the best permutation of columns:
$\text{err}_{\ell_1} (\wh{D},D) := \min_{\pi \in \mathrm{PERM}} \frac{1}{2K} \sum\mathop{}_{k} \| \wh{d}_{\pi_k} - d_{k}\|_1 \;\in [0,1]$.
The minimization is over the possible permutations $\pi \in \mathrm{PERM}$ of the columns of 
$\wh{D}$ and can be efficiently obtained with the Hungarian algorithm for bipartite matching.
For the evaluation of topic recovery in the real data case, we use an approximation of the log-likelihood for held out documents as the metric~\cite{WalEtAl2009}.
See Appendix~\ref{sec:evaluation-real} for more details.

We use our Matlab implementation of the GP/DICA cumulants, the LDA moments, and the diagonalization algorithms. The datasets and the code for reproducing our experiments are available online.\footnote{ \url{https://github.com/anastasia-podosinnikova/dica}} In Appendix~\ref{sec:code-and-compleixty}, we discuss the complexity and implementation of the algorithms.
We explain how we initialize the parameter $c_0$ for the LDA moments in Appendix~\ref{sec:initialization-c0}.













\subsection{Comparison of the diagonalization algorithms}\label{sec:diagcmp}
In Figure~\ref{plots:diag}, we compare the diagonalization algorithms on the semi-synthetic AP dataset for $K=50$ using the GP  sampling. We compare the tensor power method (TPM)~\cite{AnaEtAl2014}, the spectral algorithm (Spec), the orthogonal joint diagonalization algorithm (JD) described in Algorithm~\ref{alg:jd} with different options to choose the random projections:
JD(k) takes $P=K$ vectors $u_p$ sampled uniformly from the unit $\ell_2$-sphere in $\rr{K}$ and selects $v_p = W^{\top} u_p$ (option (a) in Algorithm~\ref{alg:jd}); JD selects the full basis $e_1,\dots,e_K$ in $\rr{K}$ and sets $v_p = W^{\top} e_p$ (as JADE~\cite{CarSou1993}) (option (b) in Algorithm~\ref{alg:jd}); $JD(f)$ chooses the full canonical basis of $\rr{M}$ as the projection vectors (computationally expensive). 
\begin{figure}[t]
\centering
\begin{tabular}{cccc}
\includegraphics[clip=true, trim=0em 1em 0em 1em,width=.36\columnwidth]{fig_1_left_l1.eps} 
 & 
 
 &
 
 &
\includegraphics[clip=true, trim=0em 1em 0em 1em,width=.36\columnwidth]{fig_1_right_l1.eps} 
\end{tabular}
\vspace{-1em}
\caption{
Comparison of the diagonalization algorithms. 
The topic matrix $D$ and Dirichlet parameter $c$ are learned for $K=50$ from AP; $c$ is scaled to sum up to $0.5$ 
and $b$ is set to fit the expected document length $\wh{L}=200$. The semi-synthetic dataset is sampled from \textit{GP}; number of documents $N$ varies from $1,000$ to $50,000$. \emp{Left:} GP/DICA moments. \emp{Right:} LDA moments.  \textit{Note}: a smaller value of the $\ell_1$-error is better. 
}
\label{plots:diag}
\vspace{-1.5em}
\end{figure}

Both the GP/DICA cumulants and LDA moments are well-specified in this setup. However, the LDA moments have a slower finite sample convergence and, hence, a larger estimation error for the same value $N$. As expected, the spectral algorithm is always slightly inferior to the joint diagonalization algorithms. With the GP/DICA cumulants, where the estimation error is low, all algorithms demonstrate good performance, which also fulfills our expectations. 
However, although TPM shows almost perfect performance in the case of the GP/DICA cumulants (left), it significantly deteriorates for the LDA moments (right), which can be explained by the larger estimation error of the LDA moments and lack of robustness of TPM. 
The running times are discussed in Appendix~\ref{sec:runtimes}.
Overall, the orthogonal joint diagonalization algorithm with initialization of random projections as $W^{\top}$ multiplied with the canonical basis in $\rr{K}$ (JD) is both computationally efficient and fast.







\subsection{Comparison of the GP/DICA cumulants and the LDA moments}\label{sec:exp:mom}
In Figure~\ref{plot:moms}, when sampling from the \textit{GP} model (top, left), both the GP/DICA cumulants and LDA moments are well specified, which implies that the approximation error (i.e., the error w.r.t. the model (mis)fit) is low for both. The GP/DICA cumulants achieve low values of the estimation error already for $N=10,000$ documents independently of the number of topics, while the convergence is slower for the LDA moments.
When sampling from the \textit{LDA-fix(200)} model (top, right), the GP/DICA cumulants are mis-specified and their approximation error is high, although the estimation error is low due to the faster finite sample convergence.  One reason of poor performance of the GP/DICA cumulants, in this case, is the absence of variance in the document length. Indeed, if documents with two different lengths are mixed by sampling from the \textit{LDA-fix2(0.5,20,200)} model (bottom, left), the GP/DICA cumulants performance improves. Moreover, the experiment with a changing fraction $\gamma$ of documents (bottom, right) shows that a non-zero variance on the length improves the performance of the GP/DICA cumulants. As in practice real corpora usually have a non-zero variance for the document length, this bad scenario for the GP/DICA cumulants is not likely to happen.
\begin{figure}[!h]
\centering
\begin{tabular}{cccc}
\includegraphics[clip=true, trim=0em 1em 0em 1em,width=.36\columnwidth]{fig_2_top_left_l1.eps} 
 & 
 
 &
 
 &
\includegraphics[clip=true, trim=0em 1em 0em 1em,width=.36\columnwidth]{fig_2_top_right_l1.eps} 
\\ 
\includegraphics[clip=true, trim=0em 1em 0em 1em,width=.36\columnwidth]{fig_2_bottom_left_l1.eps}  
&

&

&
\includegraphics[clip=true, trim=0em 1em 0em 1em,width=.36\columnwidth]{fig_2_bottom_right_l1.eps} 
\end{tabular}
\vspace{-1em}
\caption{ Comparison of the GP/DICA cumulants and LDA moments. Two topic matrices and parameters $c_1$ and $c_2$ are learned from the NIPS dataset for $K=10$ and $90$; $c_1$ and $c_2$ are scaled to sum up to $c_0=1$.  Four corpora of different sizes $N$ from $1,000$ to $50,000$: \emp{top, left:} $b$ is set to fit the expected document length $\wh{L} = 1300$; sampling from the \textit{GP} model; \emp{top, right:} sampling from the \textit{LDA-fix(200)} model; \emp{bottom, left:} sampling from the \textit{LDA-fix2(0.5,20,200)} model. \emp{Bottom, right:} the number of documents here is fixed to $N = 20,000$; sampling from the \textit{LDA-fix2($\gamma$,20,200)} model varying the values of the fraction $\gamma$ from $0$ to $1$ with the step~$0.1$. \textit{Note}: a smaller value of the $\ell_1$-error is better. }
\label{plot:moms}
\vspace{-1em}
\end{figure}









\subsection{Real data experiments}\label{section-real-experiments}
In Figure~\ref{lastfigure}, JD-GP, Spec-GP, JD-LDA, and Spec-LDA are compared with variational inference (VI) and with variational inference initialized with the output of JD-GP (VI-JD). We measure the held out log-likelihood per token (see Appendix~\ref{sec-expsup-real} for details on the experimental setup). The orthogonal joint diagonalization algorithm with the GP/DICA cumulants (JD-GP) demonstrates promising performance.
In particular, the GP/DICA cumulants significantly outperform the LDA moments. Moreover, although variational inference performs better than the JD-GP algorithm, restarting variational inference with the output of the JD-GP algorithm systematically leads to better results. Similar behavior has already been observed (see, e.g.,~\cite{CohCol2014}).
\begin{figure}[t!]
\begin{center}
\begin{tabular}{cccc}
\includegraphics[clip=true, trim=0em 1em 0em 1em,width=.36\columnwidth]{fig_3_left_ap.eps} 
 & 
\includegraphics[width=.14\columnwidth]{real_legend.eps} 
 & 
\includegraphics[clip=true, trim=0em 1em 0em 1em,width=.36\columnwidth]{fig_3_right_kos.eps} 
\end{tabular}
\vspace{-1em}
\caption{ 
Experiments with real data. \emp{Left:} the AP dataset. \emp{Right:} the KOS dataset. \textit{Note}: a higher value of the log-likelihood is better.
}
\label{lastfigure}
\end{center}
\vspace{-2em}
\end{figure}










\section{Conclusion}
In this paper, we have proposed a new set of tensors for a discrete ICA model related to LDA, where word counts are directly modelled. These moments make fewer assumptions regarding distributions, and are theoretically and empirically more robust than previously proposed tensors for LDA, both on synthetic and real data. Following the ICA literature, we showed that our joint diagonalization procedure is also more robust. Once the topic matrix has been estimated in a semi-parametric way where topic intensities are left unspecified, it would be interesting to learn the unknown distributions of the independent topic intensities.




\emp{Aknowledgements.} This work was partially supported by the MSR-Inria Joint Center. The authors would like to thank Christophe Dupuy for helpful discussions.
























\subsubsection*{References}
\renewcommand\refname{\vspace{-2em}}
\bibliographystyle{unsrt}
{\small\bibliography{lit}}


















































































\newpage
\appendix
\renewcommand{\thesubsection}{\Alph{subsection}}
\subsection{Appendix. Plate diagrams for the models from Section~\ref{sec:2}}\label{sec:pds}

\begin{figure}[htp]
\begin{subfigure}{.24\textwidth}
\centering
\begin{tikzpicture}
[
observed/.style={minimum size=28pt,circle,draw=black,fill=black!10},
unobserved/.style={minimum size=28pt,circle,draw},
arrow/.style={->,>=stealth',semithick},
]
\node (wnl) [observed] at (0,0) {$w_{n\ell}$};
\node (znl) [unobserved] at (0,1.4) {$z_{n\ell}$};
\node (theta) [unobserved] at (0,3.3) {$\theta_n$};
\node (c) [label=above:$c$] at (0,4.9) {};
\filldraw [black] (0,4.9) circle (3pt);
\node (D) [label=above:$D$,inner sep=2pt,outer sep=0pt] at (-1.5,1.5) {};
\filldraw [black] (-1.5,1.5) circle (3pt);
\path
(znl)   edge [arrow] (wnl)
(c)     edge [arrow] (theta)
(theta) edge [arrow] (znl)
(D)     edge [arrow] (wnl)
;
\node [draw,fit=(wnl) (theta), inner sep=18pt] (plate-context) {};
\node [below left] at (plate-context.north east) {$N$};
\node [draw,fit=(wnl) (znl), inner sep=14pt] (plate-token) {};
\node [below left] at (plate-token.north east) {$L_n$};
\end{tikzpicture}
\caption{LDA~\eqref{lda-tokens-n}}\label{pd-lda-tokens}
\end{subfigure}% 
\begin{subfigure}{.24\textwidth}
\centering
\begin{tikzpicture}
[
observed/.style={minimum size=28pt,circle,draw=black,fill=black!10},
unobserved/.style={minimum size=28pt,circle,draw},
arrow/.style={->,>=stealth',semithick},
]
\node (xn) [observed] at (0,0) {$x_n$};
\node (theta) [unobserved] at (0,3.3) {$\theta_n$};
\node (c) [label=above:$c$] at (0,4.9) {};
\filldraw [black] (0,4.9) circle (3pt);
\node (D) [label=above:$D$,inner sep=2pt,outer sep=0pt] at (-1.5,1.5) {};
\filldraw [black] (-1.5,1.5) circle (3pt);
\path
(c)     edge [arrow] (theta)
(theta) edge [arrow] (xn)
(D)     edge [arrow] (xn)
;
\node [draw,fit=(xn) (theta), inner sep=18pt] (plate-context) {};
\node [below left] at (plate-context.north east) {$N$};
\end{tikzpicture}
\caption{LDA~\eqref{lda-n}}\label{pd-lda} 
\end{subfigure}%
\begin{subfigure}{.24\textwidth}
\centering
\begin{tikzpicture}
[
observed/.style={minimum width=28pt,circle,draw=black,fill=black!10},
unobserved/.style={minimum size=28pt,circle,draw},
arrow/.style={->,>=stealth',semithick},
]
\node (xn) [observed] at (0,0) {$x_{nm}$};
\node (alpha) [unobserved] at (0,3.3) {$\alpha_n$};
\node (c) [label=above:$c$] at (0,4.9) {};
\filldraw [black] (0,4.9) circle (3pt);
\node (D) [label=above:$D$,inner sep=2pt,outer sep=0pt] at (-1.5,1.5) {};
\filldraw [black] (-1.5,1.5) circle (3pt);
\path
(c)     edge [arrow] (alpha)
(alpha) edge [arrow] (xn)
(D)     edge [arrow] (xn)
;
\node [draw,fit=(xn) (alpha), inner sep=18pt] (plate-context) {};
\node [below left] at (plate-context.north east) {$N$};
\node [draw,fit=(xn), inner sep=14pt] (plate-context) {};
\node [below left] at (plate-context.north east) {$M$};
\end{tikzpicture}
\caption{GP~\eqref{gp-n}}\label{pd-gp} 
\end{subfigure} 
\begin{subfigure}{.24\textwidth}
\centering
\begin{tikzpicture}
[
observed/.style={minimum width=28pt,circle,draw=black,fill=black!10},
unobserved/.style={minimum size=28pt,circle,draw},
arrow/.style={->,>=stealth',semithick},
]
\node (xn) [observed] at (0,0) {$x_{nm}$};
\node (alpha) [unobserved] at (0,3.3) {$\alpha_n$};
\node (D) [label=above:$D$,inner sep=2pt,outer sep=0pt] at (-1.5,1.5) {};
\node (c) [label=above:$ $] at (0,4.9) {};
\filldraw [white] (0,4.9) circle (3pt);
\filldraw [black] (-1.5,1.5) circle (3pt);
\path
(alpha) edge [arrow] (xn)
(D)     edge [arrow] (xn)
;
\node [draw,fit=(xn) (alpha), inner sep=18pt] (plate-context) {};
\node [below left] at (plate-context.north east) {$N$};
\node [draw,fit=(xn), inner sep=14pt] (plate-context) {};
\node [below left] at (plate-context.north east) {$M$};
\end{tikzpicture}
\caption{DICA~\eqref{dica-n}}\label{pd-dica} 
\end{subfigure} 
\caption{Plate diagrams for the models from Section~\ref{sec:2}.} 
\end{figure}

In Section~\ref{sec:2}, the index $n$, which stands for the $n$-th document, was omitted. For convenience, we recall the models.
The LDA model in the tokens representation:
\begin{equation}\label{lda-tokens-n}
  \begin{aligned}
    \theta_n &\sim \dir(c), \\
    z_{n\ell}|\theta_n &\sim \mul(1,\theta_n), \\
    w_{n\ell} | z_{n\ell},\theta_n &\sim \mul(1,d_{z_{n\ell}});
  \end{aligned}
\end{equation}
the LDA model with the marginalized out latent variable $z$:
\begin{equation} \label{lda-n}
  \begin{aligned}
    \theta_n &\sim \dir(c), \\
    x_n|\theta_n &\sim \mul(L_n,D\theta_n);
  \end{aligned}
\end{equation}
the GP model:
\begin{equation}\label{gp-n}
  \begin{aligned}
	\ga_{nk} &\sim \gam(c_k,b), \\
	x_{nm}|\ga_n &\sim \poi([D\ga_n]_m);
  \end{aligned}
\end{equation}
and the DICA model:
\begin{equation}\label{dica-n}
  \begin{aligned}
	\ga_{n1},\dots,\ga_{nK} &\sim \text{mutually independent}, \\
	x_{nm}|\ga_n & \sim \poi([D\ga_n]_m).
  \end{aligned}
\end{equation}

\subsection{Appendix. The GP model}
\subsubsection{The connection between the LDA and GP models} \label{sec:ldaproof2}
To show that the LDA model~\eqref{lda} with the additional assumption that the document length is modeled as a gamma-Poisson random variable is equivalent to the GP model~\eqref{gp}, we show that:
\begin{mi}
\item when modeling the document length $L$ as a Poisson random variable with a parameter $\gl$, the count vectors $x_{1}$, $x_{2}$, $\dots$, $x_{M}$ are mutually independent Poisson random variables;
\item the Gamma prior on $\gl$ reveals the connection $\ga_{k} = \gl \gt_{k}$ between the Dirichlet random variable $\gt$ and the mutually independent gamma random variables $\ga_{1}$, $\ga_{2}$, $\dots$, $\ga_{K}$.
\end{mi}

For completeness, we repeat the known result that if $L \sim \poi(\lambda)$ and $x | L \sim \mul(L,D\theta)$ (which thus means that $L = \sum_m x_{m}$ with probability one), then $x_{1}$, $x_{2}$, $\dots$, $x_{M}$ are mutually independent Poisson random variables with parameters $\gl\sbra{D\gt}_1$, $\gl\sbra{D\gt}_2$, $\dots$, $\gl\sbra{D\gt}_M$. Indeed, we consider the following joint probability mass function where $x$ and $L$ are assumed to be non-negative integers:
\begin{ma}
p(x, L|\gt,\gl) =& p(L|\gl) p(x|L,\gt) \\
=& \indicator{ L=\sum_m x_{m} } \, \frac{\exp\rbra{-\gl}\gl^{L}}{\cancel{L!}}\frac{\cancel{L!}}{\prod_m x_{m}!} \prod_m \sbra{D\gt}_m^{x_{m}}  \\
=& \indicator{ L=\sum_m x_{m}} \, \exp (-\gl \sum_m \sbra{D\gt}_m) \gl^{\sum_m x_{m}} \prod_m \frac{ \sbra{D\gt}_m^{x_{m}}}{x_{m}!}  \\
=& \indicator{ L=\sum_m x_{m}} \, \prod_m \frac{ \exp(-\gl\sbra{D\gt}_m) (\gl \sbra{D\gt}_m)^{x_{m}} } {x_{m}!} \\
=& \indicator{ L=\sum_m x_{m}} \, \prod_m \poi(x_{m}; \gl \sbra{D\gt}_m),
\end{ma}
where in the third equation we used the fact that 
$$\sum_m \sbra{D\gt}_m = \sum_{m,k}D_{mk}\gt_k=\sum_k\gt_k \sum_mD_{mk}=1.$$ 
We thus have $p(x,L | \theta, \lambda) = p(L|x) \prod_{m} p(x_m | \lambda [D\theta]_m)$ where $p(L|x)$ is simply the deterministic distribution $\indicator{ L=\sum_m x_{m}}$ and $p(x_m | \lambda [D\theta]_m)$ for $m=1, \ldots, M$ are independent $\poi(\lambda [D\theta]_m)$ distributions (and thus do not depend on $L$).
Note that in the notation introduced in the paper, $D_{mk} = d_{km}$.
Hence, by using the construction of the Dirichlet distribution from the normalization of independent gamma random variables, we can show that the LDA model with a gamma-Poisson prior over the length is equivalent to the following model (recall, that  $c_0 = \sum_k  c_k$):
\begin{equation}\label{intermodel}
\begin{aligned}
\gl \sim & \;\gam(c_0,b), \\
\gt \sim& \;\dir(c), \\
x_{m}|\gl, \theta \sim &\; \poi(\sbra{D(\gl\gt)}_m).
\end{aligned}
\end{equation}

More specifically, we complete the second part of the argument with the following properties. When $\ga_{1}$, $\ga_{2}$, $\dots$, $\ga_{K}$ are mutually independent gamma random variables, each $\ga_{k}\sim\gam(c_k,b)$, their sum is also a gamma random variable $\sum_k \ga_{k} \sim\gam(\sum_k c_k,b)$. The former is equivalent to $\gl$. 
It is known (e.g.,~\citesup{FriEtAl2010}) that a Dirichlet random variable can be sampled by first sampling independent gamma random variables ($\ga_k$) and then dividing each of them by their sum ($\gl$): $\theta_k = \ga_k / \sum_{k'} \ga_{k'}$, and, in other direction, the variables $\alpha_k = \gl \theta_k$ are mutually independent, giving back the GP model~\eqref{gp}.






\subsubsection{The expectation and the variance of the document length for the GP model}\label{sec:L:gp}
From the drivations in Appendix~\ref{sec:ldaproof2}, it follows that the document length of the GP model~\eqref{gp} is a gamma-Poisson random variable, i.e., $L|\gl \sim\poi(\gl)$ and $\gl \sim \gam(c_0,b)$. Therefore, the following follows from the law of total expectation and the law of total variance
\begin{ma}
\ebb(L) &=\ebb\sbra{\ebb(L|\gl)} = \ebb(\gl)=c_0/b \\
\var(L) &= \var \sbra{\ebb(L|\gl)} + \ebb\sbra{\var(L|\gl)}=\var(\gl)+\ebb(\gl) = c_0/b+c_0/b^2
\end{ma}
The first expression shows that the parameter $b$ controls the expected document length $\ebb(L)$ for a given parameter $c_0$: the smaller $b$, the larger $\ebb(L)$. On the other hand, if we allow $c_0$ to vary as well, only the ratio $c_0/b$ is important for the document length. We can then interpret the role of $c_0$ as actually controlling the concentration of the distribution for the length $L$ (through the variance). More specifically, we have that:
\begin{equation} \label{eq:concentrationL_GP}
\frac{\var(L)}{(\ebb(L))^2} = \frac{1}{\ebb(L)} + \frac{1}{c_0}.
\end{equation}
For a fixed target document length $\ebb(L)$, we can increase the variance (and thus decrease the concentration) by using a smaller $c_0$.



























\subsection{Appendix. The cumulants of the GP and DICA models} \label{sec:app:dicacum}
\subsubsection{Cumulants}\label{sec:cumulants}
For a random vector $x\in\rr{M}$,  the first three cumulant tensors\footnote{Strictly speaking, the (scalar) $n$-th cumulant $\kappa_n$ of a random variable $X$ is defined via the cumulant-generating function $g(t)$, which is the natural logarithm of the moment-generating function, i.e $g(t) := \log\ebb\sbra{e^{tX}}$. The cumulant $\kappa_n$ is then obtained from a power series expansion of the cumulant-generating function, that is $g(t) = \sum_{n=1}^{\infty}\kappa_n{t^n}/{n!}$ [Wikipedia].} are
\begin{ma}
\cum(x_m) &= \ebb(x_m), \\
\cum(x_{m_1},x_{m_2}) &= \ebb\sbra{(x_{m_1}-\ebb(x_{m_1}))(x_{m_2}-\ebb(x_{m_2})} = \cov(x_{m_1},x_{m_2}), \\
\cum(x_{m_1},x_{m_2},x_{m_3}) &= \ebb\sbra{(x_{m_1}-\ebb(x_{m_1}))(x_{m_2}-\ebb(x_{m_2}
))(x_{m_3}-\ebb(x_{m_3})))}.
\end{ma}
Note that the 2nd and 3rd cumulants coincide with the 2nd and 3rd central moments (but not for higher orders).
In the following, $\cum(x,x,x)\in\rr{M \times M\times M}$ denotes the third order tensor with elements $\cum(x_{m_1},x_{m_2},x_{m_3})$.
Some of the properties of cumulants are listed below (see~\cite[chap.~5]{ComJut2010}). The most important property that motivate us to use cumulants in this paper (and the ICA literature) is the \textbf{independence} property, which says that the cumulant tensor for a random vector with independent components is diagonal (this property \emph{does not} hold for the (non-central) moment tensors of any order, and neither for the central moments of order 4 or more).
\begin{mi}
\item \emp{Independence.} If the elements of $x\in\rr{M}$ are independent, then their cross-cumulants are zero as soon as two indices are different, i.e., $\cum(x_{m_1},x_{m_2})=\gd(m_1,m_2)\ebb[(x_{m_1}-\ebb_{m_1}))^2]$ and $\cum(x_{m_1},x_{m_2},x_{m_3})=\gd(m_1,m_2,m_3)\ebb[(x_{m_1}-\ebb(x_{m_1}))^3]$, where $\gd$ is the Kronecker delta.
\item \emp{Multilinearity.} If two random vectors $y\in\rr{M}$ and $\ga\in\rr{K}$ are linearly dependent, i.e., $y=D\ga$ for some $D\in\rr{M\times K}$, then
\begin{ma}
\cum(y_m) & = \sum_k \cum(\ga_k) D_{mk},\\
\cum(y_{m_1},y_{m_2}) &= \sum_{k_1,k_2} \cum(\ga_{k_1},\ga_{k_2}) D_{m_1k_1}D_{m_2k_2},\\
\cum(y_{m_1},y_{m_2},y_{m_3}) &= \sum_{k_1,k_2,k_3} \cum(\ga_{k_1},\ga_{k_2},\ga_{k_3}) D_{m_1k_1}D_{m_2k_2}D_{m_3k_3},
\end{ma}
which can also be denoted\footnote{In~\cite{AnaEtAl2014}, given a tensor $T\in\rr{K\times K\times K}$, $T(D^{\top},D^{\top},D^{\top})$ is referred to as the multilinear map. In~\citesup{KolBad2009}, the same entity is denoted by $T\times_1 D^{\top} \times_2 D^{\top} \times_3 D^{\top}$, where $\times_n$ denotes the $n$-mode tensor-matrix product.} 
by 
\begin{ma}
\ebb(y) &= D\ebb(\ga),\\
\cov(y,y) &=D\cov(\ga,\ga)D^{\top},\\
 \cum(y,y,y)&=\cum(\ga,\ga,\ga)(D^{\top},D^{\top},D^{\top}).
\end{ma}
\item \emp{The law of total cumulance.} For two random vectors $x\in\rr{M}$ and $y\in\rr{M}$, it holds
\begin{ma}
\cum(x_m) &= \ebb\sbra{\ebb(x_m|y)}, \\
\cum(x_{m_1},x_{m_2}) &=  \ebb\sbra{\cov(x_{m_1},x_{m_2}|y)} + \cov\sbra{\ebb(x_{m_1}|y),\ebb(x_{m_2}|y)}, \\
\cum(x_{m_1},x_{m_2},x_{m_3}) &= \ebb\sbra{\cum(x_{m_1},x_{m_2},x_{m_3}|y)} + \cum\sbra{\ebb(x_{m_1}|y),\ebb(x_{m_2}|y),\ebb(x_{m_3}|y)} \\
&+ \cov\sbra{\ebb(x_{m_1}|y),\cov(x_{m_2},x_{m_3}|y)} \\
&+ \cov\sbra{\ebb(x_{m_2}|y),\cov(x_{m_1},x_{m_3}|y)} \\
&+ \cov\sbra{\ebb(x_{m_3}|y),\cov(x_{m_1},x_{m_2}|y)}.
\end{ma}
Note that the first expression is also well known as the law of total expectation or the tower property, while the second one is known as the law of total covariance.
\end{mi}




\subsubsection{The third cumulant of the GP/DICA models} \label{sec:app:3dicacum}
In this section, by analogy with Section~\ref{sec:cum}, we derive the third GP/DICA cumulant.

As the third cumulant of a Poisson random variable $x_m$ with parameter $y_m$ is $\ebb((x_{m}-\ebb(x_{m}))^3|y_{m}) = y_{m}$, then by the independence property of cumulants from Section~\ref{sec:cumulants}, the cumulant of $x|y$ is diagonal:
$$
\cum(x_{m_1},x_{m_2},x_{m_3}|y)=\gd(m_1,m_2,m_3)\, y_{m_1}.
$$
Substituting the cumulant of $x|y$ into the law of total cumulance, we obtain
\begin{align}
&\cum( x_{m_1}, x_{m_2},x_{m_3}) = \ebb\sbra{\cum(x_{m_1},x_{m_2},x_{m_3}|y)} \notag \\
 & \quad+ \cum\sbra{\ebb(x_{m_1}|y),\ebb(x_{m_2}|y),\ebb(x_{m_3}|y)} 
  + \cov\sbra{\ebb( x_{m_1}|y),\cov(x_{m_2},x_{m_3}|y)}  \notag \\
 &\quad + \cov\sbra{\ebb(x_{m_2}|y),\cov(x_{m_1},x_{m_3}|y)} 
  + \cov\sbra{\ebb(x_{m_3}|y),\cov(x_{m_1},x_{m_2}|y)}  \notag \\
 &= \gd(m_1,m_2,m_3) \ebb(y_{m_1}) + \cum(y_{m_1},y_{m_2},y_{m_3})  \notag \\
 &\quad+ \gd(m_2,m_3)\cov(y_{m_1},y_{m_2}) 
 + \gd(m_1,m_3) \cov(y_{m_1},y_{m_2}) 
 + \gd(m_1,m_2) \cov(y_{m_1},y_{m_3})  \notag \\
 &= \gd(m_1,m_2,m_3) \ebb(x_{m_1}) + \cum(y_{m_1},y_{m_2},y_{m_3})  \notag \\
 &\quad + \gd(m_2,m_3) \cov(x_{m_1},x_{m_2}) - \gd(m_1,m_2,m_3) \ebb(x_{m_1})  \notag \\
 &\quad + \gd(m_1,m_3) \cov(x_{m_1},x_{m_2}) - \gd(m_1,m_2,m_3)\ebb(x_{m_1}) \notag \\
 &\quad + \gd(m_1,m_2) \cov(x_{m_1},x_{m_3}) - \gd(m_1,m_2,m_3)\ebb(x_{m_1})  \notag \\
 &= \cum(y_{m_1},y_{m_2},y_{m_3}) - 2\gd(m_1,m_2,m_3)\ebb(x_{m_1})   \notag \\
 &\quad + \gd(m_2,m_3) \cov(x_{m_1},x_{m_2}) 
 + \gd(m_1,m_3) \cov(x_{m_1},x_{m_2}) 
 + \gd(m_1,m_2) \cov(x_{m_1},x_{m_3})  \notag \\
 &= \sbra{\cum(\ga,\ga,\ga)(D^{\top},D^{\top},D^{\top})}_{m_1m_2m_3} - 2\gd(m_1,m_2,m_3) \ebb(x_{m_1})   \notag \\
 &\quad + \gd(m_2,m_3) \cov(x_{m_1},x_{m_2}) 
 + \gd(m_1,m_3) \cov(x_{m_1},x_{m_2}) 
 + \gd(m_1,m_2) \cov(x_{m_1},x_{m_3}),
 \label{cum3}
\end{align}
where, in the third equality, we used the previous result from~\eqref{cum2} that $\cov(y,y) = \cov(x,x) - \diag(\ebb(x))$.




\subsubsection{The diagonal structure of the GP/DICA cumulants} \label{sec:app:dicacum2}
In this section, we provide detailed derivation of the diagonal structure~\eqref{diagS} of the matrix $S$~\eqref{S} and the diagonal structure~\eqref{diagT} of the tensor $T$~\eqref{T}. 

From the independence of $\ga_{1},\ga_{2},\dots,\ga_{K}$ and by the independence property of cumulants from Section~\ref{sec:cumulants},
it follows that  $\cov(\ga,\ga)$ is a diagonal matrix and $\cum(\ga,\ga,\ga)$ is a diagonal tensor, i.e., $\cov(\ga_{k_1},\ga_{k_2})=\gd(k_1,k_2)\cov(\ga_{k_1},\ga_{k_2})$ and 
 $\cum(\ga_{k_1},\ga_{k_2},\ga_{k_3})=\gd(k_1,k_2,k_3) \cum(\ga_{k_1},\ga_{k_1},\ga_{k_1})$. Therefore, the following holds
\begin{ma}
\cov(y_{m_1},y_{m_2}) &= \sum_k  \cov(\ga_k,\ga_k) D_{m_1k}D_{m_2k}, \\
\cum(y_{m_1},y_{m_2},y_{m_3}) &= \sum_k \cum(\ga_k,\ga_k,\ga_k) D_{m_1k}D_{m_2k}D_{m_3k}, 
\end{ma}
which we can rewrite in a matrix/tensor form as
\begin{ma}
\cov(y,y) &= \sum_k \cov(\ga_k,\ga_k) d_k d_k^{\top}, \\
\cum(y,y,y) &= \sum_k \cum(\ga_k,\ga_k,\ga_k) d_k\tp d_k\tp d_k.
\end{ma}
 Moving $\cov(y,y)$ / $\cum(y,y,y)$ in the expression for $\cov(x,x)$~\eqref{cum2} / $\cum(x,x,x)$~\eqref{cum3} on one side of equality and all other terms on the other side, we define matrix $S\in\rr{M\times M}$  / tensor $T\in\rr{M\times M\times M}$ as follows
\begin{align}
\label{appSgp} S &:=\cov(x,x) - \diag\rbra{\ebb(x)}, \\
\nonumber T_{m_1m_2m_3} &:=  \cum(x_{m_1},x_{m_2},x_{m_3})+ 2\gd(m_1,m_2,m_3)\ebb(x_{m_1}) \\
\nonumber &- \gd(m_2,m_3)\cov(x_{m_1},x_{m_2})\\
\nonumber & - \gd(m_1,m_3)\cov(x_{m_1},x_{m_2})\\
\label{appTgp}& - \gd(m_1,m_2) \cov(x_{m_1},x_{m_3}).
\end{align}
By construction, $S=\cov(y,y)$ and $T=\cum(y,y,y)$ and, therefore, it holds that
\begin{align}
\label{appSdiaggp} S =& \sum_k \cov(\ga_k,\ga_k) d_k d_k^{\top}, \\
\label{appTdiaggp} T =& \sum_k \cum(\ga_k,\ga_k,\ga_k) d_k\tp d_k\tp d_k.
\end{align}
This means that both the matrix $S$ and the tensor $T$ are sums of rank-1 matrices and tensors, respectively\footnote{For tensors, such decomposition is also known under the names CANDECOMP/PARAFAC or, simply, the CP decomposition (see, e.g.,~\citesup{KolBad2009}). }. This structure of the matrix $S$ and the tensor $T$ is the basis for the algorithms considered in this paper.




\subsubsection{Unbiased finite sample estimators for the GP/DICA cumulants} \label{sec:dica:fs}
Given a sample $\cbra{x_1,x_2,\dots,x_N}$, we obtain a finite sample estimate $\wh{S}$ of $S$~\eqref{S} / $\wh{T}$ of $T$~\eqref{T} for the GP/DICA cumulants:
\begin{align}
\label{appSgp:fs} \wh{S} &:=\wcov(x,x) - \diag\rbra{\webb(x)}, \\
\nonumber \wh{T}_{m_1m_2m_3} &:=  \wcum(x_{m_1},x_{m_2},x_{m_3})+ 2\gd(m_1,m_2,m_3)\webb(x_{m_1}) \\
\nonumber &- \gd(m_2,m_3)\wcov(x_{m_1},x_{m_2})\\
\nonumber & - \gd(m_1,m_3)\wcov(x_{m_1},x_{m_2})\\
\label{appTgp:fs}& - \gd(m_1,m_2) \wcov(x_{m_1},x_{m_3}),
\end{align}
where unbiased estimators of the first three cumulants are
\begin{equation}\label{cum:empirical}
\begin{aligned}
\wh{\ebb}(x_{m_1}) =& \frac{1}{N} \sum_n x_{nm_1}, \\
\wh{\cov}(x_{m_1},x_{m_2}) =& \frac{1}{N-1} \sum_n z_{nm_1}  z_{nm_2}, \\
\wh{\cum}(x_{m_1},x_{m_2},x_{m_3}) =& \frac{N}{(N-1)(N-2)} \sum_n z_{nm_1}  z_{nm_2}  z_{nm_3},
\end{aligned}
\end{equation}
where the word vocabulary indexes are $m_1, m_2, m_3 = 1,2,\dots,M$ and the centered documents $z_{nm} := x_{nm} - \wh{\ebb}(x_m)$. (The latter is introduced only for compact representation of~\eqref{cum:empirical} and is different from $z$ in the LDA model.)



\subsubsection{On the orders of cumulants}\label{sec:on-the-orders-of-cumulants}
Note that the factorization of $S = \wt{D}\wt{D}^{\top}$ does not uniquely determine $\wt{D}$ as one can equivalently use $S = (\wt{D}U)(\wt{D}U)^{\top}$ with any orthogonal $K \times K$ matrix $U$. Therefore, one has to consider higher than the second order information. Moreover, in ICA the fourth-order tensors are used, because the third cumulant of the Gaussian distribution is zero, which is not the case in the DICA/LDA models, where the third order information is sufficient.






















\subsection{Appendix. The sketch of the proof for  Proposition~\ref{sample-complexity}} \label{sec:sample-complexity}
\subsubsection{Expected squared error for the sample expectation} \label{section-variance-expectation}
The sample expectation is $\webb(x) = \frac{1}{N} \sum_n x_n$ is an unbiased estimator of the expectation and:
\begin{ma}
\ebb\rbra{\normp{\webb(x) - \ebb(x)}_2^2} & = \sum_m \ebb\sbra{\rbra{\webb(x_m) - \ebb(x_m)}^2} \\
& = \frac{1}{N^2} \sum_m \sbra{ \ebb\rbra{ \sum_n \rbra{x_{nm} - \ebb(x_m)}^2 } + \ebb\rbra{ \sum_n \sum_{n\ne n'} \rbra{x_{nm} - \ebb(x_m)} \rbra{x_{n'm} - \ebb(x_m)}} } \\
& = \frac{1}{N} \sum_m \ebb\sbra{ \rbra{x_{m} - \ebb(x_m)}^2 } = \frac{1}{N}\sum_m \var(x_m).
\end{ma}
Further, by the law of total variance:
\begin{ma}
\ebb\rbra{\normp{\webb(x) - \ebb(x)}_2^2} &= \frac{1}{N}\sum_m \sbra{ \ebb(\var(x_m|y)) + \var(\ebb(x_m|y)) } = \frac{1}{N} \sum_m \sbra{ \ebb(y_m) + \var(y_m) } \\
& = \frac{1}{N}\sbra{ \sum_k \ebb(\ga_k) + \sum_k \innerp{d_k,d_k} \var(\ga_k)},
\end{ma}
using the fact that $\sum_m D_{mk} = 1$ for any $k$.



\subsubsection{Expected squared error for the sample covariance} \label{section-variance-covariance}
The following finite sample estimator of the covariance $\cov(x,x) = \ebb(xx^{\top}) - \ebb(x)\ebb(x)^{\top}$
\begin{equation}\label{sample-covariance}
\begin{aligned}
\wcov(x,x) &= \frac{1}{N-1} \sum_n x_nx_n^{\top} - \webb(x)\webb(x)^{\top} 
 = \frac{1}{N-1} \sum_n \rbra{x_n x_n^{\top} - \frac{1}{N^2}  \sum_{n'}\sum_{n''} x_{n'}x_{n''}^{\top} }
\\
&= \frac{1}{N}\sum_n\rbra{x_nx_n^{\top} - \frac{1}{N-1}x_n\sum_{n'\ne n} x_{n'}^{\top} }
\end{aligned}
\end{equation}
is unbiased, i.e., $\ebb(\wcov(x,x)) = \cov(x,x)$. Its squared error is
\begin{ma}
&\ebb\rbra{\normp{\wcov(x,x) - \cov(x,x)}_F^2} = \sum_{m,m'}
\ebb\sbra{ \rbra{\wcov(x_m,x_{m'}) - \ebb[\wcov(x_m,x_{m'})] }^2 }.
\end{ma}
The $m,m'$-th element of the sum above is equal to
\begin{ma}
& \frac{1}{N^2}  \sum_{n,n'} \cov\rbra{ x_{nm}x_{nm'} - \frac{1}{N-1}x_{nm}\sum_{n''\ne n}x_{n''m'}\quad , \quad x_{n'm} x_{n'm'} - \frac{1}{N-1}x_{n'm}\sum_{n'''\ne n'}x_{n'''m'}} \\
&=\frac{1}{N^2}  \sum_{n,n'} \cov\rbra{x_{nm}x_{nm'}, x_{n'm} x_{n'm'}}
-\frac{2}{N^2(N-1)} \sum_{n,n'} \cov\rbra{x_{nm}\sum_{n''\ne n} x_{n''m'}, x_{n'm}x_{n'm'}}\\
& + \frac{1}{N^2(N-1)^2} \sum_{n,n'} \cov\rbra{x_{nm}\sum_{n''\ne n} x_{n''m'},x_{n'm}\sum_{n'''\ne n'} x_{n'''m'} }\\
& =\frac{1}{N^2} \sum_n \cov\rbra{x_{nm}x_{nm'},x_{nm}x_{nm'}} \\
& - \frac{2}{N^2(N-1)} \sbra{ \sum_n\sum_{n''\ne n} \cov\rbra{x_{nm}x_{n''m'},x_{nm}x_{nm'}} 
 +  \sum_n\sum_{n'\ne n}  \cov\rbra{ x_{nm}x_{n'm'},x_{n'm}x_{n'm'}}  } \\ 
& + \frac{1}{N^2(N-1)^2} \sbra{  \sum_n \sum_{n'' \ne n} \sum_{n''' \ne n} \cov\rbra{x_{nm}x_{n''m'},x_{nm}x_{n'''m'}}  +   \sum_{n'} \sum_{n\ne n'} \sum_{n'' \ne n} \cov\rbra{ x_{nm}x_{n''m'},x_{n'm}x_{nm'}} } \\
& + \frac{1}{N^2(N-1)^2} \sbra{ \sum_{n'} \sum_{n\ne n'} \sum_{n''' \ne n'} \cov\rbra{ x_{nm}x_{n'm'},x_{n'm}x_{n'''m'}}  +  \sum_{n'} \sum_{n\ne n'} \sum_{n'' \ne n} \cov\rbra{ x_{nm}x_{n''m'},x_{n'm}x_{n''m'}} },
\end{ma}
where we used mutual independence of the observations $x_n$ in a sample $\cbra{x_n}_{n=1}^N$ to conclude that the covariance between the two expressions involving only independent variables is zero.

Further:
\begin{ma}
&\ebb\rbra{\normp{\wcov(x,x) - \cov(x,x)}_F^2} = \frac{1}{N^2} \sum_{m,m'} N \rbra{ \ebb(x_m^2x_{m'}^2) - \sbra{\ebb(x_mx_{m'})}^2 } \\
& - \frac{4}{N^2(N-1)} \sum_{m,m'} N(N-1) \rbra{ \ebb(x_m^2x_{m'})\ebb(x_{m'}) - \ebb(x_mx_{m'})\ebb(x_m)\ebb(x_{m'}) } \\
& + \frac{2}{N^2(N-1)^2} \sum_{m,m'} N (N-1) (N-2) \rbra{\ebb(x_m^2)\sbra{\ebb(x_{m'})}^2 - \sbra{\ebb(x_m)}^2 \sbra{\ebb(x_{m'})}^2 } \\
& + \frac{2}{N^2(N-1)^2} \sum_{m,m'} N(N-1)(N-2) \rbra{\ebb(x_mx_{m'})\ebb(x_{m})\ebb(x_{m'}) - \sbra{\ebb(x_m)}^2 \sbra{\ebb(x_{m'})}^2 } + O\rbra{\frac{1}{N^2} },
\end{ma}
which after simplification gives
\begin{ma}
&\ebb\rbra{\normp{\wcov(x,x) - \cov(x,x)}_F^2} = \frac{1}{N} \sum_{m,m'} \sbra{ \var(x_mx_{m'}) + 2 \sbra{\ebb(x_m)}^2\var(x_{m'}) } \\
& + \frac{1}{N}\sum_{m,m'} \sbra{ 2 \ebb(x_m)\ebb(x_{m'}) \cov(x_m,x_{m'}) - 4 \ebb(x_m)\cov(x_mx_{m'},x_{m'}) } + O\rbra{\frac{1}{N^2}},
\end{ma}
where in the last equality, by symmetry, the  summation indexes $m$ and $m'$ can be exchanged.
As $x_m \sim \poi(y_m)$, by the law of total expectation and law of total covariance, it follows, for $m \neq m'$ (and using the auxiliary expressions from Section~\ref{auxiliary-expressions-fsc}):
\begin{ma}
\var(x_mx_{m'}) & =  \ebb(x_m^2x_{m'}^2) - \sbra{\ebb[x_mx_{m'}]}^2 = \ebb\sbra{\ebb(x_m^2x_{m'}^2|y)} - \sbra{ \ebb\sbra{ \ebb(x_mx_{m'}|y) } }^2 \\
& = \ebb\sbra{ y_m^2 y_{m'}^2 + y_m^2 y_{m'} + y_my_{m'}^2 + y_my_{m'}  }   - \sbra{ \ebb(y_my_{m'}) }^2, \\
\sbra{\ebb(x_m)}^2\var(x_{m'}) & = \sbra{\ebb(y_m)}^2 \ebb(y_{m'}) + \sbra{\ebb(y_m)}^2\ebb(y_{m'}^2) - \sbra{\ebb(y_m)}^2\sbra{\ebb(y_{m'})}^2, \\
\ebb(x_m)\ebb(x_{m'}) \cov(x_m,x_{m'}) & =  \ebb(y_my_{m'}) \ebb(y_m)\ebb(y_{m'}) - \sbra{ \ebb(y_m) }^2 \sbra{ \ebb(y_{m'}) }^2, \\
 \ebb(x_m) \cov(x_mx_{m'},x_{m'}) & =  \ebb(y_m) \sbra{ \ebb(y_my_{m'}) + \ebb(y_my_{m'}^2) - \ebb(y_my_{m'}) \ebb(y_{m'}) }.
\end{ma}
Now, considering the $m = m'$ case, we have:
\begin{ma}
\var(x_m^2) & = \ebb[\ebb(x_m^4|y)] - \sbra{ \ebb[\ebb(x_m^2|y)] }^2 \\
& = \ebb\sbra{ y_m^4 + 6 y_m^3 +7 y_m^2 + y_m} - \sbra{ \ebb\sbra{ y_m^2 + y_m} }^2, \\
\ebb(x_m)\ebb(x_{m}) \cov(x_m,x_{m}) & =  \ebb(y_m)^2 \sbra{ \ebb(y_m^2) + \ebb(y_m) - \sbra{ \ebb(y_{m}) }^2} , \\
 \ebb(x_m) \cov(x_m^2,x_{m}) & =  \ebb(y_m) \sbra{ \ebb(y_m^3) +  3\ebb(y_m^2)+\ebb(y_m) - \ebb(y_m) \sbra{\ebb(y_m^2)+\ebb(y_m)  } }.
\end{ma}
Substitution of $y_m = \sum_kD_{mk}\ga_k$ gives the following
\begin{ma}
\ebb\rbra{\normp{\wcov(x,x) - \cov(x,x)}_F^2} & = \frac{1}{N}\sum_{k,k',k'',k'''}
\innerp{d_k,d_{k'}}\innerp{d_{k''},d_{k'''}}
 \acal_{kk'k''k'''}
\\
& + \frac{1}{N}\sum_{k,k',k''} \sbra{
\innerp{d_k,d_{k'}}\innerp{d_{k''},\ones}
 \bcal_{kk'k''} + \innerp{d_k\circ d_{k'}, d_{k''}} \ecal_{kk'k''} } \\
 & + \frac{1}{N}\sum_{k,k'} \sbra{ \innerp{d_k,\ones}\innerp{d_{k'},\ones} \ebb(\ga_k\ga_{k'}) + \innerp{d_k,d_{k'}} \fcal_{kk'} } \\
 & + \sum_k \innerp{d_k,\ones} \ebb(\ga_k) + O\rbra{\frac{1}{N^2}},
\end{ma}
where $\ones$ is the vector with all the elements equal to $1$ and
\begin{ma}
\acal_{kk'k''k'''} &=  \ebb(\ga_k\ga_{k'}\ga_{k''}\ga_{k'''}) 
- \ebb(\ga_k\ga_{k''}) \ebb(\ga_{k'}\ga_{k'''}) 
+ 2\ebb(\ga_k)\ebb(\ga_{k'})\ebb(\ga_{k''}\ga_{k'''})\\
&- 2 \ebb(\ga_k)\ebb(\ga_{k'})\ebb(\ga_{k''})\ebb(\ga_{k'''})
+2 \ebb(\ga_k\ga_{k''})\ebb(\ga_{k'})\ebb(\ga_{k'''})  -2\ebb(\ga_k)\ebb(\ga_{k'})\ebb(\ga_{k''})\ebb(\ga_{k'''})\\
&-4\ebb(\ga_k)\ebb(\ga_{k'}\ga_{k''}\ga_{k'''})
+4\ebb(\ga_k)\ebb(\ga_{k'}\ga_{k''})\ebb(\ga_{k'''}),\\
\bcal_{kk'k''} & = 2\ebb(\ga_k\ga_{k'}\ga_{k''}) + 2\ebb(\ga_k)\ebb(\ga_{k'})\ebb(\ga_{k''}) - 4\ebb(\ga_k)\ebb(\ga_{k'}\ga_{k''}), \\
\ecal_{kk'k''} &= 4 \ebb(\ga_k\ga_{k'}\ga_{k''}) + 6 \ebb(\ga_k)\ebb(\ga_{k'}) \ebb(\ga_{k''}) - 10\ebb(\ga_k\ga_{k'})\ebb(\ga_{k''}), \\
\fcal_{kk'} & = 6 \ebb(\ga_k\ga_{k'}) - 5 \ebb(\ga_k)\ebb(\ga_{k'}),
\end{ma}
where we used the expressions from Section~\ref{auxiliary-expressions-fsc}.





\subsubsection{Expected squared error of the  estimator \texorpdfstring{$\mathbf{\wh{S}}$}{S} for the GP/DICA cumulants} \label{varS}
As the estimator $\wh{S}$~\eqref{appSgp:fs} of $S$~\eqref{S} is unbiased, its expected squared error is
\begin{equation}\label{variance-S}
\begin{aligned}
\ebb\sbra{ \normp{\wh{S} - S}_F^2 } =&
  \ebb\sbra{ \norm{ \rbra{\wcov(x,x) - \cov(x,x)} + \rbra{ \diag [\webb(x)] - \diag\sbra{\ebb(x)} } }_F^2 } \\
& = \ebb\sbra{\normp{\webb(x)-\ebb(x)}_F^2} + \ebb\sbra{\normp{\wcov(x,x) - \cov(x,x)}_F^2} \\
& + 2\sum_{m} \ebb\sbra{ \rbra{ \webb(x_m)-\ebb(x_m) }\rbra{ \wcov(x_m,x_{m}) - \cov(x_m,x_{m}) } }.
\end{aligned}
\end{equation}
As $\webb(x_m)$ and $\wcov(x_m,x_m)$ are unbiased,  the $m$-th element of the last sum is equal to
\begin{ma}
&\cov\sbra{ \webb(x_m),\wcov(x_m,x_m) }  \\
&= \frac{1}{N^2} \sum_{n,n'} \cov\sbra{ x_{nm}, x_{n'm}^2} - \frac{1}{N^2(N-1)} \sum_{n,n',n''\ne n'} \cov\sbra{ x_{nm},  x_{n'm}x_{n''m}} \\
&= \frac{1}{N^2} \sum_{n} \cov\sbra{ x_{nm}, x_{nm}^2} - \frac{2}{N^2(N-1)} \sum_{n,n'\ne n} \cov\sbra{ x_{nm},  x_{n'm}x_{nm}} +  O\rbra{\frac{1}{N^2}}\\
& = \frac{1}{N} \ebb(x_m^3) - \frac{2}{N}\left( \ebb(x_m^2)\ebb(x_m) - \sbra{ \ebb(x_m) }^3 \right) + O\rbra{\frac{1}{N^2}} \\
& \leq \frac{1}{N} \ebb(x_m^3) + \frac{2}{N} \sbra{ \ebb(x_m) }^3  + O\rbra{ \frac{1}{N^2} } = \frac{1}{N}\sbra{ \ebb(y_m^3) + 3\ebb(y_m^2) + \ebb(y_m) + 2\sbra{\ebb(y_m)}^3 } + O\rbra{\frac{1}{N^2}},
\end{ma}
where we neglected the negative term $-\ebb(x_m^2)\ebb(x_m)$ for the inequality, and the last equality follows from the expressions in Section~\ref{auxiliary-expressions-fsc}. Further, the fact that $y_m = \sum_k D_{mk}\ga_k$ gives
\begin{ma}
\sum_m\cov\sbra{ \webb(x_m),\wcov(x_m,x_m) } & = \frac{1}{N} \sum_{k,k',k''} \innerp{d_k\circ d_{k'}, d_{k''}} \ccal_{kk'k''} \\
& + \frac{3}{N}\sum_{k,k'} \innerp{d_k,d_{k'}} \ebb(\ga_k\ga_{k'}) + \frac{1}{N} \sum_k \innerp{d_k,\ones} \ebb(\ga_k) + O\rbra{ \frac{1}{N^2} },
\end{ma}
where $\circ$ denotes the elementwise Hadamard product and 
\begin{ma}
\ccal_{kk'k''} & = \ebb(\ga_k\ga_{k'}\ga_{k''}) + 2\ebb(\ga_k)\ebb(\ga_{k'})\ebb(\ga_{k''}).
\end{ma}
Plugging this and the expressions for $\ebb(\normp{\webb(x)-\ebb(x)}_F^2)$  and $\ebb(\normp{\wcov(x,x)-\cov(x,x)}_F^2)$ from Sections~\ref{section-variance-expectation} and~\ref{section-variance-covariance}, respectively, into \eqref{variance-S} gives
\begin{ma}
& \ebb\sbra{ \normp{\wh{S} - S}_F^2 } = 
\frac{1}{N}\sbra{ \sum_k \innerp{d_k,d_k} \var(\ga_k) + \sum_k \ebb(\ga_k)
 +\sum_{k,k',k'',k'''}
\innerp{d_k,d_{k'}}\innerp{d_{k''},d_{k'''}}
 \acal_{kk'k''k'''} } + O\rbra{ \frac{1}{N^2} }\\
& + \frac{1}{N} \sbra{ \sum_{k,k',k''}  \sbra{
\innerp{d_k,d_{k'}}
 \bcal_{kk'k''}  
+ 2 \innerp{d_k\circ d_{k'}, d_{k''}} \ccal_{kk'k''} } +  \sum_{k,k'} \rbra{1 + 6 \inner{d_k,d_{k'}} }\ebb(\ga_k\ga_{k'}) + 2\sum_k  \ebb(\ga_k)},
\end{ma}
where we used that, by the simplex constraint on the topics, $\innerp{d_k,\ones} = 1$ for all $k$. To analyze this expression in more details, let us now consider the GP model, i.e., $\ga_k\sim\gam(c_k,b)$:
\begin{ma}
\sum_{k,k',k'',k'''} \acal_{kk'k''k'''} & \le \frac{30c_0^4 + 23 c_0^3 + 14c_0^2 + 8 c_0}{b^4}, \quad\mbox{and}\quad
\sum_{k,k',k''} \bcal_{kk'k''}  \le \frac{6c_0^3 + 10 c_0^2 + 4c_0}{b^3},
\\
\sum_{k,k',k''} \ccal_{kk'k''} & \le \frac{7c_0^3 + 6c_0^2 + 2c_0}{b^3}, \quad\mbox{and}\quad \sum_{k,k',k''} \ecal_{kk'k''} \le \frac{12c_0^3 + 10c_0^2 + 8c_0}{b^3}, \\
\sum_{k,k'} \fcal_{kk'} & \le \frac{2c_0^2 + c_0}{b^2}\quad\mbox{and}\quad
\sum_{k,k'} \ebb(\ga_k\ga_{k'}) \le \frac{2c_0^2 + c_0}{b^2},
\end{ma}
where we used the expressions from Section~\ref{auxiliary-expressions-fsc}, which gives
\begin{ma}
& \ebb\sbra{ \normp{ \wh{S} - S }_F^2 } \le \frac{\nu}{N} \sbra{  \max_k \normp{d_k}_2^2 \frac{c_0}{b^2} + \frac{c_0}{b} + \rbra{\max_{k,k'} \innerp{d_k,d_{k'}}}^2 \max\sbra{ \frac{c_0^4}{b^4},\frac{c_0}{b^4} } + \max_{k,k'} \innerp{d_k,d_{k'}} \max\sbra{ \frac{c_0^3}{b^3}, \frac{c_0}{b^3} } } \\
& + \frac{\nu}{N} \sbra{   \rbra{ \max_{k,k',k''}\innerp{d_k\circ d_{k'},d_{k''}} } \max\sbra{ \frac{c_0^3}{b^3}, \frac{c_0}{b^3} }  + \rbra{1+\max_{k,k'}\innerp{d_k,d_{k'}} } \max\sbra{\frac{c_0^2}{b^2},\frac{c_0}{b^2}}  } + O\rbra{\frac{1}{N^2}},
\end{ma}
where $\nu\le 30$ is a universal constant.
As, by the Cauchy-Schwarz inequality, $\max\mathop{}_{k,k'} \innerp{d_k,d_{k'}} \le \max\mathop{}_k \norm{d_k}_2^2=:\Delta_1$ and $\max\mathop{}_{k,k',k''} \innerp{d_k\circ d_{k'},d_{k''}} \le \max\mathop{}_k \norm{d_k}_\infty \norm{d_k}_2^2 \leq \max\mathop{}_k \norm{d_k}_2^3 =:\Delta_2$ (note that for the topics in the simplex, $\Delta_2 \leq \Delta_1$ as well as $\Delta_1^2 \le \Delta_1$), it follows that
\begin{ma}
& \ebb\sbra{ \normp{ \wh{S} - S }_F^2 } \le \frac{\nu}{N} \sbra{ \Delta_1\rbra{\frac{L^2}{\bar{c}_0} + \frac{L^3}{\bar{c}_0^2} }  + L + \Delta_1^2\frac{L^4}{\bar{c}_0^3}  + \frac{L^2}{\bar{c}_0^2} + \Delta_2 \frac{L^3}{\bar{c}_0^2} } + O\rbra{\frac{1}{N^2} } \\
&  \le \frac{2\nu}{N} \frac{1}{\bar{c}_0^3} \sbra{ \Delta_1^2L^4 + \bar{c}_0  \Delta_1  L^3   +\bar{c}_0^2L^2  + \bar{c}_0^3 L} + O\rbra{\frac{1}{N^2} },
\end{ma}
where $\bar{c}_0 = \min(1,c_0)\le1$
 and, from
Section~\ref{sec:L:gp}, $c_0 = bL$ where $L$ is the expected document length. The second term $\bar{c}_0\Delta_1L^3$ cannot be dominant as the system $\bar{c}_0\Delta_1L^3>\bar{c}_0^2L^2$ and $\bar{c}_0\Delta_1L^3>\Delta_1^2L^4$ is infeasible. Also, with the reasonable assumption that $L \geq 1$, we also have that the 4th term $\bar{c}_0^3 L \leq \bar{c}_0^2L^2$. 
Therefore,
\begin{ma}
& \ebb\sbra{ \normp{ \wh{S} - S }_F^2 }  \le \frac{3\nu}{N}  \max\sbra{ \Delta_1^2L^4,\,\bar{c}_0^2L^2  } + O\rbra{\frac{1}{N^2} }.
\end{ma}


\subsubsection{Auxiliary expressions}\label{auxiliary-expressions-fsc}
As $\cbra{x_m}_{m=1}^M$ are conditionally independent given $y$ in the DICA model~\eqref{gp}, we have the following expressions by using the law of total expectation for $m \neq m'$ and using the moments of the Poisson distribution with parameter $y_m$:
\begin{ma}
\ebb(x_m) &= \ebb[\ebb(x_m|y_m)] = \ebb( y_m),\\
\ebb(x_m^2) &= \ebb[\ebb(x_m^2|y_m)] =  \ebb(y_m^2) + \ebb(y_m), \\
\ebb(x_m^3) &= \ebb[\ebb(x_m^3|y_m)] = \ebb( y_m^3) + 3\ebb(y_m^2) + \ebb(y_m), \\
\ebb(x_m^4) &= \ebb[\ebb(x_m^4|y_m)] = \ebb( y_m^4) + 6\ebb(y_m^3) + 7\ebb(y_m^2) + \ebb(y_m) , \\
\ebb(x_mx_{m'}) &=\ebb[\ebb(x_mx_{m'}|y)] = \ebb[\ebb(x_m|y_m)\ebb(x_{m'}|y_{m'})] = \ebb(y_my_{m'}), \\
\ebb(x_mx_{m'}^2) &= \ebb[\ebb(x_mx_{m'}^2|y)] = \ebb[\ebb(x_m|y_m)\ebb(x_{m'}^2|y_{m'})] = \ebb(y_my_{m'}^2) + \ebb(y_m y_{m'}), \\
\ebb(x_m^2x_{m'}^2) &= \ebb[\ebb(x_m^2|y_m)\ebb(x_{m'}^2|y_{m'})] = \ebb(y_m^2y_{m'}^2) +\ebb( y_m^2y_{m'}) + \ebb(y_my_{m'}^2) + \ebb(y_my_{m'}).
\end{ma}

Moreover, the moments of $\ga_k\sim\gam(c_k,b)$ are
$$
\ebb(\ga_k) = \frac{c_k}{b},\quad \ebb(\ga_k^2) = \frac{c_k^2 + c_k}{b^2}, \quad \ebb(\ga_k^3) = \frac{c_k^3 + 3 c_k^2 + 2 c_k}{b^3}, \quad \ebb(\ga_k^4) = \frac{c_k^4 + 6c_k^3 + 11c_k^2 + 6c_k}{b^4},\quad\mbox{etc.}
$$



\subsubsection{Analysis of the whitening and recovery error}\label{section-analysis-of-whitening}
We can follow a similar analysis as in Appendix C of~\cite{AnaEtAl2013} to derive the topic recovery error given the sample estimate error. In particular, if we define the following sampling errors $E_S$ and $E_T$:
\begin{ma}
&\normp{\wh{S} - S}  \le E_S,\\
&\normp{\wh{T}(u) - T(u)} \le \norm{u}_2E_T,
\end{ma}
then the following form of their Lemma~C.2 holds for both the LDA moments and the GP/DICA cumulants: 
\begin{equation}
\label{analysis-whitening-bound}
\begin{aligned}
\normp{\wh{W} \wh{T}(\wh{W}^{\top}u) \wh{W}^{\top} - WT(W^{\top} u)W^{\top}} &\le \nu\sbra{  \frac{(\mathrm{max}_k \gamma_k) E_S }{\sigma_K \big(\wt{D}\big)^2} + \frac{E_T}{\sigma_K\big(\wt{D}\big)^3} },
\end{aligned}
\end{equation}
where $\sigma_k(\cdot)$ denotes the $k$-th singular value of a matrix, $\nu$ is some universal constant, and in both cases $\wt{D}$ was defined such that $S = \wt{D} \wt{D}^\top$. For the LDA moments, $\gamma_k = 2 \sqrt{ \frac{c_0 (c_0+1))}{c_k (c_0+2)^2 }}$, whereas for the GP/DICA cumulants, $\gamma_k$ takes the simpler form $\gamma_k := \cum(\ga_k)/[\var(\ga_k)]^{3/2}=2/\sqrt{c_k}$.

We note that the scaling for $S$ is $O(L^2)$ for the GP/DICA cumulants, in contrast to $O(1)$ for the LDA moments. Thus, to compare the upper bound~\eqref{analysis-whitening-bound} for the two types of moments, we need to put it in quantities which are common. In the first section of the Appendix~C of~\cite{AnaEtAl2013}, it was mentioned that  $\sigma_K\big(\wt{D}\big) \geq \sqrt{\frac{c_{\mathrm{min}}}{c_0 (c_0+1)}} \sigma_K(D)$ for the LDA moments, where $c_{\mathrm{min}} := \min_k c_k$. In contrast, for the GP/DICA cumulants, we can show that $\sigma_K\big(\wt{D}\big) \geq L \frac{\sqrt{c_{\mathrm{min}}}}{c_0} \sigma_K(D)$, where $L := c_0/b$ is the average length of a document in the GP model. Using this lower bound for the singular vector, we thus get the following bound in the case of the GP cumulant:
\begin{equation} \label{eq:W_bound_us}
\normp{\wh{W} \wh{T}(\wh{W}^{\top}u) \wh{W}^{\top} - WT(W^{\top} u)W^{\top}} 
 \le \frac{\nu}{c_{\mathrm{min}}^{3/2}}\sbra{\frac{E_S}{L^2} \frac{ 2c_0^2 }{\sbra{ \sigma_K\big(D\big) }^2 } + \frac{E_T}{L^3} \frac{c_0^3}{ { \sbra{\sigma_K(D)}^3 } } }.
\end{equation}
The $c_{\mathrm{min}}^{3/2}$ factor is common for both the LDA moment and GP cumulant, but as we mentioned after Proposition~\ref{sample-complexity}, the sample error $E_S$ term gets divided by $L^2$ for the GP cumulant, as expected.

The recovery error bound in~\cite{AnaEtAl2013} is based on the bound~\eqref{eq:W_bound_us}, and thus by showing that the error $E_S/L^2$ for the GP cumulant is lower than the $E_S$ term for the LDA moment, we expect to also gain a similar gain for the recovery error, as the rest of the argument is the same for both types of moments (see Appendix C.2, C.3 and~C.4 in~\cite{AnaEtAl2013} for the completion). 













































































\subsection{Appendix. The LDA moments}\label{sec:lda:moms}
\subsubsection{Our notation} \label{sec:lda:moms:notation}
The LDA moments were derived in~\cite{AnaEtAl2012}. Note that the full version of the paper with proofs appeared in~\cite{AnaEtAl2013} and a later version of the paper also appeared in~\citesup{AnaEtAl2014b}. In this section, we recall the form of the LDA moments using our notation. This section does not contain any novel results and is included for the reader's convenience. We also refer to this section when deriving the practical expressions for computation of the sample estimates of the LDA moments in Appendix~\ref{sec:lda:empirical}.

For deriving the LDA moments, a document is assumed to be composed of at least three tokens: $L \geq 3$. As the LDA generative model~\eqref{lda-tokens} is only defined \emph{conditional} on the length $L$, this is not too problematic. But given that we present models in this paper which also model $L$, we mention for clarity that we can suppose that all expectations and probabilities defined below are implicitly conditioning on $L \geq 3$.\footnote{Note that another advantage of the DICA cumulants from Section~\ref{sec:cum} is that they do not require such a somewhat artificial condition: they are well-defined for any document length (even a document of length zero!).} The theoretical LDA moments are derived only using the first three words $w_1$, $w_2$ and $w_3$ of a document. But note that since the words $w_{\ell}$'s are conditionally i.i.d. given $\theta$ (for $1 \leq \ell \leq L$), we have $M_3 := \ebb(w_{1}\tp w_{2}\tp w_{3}) = \ebb(w_{\ell_1}\tp w_{\ell_2}\tp w_{\ell_3})$ for any three distinct tokens $\ell_1$, $\ell_2$ and $\ell_3$. The tensor $M_3$ is thus symmetric, and could have been defined using any distinct $\ell_1$, $\ell_2$ and $\ell_3$ that are less than $L$. To highlight this arbitrary choice and to make the links with the U-statistics estimator presented later, we thus use generic distinct $\ell_1$, $\ell_2$ and $\ell_3$ in the definition of the LDA moments below, instead of $\ell_1 = 1$, $\ell_2=2$ and $\ell_3 =1$ as in~\cite{AnaEtAl2012}. 

Using this notation, then by the law of total expectation and the properties of the Dirichlet distribution, the non-central moments\footnote{Note, the difference in the notation for the LDA moments in papers~\cite{AnaEtAl2012} and~\cite{AnaEtAl2014}. In~\cite{AnaEtAl2012}, $M_1=\ebb(w_{\ell_1})$, $M_2=\ebb(w_{\ell_1}\tp w_{\ell_2})$, and $M_3 = \ebb(w_{\ell_1}\tp w_{\ell_2} \tp w_{\ell_3})$. However, in~\cite{AnaEtAl2014},  $M_2$ is equivalent to $S$ in our notation and to $Pairs$ in the notation of~\cite{AnaEtAl2012}; similarly, $M_3$ is $T$ in our notation or $Triples$ in the notation of~\cite{AnaEtAl2012}.}
of the LDA model~\eqref{lda-tokens} take the form~\cite{AnaEtAl2012}:
\begin{align}
\label{appM1} 
M_1 &=\ebb(w_{\ell_1}) = D\frac{c}{c_0},\\
\label{appM2}
M_2 &= \ebb(w_{\ell_1}w_{\ell_2}^{\top})=  \frac{c_0}{c_0+1} M_1  M_1^{\top} + \frac{1}{c_0(c_0+1)}D\diag\rbra{c}D^{\top}, \\
\nonumber
M_3 &=\ebb(w_{\ell_1}\tp w_{\ell_2}\tp w_{\ell_3}) \\ 
\nonumber
&= \frac{c_0}{c_0+2} \sbra{\ebb(w_{\ell_1}\tp w_{\ell_2} \tp M_1) + \ebb(w_{\ell_1} \tp M_1 \tp w_{\ell_3}) + \ebb(M_1\tp w_{\ell_2} \tp w_{\ell_3})}, \\
\label{appM3}
&-\frac{2c_0^3}{c_0(c_0+1)(c_0+2)} M_1\tp M_1 \tp M_1 + \frac{2}{c_0(c_0+1)(c_0+2)}\sumk c_k d_k\tp d_k \tp d_k.
\end{align}
where $\tp$ denotes the tensor product.

Similarly to the GP/DICA cumulants (as discussed in Appendix~\ref{sec:app:dicacum2}), moving the terms in the non-central moments~\eqref{appM1},~\eqref{appM2},~\eqref{appM3}, the following quantities are defined
\begin{align}
\label{S:lda}
(Pairs) = S &:= M_2 - \frac{c_0}{c_0+1} M_1  M_1^{\top}, \hspace{5.3cm} \text{LDA S-moment}\\
\nonumber
(Triples) = T & := M_3 - \frac{c_0}{c_0+2} \sbra{\ebb(w_{\ell_1}\tp w_{\ell_2} \tp M_1) + \ebb(w_{\ell_1} \tp M_1 \tp w_{\ell_3}) + \ebb(M_1\tp w_{\ell_2} \tp w_{\ell_3})} \\
\label{T:lda}
& + \frac{2c_0^2}{(c_0+1)(c_0+2)} M_1\tp M_1 \tp M_1. \hspace{3.7cm} \text{LDA T-moment}
\end{align}
Slightly abusing terminology, we refer to the entities $S$ and $T$ as the ``LDA moments''. 
They have the following diagonal structure
\begin{align}
\label{diagS:lda} S &=  \frac{1}{c_0(c_0+1)} \sumk c_k d_kd_k^{\top},  \\
\label{diagT:lda} T &= \frac{2}{c_0(c_0+1)(c_0+2)} \sumk c_k d_k\tp d_k\tp d_k. 
\end{align}
Note however that this form of the LDA moments has a slightly different nature than the similar form~\eqref{diagS} and~\eqref{diagT} of the GP/DICA cumulants. Indeed, the former is the result of properties of the Dirichlet distribution, while the latter is the result of the independence of $\ga$'s. However, one can think of the elements of a Dirichlet random vector as being almost independent (as, e.g., a Dirichlet random vector can be obtained from independent gamma variables through dividing each by their sum). Also, this closeness of the structures of the LDA moments and the GP cumulants can be explained by the closeness of the respective models as discussed in Section~\ref{sec:2}.







\subsubsection{Asymptotically unbiased finite sample estimators for the LDA moments} \label{sec:lda:fs}
Given realizations $w_{n\ell}$, $n=1,\dots,N$, $\ell=1,\dots,L_n$, of the token random variable $w_{\ell}$, we now give the expressions for the finite sample estimates of $S$~\eqref{S:lda} and $T$~\eqref{T:lda} for the LDA model (and we rewrite them as a function of the sample counts $x_n$).\footnote{Note that because non-linear functions of $\wh{M}_1$ appear in the expression for~$\wh{S}$~\eqref{S:lda:fs} and~$\wh{T}$~\eqref{T:lda:fs}, the estimator is biased, i.e., $\ebb(\wh{S}) \neq S$. The bias is small though: $\normp{\ebb(\wh{S}) -  S} = O(1/N)$ and the estimator is asymptotically unbiased. This is in contrast with the estimator for the GP/DICA moments which is easily made unbiased.} We use the notation $\wh{\ebb}$ below to express a U-statistics empirical expectation over the token within a documents, uniformly averaged over the whole corpus. For example, $\wh{\ebb}(w_{\ell_1}\tp w_{\ell_2} \tp \wh{M}_1) := \frac{1}{N} \sumn \frac{1}{L_n(L_n-1)}\sum_{\ell_1=1}^{L_n} \sum_{\substack{\ell_2=1\\ \ell_2\ne \ell_1}}^{L_n} w_{\ell_1}\tp w_{\ell_2}\tp \wh{M}_1 $.
\begin{align}
\label{S:lda:fs}
\wh{S} &:= \wh{M}_2 - \frac{c_0}{c_0+1} \wh{M}_1  \wh{M}_1^{\top}, \\
\nonumber
\wh{T} & := \wh{M}_3 - \frac{c_0}{c_0+2} \sbra{\wh{\ebb}(w_{\ell_1}\tp w_{\ell_2} \tp \wh{M}_1) + \wh{\ebb}(w_{\ell_1} \tp \wh{M}_1 \tp w_{\ell_3}) + \wh{\ebb}(\wh{M}_1\tp w_{\ell_2} \tp w_{\ell_3})} \\
\label{T:lda:fs}
& + \frac{2c_0^2}{(c_0+1)(c_0+2)} \wh{M}_1\tp \wh{M}_1 \tp \wh{M}_1,
\end{align}
where, as suggested in~\cite{AnaEtAl2014}, unbiased U-statistics estimates of 
$M_1$, $M_2$ and $M_3$ are:
\begin{align}
\label{m1final} \wh{M}_1 & :=\wh{\ebb}(w_{\ell})= \frac{1}{N} \sumn \frac{1}{L_n} \sum_{\ell=1}^{L_n} w_{n\ell} = \frac{1}{N}\sumn [\delta_1]_n x_n  = \frac{1}{N} X\delta_1, \\
\nonumber \wh{M}_2 &:=\wh{\ebb}(w_{\ell_1} w_{\ell_2}^{\top}) = \frac{1}{N} \sumn \frac{1}{L_n(L_n-1)}\sum_{\ell_1=1}^{L_n} \sum_{\substack{\ell_2=1\\ \ell_2\ne \ell_1}}^{L_n}w_{n\ell_1} w_{n\ell_2}^{\top} \\
\nonumber & = \frac{1}{N} \sumn [\delta_2]_n \rbra{x_n x_n^{\top} - \sum_{\ell=1}^{L_n} w_{n\ell} w_{n\ell}^{\top}} \\
\nonumber & = \frac{1}{N} \sumn [\delta_2]_n \rbra{x_n x_n^{\top} - \diag(x_n) } \\ 
\label{m2final} &= \frac{1}{N} \sbra{X \diag(\delta_2)X^{\top} - \diag(X\delta_2)}, \\ \end{align}
\begin{align}
\nonumber
\wh{M}_3&:=\wh{\ebb}(w_{\ell_1}\tp w_{\ell_2}\tp w_{\ell_3}) = \frac{1}{N} \sumn \delta_{3n} \sum_{\ell_1=1}^{L_n} \sum_{\substack{\ell_2=1\\\ell_2\ne \ell_1}}^{L_n}\sum_{\substack{\ell_3=1\\\ell_3\ne\ell_2\\ \ell_3\ne\ell_1}}^{L_n} w_{n\ell_1}\tp w_{n\ell_2}\tp w_{n\ell_3} \\
\nonumber
&= \frac{1}{N}\sumn [\delta_3]_n \left( x_n \tp x_n\tp x_n -\sum_{\ell=1}^{L_n} w_{n\ell}\tp w_{n\ell} \tp w_{n\ell} \right. \\
\nonumber
& \left. - \sum_{\ell_1=1}^{L_n}\sum_{\substack{\ell_2=1\\ \ell_2\ne\ell_1}}^{L_n}(w_{n\ell_1}\tp w_{n\ell_1}\tp w_{n\ell_2} + w_{n\ell_1}\tp w_{n\ell_2}\tp w_{n\ell_1} + w_{n\ell_1}\tp w_{n\ell_2}\tp w_{n\ell_2}) \right) \\
\nonumber
&= \frac{1}{N}\sumn [\delta_3]_n \left( x_n\tp x_n \tp x_n + 2\summ x_{nm} (e_m\tp e_m\tp e_m) \right. \\
\label{m3final}
& \left. - \sum_{m_1=1}^M\sum_{m_2=1}^M x_{nm_1} x_{nm_2} (e_{m_1}\tp e_{m_1} \tp e_{m_2} + e_{m_1}\tp e_{m_2}\tp e_{m_1} + e_{m_1}\tp e_{m_2} \tp e_{m_2}) \right).
\end{align}
Here, the vectors $\gd_1$, $\gd_2$ and $\gd_3 \in\rr{N}$ are defined as $\sbra{\delta_1}_n := L_n^{-1}$; $\sbra{\delta_2}_n := (L_n(L_n-1))^{-1}$, i.e., $\sbra{\delta_2}_n = \sbra{{L_n \choose 2} 2!}^{-1}$ is the number of times to choose an ordered pair of tokens out of $L_n$ 
tokens;  $[\delta_3]_n := (L_n(L_n-1)(L_n-2))^{-1}$, i.e., $[\delta_3]_n = \sbra{{L_n \choose 3} 3!}^{-1}$ is the number of times to choose an ordered triple of tokens out of $L_n$ tokens. Note that the vectors $\gd_1$, $\gd_2$, and $\gd_3$ have nothing to do with the Kronecker delta $\gd$.

For a vector $a\in\rr{N}$, we sometimes use notation $\sbra{a}_n$ to denote its $n$-th element. Similarly, for a matrix $A\in\rr{M\times N}$ we use notation $\sbra{A}_{mn}$ to denote its $(m,n)$-th element.

There is a slight abuse of notation in the expressions above as $w_{\ell}$ is sometimes treated as a random variable (i.e., in $\webb(w_{\ell})$, $\webb(w_{\ell_1} w_{\ell_2}^{\top})$, etc.) and sometimes as its realization. However, the difference is clear from the context.


















































\subsection{Appendix. Practical aspects and implementation details} \label{sec:app:implementation}
\subsubsection{Whitening of $\mathbf{S}$ and dimensionality reduction}\label{sec:whitening}
The algorithms from Section~\ref{sec:diag} require the computation of a whitening matrix $W$ of $S$. Due to the similar diagonal structure (\eqref{diagS:lda} and~\eqref{diagS}) of the matrix S for both the LDA moments~\eqref{S:lda} and the GP/DICA cumulants~\eqref{S}, the computation of a whitening matrix is exactly the same in both cases. 

By a whitening matrix, we mean a matrix $W\in\rr{K\times M}$ (in practice, $M\gg K$) that does not only whiten $S\in\rr{M\times M}$, but also reduces its dimensionality such that\footnote{Note that such a whitening matrix $W\in\rr{K\times M}$ is not uniquely defined as left multiplication by any orthogonal matrix $V\in\rr{K\times K}$ does not change anything. Indeed, let $\wt{W} = VW$, then $\wt{W}S\wt{W}^{\top} = VWSW^{\top} V^{\top} = I_K$.} $WSW^{\top} = I_K$.

Let $S = U \Sigma U^{\top}$ be an orthogonal eigendecomposition of the symmetric matrix $S$. Let $\Sigma_{1:K}$ denotes the diagonal matrix that contains the largest $K$ eigenvalues\footnote{We mean the largest non-negative eigenvalues. In theory, $S$ have to be PSD. In practice, when we deal with finite number of samples, respective estimate of $S$ can have negative eigenvalues. However, for $K$ sufficiently small, $S$ should have enough positive eigenvalues. Moreover, it is standard practice to use eigenvalues of $S$ for estimation of a good value of $K$, e.g., by thresholding all negative and close to zero eigenvalues.}  
of $S$ on its diagonal and let $U_{1:K}$ be a matrix with the respective eigenvalues in its columns. Then, a whitening matrix is
\begin{equation}\label{W}
W = \Sigma_{1:K}^{\dagger 1/2}U_{1:K}^{\top} ,
\end{equation}
where $\Sigma_{1:K}^{\dagger 1/2}$ is a diagonal matrix constructed from $\Sigma_{1:K}$ by taking the inverse and the square root of its non-zero diagonal values ($^{\dagger}$ stands for the pseudo-inverse).

In practice, when only a finite sample estimator $\wh{S}$ of $S$ is available, the following finite sample estimator $\wh{W}$ of $W$ can be introduced
\begin{equation}\label{W-fs}
\wh{W} :=  \wh{\Sigma}_{1:K}^{\dagger 1/2} \wh{U}_{1:K}^{\top},
\end{equation}
where $\wh{S} = \wh{U} \wh{\Sigma} \wh{U}^{\top}$.













\subsubsection{Computation of the finite sample estimators of the GP/DICA cumulants} \label{sec:dica:empirical}
In this section, we present efficient formulas for computation of the finite sample estimate (see Appendix~\ref{sec:dica:fs} for the definition of $\wh{T}$) of  $\wh{W}\wh{T}(v)\wh{W}^{\top}$ for the GP/DICA models. The construction of the finite sample estimator $\wh{W}$ is discussed in Appendix~\ref{sec:whitening}, while the computation of $\wh{S}$~\eqref{appSgp:fs} is straightforward.

By plugging the definition of the tensor $\wh{T}$~\eqref{appTgp:fs} in the formula~\eqref{projT} for the projection of a tensor onto a vector, we obtain for a given $v\in\rr{M}$:
\begin{ma}
\sbra{\wh{T}(v)}_{m_1m_2} &= \sum_{m_3} \wcum(x_{m_1},x_{m_2},x_{m_3}) v_{m_3} + 2\sum_{m_3} \gd(m_1,m_2,m_3) \webb(x_{m_3}) v_{m_3} \\
&- \sum_{m_3} \gd(m_2,m_3) \wcov(x_{m_1},x_{m_2})v_{m_3}\\
& - \sum_{m_3} \gd(m_1,m_3) \wcov(x_{m_1},x_{m_2}) v_{m_3}\\
& - \sum_{m_3} \gd(m_1,m_2) \wcov(x_{m_1},x_{m_3})v_{m_3} \\
&= \sum_{m_3} \wcum(x_{m_1},x_{m_2},x_{m_3}) v_{m_3} + 2 \gd(m_1,m_2) \webb(x_{m_1}) v_{m_1} \\
&-  \wcov(x_{m_1},x_{m_2}) v_{m_2} -  \wcov(x_{m_1},x_{m_2}) v_{m_1} 
- \gd(m_1,m_2) \sum_{m_3} \wcov(x_{m_1},x_{m_3}) v_{m_3}.
\end{ma}
This gives the following for the expression $\wh{W}\wh{T}(v)\wh{W}^{\top}$:
\begin{ma}
\sbra{\wh{W}\wh{T}(v)\wh{W}^{\top}}_{k_1k_2} &= \wh{W}{k_1}^{\top} \wh{T}(v) \wh{W}_{k_2} \\
&= \sum_{m_1,m_2,m_3} \wh{\cum}(x_{m_1},x_{m_2},x_{m_3}) v_{m_3} \wh{W}_{k_1m_1} \wh{W}_{k_2m_2}\\
&+ 2\sum_{m_1,m_2} \gd(m_1,m_2) \wh{\ebb} (x_{m_1}) v_{m_1} \wh{W}_{k_1m_1} \wh{W}_{k_2m_2} \\
&- \sum_{m_1,m_2} \wh{\cov}(x_{m_1},x_{m_2}) v_{m_2} \wh{W}_{k_1m_1} \wh{W}_{k_2m_2} \\
& - \sum_{m_1,m_2} \wh{\cov}(x_{m_1},x_{m_2}) v_{m_1} \wh{W}_{k_1m_1} \wh{W}_{k_2m_2}\\
& - \sum_{m_1,m_3} \wh{\cov}(x_{m_1},x_{m_3}) v_{m_3} \wh{W}_{k_1m_1} \wh{W}_{k_2m_1},
\end{ma}
where $\wh{W}_k$ denotes the $k$-th row of $\wh{W}$ as a column vector.
By further plugging in the expressions~\eqref{cum:empirical} for the unbiased finite sample estimates of $\wcov$ and $\wcum$, we further get
\begin{ma}
\sbra{\wh{W}\wh{T}(v)\wh{W}^{\top}}_{k_1k_2}&= \frac{N}{(N-1)(N-2)} \sum_n \inner{\wh{W}_{k_1},x_n-\webb(x)} \inner{\wh{W}_{k_2},x_n-\webb(x)} \inner{v, x_n - \webb(x)} \\
&+ 2\sum_m \webb(x_m) v_m \wh{W}_{k_1m} \wh{W}_{k_2m} \\
&- \frac{1}{N-1} \sum_n \inner{\wh{W}_{k_1}, x_n - \webb(x)} \inner{v\circ \wh{W}_{k_2}, x_n - \webb(x)} \\
&- \frac{1}{N-1} \sum_n \inner{v\circ \wh{W}_{k_1}, x_n - \webb(x)} \inner{\wh{W}_{k_2}, x_n - \webb(x)} \\
&- \frac{1}{N-1} \sum_n \inner{\wh{W}_{k_1} \circ \wh{W}_{k_2}, x_n -\webb(x)} \inner{v, x_n - \webb(x)},
\end{ma} 
where $\circ$ denotes the elementwise Hadamard product.
Introducing
the counts matrix $X\in\rr{M\times N}$ where each element $X_{mn}$ is the count of the $m$-th word in the $n$-th document (note, the matrix $X$ contain the vector $x_n$ in the $n$-th column), we further simplify the above expression
\begin{equation}\label{est:T:dica:old}
\begin{aligned}
\wh{W}\wh{T}(v)\wh{W}^{\top} & = \frac{N}{(N-1)(N-2)} (\wh{W}X)\diag[X^{\top} v](\wh{W}X)^{\top} \\
&+\frac{N}{(N-1)(N-2)}\inner{v,\webb(x)}\sbra{ 2N(\wh{W}\webb(x))(\wh{W}\webb(x))^{\top} - (\wh{W}X)(\wh{W}X)^{\top} } \\
& - \frac{N}{(N-1)(N-2)} \sbra{\wh{W}X(X^{\top} v) (\wh{W}\webb(x))^{\top} + \wh{W}\webb(x)(\wh{W}X(X^{\top} v))^{\top}} \\
&+2\wh{W} \diag[v\circ\webb(x)] \wh{W}^{\top} \\
& - \frac{1}{N-1} \sbra{ (\wh{W}X)(\wh{W}\diag(v)X)^{\top} + (\wh{W}\diag(v) X)(\wh{W}X)^{\top} + \wh{W} \diag[X(X^{\top} v)] \wh{W}^{\top} } \\
& + \frac{N}{N-1} \sbra{ (\wh{W}\webb(x))(\wh{W}\diag[v] \webb(x))^{\top} + (\wh{W}\diag[v] \webb(x))(\wh{W}\webb(x))^{\top}  } \\
& + \frac{N}{N-1} \inner{v,\webb(x)}\wh{W} \diag[\webb(x)] \wh{W}^{\top}.
\end{aligned}
\end{equation}

A more compact way to write down expression~\eqref{est:T:dica:old} is as follows
\begin{equation}
\label{est:T:dica}
\begin{aligned}
\wh{W}\wh{T}(v)\wh{W}^{\top} & = \frac{N}{(N-1)(N-2)} \sbra{ T_1 + \innerp{v,\webb(x)}\rbra{ T_2 - T_3 } - ( T_4 + T_4^{\top} ) } \\
& + \frac{1}{N-1} \sbra{ T_5 + T_5^{\top} - T_6 - T_6^{\top} +
\wh{W} \diag(a) \wh{W}^{\top} 
 }, 
\end{aligned}
\end{equation}
where
\begin{ma}
T_1 &= (\wh{W}X)\diag[X^{\top} v](\wh{W}X)^{\top}, \\
T_2 &= 2N(\wh{W}\webb(x))(\wh{W}\webb(x))^{\top}, \\
T_3 &= (\wh{W}X)(\wh{W}X)^{\top}, \\
T_4 &= \wh{W}X(X^{\top} v) (\wh{W}\webb(x))^{\top}, \\
T_5 &= (\wh{W}X)(\wh{W}\diag(v)X)^{\top}, \\
T_6 &= (\wh{W}\diag(v) \webb(x))(\wh{W}\webb(x))^{\top}, \\
a &= 2(N-1)[v\circ \webb(x)] + \innerp{v,\webb(x)}\webb(x) - X(X^{\top} v).
\end{ma}

\subsubsection{Computational complexity of the GP/DICA T-cumulant estimator~\eqref{est:T:dica}}\label{sec:complexity-t-cumulant}
When computing the T-cumulant $P$ times with the formula above, the following terms are dominant: $O(RNK) + O(NK^2) + O(MK)$, where $R$ is the largest number of unique words (non-zero counts) in a document over the corpus. In practice, almost always $K < M < N$, which gives the overall complexity of $P$ computations of the estimator~\eqref{est:T:dica} to be equal to $O(PRNK) + O(PNK^2)$.














\subsubsection{Computation of the finite sample estimators of the LDA moments} \label{sec:lda:empirical}
In this section, we present efficient formulas for computation of the finite sample estimate (see Appendix~\ref{sec:lda:fs} for the definition of $\wh{T}$) of $\wh{W}\wh{T}(v)\wh{W}^{\top}$ for the LDA model. Note that the construction of the sample estimator $\wh{W}$ of a whitening matrix $W$ is discussed in Appendix~\ref{sec:whitening}). The computation of $\wh{S}$~\eqref{S:lda:fs} is straightforward.
This approach to efficient implementation was discussed in~\cite{AnaEtAl2014}, however, to the best of our knowledge, the final expressions were not explicitly stated before. All derivations are straightforward, but quite tedious. 

By analogy with the GP/DICA case, a projection~\eqref{projT} of the tensor $\wh{T}\in\rr{M\times M\times M}$~\eqref{T:lda:fs} onto some vector $v\in\rr{M}$ in the LDA is
\begin{ma}
&\sbra{\wh{T}(v)}_{m_1m_2} = \sum_{m_3=1}^M\sbra{\wh{M}_3}_{m_1m_2m_3} v_{m_3} +\frac{2c_0^2}{(c_0+1)(c_0+2)}\sum_{m_3}[\wh{M}_1]_{m_1}[\wh{M}_1]_{m_2}[\wh{M}_1]_{m_3}v_{m_3} \\
&- \frac{c_0}{c_0+2} \sum_{m_3=1}^M
\sbra{\webb(w_{\ell_1}\tp w_{\ell_2} \tp \wh{M}_1)
+\webb(w_{\ell_1}\tp \wh{M}_1 \tp w_{\ell_3}) 
+ \webb(\wh{M}_1\tp w_{\ell_2}\tp w_{\ell_3})}_{m_1m_2m_3} v_{m_3}.
\end{ma}
Plugging in the expression~\eqref{m3final} for an unbiased sample estimate $\wh{M_3}$ of $M_3$, we get
\begin{ma}
\sbra{\wh{T}(v)}_{m_1m_2} 
&= \frac{1}{N} \sumn [\delta_3]_n \rbra{x_{nm_1}x_{nm_2}\inner{x_n,v} + 2\sum_{m_3}\gd(m_1,m_2,m_3)x_{nm_3}v_{m_3}} \\
& - \frac{1}{N} \sumn [\delta_3]_n \sum_{m_3=1}^M \sbra{\sum_{i,j=1}^M x_{ni}x_{nj}\rbra{ e_i\tp e_i \tp e_j +e_i\tp e_j \tp e_i + e_i \tp e_j \tp e_j}}_{m_1m_2m_3} v_{m_3} \\
& + \frac{2c_0^2}{(c_0+1)(c_0+2)} [\wh{M}_1]_{m_1}[\wh{M}_1]_{m_2} \inner{\wh{M}_1,v}\\
& - \frac{c_0}{c_0+2} \rbra{[\wh{M}_2]_{m_1m_2} \inner{\wh{M}_1,v} + \sum_{m_3=1}^M \rbra{ [\wh{M}_2]_{m_1m_3}[\wh{M}_1]_{m_2}v_{m_3} + [\wh{M}_2 ]_{m_2m_3} [\wh{M}_1]_{m_1}v_{m_3} } },
\end{ma}
where $e_1$, $e_2$, \dots, $e_M$ denote the canonical vectors of $\rr{M}$ (i.e., the columns of the identity matrix $I_M$). 
Further, this gives the following for the expression $\wh{W}\wh{T}(v)\wh{W}^{\top}$:
\begin{align}
\nonumber \sbra{\wh{W}\wh{T}(v)\wh{W}^{\top}}_{k_1k_2} &= \frac{1}{N} \sumn [\delta_3]_n \rbra{\inner{x_n,v}\inner{x_n,\wh{W}_{k_1}}\inner{x_n,\wh{W}_{k_2}} +2\summ x_{nm}v_m \wh{W}_{k_1m}\wh{W}_{k_2m}} \\
\nonumber &- \frac{1}{N} \sumn \delta_{3n} \sum_{i,j=1}^M x_{ni}x_{nj}\rbra{ \wh{W}_{k_1i}\wh{W}_{k_2i}v_j + \wh{W}_{k_1i}\wh{W}_{k_2j}v_i + \wh{W}_{k_1i}\wh{W}_{k_2j}v_j} \\
\nonumber &- \frac{c_0}{c_0+2} \rbra{ \inner{\wh{W}_{k_1},\sbra{\wh{M}_2}\wh{W}_{k_2}} + \inner{\wh{W}_{k_1},\wh{M}_2v}\inner{\wh{M}_1\wh{W}_{k_2} } +  \inner{\wh{W}_{k_2},\wh{M}_2 v}\inner{\wh{M}_1,\wh{W}_{k_1}} } \\
\nonumber &+ \frac{2c_0^2}{(c_0+1)(c_0+2)}\inner{\wh{M}_1,\wh{W}_{k_1}} \inner{\wh{M}_1,\wh{W}_{k_2}}\inner{\wh{M}_1,v},
\end{align}
where $\wh{W}_{k}$ denotes the $k$-th row of $\wh{W}$ as a column-vector.
This  further simplifies to
\begin{equation}
\begin{aligned}
 \wh{W}\wh{T}(v)\wh{W}^{\top} &= \frac{1}{N} (\wh{W}X)\diag\sbra{ (X^{\top} v)\circ \delta_3} (\wh{W}X)^{\top} \\
 & + \frac{1}{N} \wh{W}\diag\sbra{2[(X\delta_3)\circ v]-X[(X^{\top} v)\circ \delta_3] } \wh{W}^{\top}  \\
 &-\frac{1}{N} (\wh{W}\diag[v]X)\diag[\delta_3](\wh{W}X)^{\top} \\
 &-\frac{1}{N}  (\wh{W}X)\diag[\delta_3] (\wh{W}\diag[v]X)^{\top} \\
 &-\frac{c_0}{c_0+2}\sbra{\inner{\wh{M}_1,v}(\wh{W}\wh{M}_2\wh{W}^{\top}) + (\wh{W}(\wh{M}_2v))(\wh{W}\wh{M}_1)^{\top} + (\wh{W}\wh{M}_1)(\wh{W}(\wh{M}_2v))^{\top}}  \\
 & + \frac{2c_0^2}{(c_0+1)(c_0+2)}\inner{\wh{M}_1,v}(\wh{W}\wh{M}_1) (\wh{W}\wh{M}_1)^{\top}.
\end{aligned}
\end{equation}
A more compact representation gives:
\begin{equation}
\label{est:T:lda}
\begin{aligned}
 \wh{W}\wh{T}(v)\wh{W}^{\top} &= \frac{1}{N} \sbra{ T_1 + T_2 -T_3 - T_3^{\top}}  
 -\frac{c_0}{c_0+2}\sbra{\innerp{\wh{M}_1,v}(\wh{W}\wh{M}_2\wh{W}^{\top}) + T_4 + T_4^{\top}}  \\
&  + \frac{2c_0^2}{(c_0+1)(c_0+2)}\innerp{\wh{M}_1,v}(\wh{W}\wh{M}_1) (\wh{W}\wh{M}_1)^{\top},
\end{aligned}
\end{equation}
where
\begin{ma}
T_1 & = (\wh{W}X)\diag\sbra{ (X^{\top} v)\circ \delta_3} (\wh{W}X)^{\top}, \\
T_2 & = \wh{W}\diag\sbra{2[(X\delta_3)\circ v]-X[(X^{\top} v)\circ \delta_3] } \wh{W}^{\top}, \\
T_3 & = [\wh{W}\diag(v)X]\diag(\delta_3)(\wh{W}X)^{\top}, \\
T_4 & = [\wh{W}(\wh{M}_2v)](\wh{W}\wh{M}_1)^{\top}.
\end{ma}

\subsubsection{Computational complexity of the LDA T-moment estimator~\eqref{est:T:lda}}\label{sec:complexity-t-moment}
By analogy with Appendix~\ref{sec:complexity-t-cumulant}, the computational complexity of the T-moment is $O(RNK) + O(NK^2)$. However, in practice we noticed that the computation of~\eqref{est:T:dica} is slightly faster for larger datasets than the computation of~\eqref{est:T:lda} (although the code for both was equally well optimized). This means that the constants in $O(RNK)+O(NK^2)$ for the LDA T-moment are, probably, slightly larger than for the GP/DICA T-cumulant.











\subsubsection{Estimation of the model parameters for GP/DICA model} \label{estimation-model-parameters}
Below we briefly discuss the recovery of the model parameters for the GP/DICA and LDA models from 
a joint diagonalization matrix $A \in \rr{K \times M}$ estimated in Algorithm~\ref{alg:jd}. This matrix has the property that $A D$ should be approximately diagonal up to a permutation of the columns of $D$. The standard approach~\cite{AnaEtAl2012} of taking the pseudo-inverse of $A$ to get an estimate of the topic matrix $D$ has a problem that it does not preserve the simplex constraint of the topics (in particular, the non-negativity of $\wt{D}$). Due to the space constraints, we do not discuss this issue here, but we observed experimentally that this can potentially significantly deteriorate performance of all moment matching algorithms for topic models considered in this paper. We made an attempt to solve this problem by integrating the non-negativity constraint into the Jacobi-updates procedure of the orthogonal joint diagonalization algorithm, but the obtained results did not lead to any significant improvement. Therefore, in our experiments for both GP/DICA cumulants and LDA moments, we estimate the topic matrix by thresholding the negative values of the pseudo-inverse of $A$: 
$$
\wh{d}_k := \tau_k\max(0,[A\pinv]_{:k})/\normp{\max(0,[A\pinv]_{:k})}_1,
$$ 
where $[A\pinv]_{:k}$ is the $k$-th column of the pseudo-inverse $A\pinv$ of $A$, and $\tau_k=\rpm 1$ set to $-1$ if $[A\pinv]_{:k}$ has more negative than positive values. This might not be the best option, and we leave this issue for the future research.

To estimate the parameters for the prior distribution over the topic intensities $\ga_k$ for the DICA model~\eqref{dica}, we use the diagonalized form of the projected tensor from~\eqref{orthT} and relate it to the output diagonal elements $a_p$ for the $p$-th projection:
\begin{equation} \label{eq:ap_tk}
[a_p]_k = \wt{t}_k \innerp{z_k,u_p} = \frac{t_k}{s_k^{3/2}}  \innerp{z_k,u_p} =
\frac{\cum(\ga_k,\ga_k,\ga_k)}{[\var(\ga_k)]^{3/2}} \inner{\tau_k\wt{d}_k,W^{\top}u_p},
\end{equation}
where $\wt{d}_k = \tau_k \max(0,[A\pinv]_{:k})$.
This formula is valid for any prior on $\ga_k$ in the DICA model. For the GP model~\eqref{gp} where $\ga_k \sim \gam(c_k,b)$, we have that $\var(\ga_k) = \frac{c_k}{b^2}$ and  $\cum(\ga_k,\ga_k,\ga_k) = \frac{2 c_k}{b^3}$, and thus $ \wt{t}_k  = \frac{2}{\sqrt{c_k}}$, which enables us to estimate $c_k$. Plugging this value of $\wt{t}_k$ in~\eqref{eq:ap_tk}, and solving for $c_k$ gives the following expression:
$$
c_k = \frac{4 \inner{\wt{d}_k,W^{\top}u_p}^2}{[a_p]_k^2}.
$$
By replacing the quantities on the RHS with their estimated ones, we get one estimate for $c_k$ per projection. We use as our final estimate the average estimate over the projections:
\begin{equation} \label{eq:ck_estimate}
\wh{c}_k := \frac{1}{P} \sum_{p=1}^P  \frac{4 \inner{\wt{d}_k,\wh{W}^{\top}u_p}^2}{[a_p]_k^2}.
\end{equation}
Reusing the properties of the length of documents for the GP model as described in Appendix~\ref{sec:L:gp}, we finally use the following estimates for rate parameter $b$ of the gamma distribution:
\begin{equation} \label{eq:b_estimate}
 \wh{b} := \frac{\wh{c}_0} {\wh{L}},
\end{equation}
where $\wh{c}_0 := \sum_k \wh{c}_k$ and $\wh{L}$ is the average document length in the corpus.

By analogy, similar formulas for the estimation of the Dirichlet parameter $c$ of the LDA model can be derived and are a straightforward extension of the expression in~\cite{AnaEtAl2012}.


































\subsection{Appendix. Complexity of algorithms and details on the experiments}

\subsubsection{Code and complexity}\label{sec:code-and-compleixty}

Our (mostly Matlab) implementations of the diagonalization algorithms (JD, Spec, and TPM) for both the GP/DICA cumulants and LDA moments are available online.\footnote{\url{https://github.com/anastasia-podosinnikova/dica-light}} Moreover, all datasets and the code for reproducing our experiments are available.\footnote{\url{https://github.com/anastasia-podosinnikova/dica}}
To our knowledge, no efficient implementation of these algorithms was available for LDA. Each experiment was run in a single thread.

The bottleneck for the spectral, JD, and TPM algorithms is the computation of the cumulants/moments.
However, the expressions~\eqref{est:T:dica} and~\eqref{est:T:lda} provide efficient formulas for fast computation of the GP/DICA cumulants and LDA moments ($O(RNK + NK^2)$, where $R$ is the largest number of non-zeros in the count vector $x$ over all documents, see Appendix~\ref{sec:complexity-t-cumulant} and~\ref{sec:complexity-t-moment}), which makes even the Matlab implementation fast for large datasets. Since all diagonalization algorithms (spectral, JD, TPM) perform the whitening step once,
it is sufficient to compare their complexities by the number of times the cumulants/moments are computed.


\textbf{Spectral.} The spectral algorithm estimates the cumulants/moments only once leading to $O(NK(R+K))$ complexity and, therefore, is the fastest.

\textbf{JD.}
For JD, rather than estimating $P$ cumulants/moments separately, one can jointly estimate these values by precomputing and reusing some terms (e.g., $W X$).
However, the complexity is still $O(PNK(R + K))$, although in practice it is sufficient to have $P=K$ or even smaller.

\textbf{TPM.} For TPM some parts of the cumulants/moments can also be precomputed, but as TPM normally does many more iterations than $P$, it can be significantly slower. 
In general, the complexity of TPM can be significantly influenced by the initialization of the parameters of the algorithm. There are two main parameters: $L_{tpm}$ is the number of random restarts within one deflation step and $N_{tpm}$ is the maximum number of iterations for each of $L_{tpm}$ random restarts (different from $N$ and $L$). Some restarts converge very fast (in much less than $N_{tpm}$ iterations), while others are slow. Moreover, as follows from theoretical results~\cite{AnaEtAl2014} and, as we observed in practice, the restarts which converge to a good solution converge fast, while slow restarts, normally, converge to a worse solution. Nevertheless, in the worst case, the complexity is $O(N_{tpm} L_{tpm}NK(R+K))$.

Note that for the experiment in Figure~\ref{plots:diag}, $L_{tpm}=10$ and $N_{tpm} = 100$ and the run with the best objective is chosen. We believe that these values are reasonable in a sense that they provide a good accuracy solution ($\varepsilon = 10^{-5}$ for the norm of the difference of the vectors from the previous and the current iteration) in a little number of iterations, however, they may not be the best ones. 

\textbf{JD implementation.} 
For the orthogonal joint diagonalization algorithm, we implemented a faster C++ version of the previous Matlab implementation\footnote{\url{http://perso.telecom-paristech.fr/~cardoso/Algo/Joint_Diag/joint_diag_r.m}} by J.-F.~Cardoso. Moreover, the orthogonal joint diagonalization routine can be initialized in different ways: (a) with the $K\times K$ identity matrix or (b) with a random orthogonal $K\times K$ matrix. We tried different options and in nearly all cases the algorithm converged to the same solution, implying that initialization with the identity matrix is sufficient.


\textbf{Whitening matrix.} For the large vocabulary size $M$,  computation of a whitening matrix can be expensive (in terms of both memory and time).
One possible solution would be to reduce the vocabulary size with, e.g., TF-IDF score, which is a standard practice in the topic modeling context.
Another option is using a stochastic eigendecomposition (see, e.g.,~\citesup{HalEtAl2011}) to approximate the whitening matrix.

\textbf{Variational inference.} For variational inference, we used the code of D.~Blei and modified it for the estimation of a non-symmetric Dirichlet prior $c$, which is known to be important~\citesup{WalEtAl2009b}. The default values of the tolerance/maximum number of iterations parameters are used for variational inference. The computational complexity of one iteration for one document of the variational inference algorithm is $O(RK)$, where $R$ is the number of non-zeros in the count vector for this document, which is then performed a significant number of times for each document. 

\subsubsection{Runtimes of the algorithms}\label{sec:runtimes}
In Table~\ref{tab:runtimes}, we present the running times of the algorithms from Section~\ref{sec:diagcmp}. 
\begin{table}[t!]
\centering
\begin{tabular}{| l | l l l |} \hline
           & min   & mean  & max \\ \hline
 JD-GP     & 148   & 192   & 247 \\
 JD-LDA    & 252   & 284   & 366 \\ \hline
 JD(k)-GP  & 157   & 190   & 247 \\
 JD(k)-LDA & 264   & 290   & 318 \\ \hline
 JD(f)-GP  & 1628  & 1846  & 2058 \\
 JD(f)-LDA & 2545  & 2649  & 2806 \\ \hline
 Spec-GP   & 101   & 107   & 111 \\
 Spec-LDA  & 107   & 140   & 193 \\ \hline
 TPM-GP    & 1734  & 2393  & 2726 \\
 TPM-LDA   & 12723 & 16460  & 19356 \\ \hline
\end{tabular}
\caption{The running times in seconds of the algorithms from Figure~\ref{plots:diag}, corresponds to the case when $N=50,000$. Each algorithm was run $5$ times, so the times in the table display the minimum (min), mean, and maximum (max) time. }
\label{tab:runtimes}
\end{table} 
JD and JD(k) are significantly faster than JD(f) as expected, although the performance in terms of the $\ell_1$-error is nearly the same for all of them. This indicates that preference should be given to the JD or JD(k) algorithms.

The running time of all LDA-algorithms is higher than the one of the GP/DICA-algorithms. This indicates that the computational complexity of the LDA-moments is slightly higher than the one of the GP/DICA-cumulants (compare, e.g., the times for the spectral algorithm which almost completely consist of the computation of the moments/cumulants). Moreover, the runtime of TPM-LDA is significantly higher (half an hour vs. several hours) than the one of TPM-GP/DICA. This can be explained by the fact that the LDA-moments have more noise than the GP/DICA-cumulants and, hence, the algorithm is slower. Interestingly, all versions of JD algorithm are not that sensitive to noise. 

Computation of a whitening matrix is roughly 30 sec (this time is the same for all algorithms and is included in the numbers above). 

\subsubsection{Initialization of the parameter c\textsubscript{0} for the LDA moments}\label{sec:initialization-c0}
The construction of the LDA moments requires the parameter $c_0$, which is not trivial to set in the unsupervised setting of topic modeling, especially taking into account the complexity of the evaluation for topic models~\cite{WalEtAl2009}. For the semi-synthetic experiments, the true value of $c_0$ is provided to the algorithms. It means that the LDA moments, in this case, have access to some oracle information, which in practice is never available. For real data experiments, $c_0$ is set to the value obtained with variational inference. The experiments in Appendix~\ref{sec:app:c0lda} show that this choice was somewhat important. However, this requires more thorough investigation.

\subsubsection{The LDA moments vs parameter c\textsubscript{0}}\label{sec:app:c0lda}
In this section, we experimentally investigate dependence of the LDA moments on the parameter~$c_0$.
 In Figure~\ref{fig:4}, the joint diagonalization algorithm with the LDA moment is compared for different values of $c_0$ provided to the algorithm. The data is generated similarly to Figure~\ref{plot:moms}. The experiment indicates that the LDA moments are somewhat sensitive to the choice of $c_0$. For example, the recovery $\ell_1$-error doubles when moving from the correct choice $c_0=1$ to the plausible alternative $c_0 = 0.1$ for $K=10$ on the \textit{LDAfix($200$)} dataset (JD-LDA(10) line on the right of Figure~\ref{fig:4}). 
\begin{figure}[t]
\centering
\begin{tabular}{cccc}
\includegraphics[width=.36\columnwidth]{fig_5_left_l1.eps} 
 & 
 
 &
 
 &
\includegraphics[width=.36\columnwidth]{fig_5_right_l1.eps} 
\end{tabular}
\caption{ Performance of the LDA moments depending on the parameter $c_0$. $D$ and $c$ are learned from the AP dataset for $K=10$ and $K=50$ and true $c_0=1$. JD-GP(10) for $K=10$ and JD-GP(50) for $K=50$. Number of sampled documents $N=20,000$. For the error bars, each dataset is resampled 5 times. Data \textbf{(left)}: \textit{GP} sampling; \textbf{(right)}: \textit{LDAfix($200$)} sampling.  
\textit{Note}: a smaller value of the $\ell_1$-error is better.  }
\label{fig:4}
\end{figure}



\subsubsection{Comparison of the \texorpdfstring{$\mathbf{\ell_1}$}{L1}- and \texorpdfstring{$\mathbf{\ell_2}$}{L2}-errors}
The sample complexity results~\cite{AnaEtAl2012} for the spectral algorithm for the LDA moments allow straightforward extension to the GP/DICA cumulants, if the results from Proposition~\ref{sample-complexity} are taken into account. The analysis is, however, in terms of the $\ell_2$-norm. Therefore, in Figure~\ref{l1vsl2}, we provide experimental comparison of the $\ell_1$- and $\ell_2$-errors to verify that they are indeed behaving similarly.
\begin{figure}[t]
\centering
\begin{tabular}{cccc}
\includegraphics[width=.36\columnwidth]{fig_6_left.eps} 
 & 
 
 &
 
 &
\includegraphics[width=.36\columnwidth]{fig_6_right.eps} 
\end{tabular}
\caption{Comparison of the $\ell_1$- and $\ell_2$- errors on the NIPS semi-synthetic dataset as in Figure~\ref{plot:moms} (top, left). The $\ell_2$ norms of the topics were normalized to [0,1] for the computation of the $\ell_2$ error. }
\label{l1vsl2}
\end{figure}


\subsubsection{Evaluation of the real data experiments}\label{sec:evaluation-real}
For the evaluation of topic recovery in the real data case, we use an approximation of the log-likelihood for held out documents as the metric. The approximation is computed using a Chib-style method as described by~\cite{WalEtAl2009} using the implementation by the authors.\footnote{\url{http://homepages.inf.ed.ac.uk/imurray2/pub/09etm}}
Importantly, this evaluation methods is applicable for both the LDA model as well as the GP model. Indeed, as it follows from Section~\ref{sec:2} and Appendix~\ref{sec:ldaproof2}, the GP model is equivalent to the LDA model when conditioning on the length of a document $L$ (with the same $c_k$ hyper parameters), while the LDA model does not make any assumption on the document length. For the test log-likelihood comparison, we thus treat the GP model as a LDA model (we do not include the likelihood of the document length).


\subsubsection{More on the real data experiments}\label{sec-expsup-real}
The detailed experimental setup is as follows.
Each dataset is separated into 5 training/evaluation pairs, where the documents for evaluation are chosen randomly and non-repetitively  among the folds (600 documents are held out for KOS; 400 documents are held out for AP; 450 documents are held out for NIPS). Then, the model parameters are learned  for a different number of topics. The evaluation of the held-out documents is performed with averaging over 5 folds. In Figure~\ref{lastfigure} and Figure~\ref{lastfigure-2}, on the y-axis, the predictive log-likelihood in bits averaged per token is presented. 

In addition to the experiments with AP and KOS in Figure~\ref{lastfigure}, we demonstrate one more experiment with the NIPS dataset in Figure~\ref{lastfigure-2} (right).

Note that, as the LDA moments require at least 3 tokens in each document, $1$ document from the NIPS dataset and 3 documents from the AP dataset, which did not fulfill this requirement, were removed.

\begin{figure}[!h]
\begin{center}
\begin{tabular}{ccc}
\includegraphics[width=.41\columnwidth]{fig_3_right_kos.eps} 
 & 
\includegraphics[width=.14\columnwidth]{real_legend.eps} 
 & 
\includegraphics[width=.41\columnwidth]{real_nips.eps} 
\end{tabular}
\vspace{-1em}
\caption{ 
Experiments with real data. \emp{Left:} the KOS dataset. \emp{Right:} the NIPS dataset. \textit{Note}: a higher value of the log-likelihood is better.
}
\label{lastfigure-2}
\end{center}
\end{figure}

Importantly, we observed that VI when initialized with the output of the JD-GP is consistently better in terms of the predictive log-likelihood.
Therefore, the new algorithm can be used for more clever initialization of other LDA/GP inference methods.

We also observe that the joint diagonalization algorithm for the LDA moments is worse than the spectral algorithm. This indicates that the diagonal structure~\eqref{diagS:lda} and~\eqref{diagT:lda} might not be present in the sample estimates~\eqref{S:lda:fs} and~\eqref{T:lda:fs} due to either model misspecification or to finite sample complexity issues.










\bibliographysup{litsup}
\bibliographystylesup{plain}









\end{document}
\section{Preliminaries}

\subsection{Summary of Supplementary Material}
The supplementary material is divided into two parts: 

(1) The first part is dedicated to the exposition
of the theoretical results presented in the main paper. 
Section~\ref{sec:FW_alg} details the variants of the Frank-Wolfe algorithm that we used and analyzed.
Section~\ref{sec:theory_fixed_eps} gives the proof to Theorem~\ref{thm:convergence_fixed_eps_main} (fixed $\shrinkAmount$) while Section~\ref{sec:theory_adaptive_eps} gives the proof to Theorem~\ref{thm:convergence_adaptive_eps_main} (adaptive $\shrinkAmount$). Finally, Section~\ref{sec:trw_properties} applies the convergence theorem to the TRW objective and investigates the relevant constants.

(2) The remainder of the supplementary material provides more information about the experimental setup as well as additional experimental results.

%
%


\subsection{Descent Lemma}
The following descent lemma is proved in \citesup{bertsekas1999nonlinear} (Prop. A24) and is standard for any convergence proof of first order methods. We provide a proof here for completeness. It also highlights the origin of the requirement that we use dual norm pairings between $\x$ and the gradient of $f(\x)$ (because of the generalized Cauchy-Schwartz inequality).
\begin{lemma}
	\label{lem:descent_lemma}
	\textbf{Descent Lemma}
	
	Let $\x_\stepsize :=  \x + \stepsize \dd$ and suppose that $f$ is continuously differentiable on the line segment from $\x$ to ${\x}_{\stepmax}$ for some $\stepmax > 0$.
Suppose that $L = {\sup}_{\alpha \in ]0,\stepmax]} \frac{|| \nabla f(\x+\alpha\dd)-\nabla f(\x)||_*}{||\alpha \dd||}$ is finite, then we have: 
	\begin{equation} \label{eq:descent_lemma}
	f(\x_\stepsize)\leq f(\x)+\stepsize \brangle{\nabla f(\x),\dd} + \frac{\stepsize^2}{2} L||\dd||^2, \quad \forall \stepsize \in [0, \stepmax].
	\end{equation}
\end{lemma}
\begin{proof}
	Let $0 < \stepsize \leq \stepmax$. Denoting $l(\alpha) =f(\x+\alpha\dd)$, we have that:
	\begin{dmath*}
		f(\x_\stepsize)-f(\x)=l(\stepsize)-l(0)=\int_{0}^{\stepsize}\nabla_{\alpha}l(\alpha)d\alpha\\
		=\int_{0}^{\stepsize} \brangle{\dd,\nabla f(\x+\alpha\dd)} d\alpha\\  %
		= \int_{0}^{\stepsize} \brangle{\dd,\nabla f(\x)} d\alpha+ 
		\int_{0}^{\stepsize} \dd^T (\nabla f(\x+\alpha\dd)-\nabla f(\x))d\alpha\\
		\leq \int_{0}^{\stepsize} \brangle{\dd,\nabla f(\x)}d\alpha+ 
		\left|\int_{0}^{\stepsize} \dd^T (\nabla f(\x+\alpha\dd)-\nabla f(\x))\right|d\alpha\\
		\leq \int_{0}^{\stepsize} \brangle{\dd,\nabla f(\x)}d\alpha+ 
		\int_{0}^{\stepsize} ||\dd||\;\;||\nabla f(\x+\alpha\dd)-\nabla f(\x)||_* \, d\alpha\\ %
		= \stepsize\brangle{\dd,\nabla f(\x)} +  
		\int_{0}^{\stepsize} ||\dd||\;\;\frac{||\nabla f(\x+\alpha\dd)-\nabla f(\x)||_*}{\alpha||\dd||}\alpha||\dd|| \, d\alpha\\
		\leq \stepsize\brangle{\dd,\nabla f(\x)} +  
		\int_{0}^{\stepsize} ||\dd||\;\;L||\dd||\alpha \, d\alpha\\
		= \stepsize\brangle{\dd,\nabla f(\x)} +  \frac{L}{2}\stepsize^2||\dd||^2\\
	\end{dmath*}
	Rearranging terms, we get the desired bound.
\end{proof}


\section{Frank-Wolfe Algorithms} \label{sec:FW_alg}

In this section, we present the various algorithms that we use to do fully corrective Frank-Wolfe (FCFW) with adaptive contractions over the domain $\DOM$, as was done in our experiments. 

\subsection{Overview of the Modified Frank-Wolfe Algorithm (FW with Away Steps)}
To implement the approximate correction steps in the fully corrective Frank-Wolfe (FCFW) algorithm, we use the Frank-Wolfe algorithm with away steps~\citepsup{Wolfe:1970wy}, also known as the modified Frank-Wolfe (MFW) algorithm~\citepsup{Guelat:1986fq}. 
We give pseudo-code for MFW in Algorithm~\ref{alg:MFW} (taken from~\citep{lacoste2015MFW}). 
This variant of Frank-Wolfe adds the possibility to do an ``away step'' (see step~5 in Algorithm~\ref{alg:MFW}) in order to avoid the zig zagging phenomenon that slows down Frank-Wolfe when the solution is close to the boundary of the polytope. 
For a strongly convex objective (with Lipschitz continuous gradient), the MFW was known to have asymptotic linear convergence~\citepsup{Guelat:1986fq} and its global linear convergence rate was shown recently~\citep{lacoste2015MFW}, accelerating the slow general sublinear rate of Frank-Wolfe. 
When performing a correction over the convex hull over a (somewhat small) set of vertices of $\DOMeps$, this convergence difference was quite significant in our experiments (MFW converging in a small number of iterations to do an approximate correction vs. FW taking hundreds of iterations to reach a similar level of accuracy). 
We note that the TRW objective is strongly convex when all the edge probabilities are non-zero~\citep{wainwright2005new}; and that it has Lipschitz gradient over $\DOMeps$ (but not $\DOM$).

The gap computed in step~6 of~Algorithm~\ref{alg:MFW} is non-standard; it is a sufficient condition to ensure the global linear convergence of the outer FCFW algorithm when using Algorithm~\ref{alg:MFW} as a subroutine to implement the approximate correction step. See~\citet{lacoste2015MFW} for more details.

The MFW algorithm requires more bookkeeping than standard FW: in addition to the current iterate $\xk$, it also maintains both the active set $\Coreset^{(k)}$ (to search for the ``away vertex'') as well as the barycentric coordinates $\bm{\alpha}^{(k)}$ (to know what are the away step-sizes that ensure feasibility -- see step~13) i.e. $\xk= \sum_{\vv \in \Coreset^{(k)}} \alpha^{(k)}_{\vv} \vv$.

\begin{algorithm}
	\caption{Modified Frank-Wolfe algorithm (FW with Away Steps) -- used for approximate correction}
	\label{alg:MFW}
	\begin{algorithmic}[1]
	\STATE Function \textbf{MFW}$(\x^{(0)}, \bm{\alpha}^{(0)}, \Vertices, \stopCrit)$ to optimize over $\conv(\Vertices)$: 
	\STATE \textbf{Inputs:} Set of atoms $\Vertices$, starting point $\x^{(0)}= \sum_{\vv \in \Coreset^{(0)}} \alpha^{(0)}_{\vv} \vv$ where $\Coreset^{(0)}$ is active set and $\bm{\alpha}^{(0)}$ the active coordinates, stopping criterion $\stopCrit$.
	\FOR{$k=0\dots K$}
		\STATE Let $\s_k \in \displaystyle\argmin_{\vv \in \Vertices} \textstyle\brangle{\nabla f(\x^{(k)}),\vv}$ and $\dd_k^\FW := \s_k - \x^{(k)}$ \qquad~~ \emph{\small(the FW direction)}
		\STATE Let $\vv_k \in \displaystyle\argmax_{\vv \in \Coreset^{(k)} } \textstyle\left\langle \nabla f(\x^{(k)}), \vv \right\rangle$ and $\dd_k^\away := \x^{(k)} - \vv_k$ \qquad \emph{\small(the away direction)}
		\STATE $g_k^\pFW  := \left\langle -\nabla f(\x^{(k)}), \dd_k^\FW + \dd_k^\away\right\rangle$ \qquad \emph{\small(stringent gap is FW + away gap to work better for FCFW)}
		\IF{$g_k^\pFW \leq \stopCrit$}
			\STATE \textbf{return} $\x^{(k)}$, $\bm{\alpha}^{(k)}$, $\Coreset^{(k)}$.
		\ELSE
  				\IF{$\left\langle -\nabla f(\x^{(k)}), \dd_k^\FW\right\rangle  \geq \left\langle -\nabla f(\x^{(k)}), \dd_k^\away\right\rangle$ }
		 		  \STATE $\dd_k :=  \dd_k^\FW$, and $\stepmax := 1$  
		 			     \hspace{20mm}\emph{\small(choose the FW direction)}
		 		  \ELSE
		 		  \STATE $\dd_k :=  \dd_k^\away$, and $\stepmax := \frac{\alpha_{\vv_k}}{(1- \alpha_{\vv_k})}$
		 		  	\hspace{12mm}\emph{\small(choose away direction; maximum feasible step-size)}
		 		  \ENDIF	
		 		  \STATE Line-search: $\stepsize_k \in \displaystyle\argmin_{\stepsize \in [0,\stepmax]} \textstyle f\left(\x^{(k)} + \stepsize \dd_k\right)$
			\STATE Update $\x^{(k+1)} := \x^{(k)} + \stepsize_k \dd_k$
			\STATE Update coordinates $\bm{\alpha}^{(k+1)}$ accordingly (see \citet{lacoste2015MFW}).
			\STATE Update $\Coreset^{(k+1)} := \{\vv \: s.t. \: \alpha^{(k+1)}_{\vv} > 0\}$
		 \ENDIF	
	\ENDFOR
	
	
	\end{algorithmic}
\end{algorithm}


\subsection{Fully Corrective Frank-Wolfe (FCFW) with Adaptive-$\shrinkAmount$}

We give in Algorithm~\ref{alg:adaptive_eps} the pseudo-code to perform fully corrective Frank-Wolfe optimization over $\DOM$ by iteratively optimizing over $\DOMeps$ with adaptive-$\shrinkAmount$ updates.
If $\shrinkAmount$ is kept constant (skipping step 10), then Algorithm~\ref{alg:adaptive_eps} implements the
fixed~$\shrinkAmount$ variant over $\DOMeps$. We describe the algorithm as maintaining the correction set of atoms~$V^{(k+1)}$ over $\DOM$ (rather than $\DOMeps$), as $\shrinkAmount$ is constantly changing. One can easily move back and forth between $V^{(k+1)}$ and its contraction $V_\shrinkAmount = (1-\epsk{k})V^{(k+1)} + \epsk{k} \unif$, and so we note that an efficient implementation might work with either representation cheaply (for example, by storing only $V^{(k+1)}$ and $\shrinkAmount$, not the perturbed version of the correction polytope). The approximate correction over $V_\shrinkAmount$ is implemented using the MFW algorithm described in Algorithm~\ref{alg:MFW}, which requires a barycentric representation $\bm{\alpha}^{(k)}$ of the current iterate $\xk$ over the correction polytope $V_\shrinkAmount$. Our notation in Algorithm~\ref{alg:adaptive_eps} uses the elements of $\Vertices$ as indices, rather than their contracted version; that is, we maintain the property that $\xk = \sum_{\vv \in \Vertices} \alpha_{\vv}^{(k)} [(1-\epsk{k}) \vv + \epsk{k} \unif]$. As $V_\shrinkAmount$ changes when $\shrinkAmount$ changes, we need to update the barycentric representation of $\xk$ accordingly -- this is done in step~11 with the following equation. Suppose that we decrease $\shrinkAmount$ to $\shrinkAmount'$. Then the old coordinates $\bm{\alpha}$ can be updated to new coordinates $\bm{\alpha}'$ for the new contraction polytope as follows:
\begin{align}
\begin{split} \label{eq:alpha_update}
\alpha'_{\vv} &= \alpha_{\vv} \frac{1-\shrinkAmount}{1-\shrinkAmount'} \qquad \text{for} \quad \vv \in \Vertices \setminus \{\unif \}, \\
\alpha'_{\unif} &= 1-\sum_{\vv \neq \unif} \alpha'_{\vv}.
\end{split}
\end{align} 
This ensures that $\sum_{\vv} \alpha_{\vv} \vv_{(\shrinkAmount)} = \sum_{\vv} \alpha'_{\vv} \vv_{(\shrinkAmount')}$, where $\vv_{(\shrinkAmount)} := (1-\shrinkAmount) \vv + \shrinkAmount \unif$, and that the coordinates form a valid convex combination (assuming that $\shrinkAmount' \leq \shrinkAmount$), as can be readily verified.

%
\begin{algorithm}
	\caption{Optimizing $f$ over $\DOM$ using Fully Corrective Frank-Wolfe (FCFW) with Adaptive-$\shrinkAmount$ Algorithm.}
	\label{alg:adaptive_eps}
	\begin{algorithmic}[1]
		\STATE \textbf{FCFW}$(\x^{(0)}, \Vertices, \stopCrit, \epsk{\mathrm{init}})$
		\STATE \textbf{Inputs:} Set of atoms $\Vertices$ so that $\DOM = \conv({\Vertices})$, active set $\Coreset^{(0)}$, starting point $\x^{(0)}= \sum_{\vv \in \Coreset^{(0)}} \alpha^{(0)}_{\vv}  [(1-\epsk{\mathrm{init}})\vv + \epsk{\mathrm{init}} \unif]$ where $\bm{\alpha}^{(0)}$ are the active coordinates, $\epsk{\mathrm{init}}\leq \frac{1}{4}$ describes the initial contraction of the polytope, stopping criterion $\stopCrit$, $\unif$ is a fixed reference point in the relative interior of $\DOM$.
		\STATE Let $V^{(0)} := \Coreset^{(0)}$ \quad (optionally, a bigger $V^{(0)}$ could be passed as argument for a warm start), $\epsk{-1} := \epsk{\mathrm{init}}$ 
	\FOR{$k=0\dots K$}
		\STATE Let $\sk \in \displaystyle\argmin_{\vv \in \Vertices} \textstyle\innerProd{\nabla f(\x^{(k)})}{\vv}$ \qquad \emph{\small(the FW vertex)} 
		\STATE Compute $\gk=\brangle{-\nabla f(\xk),\sk-\xk}$ \quad \emph{\small(FW gap)} 
		\IF{$\gk \leq \stopCrit$}
			\STATE \textbf{return} $\x^{(k)}$
		\ENDIF
%
%
%
%
%
%
%
		\STATE Let $\epsk{k}$ be $\epsk{k-1}$ updated according to Algorithm~\ref{alg:adaptive_update}.
		\STATE Update $\bm{\alpha}^{(k)}$ accordingly (using~\eqref{eq:alpha_update})
		\STATE Let $\skeps := (1-\epsk{k})\sk + \epsk{k} \unif$ 
		\STATE Let $\dd_k^\FW := \skeps - \x^{(k)}$ 
		\STATE Line-search: $\stepsize_k \in \displaystyle\argmin_{\stepsize \in [0,1]} \textstyle f\left(\x^{(k)} + \stepsize \dd_k^\FW\right)$
		\STATE Set $\x^{(\mathrm{temp)}} := \x^{(k)} + \stepsize_k  \dd_k^\FW$  \algComment{initialize correction to the update after a FW step with line search}
		\STATE $\bm{\alpha}^{(\mathrm{temp})} = (1-\stepsize_k) \bm{\alpha}^{(k)}$
		\STATE $\alpha_{\sk}^{(\mathrm{temp)}} \leftarrow \alpha_{\sk}^{(\mathrm{temp)}} + \stepsize_k$ \algComment{update coordinates according to the FW step}
		\STATE Update (non-contracted) correction polytope: $V^{(k+1)} := V^{(k)} \cup \{ \sk \}$
		\STATE Let $V_\shrinkAmount = (1-\epsk{k})V^{(k+1)} + \epsk{k} \unif$ \algComment{contracted correction polytope}
		\STATE $\x^{(k+1)}\!\!$, $\bm{\alpha}^{(k+1)} := \textbf{\textrm{MFW}}(\x^{(\mathrm{temp)}},\bm{\alpha}^{(\mathrm{temp})}, V_\shrinkAmount, \stopCrit)$ \quad \emph{\small (approximate correction step on $V_\shrinkAmount$ using MFW)}
	\ENDFOR
	\end{algorithmic}
\end{algorithm}



%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% ICML 2016 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use the following line _only_ if you're still using LaTeX 2.09.
%\documentstyle[icml2016,epsf,natbib]{article}
% If you rely on Latex2e packages, like most moden people use this:
\documentclass{article}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts

% use Times
\usepackage{times}
% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure} 
\usepackage{framed}
% For citations
\usepackage{natbib}

\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{todonotes}
\usepackage{wrapfig}
\usepackage{xfrac}

\usepackage[fleqn]{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{example}{Example}
\newtheorem{remark}{Remark}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{assumption}{Assumption}
\newtheorem{property}{Property}
\newtheorem{observation}{Observation}
\newtheorem{hypothesis}{Hypothesis}

%\newcommand*\diff{\mathop{}\!\mathrm{d}}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\prob}{Pr}
\newcommand\given[1][]{\:#1\vert\: }
\newcommand\givenb{\given[\Big]}

\newcommand\ceil[1]{\lceil#1\rceil}

\newcommand{\bX}{\mathbf{X}}
\newcommand{\bY}{\mathbf{Y}}
\newcommand{\bH}{\mathbf{H}}
\newcommand{\bZ}{\mathbf{Z}}

% COLORED TEXT
\iffalse
\newcommand{\af}[1]{\textcolor{green}{#1}}
\newcommand{\dk}[1]{\textcolor{orange}{#1}}
\newcommand{\nb}[1]{\textcolor{blue}{#1}}
\newcommand{\da}[1]{\textcolor{brown}{#1}}
\fi

\newcommand{\af}[1]{\textcolor{black}{#1}}
\newcommand{\dk}[1]{\textcolor{black}{#1}}
\newcommand{\nb}[1]{\textcolor{black}{#1}}
\newcommand{\da}[1]{\textcolor{black}{#1}}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2016} with
% \usepackage[nohyperref]{icml2016} above.
\usepackage{hyperref}

% if I may:
\usepackage[all]{hypcap} % this makes the figure references point to the top of the figure
                         % rather than the caption

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
%\usepackage{icml2017} 
% \renewcommand{\theenumi}{\Alph{enumi}}
\usepackage{enumitem}
% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Proceedings of the...''
\usepackage[accepted]{icml2017}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{A Closer Look at Memorization in Deep Networks}

\begin{document} 

\twocolumn[
\icmltitle{A Closer Look at Memorization in Deep Networks}
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Devansh Arpit}{equal,mila,udem}
\icmlauthor{Stanisław Jastrzębski}{equal,jag}
\icmlauthor{Nicolas Ballas}{equal,mila,udem}
\icmlauthor{David Krueger}{equal,mila,udem}
\icmlauthor{Emmanuel Bengio}{mcgill}
\icmlauthor{Maxinder S. Kanwal}{ucb}
\icmlauthor{Tegan Maharaj}{mila,poly}
\icmlauthor{Asja Fischer}{bonn}
\icmlauthor{Aaron Courville}{mila,udem,cifar1}
\icmlauthor{Yoshua Bengio}{mila,udem,cifar}
\icmlauthor{Simon Lacoste-Julien}{mila,udem}
\end{icmlauthorlist}

\icmlaffiliation{mila}{Montr\'eal Institute for Learning Algorithms, Canada}
\icmlaffiliation{udem}{Universit\'e de Montr\'eal, Canada}
\icmlaffiliation{jag}{Jagiellonian University, Krakow, Poland}
\icmlaffiliation{poly}{Polytechnique Montr\'eal, Canada}
\icmlaffiliation{bonn}{University of Bonn, Bonn, Germany}
\icmlaffiliation{cifar1}{CIFAR Fellow}
\icmlaffiliation{cifar}{CIFAR Senior Fellow}
\icmlaffiliation{ucb}{University of California, Berkeley, USA}
\icmlaffiliation{mcgill}{McGill University, Canada}

\icmlcorrespondingauthor{}{david.krueger@umontreal.ca}

% You may provide any keywords that you 
% find helpful for describing your paper; these are used to populate 
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{deep learning, deep networks, capacity, regularization}


\vskip 0.3in
]

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
We examine the role of memorization in deep learning, drawing connections to capacity, generalization, and adversarial robustness.
While deep networks are capable of memorizing noise data, our results suggest that they tend to prioritize learning simple patterns first.
In our experiments, we expose qualitative differences in gradient-based optimization of deep neural networks (DNNs) on noise vs.~real data.
We also demonstrate that for appropriately tuned explicit regularization (e.g.,~dropout) we can degrade DNN training performance on noise datasets without compromising generalization on real data.
Our analysis suggests that the notions of effective capacity which are dataset independent are unlikely to explain the generalization performance of deep networks when trained with gradient based methods because training data itself plays an important role in determining the degree of memorization.
\end{abstract}






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

% overfitting as memorization
The traditional view of generalization holds that a model with sufficient capacity (e.g. more parameters than training examples) will be able to ``memorize'' each example, overfitting the training set and yielding poor generalization to validation and test sets \citep{dl_book}.
Yet deep neural networks (DNNs) often achieve excellent generalization performance with massively over-parameterized models. This phenomenon is not well-understood.

% DL priors don't explain generalization
From a representation learning perspective, the generalization capabilities of DNNs \da{are believed to} stem from their incorporation of good generic priors (see, e.g., \citet{bengio2009}). 
\citet{Tegmark} further suggest that the priors of deep learning are well suited to the physical world. 
But while the priors of deep learning may help explain why DNNs learn to efficiently represent complex real-world functions, they are not restrictive enough to rule out memorization.


On the contrary, deep nets are known to be universal approximators, capable of representing arbitrarily complex functions given sufficient capacity \cite{cybenko1989approximation, hornik1989multilayer}.  
Furthermore, recent work has shown that the expressiveness of DNNs grows exponentially with depth \cite{montufar2014,ganguli2016}.
These works, however, only examine the \textit{representational capacity}, that is, the set of hypotheses a model is capable of expressing via some value of its parameters.

% introduce / define effective capacity 
Because DNN optimization is not well-understood, it is unclear which of these hypotheses can actually be reached by gradient-based training \cite{bottou1998online}.
In this sense, optimization and generalization are entwined in DNNs.
To account for this, we formalize a notion of the \textit{effective capacity (EC)} of a learning algorithm $\mathcal{A}$ (defined by specifying both the model {\it and the training procedure}, e.g.,``train the LeNet architecture \citep{lecun1998mnist} for 100 epochs using \af{stochastic gradient descent} (SGD) with a learning rate of $0.01$'') as the set of hypotheses which can be reached by applying that learning algorithm on {\it some} dataset.  Formally, using set-builder notation:
$$
EC(\mathcal{A}) = \{h \mid  \exists \mathcal{D} \text{ such that } h \in \mathcal{A}(\mathcal{D}) \} \enspace,
$$
where $\mathcal{A}(\mathcal{D})$
represents the set of hypotheses that is reachable by $\mathcal{A}$ on a dataset $\mathcal{D}$\footnote{
Since $\mathcal{A}$ can be stochastic, $\mathcal{A}(\mathcal{D})$ is a set.
}.

% effective capacity... is not the answer! (Zhang et al)
One might suspect that DNNs effective capacity is sufficiently limited by gradient-based training and early stopping to resolve the apparent paradox between DNNs' excellent generalization and their high representational capacity.
However, the experiments of \citet{understanding_DL} suggest that this is not the case.
They demonstrate that DNNs are able to fit pure noise without even needing substantially longer training time. 
Thus even the {\it effective} capacity of DNNs may be too large, from the point of view of traditional learning theory.

% link to memorization
By demonstrating the ability of DNNs to ``memorize'' random noise, \citet{understanding_DL} also raise the question whether deep networks use similar memorization tactics on real datasets.
Intuitively, a brute-force memorization approach to fitting data does not capitalize on patterns shared between training examples or features; the \emph{content} of what is memorized is irrelevant.
A paradigmatic example of a memorization algorithm is k-nearest neighbors \citep{fix1951discriminatory}.
Like \citet{understanding_DL}, we do not formally define memorization; rather, we investigate this intuitive notion of memorization by training DNNs to fit random data.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Main Contributions}
 We operationalize the definition of ``memorization'' as {\it the behavior exhibited by DNNs trained on noise}, and conduct a series of experiments that contrast the learning dynamics of DNNs on real vs.~noise data.
 Thus, our analysis builds on the work of \citet{understanding_DL} and further investigates the role of memorization in DNNs. 

Our findings are summarized as follows:
\begin{enumerate}
\item There are qualitative differences in DNN optimization behavior on real data vs.~noise.
In other words, DNNs do not just memorize real data (Section ~\ref{sec:qualitative_differences}).
\item DNNs learn simple patterns first, before memorizing (Section ~\ref{sec:simple_patterns_first}).
In other words, DNN optimization is {\it content-aware}, taking advantage of patterns shared by multiple training examples.
\item Regularization techniques can differentially hinder memorization in DNNs while preserving their ability to learn about real data (Section ~\ref{sec:regularization}). 
\end{enumerate}












%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiment Details}

We perform experiments on MNIST \citep{lecun1998mnist} and CIFAR10 \citep{CIFAR10} datasets.
We investigate two classes of models: 2-layer multi-layer perceptrons (MLPs) with rectifier linear units (ReLUs) on MNIST and convolutional neural networks (CNNs) on CIFAR10.
If not stated otherwise, the MLPs have 4096 hidden units per layer and are trained for $1000$ epochs with SGD and learning rate $0.01$. 
The CNNs are a small Alexnet-style CNN\footnote{Input $\rightarrow$ Crop(2,2) $\rightarrow$ Conv(200,5,5) $\rightarrow$ BN $\rightarrow$ ReLU $\rightarrow$ MaxPooling(3,3) $\rightarrow$ Conv(200,5,5) $\rightarrow$ BN$\rightarrow$ ReLU$\rightarrow$ MaxPooling(3,3) $\rightarrow$ Dense(384) $\rightarrow$ BN $\rightarrow$ ReLU $\rightarrow$ Dense(192) $\rightarrow$ BN $\rightarrow$ ReLU $\rightarrow$ Dense($\#$classes) $\rightarrow$ Softmax. Here Crop(. , .) crops height and width from both sides with respective values. } (as in \citet{understanding_DL}), and are trained using SGD with momentum=$0.9$ and learning rate of $0.01$, scheduled to drop by half every 15 epochs.

Following \citet{understanding_DL}, in many of our experiments we replace either (some portion of) the labels (with random labels), or the inputs (with i.i.d.~Gaussian noise matching the real dataset's mean and variance) for some fraction of the training set. 
We use \emph{randX} and \emph{randY} to denote datasets with (100\%, unless specified) noisy inputs and labels (respectively).




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Qualitative Differences of DNNs Trained on Random vs. Real Data}
\label{sec:qualitative_differences}

\citet{understanding_DL} empirically demonstrated that DNNs are capable of fitting random data, which implicitly necessitates some high degree of memorization.
In this section, we investigate whether DNNs employ similar memorization strategy when trained on real data.  
In particular, our experiments highlight some qualitative differences between DNNs trained on real data vs.~random data, supporting the fact that DNNs do not use brute-force memorization to fit real datasets.



\subsection{Easy Examples as Evidence of Patterns in Real Data}
\label{sec:easy_examples}
A brute-force memorization approach to fitting data should apply equally well to different training examples.
However, if a network is learning based on patterns in the data, some examples may fit these patterns better than others.
We show that such ``easy examples'' (as well as correspondingly ``hard examples'') are common in real, but not in random, datasets.
Specifically, for each setting (real data, randX, randY), we train an MLP for a single epoch starting from 100 different random initializations and shufflings of the data.
We find that, for real data, many examples are consistently classified (in)correctly after a single epoch, suggesting that different examples are significantly easier or harder in this sense.
For noise data, the difference between examples is much less, indicating that these examples are fit (more) independently.
Results are presented in Figure~\ref{fig:easy_hard.pdf}.

For randX, apparent differences in difficulty are well modeled as random Binomial noise.
For randY, this is not the case, indicating some use of shared patterns. 
Visualizing first-level features learned by a CNN supports this hypothesis (Figure~\ref{fig:filters.pdf}).

% easy/hard
\begin{figure}[!t]
  \center
  \label{fig:easy_hard.pdf}
  \includegraphics[width=7.5cm]{fig_png/replacement_figure_1.png}
  \caption{Average (over 100 experiments) misclassification rate for each of 1000 examples after one epoch of training.
           This measure of an example's difficulty is much more variable in real data.
       We conjecture this is because the easier examples are explained by some simple patterns, which are reliably learned within the first epoch of training.
	   We include 1000 points samples from a binomial distribution with $n=100$ and $p$ equal to the average estimated P(correct) for randX, and note that this curve closely resembles the randX curve, suggesting that random inputs are all equally difficult.
}
\end{figure}

% filters
\begin{figure}[!t]
  \center
  \label{fig:filters.pdf}
  \includegraphics[width=7.5cm]{fig_png/filters.pdf}
  \caption{Filters from first layer of network trained on CIFAR10 (left) and randY (right).}
\end{figure}

% GINI
\begin{figure*}[!ht]
  \centering
  \includegraphics[width=6.1cm]{fig_png/emmanuel_gini_overall.pdf}
  \hspace{-1em}
  \includegraphics[width=6.1cm]{fig_png/emmanuel_gini_IY.pdf}
  \caption{Plots of the Gini coefficient of $\bar{g}_\mathbf{x}$ over examples $\mathbf{x}$ (see section \ref{sec:grad_sens}) as training progresses, for a 1000-example real dataset (14x14 MNIST) versus random data. On the left, $Y$ is the normal class label; on the right, there are as many classes as examples, the network has to learn to map each example to a unique class.
    }
  \label{fig:grad_sensitivity}
\end{figure*}
        


\subsection{Loss-Sensitivity in Real vs. Random Data}
\label{sec:grad_sens}

To further investigate the difference between real and fully random inputs, we propose a proxy measure of memorization via gradients. Since we cannot measure quantitatively how much each training sample $\mathbf{x}$ is memorized, we instead measure the effect of each sample on the average loss. 
That is, we measure the norm of the loss gradient with respect to a previous example $\mathbf{x}$ after $t$ SGD updates. 
Let $\mathcal{L}_t$ be the loss after $t$ updates; then the sensitivity measure is given by
$$g^t_\mathbf{x}=\|\partial \mathcal{L}_t/\partial \mathbf{x}\|_1 \enspace.$$
The parameter update from training on $\mathbf{x}$ influences all future $\mathcal{L}_t$ indirectly by changing the subsequent updates on different training examples.
We denote the average over $g^t_\mathbf{x}$ after $T$ steps as $\bar{g}_\mathbf{x}$, and refer to it as \emph{loss-sensitivity}. Note that we only report $\ell^1$-norm results, but that results stay very similar using  $\ell^2$-norm and infinity norm. 

We compute $g^t_\mathbf{x}$ by unrolling $t$ SGD steps and applying backpropagation over the unrolled computation graph, as done by \citet{maclaurin2015gradient}.
Unlike \citet{maclaurin2015gradient}, we only use this procedure to compute $g^t_\mathbf{x}$, and do not modify the training procedure in any way.

We find that for real data, only a subset of the training set has high $\bar{g}_\mathbf{x}$, while for random data, $\bar{g}_\mathbf{x}$ is high for virtually all examples. We also find a different behavior when \textit{each example} is given a unique class; in this scenario, the network has to learn to identify each example uniquely,
yet still behaves differently when given real data than when given random data as input.

We visualize  (Figure \ref{fig:grad_sensitivity}) the spread of $\bar{g}_\mathbf{x}$ as training progresses by computing the Gini coefficient over $\mathbf{x}$'s. The Gini coefficient \citep{gini} is a measure of the inequality among values of a frequency distribution; a coefficient of 0 means \af{exact equality (i.e.,~all values are the same), while a coefficient of 1 means maximal inequality among values.}
We observe that, when trained on real data, the network has a high $\bar{g}_\mathbf{x}$ for a few examples, while on random data the network is sensitive to most examples. The difference between the random data scenario, where we know the neural network needs to do memorization, and the real data scenario, where we're trying to understand what happens, leads us to believe that this measure is indeed sensitive to memorization. Additionally, these results suggest that when being trained on real data, the neural network probably does not memorize, or at least not in the same manner it needs to for random data.

In addition to the different behaviors for real and random data described above, we also consider a class specific loss-sensitivity: $\bar{g}_{i, j} =\mathbb{E}_{(x,y)}\sfrac{1}{T}\sum_t^T|\partial \mathcal{L}_t(y=i)/\partial x_{y=j}|$, where $\mathcal{L}_t(y=i)$ is the term in the crossentropy sum corresponding to class $i$.
We observe that the loss-sensitivity 
\af{w.r.t.~class $i$ for}
training examples of class $j$
is higher when $i=j$, but more spread out for real data (see Figure \ref{fig:grad_class_sensitivity}).
An interpretation of this is that for real data there are more interesting cross-category patterns that can be learned than for random data.

\begin{figure}[h]
  \centering
  \hspace{-2.0em}
  \includegraphics[width=8.6cm]{fig_png/emmanuel_class_sensitivity_both.pdf}
  \caption{Plots of per-class $g_x$ (see previous figure; log scale), a cell $i,j$ represents the average $|\partial \mathcal{L}(y=i)/\partial x_{y=j}|$, i.e. the loss-sensitivity of examples of class $i$ w.r.t. training examples of class $j$. Left is real data, right is random data.}
  \label{fig:grad_class_sensitivity}
\end{figure}

Figure \ref{fig:grad_sensitivity} and \ref{fig:grad_class_sensitivity} were obtained by training a fully-connected network with 2 layers of 16 units on 1000 downscaled $14\times 14$ MNIST digits using SGD. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Capacity and Effective Capacity}

In this section, we investigate the impact of capacity and effective capacity on learning of datasets having different amounts of random input data or random labels.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Effects of capacity and dataset size on validation performances}

In a first experiment, we study how overall model capacity impacts the validation performances for datasets with different amounts of noise.
On MNIST, we found that the optimal validation performance requires a higher capacity model in the presence of noise examples (see Figure~\ref{fig:val_acc__vs__capacity}).
This trend was consistent for noise inputs on CIFAR10, but we did not notice any relationship between capacity and validation performance on random \emph{labels} on CIFAR10. 

This result contradicts the intuitions of traditional learning theory, which suggest that capacity should be restricted, in order to enforce the learning of (only) the most regular patterns.
Given that DNNs can perfectly fit the training set in any case, we hypothesize that that higher capacity allows the network to fit the noise examples in a way that does not interfere with learning the real data.
In contrast, if we were simply to \emph{remove} noise examples, yielding a smaller (clean) dataset, a \emph{lower} capacity model would be able to achieve optimal performance. 
	
%%%%%%%
% PLOT: val_acc as a function of capacity
\begin{figure}[!ht]
\centering
%\includegraphics[scale=.3]{fig_png/plot1.pdf}
\includegraphics[scale=.072]{fig_png/plot1_.png}
\caption{Performance as a function of capacity in 2-layer MLPs \af{trained on (noisy versions of) MNIST}. For real data, performance is already very close to maximal with 4096 hidden units, but when there is noise in the dataset, higher capacity is needed.}
\label{fig:val_acc__vs__capacity}
\end{figure}
	
	
%%%%%%%
% PLOT: time-to-convergence
\begin{figure*}[!t]
   \center
  \subfigure
  {
%    \includegraphics[scale=.32]{fig_png/plot2.pdf}
    \includegraphics[scale=.08]{fig_png/plot2_.png}
  }
  \hspace{1mm}
  \subfigure
  {
%    \includegraphics[scale=.33]{fig_png/plot3.pdf}
    \includegraphics[scale=.08]{fig_png/plot3.png}
  }
   \caption{
Time to convergence as a function of capacity with dataset size fixed to 50000 (left), or dataset size with capacity fixed to 4096 units (right).
``Noise level'' denotes to the proportion of training points whose inputs are replaced by Gaussian noise.
Because of the patterns underlying real data, having more capacity/data does not decrease/increase training time as much as it does for noise data.
}
   \label{fig:ttc__vs__capacity}
\end{figure*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Effects of capacity and dataset size on training time}
Our next experiment measures time-to-convergence, i.e. how many epochs it takes to reach 100\% training accuracy.
Reducing the capacity or increasing the size of the dataset slows down training as well for real as for noise data\footnote{
Regularization can also increase time-to-convergence; see section~\ref{sec:regularization}.
}.
However, the effect is more severe for datasets containing noise, as our experiments in this section show (see Figure~\ref{fig:ttc__vs__capacity}).

% capacity
Effective capacity of a DNN can be increased by increasing the representational capacity (e.g.~adding more hidden units) or training for longer.
Thus, increasing the number of hidden units decreases the number of training iterations needed to fit the data, up to some limit.
We observe \emph{stronger} diminishing returns from increasing representational capacity for real data, indicating that this limit is lower, and a smaller representational capacity is sufficient, for real datasets.

% dataset size
Increasing the number of examples (keeping representational capacity fixed) also increases the time needed to memorize the training set.
In the limit, the representational capacity is simply insufficient, and memorization is not feasible.
On the other hand, when the relationship between inputs and outputs is meaningful, new examples simply give more (possibly redundant) clues as to what the input $\rightarrow$ output mapping is.
Thus, in the limit, an idealized learner should be able to predict unseen examples perfectly, absent noise.
Our experiments demonstrate that time-to-convergence is not only longer on noise data (as noted by \citet{understanding_DL}), but also, \emph{increases} substantially as a function of dataset size, relative to real data.
Following the reasoning above, this suggests that our networks are learning to extract patterns in the data, rather than memorizing. 


        \begin{figure*}[!t]
		\center
		\subfigure[Noise added on classification inputs.]
		{
			\label{fig:mnist_noisey_acc}
%			\includegraphics[width=4.1cm]{fig_png/mnist_noisex_accuracy_2.pdf}
%            \includegraphics[width=4.1cm]{fig_png/mnist_noisex_critical_valid_2.pdf}
			\includegraphics[width=4.1cm]{fig_png/mnist_noisex_accuracy_2.png}
            \includegraphics[width=4.1cm]{fig_png/mnist_noisex_critical_valid_2.png}
                                                
		}
		\hspace{0.3mm}
		\subfigure[Noise added on classification labels.]
		 {
%                   \includegraphics[width=4.1cm]{fig_png/mnist_noisey_accuracy_2.pdf}
%                   \includegraphics[width=4.1cm]{fig_png/mnist_noisey_critical_valid_2.pdf}
                   \includegraphics[width=4.1cm]{fig_png/mnist_noisey_accuracy_2.png}
                   \includegraphics[width=4.1cm]{fig_png/mnist_noisey_critical_valid_2.png}
		   \label{fig:mnist_noisey_critical}
		}
		\caption{Accuracy (left in each pair, solid is train, dotted is validation) and Critical sample ratios (right in each pair) for MNIST.}

	\label{fig:mnist_nicolas}
	\end{figure*}


        \begin{figure*}[!t]
		\center
		\subfigure[Noise added on classification inputs.]
		{
			\label{fig:cifar10_noisey_acc}
%			\includegraphics[width=4.1cm]{fig_png/cifar10_noisex_accuracy_2.pdf}
%            \includegraphics[width=4.1cm]{fig_png/cifar10_noisex_critical_valid_2.pdf}
			\includegraphics[width=4.1cm]{fig_png/cifar10_noisex_accuracy_2.png}
            \includegraphics[width=4.1cm]{fig_png/cifar10_noisex_critical_valid_2.png}
                                                
		}
		\hspace{0.3mm}
		\subfigure[Noise added on classification labels.]
		 {
%                   \includegraphics[width=4.1cm]{fig_png/cifar10_noisey_accuracy_2.pdf}
%                   \includegraphics[width=4.1cm]{fig_png/cifar10_noisey_critical_valid_2.pdf}
                   \includegraphics[width=4.1cm]{fig_png/cifar10_noisey_accuracy_2.png}
                   \includegraphics[width=4.1cm]{fig_png/cifar10_noisey_critical_valid_2.png}
		   \label{fig:cifar10_noisey_critical}
		}
		\caption{Accuracy (left in each pair, solid is train, dotted is validation) and Critical sample ratios (right in each pair) for CIFAR10.}
	
    \label{fig:cifar_nicolas}
	\end{figure*}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{DNNs Learn Patterns First}
\label{sec:simple_patterns_first}
This section aims at studying how the complexity of \af{the hypotheses} learned by DNNs evolve during training for real data vs.~noise data. 
To achieve this goal, we build on the intuition that the number of different decision regions into which an input space is partitioned reflects the complexity of the learned hypothesis~\citep{sokolic2016robust}. 
This notion is similar in spirit to the degree to which a function can scatter random labels: a higher density of decision boundaries in the data space allows more samples to be scattered.

\af{Therefore, we estimate the complexity} by measuring how densely points on the data manifold are present around the model's decision boundaries. 
Intuitively, if we were to randomly sample points from the data distribution, a smaller fraction of points in the proximity of a decision boundary suggests that the learned hypothesis is simpler.


\subsection{Critical Sample Ratio (CSR)}
Here we introduce the notion of a {\it critical sample}, which we use to estimate the density of decision boundaries as discussed above.
Critical samples are a subset of a dataset such that for each such sample $\mathbf{x}$, there exists at least one adversarial example $\hat{\mathbf{x}}$ in the proximity of $\mathbf{x}$. 
Specifically, consider a classification network's output vector $ f(\mathbf{x}) =(f_1(\mathbf{x}), \dots, f_k(\mathbf{x})) \in \mathbb{R}^k $ for a given input sample $ \mathbf{x} \in \mathbb{R}^{n}$ from the data manifold. 
Formally we call a dataset sample $ \mathbf{x} $ a \emph{critical sample} if there exists a point $ \hat{\mathbf{x}} $ such that,
\begin{align}
\label{eq_sens}
&\arg \max_{i} f_{i}(\mathbf{x}) \neq \arg \max_{j} f_{j}(\hat{\mathbf{x}}) \mspace{5mu}  \\ \nonumber
& \mbox{s.t.  } \lVert \mathbf{x} - \hat{\mathbf{x}} \rVert_{\infty} \leq r
\end{align}
where $r$ is a fixed box size.
As in recent work on adversarial examples \citep{kurakin2016adversarial} the above definition depends only on the predicted label $ \arg \max_{i} f_{i}(\mathbf{x}) $ of $ \mathbf{x} $, and not the true label (as in earlier work on adversarial examples, such as \citet{adversarial_examples,goodfellow2014explaining}).


\af{Following the above argument relating complexity to decision boundaries}, a higher number of critical samples indicates a more complex hypothesis. Thus, we measure complexity as the {\it critical sample ratio (CSR)}, that is, the fraction of data-points in a set $|\mathcal{D}|$ for which we can find a critical sample: $ \frac{\# \text{critical samples}}{|\mathcal{D}|}$.

To identify whether a given data point $ \mathbf{x} $ is a critical samples, we search for an adversarial sample $ \hat{\mathbf{x}} $ within a box of radius $r$.
To perform this search, we propose using Langevin dynamics applied to the fast gradient sign method (FGSM, \citet{goodfellow2014explaining}) as shown in algorithm \ref{algo_sens_search}\footnote{In our experiments, we set $\alpha=0.25$, $\beta=0.2$ and $\eta$ is samples from standard normal distribution.}.
We refer to this method as Langevin adversarial sample search (LASS).
While the FGSM search algorithm can get stuck at a points with zero gradient, LASS explores the box more thoroughly.
Specifically, a problem with first order gradient search methods (like FGSM) is that there might exist training points where the gradient is 0, but with a large $2^{nd}$ derivative corresponding to a large change in prediction in the neighborhood. 
The noise added by the LASS algorithm during the search enables escaping from such points.

% LASS
\begin{algorithm}[!ht]
	\caption{Langevin Adversarial Sample Search (LASS)}
	\begin{algorithmic}[1]
		\REQUIRE $\mathbf{x} \in \mathbb{R}^{n}$, $\alpha$, $\beta$, $ r $, noise process $ \eta $
		\ENSURE $\hat{\mathbf{x}}$
        \STATE converged = FALSE
		\STATE $ \tilde{\mathbf{x}} \leftarrow \mathbf{x} $; $ \hat{\mathbf{x}} \leftarrow \emptyset$
		\WHILE {not converged or max iter reached}
		\STATE $\Delta = \alpha \cdot \mbox{sign}(\frac{\partial f_{k}(\mathbf{x})}{\partial \mathbf{x}}) + \beta \cdot \eta$
		\STATE $ \tilde{\mathbf{x}}  \leftarrow \tilde{\mathbf{x}} + \Delta$
		\FOR { $ i \in [n]$}
		\STATE  $ \tilde{\mathbf{x}}_{i}  \leftarrow \left\{ \begin{array}{ll}
		{\mathbf{x}}_{i} + r \cdot \mbox{sign}(\tilde{\mathbf{x}}_{i}- {\mathbf{x}}^{i}) & if \lvert \tilde{\mathbf{x}}_{i}- {\mathbf{x}}_{i} \rvert > r \\
		\tilde{\mathbf{x}}_{i} &  otherwise \\
		\end{array} 
		\right.$
		\ENDFOR
        \IF{$\arg \max_{i} f(\mathbf{x}) \neq \arg \max_{i} f(\tilde{\mathbf{x}})$}
        \STATE converged = TRUE
        \STATE $\hat{\mathbf{x}} \leftarrow \tilde{\mathbf{x}}$
        \ENDIF
		\ENDWHILE
	\end{algorithmic}
    \label{algo_sens_search}
\end{algorithm}

\begin{figure}[!t]
	\center
	\label{fig_valid_stability}
	\includegraphics[width=5cm]{fig_png/valid_stability.png}
	\caption{Critical sample ratio throughout training on CIFAR-10, random input (randX), and random label (randY) datasets.}
\end{figure}

\subsection{Critical Samples Throughout Training}
\label{sec_CS}
We now show that the number of critical samples is much higher for a deep network (specifically, a CNN) trained on noise data compared with real data.
To do so, we measure the number of critical samples in the validation set\footnote{
We also measure the number of critical samples in the training sets. Since we train our models using log loss, training points are pushed away from the decision boundary even after the network learns to classify them correctly. This leads to an initial rise and then fall of the number of critical samples in the training sets.
},
throughout training\footnote{We use a box size of 0.3, which is small enough in a 0-255 pixel scale to be unnoticeable by a human evaluator. \af{Different values for $r$ were tested but did not change results qualitatively and lead to the same conclusions}}.
Results are shown in Figure \ref{fig_valid_stability}.
A higher number of critical samples \af{for models trained on} noise data compared with those \af{trained on} real data suggests that the learned decision surface is more complex for noise data (randX and randY).
We also observe that the CSR increases gradually with increasing number of epochs and then stabilizes. This suggests that the networks learn gradually more complex hypotheses during training for all three datasets.


In our next experiment, we evaluate the performance and critical sample ratio of datasets with $ 20\% $ to  $ 80 \% $ of the training data replaced with either input or label noise.
Results for MNIST and CIFAR-10 are shown in Figures \ref{fig:mnist_nicolas} and \ref{fig:cifar_nicolas}, respectively.
For both randX and randY datasets, the CSR is higher for noisier datasets, reflecting the higher level of complexity of the learned prediction function. 
The final and maximum validation accuracies are also both lower for noisier datasets, indicating that the noise examples interfere somewhat with the networks ability to learn about the real data.

More significantly, for randY datasets (Figures \ref{fig:mnist_noisey_critical} and \ref{fig:cifar10_noisey_critical}), the network achieves maximum accuracy on the validation set before achieving high accuracy on the training set.
Thus the model first learns the simple and general patterns of the real data before fitting the noise (which results in decreasing validation accuracy).
Furthermore, as the model moves from fitting real data to fitting noise, the CSR greatly increases, indicating the need for more complex hypotheses to explain the noise.
Combining this result with our results from Section~\ref{sec:easy_examples}, we conclude that real data examples are easier to fit than noise.










%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Effect of Regularization on Learning}
\label{sec:regularization}
Here we demonstrate the ability of regularization to degrade training performance on data with random labels, while maintaining generalization performance on real data.
\citet{understanding_DL} argue that explicit regularizations are not the main explanation of good generalization performance, rather SGD based optimization is largely responsible for it. Our findings extend their claim and indicate that explicit regularizations can substantially limit the speed of memorization of noise data without significantly impacting learning on real data.

{
We compare the performance of CNNs trained on CIFAR-10 and randY with the following regularizers: dropout (with dropout rates in range  $0$-$0.9$), input dropout (range $0$-$0.9$),  input Gaussian noise (with standard deviation in range $0$-$5$), hidden Gaussian noise (range $0$-$0.3$), weight decay (range $0$-$1$) and additionally dropout with adversarial training (with weighting factor in range $0.2$-$0.7$ and dropout in rate range $0.03$-$0.5$).\footnote{We perform adversarial training using critical samples found by LASS algorithm with default parameters. }
We train a separate model for every combination of dataset, regularization technique, and regularization parameter.

\da{The results are summarized in Figure~\ref{fig:reg_pattern}.
\af{For each combination of dataset and regularization technique, the final training accuracy on randY (x-axis) is plotted against the best validation accuracy on CIFAR-10 from amongst the models trained with different regularization parameters (y-axis).} 
Flat curves indicate that the corresponding regularization technique can reduce memorization when applied on random labeling, while resulting in the same validation accuracy \af{on the clean validation set}.}
Our results show that different regularizers target memorization behavior to different extent -- dropout being the most effective. 
We find that dropout, especially coupled with adversarial training, is best at hindering memorization without reducing the model's ability to learn.
Figure~\ref{fig:randY_capping} additionally shows this effect for selected experiments (i.e. selected hyperparameter values) in terms of train loss.}


    \begin{figure}[!t]
        \center
        \label{fig:reg_pattern}
        \includegraphics[width=5.5cm]{fig_png/reg_pattern.png}
        \caption{Effect of different regularizers on train accuracy (on noise dataset) vs.~validation accuracy (on real dataset). Flatter curves indicate that memorization (on noise) can be capped without sacrificing generalization (on real data).}
    \end{figure}
    
    \begin{figure}[!t]
        \center
        %	\subfigure
       % 	{
        \label{fig:randY_capping}
        \includegraphics[width=4cm]{fig_png/randY_capping.png}
        \includegraphics[width=3.8cm]{fig_png/realY_capping.png}
        %		}
        \caption{Training curves for different regularization techniques on random label (left) and real (right) data.  The vertical ordering of the curves is different for random labels than for real data, indicating differences in the propensity of different regularizers to slow-down memorization.}
    \end{figure}

%\floatbarrier

















%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}

% Zhang et al / data-set dependent capacity
Our work builds on the experiments and challenges the interpretations of \citet{understanding_DL}.
We make heavy use of their methodology of studying DNN training in the context of noise datasets. \citet{understanding_DL} show that DNNs can perfectly fit noise and thus that their generalization ability cannot be explained through traditional statistical learning theory (e.g., see~\cite{vapnik1998statistical, bartlett2005local}). 
We agree with this finding, but show in addition that the degree of memorization and generalization in DNNs depends not only on the architecture and training procedure (including explicit regularizations), \textit{but also on the training data itself}\footnote{We conclude the latter part based on experimental findings in sections \ref{sec:qualitative_differences} and \ref{sec_CS}}. 

% relationship btw regularization and generalization
Another direction we investigate is the relationship between regularization and memorization. 
\citet{understanding_DL} argue that explicit and implicit regularizers (including SGD) might not explain or limit shattering of random data. 
In this work we show that regularizers (especially dropout) {\it do} control the \emph{speed} at which DNNs memorize. This is interesting since dropout is also known to prevent catastrophic forgetting \cite{goodfellow2013empirical} and thus in general it seems to help DNNs retain patterns.

% SGD --> simple firstw
A number of arguments support the idea that SGD-based learning imparts a regularization effect, especially with a small batch size~\citep{wilson2003general} or a small number of epochs~\citep{hardt2015train}.
Previous work also suggests that SGD prioritizes the learning of simple hypothesis first. 
\citet{CIS-125961} showed that, for linear models, SGD first learns models with small $\ell^2$ parameter norm.
More generally, the efficacy of early stopping shows that SGD first learns simpler models~\citep{yao2007early}. 
We extend these results, showing that DNNs trained with SGD learn patterns before memorizing, {\it even in the presence of noise examples}. 

% why do DNNs generalize?
Various previous works have analyzed explanations for the generalization power of DNNs.
\citet{Montavon2011} use kernel methods to analyze the complexity of deep learning architectures, and find that network priors (e.g.~implemented by the network structure of a CNN or MLP) control the speed of learning at each layer. 
\citet{neyshabur2014search} note that the number of parameters does not control the effective capacity of a DNN, and that the reason for DNNs' generalization is unknown. 
We supplement this result by showing how the impact of representational capacity changes with varying noise levels. % relationship btw noise and learning dynamics
While exploring the effect of noise samples on learning dynamics has a long tradition~\citep{bishop1995training, an1996effects}, we are the first to examine {\it relationships} between the fraction of noise samples and other attributes of the learning algorithm, namely: capacity, training time and dataset size.

% analyzing DNN training, critical samples
Multiple techniques for analyzing the training of DNNs have been proposed before, including looking at generalization error, trajectory length evolution~\citep{raghu2016expressive}, analyzing Jacobians associated to different layers~\citep{wang2016analysis, saxe2013exact}, or the shape of the loss minima found by SGD~\citep{im2016empirical, chaudhari2016entropy, keskar2016large}.
Instead of measuring the sharpness of the loss for the learned hypothesis, we investigate the complexity of the learned hypothesis throughout training and across different datasets and regularizers, as measured by the critical sample ratio. 
%Critical samples are closely related to adversarial examples  
Critical samples refer to real data-points that have adversarial examples \citep{adversarial_examples,goodfellow2014explaining} nearby.
Adversarial examples originally referred to imperceptibly perturbed data-points that are confidently misclassified.
\citep{ miyato2015distributional} define {\it virtual} adversarial examples via changes in the predictive distribution instead, thus extending the definition to unlabeled data-points.
\citet{kurakin2016adversarial} recommend using this definition when training on adversarial examples, and it is the definition we use.



% contemporary
Two contemporary works perform in-depth explorations of topics related to our work.
\citet{bojanowski2017} show that predicting random noise targets can yield state of the art results in unsupervised learning, corroborating our findings in Section ~\ref{sec:easy_examples}, especially Figure ~\ref{fig:filters.pdf}.
\citet{koh2017understanding} use {\it influence functions} to measure the impact on parameter changes during training, as in our Section~\ref{sec:grad_sens}.
They explore several promising applications for this technique, including generation of adversarial {\it training} examples.
















%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}

Our empirical exploration demonstrates qualitative differences in DNN optimization on noise vs.~real data, all of which support the claim that DNNs trained with SGD-variants first use patterns, not brute force memorization, to fit real data. 
However, since DNNs have the demonstrated ability to fit noise, it is unclear why they find generalizable solutions on real data; we believe that the deep learning priors including distributed and hierarchical representations likely play an important role.
Our analysis suggests that memorization and generalization in DNNs depend on network architecture and optimization procedure, but also on the data itself.
We hope to encourage future research on how properties of datasets influence the behavior of deep learning algorithms, and suggest a data-dependent understanding of DNN capacity as a research goal.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection*{Acknowledgments}
We thank Akram Erraqabi, Jason Jo and Ian Goodfellow for helpful discussions. 
SJ was supported by Grant No.~DI 2014/016644 from Ministry of Science and Higher Education, Poland. DA was supported by IVADO, CIFAR and NSERC.
EB was financially supported by the Samsung Advanced Institute of Technology (SAIT).
MSK and SJ were supported by MILA during the course of this work.
We acknowledge the computing resources provided by ComputeCanada and CalculQuebec.
Experiments were carried out using Theano~\citep{theano} and Keras~\citep{keras}. 
    
\bibliography{main}
\bibliographystyle{icml2017}
\end{document} 

\documentclass{article} %
\usepackage{nips13submit_e,times}

\usepackage{graphicx, amsmath, amssymb,amsthm}
\usepackage[breaklinks]{hyperref} %
\usepackage{url}
\usepackage{bm,color}
\usepackage{mathdots}

%
%

%
\usepackage{wrapfig}
\usepackage{multicol}
%
\usepackage[boxruled,commentsnumbered]{algorithm2e}
\newenvironment{algo}{%
  \renewenvironment{algocf}[1][h]{}{}%
  \algorithm
}{%
  \endalgorithm
}

\definecolor{darkgreen}{rgb}{0,.4,.2}
\definecolor{darkblue}{rgb}{.1,.2,.6}
\definecolor{brightblue}{rgb}{0,0.6,0.8}
\hypersetup{
  colorlinks=true,
  linkcolor=darkblue,
  citecolor=darkgreen,
  filecolor=darkblue,
  urlcolor=darkblue
}

%
%

%
%
%
%
%
%
%
%
%
%

%
\newtheorem*{rep@theorem}{\rep@title}
\newcommand{\newreptheorem}[2]{%
\newenvironment{rep#1}[1]{%
 \def\rep@title{#2 \ref{##1}}%
 \begin{rep@theorem}}%
 {\end{rep@theorem}}}

\newreptheorem{lemma}{Lemma'}
\newreptheorem{proposition}{Proposition'}
\newreptheorem{theorem}{Theorem'}
\newreptheorem{remark}{Remark'}

%
%
%

\newtheorem{definition}{Definition}
\newtheorem{theorem}[definition]{Theorem}
\newtheorem{proposition}[definition]{Proposition}
\newtheorem{lemma}[definition]{Lemma}
\newtheorem{corollary}[definition]{Corollary}
\newtheorem{observation}[definition]{Observation}
\newtheorem{conjecture}[definition]{Conjecture}
\newtheorem{remark}[definition]{Remark}
\newtheorem{problem}{Problem}
\newtheorem{claim}{Claim}
\newtheorem{open}{Open Problem}
\newtheorem*{example}{Example}

%
\DeclareMathOperator{\st}{\textrm{s.t.}}
\DeclareMathOperator*{\conv}{conv}
\DeclareMathOperator{\diam}{diam}
\DeclareMathOperator*{\E}{\rm E}
\DeclareMathOperator*{\argmin}{\arg\min}
\DeclareMathOperator*{\argmax}{\arg\max}
\DeclareMathOperator{\lmax}{\lambda_{\max}}
\DeclareMathOperator{\lmin}{\lambda_{\min}}
\DeclareMathOperator*{\supp}{supp}
\DeclareMathOperator*{\diag}{diag}
\DeclareMathOperator*{\sign}{sign}
\DeclareMathOperator*{\tr}{Tr}
\DeclareMathOperator*{\rk}{Rk}
\DeclareMathOperator*{\card}{card}
\DeclareMathOperator*{\relint}{\mathop{relint}}

%
\providecommand{\abs}[1]{\left\lvert#1\right\rvert}
\providecommand{\norm}[1]{\left\lVert#1\right\rVert}
\providecommand{\dualnorm}[1]{\norm{#1}_*}
\providecommand{\frobnorm}[1]{\norm{#1}_{Fro}}
\providecommand{\tracenorm}[1]{\norm{#1}_{tr}}
\providecommand{\opnorm}[1]{\norm{#1}_{op}}
\providecommand{\maxnorm}[1]{\norm{#1}_{\max}}
\providecommand{\latGrNorm}[1]{\norm{#1}_{\groups}}

\newcommand{\ball}{\mathcal{B}}
%

\newcommand{\bigO}{O}
\newcommand{\R}{\mathbb{R}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Sym}{\mathbb{S}}

%
\newcommand{\domain}{\mathcal{D}}
\newcommand{\stepsize}{\gamma}
\newcommand{\stepmax}{\stepsize_{\textrm{max}}} %
\newcommand{\stepbound}{\stepsize^\textrm{B}} %
\newcommand{\FW}{{\hspace{0.05em}\textsf{FW}}}
\newcommand{\away}{{\hspace{0.06em}\textsf{A}}}
\newcommand{\Cf}{C_{\hspace{-0.08em}f}}
\newcommand{\CfA}{C_{\hspace{-0.08em}f}^-}
\newcommand{\CfMFW}{C_{\hspace{-0.08em}f}^\away}
\newcommand{\strongConvFW}{\mu_{\hspace{-0.08em}f}^\FW}
\newcommand{\strongConvMFW}{\mu_{\hspace{-0.08em}f}^\away}
\newcommand{\distToBoundary}{{\delta_{\x^*\!,\domain}}}
\newcommand{\dirW}{\mathop{dirW}}
\newcommand{\x}{\bm{x}}
\newcommand{\y}{\bm{y}}
\newcommand{\z}{\bm{z}}
\newcommand{\s}{\bm{s}}
\newcommand{\dd}{\bm{d}}
\newcommand{\uu}{\bm{u}}
\newcommand{\vv}{\bm{v}} %
\newcommand{\atoms}{\mathcal A}
\newcommand{\groups}{\mathcal G}
\newcommand{\row}{\text{row}}
\newcommand{\col}{\text{col}}
\newcommand{\lft}{\text{left}}
\newcommand{\rgt}{\text{right}}
\newcommand{\mapprox}{\nu} %
\newcommand{\CfTotal}{C_f^{\text{\tiny{prod}}}}

%
\newcommand{\Vertices}{\mathcal{V}}
\newcommand{\Coreset}{\mathcal{S}}
\renewcommand{\S}{\mathcal{S}}
\renewcommand{\aa}{\bm{\alpha}}
\renewcommand{\r}{\bm{r}}
\newcommand{\PdirW}{\mathop{PdirW}}
\newcommand{\innerProd}[2]{\left\langle #1 , #2 \right\rangle}
\newcommand{\C}{\mathcal{C}}
\newcommand{\proj}{\bm{P}}
\newcommand{\K}{\bm{K}}
\newcommand{\Kface}{\mathcal{K}}

%
\newcommand{\Spectahedron}{\mathcal{S}}
\newcommand{\SpectahedronLeOne}{\mathcal{S}_{\le 1}} %
\newcommand{\SpectahedronLeT}{\mathcal{S}_{t}} %
\newcommand{\MaxCutPolytope}{\boxplus}
\newcommand{\FourPointPSD}{\Spectahedron^4_{\text{sparse}}}
\newcommand{\FourPointPSDplus}{\Spectahedron^{4+}_{\text{sparse}}}
\newcommand{\FourPointPSDminus}{\Spectahedron^{4-}_{\text{sparse}}}
\newcommand{\LOneBall}{\diamondsuit}
\newcommand{\signVec}{\mathbf{s}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\id}{\mathbf{I}} %
\newcommand{\ind}{\mathbf{1}} %
\newcommand{\0}{\mathbf{0}} %
\newcommand{\unit}{\mathbf{e}} %
\newcommand{\one}{\mathbf{1}} %
\newcommand{\zero}{\mathbf{0}}
\newcommand\SetOf[2]{\left\{#1\vphantom{#2}\right.\left|\vphantom{#1}\,#2\right\}}
\newcommand{\ignore}[1]{}%


\newcommand{\todo}[1]{\marginpar[\hspace*{4.5em}\textbf{TODO}\hspace*{-4.5em}]{\textbf{TODO}}\textbf{TODO:} #1}
\newcommand{\note}[1]{{\textbf{\color{red}#1}}}
\newcommand{\remove}[1]{} %



%

\title{An Affine Invariant Linear Convergence Analysis\\ for Frank-Wolfe Algorithms}
%

\author{
Simon Lacoste-Julien \\
INRIA - SIERRA project-team\\
{\'E}cole Normale Sup{\'e}rieure, Paris, France \\
\And
Martin Jaggi \\
Simons Institute for the Theory of Computing \\
UC Berkeley, USA \\
}

%
%
%
%
%
%
%

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\nipsfinalcopy %

\begin{document}


\maketitle
\vspace{-2mm}

\begin{abstract}\vspace{-2mm}
%
We study the linear convergence of variants of the Frank-Wolfe algorithms for some classes of strongly convex problems, using only affine-invariant quantities. 
As in~\cite{Guelat:1986fq}, we show the linear convergence of the standard Frank-Wolfe algorithm when the solution is in the interior of the domain, but with affine invariant constants. We also show the linear convergence of the away-steps variant of the Frank-Wolfe algorithm, but with constants which only depend on the geometry of the domain, and not any property of the location of the optimal solution.
Running these algorithms does not require knowing any problem specific parameters.
%
%
\end{abstract}

The Frank-Wolfe algorithm \cite{Frank:1956vp} (also known as \emph{conditional gradient}) is one of the earliest existing methods for constrained convex optimization, and has seen an impressive revival recently due to its nice properties compared to projected or proximal gradient methods, in particular for sparse optimization and machine learning applications.

On the other hand, the classical projected gradient and proximal methods have been known to exhibit a very nice adaptive acceleration property, 
namely that the the convergence rate becomes linear for strongly convex objective, i.e. that  the optimization error of the same algorithm after $k$ iterations will decrease geometrically with $O(\rho^{-k})$ instead of the usual $O(1/k)$ for general convex objective functions.
It has become an active research topic recently whether such an acceleration is also possible for Frank-Wolfe type methods.
%
%
%

\vspace{-2mm}
\paragraph{Contributions.}
We show that the Frank-Wolfe algorithm with away-steps converges linearly (i.e. with a geometric rate) for any strongly convex objective function optimized over a polytope domain, with a constant bounded away from zero that only depends on the geometry of the polytope. Our convergence analysis is affine invariant (both the algorithm and the convergence rate are unaffected by an affine transformation of the variables). 
%
%
%
%
%
Also, our analysis does not depend on the location of the true optimum with respect to the domain, which was a disadvantage of earlier existing results such as \cite{Wolfe:1970wy,Guelat:1986fq,Beck:2004jm}, and the later results \cite{Ahipasaoglu:2008il,Kumar:2010ku,Allende:2013vw} that need Robinson's condition \cite{Robinson:1982ii}. Our analysis yields a weaker sufficient condition than Robinson's condition; in particular we can have linear convergence even in some cases when the function has more than one global minima and is not globally strongly convex.
As a second contribution, we provide an affine invariant version of the analysis of~\cite{Guelat:1986fq} showing that the classical (unmodified) Frank-Wolfe algorithm converges linearly on strongly convex functions when the optimum lies in the interior.

\vspace{-2mm}
\paragraph{Related Work.}
%
The away-steps variant of the Frank-Wolfe algorithm, that can also remove weight from ``bad'' ones of the currently active atoms, was proposed in \cite{Wolfe:1970wy}, and later also analyzed in \cite{Guelat:1986fq}. The precise algorithm is stated below in Algorithm \ref{alg:MFW}.
An alternative away-step algorithm (with a sublinear convergence rate) has been considered by \cite{Clarkson:2010hv}, namely performing an away step whenever the number of atoms of non-zero weight has exceeded a fixed target size. The disadvantage of this method is that it requires knowledge of the curvature constant, which is not realistic in many practical applications. %
%
For the classical Frank-Wolfe algorithm, the early work of \cite[Theorem 6.1]{Levitin:1966gf} has shown a linear convergence rate under the strong requirement that the objective is strongly convex, and furthermore the domain is strongly convex as a set. %
\cite{Beck:2004jm} has shown a linear rate for the special case of quadratic objectives when the optimum is in the strict interior of the domain, 
%
but their result was already subsumed by~\cite{Guelat:1986fq}.
More recently \cite{Ahipasaoglu:2008il,Kumar:2010ku,Allende:2013vw} have obtained linear convergence results in the case that the optimum solution satisfies Robinson's condition \cite{Robinson:1982ii}.
In a different recent line of work, \cite{Garber:2013vl,Garber:2013dl} has studied an algorithm variation\footnote{This can be interpreted as a concrete instantiation of the stronger oracle proposed in~\cite{Lan:2013um}.} that moves mass from the worst vertices to the ``towards'' vertex until a specific condition is satisfied, yielding a linear convergence rate. %
Their algorithm requires the knowledge of several constants though, and moreover is not adaptive to the best-case scenario, unlike the Frank-Wolfe algorithm with away steps and line-search. 
None of these previous works was shown to be affine invariant, and most require additional knowledge about problem specific parameters.%
%


\vspace{-2mm}
%
\section{Frank-Wolfe Algorithms, and Away-Steps}\vspace{-2mm}

%
%
%
%
%
%
%
%
%
%
%
%
%
%

\begin{wrapfigure}{r}{9.3cm}\vspace{-1.5em} %
\begin{algo}[H]
  \caption{Frank-Wolfe Algorithm with Away Steps}
  \label{alg:MFW}
\SetAlgoNoLine\DontPrintSemicolon
  Let $\x^{(0)} \in \Vertices$, and $\Coreset^{(0)} := \{\x^{(0)}\}$ \vspace{-1mm}\;
   \hspace{3cm}\emph{\small(so that $\alpha^{(0)}_{\vv} = 1$ for $\vv=\x^{(0)}$ and $0$ otherwise)} \vspace{-1mm}\;
  %
  \For{$k=0\dots K$}{
  %
  Let $\s_k \in \displaystyle\argmin_{\vv \in \Vertices} \textstyle\left\langle \nabla f(\x^{(k)}), \vv \right\rangle$ and $\dd_k^\FW := \s_k - \x^{(k)}$ \vspace{-2.5mm}\;
    \hspace{6.3cm}\emph{\small(the FW direction)} \;
  Let $\vv_k \in \displaystyle\argmax_{\vv \in \Coreset^{(k)} } \textstyle\left\langle \nabla f(\x^{(k)}), \vv \right\rangle$ and $\dd_k^\away := \x^{(k)} - \vv_k$ \vspace{-3mm}\;
    \hspace{6.1cm}\emph{\small(the away direction)} \;
  \eIf{$\left\langle \nabla f(\x^{(k)}), \dd_k^\FW\right\rangle  \leq \left\langle \nabla f(\x^{(k)}), \dd_k^\away\right\rangle$ }{
  	$\dd_k :=  \dd_k^\FW$, and $\stepmax := 1$  
	     \hspace{5mm}\emph{\small(choose the FW direction)} \;
  	}{
	$\dd_k :=  \dd_k^\away$, and $\stepmax := \alpha_{\vv_k} / (1- \alpha_{\vv_k})$   \;
	     \hspace{0.5cm}\emph{\small(choose the away direction, and maximum feasible step-size)} \vspace{-2mm}\;
  }	
  Line search: $\stepsize_k \in \displaystyle\argmin_{\stepsize \in [0,\stepmax]} \textstyle f\left(\x^{(k)} + \stepsize \dd_k\right)$ \;
  Update $\x^{(k+1)} := \x^{(k)} + \stepsize_k \dd_k$ \;
       \hspace{2cm}\emph{\small(and accordingly for the weights $\aa^{(k+1)}$, see text)} \;
  Update $\Coreset^{(k+1)} := \{\vv \: s.t. \: \alpha^{(k+1)}_{\vv} > 0\}$\;
  }\vspace{-1mm}%
\end{algo}
\end{wrapfigure}


%
We consider general constrained convex optimization problems of the form\vspace{-1mm}
\begin{equation*}\label{eq:optGenConvex}
   \min_{\x \in \domain} \, f(\x) \ .\vspace{-1mm}
\end{equation*}
We assume~$f$ is convex and differentiable, and that the domain~$\domain$ %
is a bounded convex subset of a vector space.
%
The Frank-Wolfe method~\cite{Frank:1956vp}, also known as \emph{conditional gradient}~\cite{Levitin:1966gf} works as follows:
%
%
%
%
%
%
%
%
%
At a current~$\x^{(k)}$, the algorithm considers the linearization of the objective function, and moves slightly towards a minimizer of this linear function (taken over the same domain).
In terms of convergence, it is known that the iterates of Frank-Wolfe satisfy $f(\x^{(k)}) - f(\x^*) \le O\big(1/k\big)$, for $\x^*$ being an optimal solution~\cite{Frank:1956vp,Dunn:1978di,Jaggi:2013wg}. %
%
One of the main reasons for the recent increased popularity of Frank-Wolfe-type algorithms is the sparsity of the iterates, i.e. that the iterate is always represented as a sparse convex combination of at most $k$ vertices $\Coreset^{(k)}\subseteq \Vertices$ of the domain~$\domain$, which we write as $\x^{(k)}= \sum_{\vv \in \Coreset^{(k)}} \alpha^{(k)}_{\vv} \vv$.
Here $\Vertices$ is defined to be the set of vertices (extreme points) of $\domain$, so that $\domain = \conv(\Vertices)$.
We assume that the \emph{linear oracle} defining $\s_k$ always returns a point from $\Vertices$ as a minimizer.

%
%

\vspace{-4mm}
\paragraph{Away-Steps.}
The away-steps variant of Frank-Wolfe, as stated in Algorithm \ref{alg:MFW}, was proposed in \cite{Wolfe:1970wy}, with the idea to also remove weight from ``bad'' ones of the currently active atoms. %
Note that the classical Frank-Wolfe algorithm is obtained by only using the FW direction in Algorithm~\ref{alg:MFW}.
If $\stepsize_k = \stepmax$, then we call this step a \emph{drop step}, as it fully removes the vertex $\vv_k$ from the currently active set of atoms  $\Coreset^{(k)}$.
%
The updates of the algorithm are of the following form:
For a FW step, we have $\Coreset^{(k+1)} = \{\s_k\}$ if $\stepsize_k = 1$; otherwise $\Coreset^{(k+1)} = \Coreset^{(k)} \cup \{\s_k\}$. Also, we have $\alpha^{(k+1)}_{\s_k} := (1-\stepsize_k) \alpha^{(k)}_{\s_k} + \stepsize_k$ and $\alpha^{(k+1)}_{\vv} := (1-\stepsize_k) \alpha^{(k)}_{\vv}$ for $\vv \in \Coreset^{(k)} \setminus  \{\s_k\}$. 
For an away step, we have $\Coreset^{(k+1)} = \Coreset^{(k)} \setminus \{\vv_k\}$ if  $\stepsize_k = \stepmax$ (a \emph{drop step}); 
otherwise $\Coreset^{(k+1)} = \Coreset^{(k)}$.  Also, we have $\alpha^{(k+1)}_{\vv_k} := (1+\stepsize_k) \alpha^{(k)}_{\vv_k} - \stepsize_k$ and $\alpha^{(k+1)}_{\vv} := (1+\stepsize_k) \alpha^{(k)}_{\vv}$ for $\vv \in \Coreset^{(k)} \setminus  \{\vv_k\}$. 



\vspace{-2mm}
%
\section{Affine Invariant Measures of Smoothness and Strong Convexity}\vspace{-2mm}
%
\paragraph{Affine Invariance.}
%
%
An optimization method is called \emph{affine invariant} if it is invariant under affine transformations of the input problem: If one chooses any re-parameterization of the domain~$\domain$, by a \emph{surjective} linear or affine map $M:\hat\domain\rightarrow\domain$, then the ``old'' and ``new'' optimization problems $\min_{\x\in\domain}f(\x)$ and $\min_{\hat\x\in\hat\domain}\hat f(\hat\x)$ for $\hat f(\hat\x):=f(M\hat\x)$ look completely the same to the algorithm.
%
More precisely, every ``new'' iterate must remain exactly the transform of the corresponding old iterate; an affine invariant analysis should thus yield the convergence rate and constants unchanged by the transformation. It is well known that Newton's method is affine invariant under invertible $M$, and the Frank-Wolfe algorithm is affine invariant in the even stronger sense under arbitrary $M$ \cite{Jaggi:2013wg}. (This is directly implied if the algorithm and all constants appearing in the analysis only depend on inner products with the gradient, which are preserved since $\nabla \hat f = M^T\nabla f$.)
%
%
%
%
%
%
%
%
%
%
%
%
%

%

%
%
%
%

%
\vspace{-2mm}
\paragraph{Affine Invariant Measures of Smoothness.}
The affine invariant convergence analysis of the standard Frank-Wolfe algorithm by \cite{Jaggi:2013wg} crucially relies on the following measure of non-linearity of the objective function $f$ over the domain $\domain$. The \emph{curvature constant} $C_{f}$ of a convex and differentiable function $f:\R^n\rightarrow\R$, with respect to a compact domain $\domain$ is defined as\vspace{-1mm}
\begin{equation}\label{eq:Cf}
  \Cf := \sup_{\substack{\x,\s\in \domain,  ~\stepsize\in[0,1],\\
                      \y = \x+\stepsize(\s-\x)}} \textstyle
           \frac{2}{\stepsize^2}\big( f(\y)-f(\x)-\innerProd{\nabla f(\x)}{\y-\x}\big) \ .\vspace{-4mm}
\end{equation}

%
%
%
%

The assumption of bounded curvature $\Cf$ closely corresponds to a Lipschitz assumption on the gradient of~$f$. %
More precisely, if $\nabla f$ is $L$-Lipschitz continuous on $\domain$ with respect to some arbitrary chosen norm $\norm{.}$, then\vspace{-2mm}
\begin{equation} \label{eq:CfBound}
\Cf \le \diam_{\norm{.}}(\domain)^2 L \ ,\vspace{-0.5mm}
\end{equation}
where $\diam_{\norm{.}}(.)$ denotes the $\norm{.}$-diameter, see \cite[Lemma 7]{Jaggi:2013wg}.
%
While the early papers \cite{Frank:1956vp,Dunn:1979da} on the Frank-Wolfe algorithm relied on such Lipschitz constants with respect to a norm, 
the curvature constant $\Cf$ here is affine invariant, does not depend on any norm, and gives tighter convergence rates. 
$\Cf$ combines the complexity of $\domain$ and the curvature of $f$ into a single quantity.
%
%
%
%
%


\vspace{-2mm}
\paragraph{An Affine Invariant Notion of Strong Convexity.}
Inspired by the affine invariant curvature measure, one can also define a related affine invariant measure of strong convexity, when combined with the assumption of the optimum $\x^*$ being in the strict interior of $\domain$:\vspace{-2mm}
\begin{equation}\label{eq:mufFW}
  \strongConvFW := \inf_{\substack{\x\in \domain\setminus\{\x^*\}, ~\stepsize\in(0,1],\\
                      \overline\s = \overline\s(\x, \x^*,\domain),\\%
                      \y = \x+\stepsize(\overline\s-\x)}} \textstyle
           \frac{2}{\stepsize^2}\big( f(\y)-f(\x)-\innerProd{\nabla f(\x)}{\y-\x} \big) \ .\vspace{-1mm}
\end{equation}
Here the point $\overline\s$ is defined to be the point where the ray from $\x$ to the optimum $\x^*$ pinches the boundary of the set $\domain$, i.e. furthest away from $\x$ while still in $\domain$, %
$\overline\s(\x, \x^*,\domain) := \textrm{ray}(\x, \x^*) \cap \partial \domain$.
%
%
 We will later show that this strict interior assumption, which can be very prohibitive, can be removed for the Frank-Wolfe algorithm with \emph{away steps}, as we explain in Section~\ref{sec:convMFV}.
 %
Clearly, the quantity $\strongConvFW$ is affine invariant, as it only depends on the inner products of feasible points with the gradient. %
%
%
%
%
%
%
%

\begin{remark}
For all pairs of functions $f$ and bounded sets $\domain$, it holds that $\strongConvFW \le \Cf$.
\end{remark}\vspace{-2mm}

The following simple lemma gives an interpretation of the very abstract (affine invariant) quantity defined above, in terms of classical norms and strong-convexity properties.

\begin{lemma}\label{lem:muFWinterpretation}
Let $f$ be a convex differentiable function and suppose $f$ is \emph{strongly convex} w.r.t. some arbitrary norm $\norm{.}$ over the domain $\domain$ with strong-convexity constant $\mu>0$.
%
Furthermore, suppose that the (unique) optimum $x^*$ lies in the relative interior of $\domain$, i.e. $\distToBoundary := \inf_{\s\in\partial \domain} \norm{\s-\x^*} > 0$.
Then\vspace{-3mm}
\[
\strongConvFW \geq \mu \cdot \distToBoundary^2  \ .
\] %
\end{lemma}


%


\vspace{-3mm}
%
\section{Linear Convergence of Frank-Wolfe}\vspace{-2mm}%
%
We obtain an affine invariant linear convergence proof for the standard FW algorithm when $f$ is strongly convex and the solution $\x^*$ lies in the relative interior of $\domain$ (an improvement over~\cite{Guelat:1986fq}). %

\begin{theorem}\label{thm:linearConvergenceFW}
Suppose that $f$ has smoothness constant $\Cf$ as defined in~(\ref{eq:Cf}),
as well as ``interior'' strong convexity constant~$\strongConvFW$ as defined in~(\ref{eq:mufFW}).
%
%
Then the error of the iterates of the Frank-Wolfe algorithm with step-size $\stepsize := \min\{1, \frac{g_k}{\Cf} \}$
(or using line-search) decreases geometrically, that is\vspace{-2mm}
\[
h_{k+1} \leq \left(1-\rho_f^\FW\right) h_k \ , \vspace{-3mm}
\]
where $\rho_f^\FW := \min \{\frac{1}{2}, \frac{\strongConvFW}{\Cf} \}$. Here in each iteration, $h_k := f(\x^{(k)}) - f(\x^*)$ denotes the primal error, and $g_k :=  g(\x^{(k)}) := \displaystyle\max_{\s \in \domain} \,\big\langle \nabla f(\x^{(k)}), \x^{(k)} - \s  \big\rangle$ is the duality gap as defined by \cite{Jaggi:2013wg}.%
\end{theorem}



%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%




\vspace{-2mm}
%
\section{Linear Convergence of Frank-Wolfe with Away-Steps}\vspace{-2mm}%
\label{sec:convMFV}

We now show the linear convergence of FW with away-steps under strong convexity, without any assumption on the location of the optimum with respect to the domain.
However, our convergence rate will depend on a purely geometric complexity constant of the domain $\domain$, as we show below.
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%



\vspace{-2mm}
\paragraph{An Affine Invariant Notion of Strong Convexity which Depends on the Geometry of $\domain$.}

The trick is to use anchor points in the domain in order to define standard lengths (by looking at proportions on lines). These anchor points ($\s_f(\x)$ and $\vv_f(\x)$ defined below) are motivated directly from the away-steps algorithm.\\
%
Let $\s_f(\x) :=$%
$~\argmin_{\vv \in \Vertices} \left\langle \nabla f(\x), \vv \right\rangle$ (the standard Frank-Wolfe direction).
To define the away-vertex, we consider all possible expansions of $\x$ as a convex combination of vertices.
%
Let $\S_{\x} := \{ \S \, | \, \S \subseteq \Vertices$ such that $\x$ is a proper\footnote{By \emph{proper} convex combination, we mean that all coefficients are non-zero in the convex combination.} convex combination of all the elements in $\S\}$.
For a given set~$\S$, we write $\vv_{\S}(\x) := \argmax_{\vv \in \S } \left\langle \nabla f(\x), \vv \right\rangle$ for the away vertex in the algorithm supposing that the current set of active vertices was $\S$.
Finally, we define $\vv_f(\x) := \displaystyle\argmin_{\{\vv = \vv_{\S}(\x) \,|\, \S \in \S_{\x} \}} \textstyle\left\langle \nabla f(\x), \vv \right\rangle$ to be the worst-case away vertex (that is, the vertex which would yield the smallest away descent).

We can now define the strong convexity constant $\strongConvMFW$ which depends \emph{both} on the function~$f$ and the domain~$\domain$:\vspace{-3mm}
\begin{equation}\label{eq:muf}
  \strongConvMFW := \inf_{\x\in \domain} \inf_{\substack{\x^* \in \domain\\
                        \textrm{s.t. } \left\langle \nabla f(\x), \x^*-\x \right\rangle < 0 }}
           \frac{2}{{\stepsize^\away(\x,\x^*)}^2}
           %
           \big( f(\x^*)-f(\x)-\left\langle \nabla f(\x),  \x^*-\x \right\rangle \big) \ .
\end{equation}\vspace{-4mm}

Here the positive quantity $\stepsize^\away(\x,\x^*) := \frac{\innerProd{\nabla f(\x)}{\x^*-\x} }{\innerProd{\nabla f(\x)}{\s_f(\x) - \vv_f(\x)} }$ plays the role of $\stepsize$ in definition~\eqref{eq:Cf}.
%

%

%
%
%
%
%

\vspace{-3mm}
\paragraph{Interpretation.}
The above complexity definition is already sufficient for us to prove the linear convergence. 
Additionally, the constant can be understood in terms of the geometry of $\domain$, as follows:

\vspace{-3mm}
\paragraph{Directional Width.}
The directional width of a set $\domain$ with respect to a direction $\dd$ (and underlying inner product norm $\norm{\cdot}$) is defined as $\dirW(\domain,\dd) := \max_{\x\in\domain} \big\langle \frac{\dd}{\dualnorm{\dd}}, \x \big\rangle-\min_{\x \in\domain} \big\langle \frac{\dd}{\dualnorm{\dd}}, \x \big\rangle$.\vspace{-2mm}

\vspace{-2mm}
\paragraph{Pyramidal Width.}
We define the pyramidal directional width of a set $\domain$ with respect to a direction $\dd$ and a base point $\x \in \domain$ to be
$
\PdirW(\domain,\dd, \x) := \min_{\S \in \S_{\x}} \dirW( \S \cup \{\s(\domain, \dd) \} , \; \dd)
$
where $\s(\domain,\dd) := \argmax_{\vv \in \domain}  \left\langle \dd, \vv \right\rangle$.
%
To define the pyramidal width of a set, we take the infimum over a set of possible feasible directions~$\dd$ (in order to avoid the problem of zero width).
%
A direction~$\dd$ is \emph{feasible} for $\domain$ from $\x$ if it points inwards the set, 
%
(i.e. $\dd \in \text{cone}(\domain-\x)$). 

We define the \emph{pyramidal width} of a set $\domain$ to be the smallest pyramidal width of all its faces, i.e.\vspace{-1mm}
\begin{equation} \label{eq:PdirW}
\PdirW(\domain) := \displaystyle \inf_{\substack{\Kface \in \textrm{faces}(\domain) \\
												  \x \in \Kface \\
												  \dd \in \text{cone}(\Kface-\x) \setminus \{\0\}} 
                                   } \PdirW(\Kface,\dd, \x) .    \vspace{-1mm}                             
\end{equation}
%

\begin{remark}
Any curved domain will yield a pyramidal width of zero, because then the set of active atoms $\S_{\x}$ can contain vertices arbitrary close to the boundary forming a very narrow pyramid. The pyramidal width quantity is thus only useful on polytopes (the convex hull of a finite set of points). %
\end{remark}

%
%
%
%
%
%

%
%
%
%
%

%

%




\begin{remark} \label{thm:PdirWproperty}
Let $ \vv(\x, \dd) :=$ the vertex which achieves the minimum in $\min_{\S \in \S_{\x}} \max_{\vv \in \S } \innerProd{\dd}{-\vv}$ for a polytope $\Kface$ and $\x \in \Kface$. Then we have
$
	\PdirW(\Kface,\dd, \x) = \big\langle \frac{\dd}{\dualnorm{\dd}} , \s(\Kface,\dd) - \vv(\x, \dd) \big\rangle.
$\vspace{-3mm}
\end{remark}
We conjecture that $\PdirW(\domain)$ for $\domain$ being the unit simplex in $\R^d$ is $\frac2{\sqrt{d}}$.

\begin{lemma} \label{lem:muFdirWinterpretation}
Let $f$ be a convex differentiable function and suppose that $f$ is $\mu$-\emph{strongly convex} w.r.t. some \emph{inner product norm} $\norm{\cdot}$ over the domain $\domain$ with strong-convexity constant $\mu \geq 0$. Then\vspace{-1mm}
\[
\strongConvMFW \geq \mu \cdot \left( \PdirW(\domain) \right)^2 \ .
\] 
\end{lemma}

\vspace{-2mm}
%

\begin{theorem}\label{thm:linear_convergenceMFW}
Suppose that $f$ has smoothness constant $\CfMFW$,\footnote{%
For a convenience in the proof, we use a slightly modified curvature constant $\CfMFW$, which is identical to the definition of $\Cf$, except that both positive and negative step-sizes are allowed, i.e. the range of $\stepsize$ in the definition for $\Cf$ is replaced by $[-1,1]$ instead of just $[0,1]$. 
Note that boundedness of this (again affine invariant) $\CfMFW$ is still implied by the Lipschitz continuity of the gradient of $f$ (over the slightly larger domain $\domain + \domain - \domain$, but with the same diameter constant).}%
%
as well as geometric strong convexity constant~$\strongConvMFW$ as defined in~(\ref{eq:muf}).
%
Then the error of the iterates of the FW algorithm with away-steps\footnote{%
In the algorithm, one can either use line-search or set the step-size as the feasible one that minimizes the quadratic upper bound given by the curvature $\CfMFW$%
, i.e. $\stepsize_k := \min\{1,\stepmax, \stepbound_k\}$ where $\stepbound_k := \frac{g_k}{2\CfMFW}$ and\vspace{-2mm} $g_k :=  \langle  -\nabla f(\x^{(k)}), \s_k - \vv_k \rangle$.
}
%
(Algorithm~\ref{alg:MFW})
decreases geometrically at each step that is not a drop step (i.e. when $\stepsize_k < \stepmax$), that is\vspace{-2mm}
\[
h_{k+1} \leq \left(1-\rho_f^\away\right) h_k \ ,\vspace{-2mm}
\]
where $\rho_f^\away := \frac{\strongConvMFW}{4\CfMFW}$. 
%
Moreover, the number of drop steps up to iteration $k$ is bounded by $k/2$. %
This yields the global linear convergence rate of $h_k \leq h_0 \exp(-\frac12 \rho_f^\away k)$.
\end{theorem}




\paragraph{Acknowledgements.}
Simon Lacoste-Julien acknowledges support by the ERC (SIERRA-ERC-239993).
Martin Jaggi acknowledges support
by the Simons Institute for the Theory of Computing,
by the Swiss National Science Foundation (SNSF), 
and by the ERC Project SIPA. 


%
\bibliographystyle{alphaurl} %
{\small
\bibliography{bibliography}
}%




%

\newpage
%
%
%
\appendix

%
\section{Linear Convergence of Frank-Wolfe for Strongly Convex Functions with Optimum in the Interior}

%

%

\subsection{An Affine Invariant Notion of Strong Convexity}
We re-state and interpret the ``interior'' strong convexity constant~$\strongConvFW$ as defined in~(\ref{eq:mufFW}), that is
\[
  \strongConvFW := \inf_{\substack{\x\in \domain\setminus\{\x^*\}\\
                      \overline\s = \overline\s(\x, \x^*,\domain),\\%
                      \stepsize\in(0,1],\\
                      \y = \x+\stepsize(\overline\s-\x)}}
           \frac{2}{\stepsize^2}\big( f(\y)-f(\x)-\langle \nabla f(\x), \y-\x\rangle \big) \ .
\]
Here the point $\overline\s$ is defined to be the point where the ray from $\x$ to $\x^*$ pinches the boundary of the set $\domain$, %
i.e. $\overline\s(\x, \x^*,\domain) := \textrm{ray}(\x, \x^*) \cap \partial \domain$.
%
%
 
Recalling that the curvature $C_f$ by definition (\ref{eq:Cf}) provides an affine-invariant quadratic upper bound on the function $f$, the strong convexity constant $\strongConvFW$ here gives rise to an analogous quadratic lower bound, that is
\begin{equation}\label{eq:quadLowerBound}
f(\y)-f(\x)-\langle \nabla f(\x), \y-\x \rangle
\geq \frac{\stepsize^2}{2} \strongConvFW
\end{equation}
if the  point $\y = \x+\stepsize(\overline\s-\x)$ is determined by the boundary point $\overline\s(\x, \x^*,\domain)$ %
and an arbitrary step-size $\stepsize\in[0,1]$, i.e., if the point $\y$ lies on the segment which is between $\x$ and the boundary of~$\domain$, and passes through $\x^*$.
%


%
Here we prove Lemma \ref{lem:muFWinterpretation}, which gives a simple geometric interpretation of the abstract (affine invariant) quantity $\strongConvFW$ defined above, in terms of classical norms and strong-convexity properties.

\begin{replemma}{lem:muFWinterpretation}
Let $f$ be a convex differentiable function and suppose $f$ is \emph{strongly convex} w.r.t. some arbitrary norm $\norm{.}$ over the domain $\domain$ with strong-convexity constant $\mu>0$.

Furthermore, suppose that the (unique) optimum $x^*$ lies in the relative interior of $\domain$, i.e. $\distToBoundary := \inf_{\s\in\partial \domain} \norm{\s-\x^*} > 0$.
Then\vspace{-1mm}
\[
\strongConvFW \geq \mu \cdot \distToBoundary^2  \ .
\] %
\end{replemma}
%
\begin{proof}
By definition of strong convexity with respect to a norm, we have that for any $\x,\y\in\domain$,
\[
f(\y)- f(\x)- \langle\nabla f(\x), \y-\x \rangle
\geq \textstyle\frac{\mu}{2} \norm{\y-\x}^2 \ .
\]
We  want to use this lower bound in the definition (\ref{eq:mufFW}) of the affine invariant strong convexity constant. 
Observe that $\frac{1}{\stepsize^2} \norm{\y-\x}^2 = \norm{\overline\s-\x}^2$ for any $\x$ used in~\eqref{eq:mufFW}  since $\y := \x+\stepsize(\overline\s-\x) \in\domain$ by convexity. Moreover, by the definition of $\overline\s$ and $\distToBoundary$, $\norm{\overline\s-\x} \geq \norm{\overline\s-\x^*} \geq \distToBoundary$. Therefore, we can lower bound $\strongConvFW$ as
\[
\strongConvFW  
\geq \inf
          \textstyle \frac{2}{\stepsize^2}
          \frac{\mu}{2} \norm{\y-\x}^2
= \inf \mu \norm{\overline\s-\x}^2
\geq \mu \cdot \distToBoundary^2 \ ,
\]
which is the claimed bound.
\end{proof}

%
\subsection{Convergence Analysis}
\paragraph{Curvature.}
The definition of the curvature constant $\Cf$ as in (\ref{eq:Cf}) directly gives an affine invariant quadratic upper bound on the objective function, as follows:

Let $\x_\stepsize :=  \x + \stepsize (\s-\x)$ be the point obtained by moving with step-size $\stepsize$ in direction $\s\in\domain$. By definition of $\Cf$, we have
\[\textstyle
%
f(\x_\stepsize) ~\leq~ f(\x) + \stepsize \left\langle  \nabla f(\x), \s-\x \right\rangle + \frac{\stepsize^2}{2} \Cf,~~~~~\forall \stepsize \in [0,1] \ .
\]
This crucial bound enables us to analyze the objective improvement in each iteration in Frank-Wolfe-type algorithms, as in \cite{Jaggi:2013wg}:
If the point $\s$ is the standard Frank-Wolfe direction returned by an exact linear oracle, then the middle quantity is exactly the negative of the duality gap, $\left\langle  \nabla f(\x), \s-\x \right\rangle = -g(\x)$.
If an inexact linear oracle is used instead, which has multiplicative approximation quality~$\mapprox$ (to be defined below), then we always have the upper bound
\begin{equation}\label{eq:CfUpperBound}\textstyle
f(\x_\stepsize) ~\leq~ f(\x) - \stepsize \mapprox g(\x) + \frac{\stepsize^2}{2} \Cf
,~~~~~\forall \stepsize \in [0,1] \ .
\end{equation}


\paragraph{Inexact Linear Oracles.}
The standard linear oracle used inside the classical Frank-Wolfe algorithm is given by $\s \in \argmin_{\vv \in \domain} \left\langle \nabla f(\x), \vv \right\rangle$.
We say that the linear oracle %
satisfies multiplicative accuracy~$\mapprox$ for some $\mapprox\in[0,1]$, if for any $\x\in\domain$, the returned $\s$ is such that
\begin{equation}\label{eq:qualityMult}
\left\langle \nabla f(\x),  \x-\s  \right\rangle
~\ge~
\displaystyle \mapprox\cdot \max_{\s' \in \domain} \textstyle\left\langle \nabla f(\x), \x-\s'  \right\rangle
%
\ .
\end{equation}
Note that the classical Frank-Wolfe direction $\s$ satisfies this inequality with $\mapprox=1$.
The inequality means that the oracle answer $\s$ attains at least a $\mapprox$-fraction of the current \emph{duality gap} $g(\x) := \displaystyle\max_{\s \in \domain} \,\big\langle \nabla f(\x^{(k)}), \x - \s  \big\rangle$ as defined by \cite{Jaggi:2013wg}.
%

\emph{Related work.} The sublinear convergence of Frank-Wolfe with $O(1/k)$ is known to also hold if this linear subproblems are only solved approximately (meaning that the linear oracle is inexact).
For additive approximation accuracy, this was shown by \cite{Dunn:1978di,Dunn:1979da} for the line-search case, and by \cite{Jaggi:2011ux,Jaggi:2013wg} for the simpler $\frac2{k+2}$ step-size and the primal-dual convergence. For multiplicative accuracy (relative to the duality gap), it was shown by \cite[Appendix C]{LacosteJulien:2013ue}.
The case of the inexact or noisy gradient information can also be analyzed in the same way, as discussed in $\cite{Jaggi:2013wg,Freund:2013uk}$.



\paragraph{Linear Convergence Proof.}
Here we prove a slightly stronger version of Theorem \ref{thm:linearConvergenceFW}, showing the linear convergence also in the case where the linear subproblems in each iteration are only solved approximately.
The exact oracle case is obtained for $\mapprox :=1$.

\begin{reptheorem}{thm:linearConvergenceFW}
Suppose that $f$ has smoothness constant $\Cf$ as defined in~(\ref{eq:Cf}),
as well as ``interior'' strong convexity constant~$\strongConvFW$ as defined in~(\ref{eq:mufFW}).
%

Then the error of the iterates of the Frank-Wolfe algorithm with step-size $\stepsize := \min\{1, \frac{\mapprox g_k}{\Cf} \}$
(or using line-search) decreases geometrically, that is\vspace{-1mm}
\[
h_{k+1} \leq \left(1-\rho_f^\FW\right) h_k \ , \vspace{-2mm}
\]
where $\rho_f^\FW := \min \{\frac\mapprox2, \mapprox^2 \frac{\strongConvFW}{\Cf} \}$. Here in each iteration, $h_k := f(\x^{(k)}) - f(\x^*)$ denotes the primal error, and $g_k :=  g(\x^{(k)}) := \displaystyle\max_{\s \in \domain} \,\big\langle \nabla f(\x^{(k)}), \x^{(k)} - \s  \big\rangle$ is the duality gap as defined by \cite{Jaggi:2013wg}, 
and $\mapprox\in[0,1]$ is the multiplicative approximation quality to which the linear sub-problems are solved.
\end{reptheorem}
%
\begin{proof}
Applying the strong convexity bound (\ref{eq:quadLowerBound}) at the current iterate $\x:=\x^{(k)}$ for the special step-size $\overline\stepsize$ such that $\y = \x^{(k)}+\overline\stepsize (\overline\s-\x^{(k)}) = \x^*$ gives
\[
\begin{array}{rll}
\frac{\overline\stepsize^2}{2} \strongConvFW \  \leq
&f(\y)-f(\x) -\langle \nabla f(\x), \y-\x \rangle \\
=&f(\x^*)-f(\x^{(k)}) - \overline\stepsize \left\langle \nabla f(\x^{(k)}), \overline\s-\x^{(k)}\right\rangle \\
\le &-h_k + \overline\stepsize  g_k \ .
%
%
\end{array}
\]
%
Therefore $h_k \le -\frac{\overline\stepsize^2}{2} \strongConvFW + \overline\stepsize g_k$,
which is upper bounded by $%
%
%
\frac{{g_k}^2}{2\strongConvFW}$.\\
{\small(Here we have used the trivial inequality $0 \le %
a^2-2ab+b^2$ for the choice of numbers $a:=\frac{g_k}{\strongConvFW}$ and $b:=\overline\stepsize$)}
%

We now want to use the curvature definition to lower bound the absolute progress~$h_k - h_{k+1}$. The definition of the curvature $\Cf$ in the form of the quadratic upper bound \eqref{eq:CfUpperBound} reads as %
$h_k - h_{k+1} \ge \stepsize \mapprox g_k - \frac{\stepsize^2}{2} \Cf$. Using this for the particular step-size $\stepsize:=\frac{\mapprox g_k}{\Cf}$, the r.h.s. is $= \frac{\mapprox^2 g_k^2}{2\Cf}$. (The border case when $\frac{\mapprox g_k}{\Cf}>1$ will be discussed separately below). The same inequality also holds in the line-search case, as the improvement only gets better.
Combining the two bounds, we have obtained
\[
\frac{h_k - h_{k+1} }{h_k} \ge \mapprox^2 \frac{\strongConvFW}{\Cf}\vspace{-1mm}
\]
implying that we have a geometric rate of decrease $h_{k+1} \leq \Big(1-\mapprox^2\frac{\strongConvFW}{\Cf}\Big) h_k$.

\emph{Border case.} In the above analysis, we have assumed that the step-size $\stepsize:=\frac{\mapprox g_k}{\Cf} \le 1$. If this is not the case (i.e. if $\mapprox g_k > \Cf$), then the actual step-size in the algorithm is clipped to $1$, in which case the curvature upper bound~\eqref{eq:CfUpperBound} for $\stepsize:=1$ gives 
$h_k - h_{k+1} 
~\geq~ \mapprox g_k - \frac12 \Cf 
> \mapprox g_k - \frac12 \mapprox g_k
= \frac\mapprox2 g_k$. Using that the main property $h_k \leq g_k$ of the duality gap (by convexity), we therefore have $\frac{h_k - h_{k+1}}{h_k} > \frac\mapprox2$, which gives a geometric decrease of the error with constant $1-\frac\mapprox2$.
\end{proof}

%
%
%


%
%
%
%
%
%
%
%

%



%
\section{Linear Convergence of FW with Away-Steps under Strong Convexity}
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


%
\subsection{Interpretation of the Geometric Strong Convexity Constant $\strongConvMFW$}
The geometric strong convexity constant $\strongConvMFW$, as defined in (\ref{eq:muf}), is affine invariant, since it only depends on the inner products of feasible points with the gradient. Also, it combines both the complexity of the function~$f$ and the geometry of the domain $\domain$.
The goal of this subsection is to prove Lemma~\ref{lem:muFdirWinterpretation}, which provides a geometric interpretation of $\strongConvMFW$. The lemma allows us to bound the constant $\strongConvMFW$ in terms of the strong convexity of the objective function, combined with a purely geometric complexity measure of the domain $\domain$. In the following Section \ref{sec:MFWconv} below, we will show the linear convergence of Algorithm~\ref{alg:MFW} under the assumption that $\strongConvMFW > 0$. From the view of Lemma~\ref{lem:muFdirWinterpretation}, $\strongConvMFW > 0$ is a slightly weaker condition than the strong convexity of the function over a polytope domain (it is implied by strong convexity).

We recall the definition of $\strongConvMFW$ as given in (\ref{eq:muf}):
\[
  \strongConvMFW := \inf_{\x\in \domain} \inf_{\substack{\x^* \in \domain\\
                        \textrm{s.t. } \left\langle \nabla f(\x), \x^*-\x \right\rangle < 0 }}
           \frac{2}{{\stepsize^\away(\x,\x^*)}^2}
           %
           \big( f(\x^*)-f(\x)-\left\langle \nabla f(\x),  \x^*-\x \right\rangle \big) \ .
\]\vspace{-2mm}

Here the positive quantity $\stepsize^\away(\x,\x^*) := \frac{\innerProd{\nabla f(\x)}{\x^*-\x} }{\innerProd{\nabla f(\x)}{\s_f(\x) - \vv_f(\x)} }$ plays the role of $\stepsize$ in the analogous upper bound definition~\eqref{eq:Cf} for the curvature. We recall that $\s_f(\x) := \argmin_{\vv \in \Vertices} \left\langle \nabla f(\x), \vv \right\rangle$ and that $\vv_f(\x) := \displaystyle\argmin_{\{\vv = \vv_{\S}(\x) \,|\, \S \in \S_{\x} \}} \textstyle\left\langle \nabla f(\x), \vv \right\rangle$. 

We recall the definition of the pyramidal directional width of a set $\domain$ with respect to a direction $\dd$ and a base point $\x \in \domain$:
$
\PdirW(\domain,\dd, \x) := \min_{\S \in \S_{\x}} \dirW( \S \cup \{\s(\domain, \dd) \} , \; \dd)
$
where $\s(\domain,\dd) := \argmax_{\vv \in \domain}  \left\langle \dd, \vv \right\rangle$. We now provide a proof of Remark~\ref{thm:PdirWproperty}, which will be useful at the end of the proof of Lemma~\ref{lem:muFdirWinterpretation}.

\begin{repremark}{thm:PdirWproperty}
Let $ \vv(\x, \dd) :=$ the vertex which achieves the minimizer of  $\min_{\S \in \S_{\x}} \max_{\vv \in \S } \innerProd{\dd}{-\vv}$ for a polytope $\Kface$ and $\x \in \Kface$. Then we have
\begin{equation} \label{eq:PdirWproperty}
	\PdirW(\Kface,\dd, \x) = \innerProd{\frac{\dd}{\dualnorm{\dd}}}{ \s(\Kface,\dd) - \vv(\x, \dd)}.
\end{equation}
\end{repremark}
\begin{proof}
\begin{align*}
\PdirW(\Kface,\dd, \x) &= \frac{1}{\norm{\dd}_*} \min_{\S \in \S_{\x}} \left( \max_{\y \in  \S \cup \{\s(\Kface,\dd) \}} \innerProd{\dd}{\y} - \min_{\y \in \S \cup \{\s(\Kface,\dd) \} } \innerProd{\dd}{\y} \right) \\
	&= \frac{1}{\norm{\dd}_*} \min_{\S \in \S_{\x}} \left( \innerProd{\dd}{\s(\Kface,\dd)} + \max_{\y \in \S} \innerProd{\dd}{-\y} \right) \\
	&= \frac{1}{\norm{\dd}_*} \left( \innerProd{\dd}{\s(\Kface,\dd)} +  \min_{\S \in \S_{\x}} \max_{\y \in \S} \innerProd{\dd}{-\y} \right) \\
	&=\innerProd{\frac{\dd}{\dualnorm{\dd}}}{ \s(\Kface,\dd) - \vv(\x, \dd)}.
	\vspace{-5mm}
\end{align*}
\end{proof}

\paragraph{Exposing a facet of a polytope.} Finally, we introduce a final concept that will be useful in the proof. We say that a direction $\dd$ \textbf{exposes a facet}\footnote{As a reminder, we define a \emph{k-face} of $\domain$ (a $k$-dimensional face of $\domain$) a set $\Kface$ such that $\Kface = \domain \cap \{ \y : \innerProd{\r}{\y - \x} = \0 \}$ for some normal vector $\r$ and fixed reference point $\x \in \Kface$ with the additional property that $\domain$ lies on one side of the given half-space determined by $\r$ i.e. $ \innerProd{\r}{\y - \x} \leq \0$ $\forall \y \in \domain$. $k$ is the dimensionality of the affine hull of $\Kface$. We call a $k$-face of dimensions $k = 0$, $1$, $\textrm{dim}(\domain)-2$ and $\textrm{dim}(\domain)-1$ a \emph{vertex}, \emph{edge}, \emph{ridge} and \emph{facet} respectively. $\domain$ is a $k$-face of itself with $k = \textrm{dim}(\domain)$. See definition 2.1 in~\cite{Ziegler:1995td}.}  $\mathcal{F}$ of the polytope $\domain$ \textbf{at $\x$} if 1) $\mathcal{F}$ includes $\x$ and is a facet of $\domain$; and 2) the orthogonal component of $\dd$ to this facet defines this facet with $\dd$ on one side and $\domain - \x$ on the other side. In other words, let $\mathcal{F}_{\s} := \textrm{span}(\domain - \x)$ be the affine hull of $\mathcal{F}$ re-centered at $\x$; let $\proj_{\mathcal{F}_{\s}}$ be the orthogonal projection operator onto $\mathcal{F}_{\s}$; then the second condition can be expressed as $\mathcal{F} = \{\y \in \domain : \innerProd{(\id - \proj_{\mathcal{F}_{\s}})\dd}{\y-\x} = 0 \}$ and $\innerProd{(\id - \proj_{\mathcal{F}_{\s}})\dd}{\y-\x} \leq 0$ $\forall \y \in \domain$ (note that $(\id - \proj_{\mathcal{F}_{\s}})\dd$ is the orthogonal component of $\dd$ to the facet $\mathcal{F}$). Note that these conditions imply that $\dd$ cannot be a feasible direction, i.e. $\dd \notin \textrm{cone}(\domain-\x)$ and that $\x$ must be on the (relative) boundary of $\domain$. It turns out that the converse is also true: if $\dd \notin \textrm{cone}(\domain-\x)$, then there must exist at least a facet of $\domain$ exposed by $\dd$ at $\x$.\footnote{To find such an exposed facet, consider the $\mathcal{H}$-polyhedron representation of $\textrm{cone}(\domain-\x)$ (see~\cite{Ziegler:1995td}). As $\dd$ is not feasible, at least one halfspace constraint must be violated; the intersection of the hyperplane determining this halfspace constraint with $\domain-\x$ yields (the translation of) one exposed facet.}

%

\begin{replemma}{lem:muFdirWinterpretation}
Let $f$ be a convex differentiable function and suppose that $f$ is $\mu$-\emph{strongly convex} w.r.t. some \emph{inner product norm} $\norm{\cdot}$ over the domain $\domain$ with strong-convexity constant $\mu \geq 0$. Then
\[
\strongConvMFW \geq \mu \cdot \left( \PdirW(\domain) \right)^2 \ .
\] 
\end{replemma}
%
\begin{proof}
By definition of strong convexity with respect to a norm, we have that for any $\x,\y\in\domain$,
\begin{equation} \label{eq:strongConv}
f(\y)- f(\x)- \langle\nabla f(\x), \y-\x \rangle
\geq \textstyle\frac{\mu}{2} \norm{\y-\x}^2 \ .
\end{equation}
Using the strong convexity bound~\eqref{eq:strongConv} with $\y := \x^*$ on the right hand side of equation~\eqref{eq:muf} (and using the shorthand $\r_{\x} := -\nabla f(\x)$ ), we thus get:
\begin{align}
\strongConvMFW \geq&  \inf_{\substack{\x, \x^* \in \domain\\
                                   \textrm{s.t. } \innerProd{\r_{\x}}{\x^*-\x} > 0}}
                      \mu \left(  \frac{\innerProd{\r_{\x}}{ \s_f(\x) - \vv_f(\x)}}{\innerProd{\r_{\x}}{\x^*-\x}} \norm{{\x^*-\x}} \right)^2 \nonumber \\
		&=  \mu \inf_{\substack{\x \neq \x^* \in \domain \\
                        \textrm{s.t. } \innerProd{\r_{\x}}{ \hat{\r}_{\x,\x^*}} > 0}}
           \left(  \frac{\innerProd{\r_{\x}}{ \s_f(\x) - \vv_f(\x)}}{\innerProd{\r_{\x}}{ \hat{\r}_{\x,\x^*}}} \right)^2  , \label{eq:mufInitial}
\end{align}
where $\hat{\r}_{\x,\x^*} := \frac{\x^*-\x}{\norm{\x^*-\x}}$ is the unit norm feasible direction from $\x$ to $\x^*$. We are thus taking an infimum over all possible feasible directions starting from $\x$ (i.e. which moves within $\domain$) with the additional constraint that it makes a positive inner product with the negative gradient $\r_{\x}$ i.e. it is a strict descent direction. This is only possible if $\x$ is not already optimal, i.e. $\x \in \domain \setminus \X^*$ where $\X^* := \{\x^* \in \domain : \innerProd{\r_{\x^*}}{\x-\x^*} \leq 0 \,\, \forall \x \in \domain \}$ is the set of optimal points. {\small[NOTE: I know that by strong convexity it only contains one point; but I wanted to keep it general here just to see the effect of the constraints and to get more intuition about the constants]}.

The goal in the rest of the proof is to equivalently project $\r_{\x}$ onto facets of $\domain$ and then to characterize the property of its projection so that we can consider a wider set of valid directions that will thus yield a lower bound on the infimum of $\frac{\innerProd{\r_{\x}}{ \s_f(\x) - \vv_f(\x)}}{\innerProd{\r_{\x}}{ \hat{\r}_{\x,\x^*}}}$. For the rest of the proof, we fix $\x \notin \X^*$ and we work on the centered polytope at $\x$ i.e. let $\tilde{\domain} = \domain - \x$. During the proof, we work on faces $\Kface_l$ of $\tilde{\domain}$ of decreasing dimensions which all include $\x$ at their origin, as well as maintain a projection of the gradient $\dd_l \in \C_l := \text{span}(\Kface_l)$. We let $\proj_l$ be the orthogonal projection operator onto $\C_l$. We will keep projecting the gradient as $\dd_l := \proj_l \dd_{l-1}$ until they become a non-zero feasible direction from the origin i.e. $\dd_l \in \text{cone}(\Kface_l) \setminus \{\0\}$, at which point we will exit the loop with $\dd = \dd_l$ and $\Kface = \Kface_l$ for the last considered face. 

We start with $\Kface_0 = \tilde{\domain}$ and we note that since both $\s_f(\x) - \vv_f(\x)$ and $\hat{\r}_{\x,\x^*}$ belong to $\C_0 = \text{span}(\Kface_0)$, if we let $\dd_0 = \proj_0 \r_{\x}$, then we have $\frac{\innerProd{\r_{\x}}{ \s_f(\x) - \vv_f(\x)}}{\innerProd{\r_{\x}}{ \hat{\r}_{\x,\x^*}}} = \frac{\innerProd{\dd_0}{ \s_f(\x) - \vv_f(\x)}}{\innerProd{\dd_0}{ \hat{\r}_{\x,\x^*}}}$ for any $\x^*$ such that $\innerProd{\r_{\x}}{\x^*-\x} \neq 0$. Then we consider whether $\dd_0$ is a feasible direction in $\Kface_0$. If $\dd_0$ is feasible i.e. $\dd_0 \in \text{cone}(\Kface_0)$, then we stop with $\dd = \dd_0 = \proj_0 \r_{\x}$ and $\Kface = \Kface_0$. By the definition of the dual norm $\dualnorm{\cdot}$ (generalized Cauchy-Schwartz), we have $\innerProd{\dd}{ \hat{\r}_{\x,\x^*}} \leq \dualnorm{\dd}\norm{\hat{\r}_{\x,\x^*}} = \dualnorm{\dd} \cdot 1$, and thus for this $\x$ we have:
\[ 
	\inf_{\substack{\x^* \in \domain\\
               \textrm{s.t. } \innerProd{\r_{\x}}{ \hat{\r}_{\x,\x^*}} > 0}}
                       \frac{\innerProd{\r_{\x}}{ \s_f(\x) - \vv_f(\x)}}{\innerProd{\r_{\x}}{ \hat{\r}_{\x,\x^*}}} \geq  \innerProd{\frac{\dd_0}{\dualnorm{\dd_0}}}{ \s_f(\x) - \vv_f(\x)} .
\]
In the other possibility ($\dd_0 \notin \text{cone}(\Kface_0)$), then there must exist a least one facet $\Kface_1$ of $\Kface_0$ that is exposed by $\dd_0$ at $\0$ (note that we cannot have $\dd_0 = \0$ since $\x \notin \X^*$). %
%
%
%
%
%
We now project $\dd_0$ on $\text{span}(\Kface_1)$: $\dd_1 := \proj_1 \dd_0$, and we show how the lower bound transforms.
This yields the following inequalities:
\begin{align}
\innerProd{\r_{\x}}{ \s_f(\x) - \vv_f(\x)}  &= 
	 \max_{\s \in \domain} \innerProd{\r_{\x}}{ \s - \x} +
	 \min_{\S \in \S_{\x}} \max_{\vv \in \S} \innerProd{-\r_{\x}}{ \vv - \x}  \nonumber \\
	 &= \max_{\y \in \Kface_0} \innerProd{\dd_0}{ \y } +
	 	 \min_{\S \in \S_{\x}} \max_{\vv \in \S} \innerProd{-\dd_0}{ \vv - \x} \nonumber \\
	 &\ge \max_{\y \in \Kface_1} \innerProd{\dd_0}{ \y } +
	 	 	 \min_{\S \in \S_{\x}} \max_{\vv \in \S \cap (\Kface_1 + \x)} \innerProd{-\dd_0}{ \vv - \x} \nonumber \\
	 &= \max_{\y \in \Kface_1} \innerProd{\dd_1}{ \y } +
	 	 	 \min_{\S \in \S_{\x}} \max_{\vv \in \S } \innerProd{-\dd_1}{ \vv - \x} \nonumber \\
	 &= \innerProd{\dd_1}{\s(\Kface_1, \dd_1)} + \innerProd{-\dd_1}{ \vv(\x, \dd_1) - \x} . \label{eq:d1sf}
\end{align}
From the first to the second line, we used the fact that $\innerProd{\r_{\x} - \dd_0}{\y} = 0$ for any $\y \in \Kface_0 = \domain - \x$ as $\dd_0$ is the orthogonal projection of $\r_{\x}$ on $\C_0 = \text{span}(\Kface_0)$ (and thus we also have that $\innerProd{\r_{\x}}{\s_f(\x) - \x} = \innerProd{\dd_0}{\s(\Kface_0, \dd_0)}$). To go from the second to the third line, we use the fact that the first term yields an inequality as $\Kface_1 \subseteq \Kface_0$. Also, let $\Kface_{\x}$ be the minimal dimensional face of $\domain$ containing $\x$ (and thus $\x$ is in the relative interior of $\Kface_{\x}$). Note that $\bigcup \S_{\x} = \text{vertices}(\Kface_{\x})$, and also that $\Kface_{\x}$ is included in any other face containing $\x$. We thus have $\S \subseteq \Kface_1 + \x$ for any $\S \in \S_{\x}$ and thus the second term on the second line yielded an equality. The fourth line used the fact that $\dd_0 - \dd_1$ is orthogonal to members of $\Kface_1$. The fifth line used the definition of $\s(\Kface_1, \dd_1)$ and introduced the notation $ \vv(\x, \dd) :=$ the vertex $\vv \in \Kface_{\x}$ which achieves the minimizer of $\min_{\S \in \S_{\x}} \max_{\vv \in \S } \innerProd{\dd}{ -\vv}$.

To deal with $\innerProd{\r_{\x}}{ \hat{\r}_{\x,\x^*}} = \innerProd{\dd_0}{ \hat{\r}_{\x,\x^*}}$, we use the crucial fact that $\dd_0$ \emph{exposes the facet} $\Kface_1$ of $\Kface_0$. This implies that $\innerProd{\dd_0-\proj_0 \dd_0}{ \hat{\r}_{\x,\x^*}} \leq 0$ for all $\x^*-\x \in \Kface_0 \setminus \{\0\}$. So consider $\r_0 := \displaystyle \argmax_{\substack{\y \in \Kface_0 \\ 
				 \innerProd{\dd_0}{\y} > 0}} \innerProd{\dd_0}{ \frac{\y}{\norm{\y}}}$. 
We claim that we can choose $\r_0 \in \Kface_1$. To see this, let $\r_1 = \proj_1 \r_0$ and write $\r_1^\perp = \r_0 - \r_1$ and $\dd_1^\perp = \dd_0 - \dd_1$. Then we have:
\begin{align}
  \innerProd{\dd_0}{\frac{\r_0}{\norm{\r_0} }}  &= \frac{1}{\norm{\r_0}} \innerProd{\dd_1 + \dd_1^\perp}{\r_1 + \r_1^\perp} \nonumber \\
	&= \frac{1}{\norm{\r_0}} \big( \innerProd{\dd_1}{\r_1} + 0 + \underbrace{\innerProd{\dd_1^\perp}{\r_1+\r_1^\perp}}_{\leq 0} \big) \nonumber \\
	&\leq  \frac{1}{\norm{\r_0}} \innerProd{\dd_1}{\r_1} \leq  \frac{1}{\norm{\r_1}} \innerProd{\dd_1}{\r_1}, \text{ and thus} , \nonumber \\
\max_{\substack{\y \in \Kface_0 \\ 
				 \innerProd{\dd_0}{\y} > 0}} \innerProd{\dd_0}{ \frac{\y}{\norm{\y}}} &= 
		\max_{\substack{\y \in \Kface_1 \\ 
						 \innerProd{\dd_1}{\y} > 0}} \innerProd{\dd_1}{ \frac{\y}{\norm{\y}}} .		 
 \label{eq:d1r1}
\end{align}
Note that in the third line, we have used that $\norm{\r_1} = \norm{\proj_1 \r_0 - \proj_1 \0} \leq \norm{\r_0 - \0}$ by the contraction property of the orthogonal projection for inner product norms.\footnote{The contraction property is only valid for inner product norms (i.e. $\norm{\cdot} = \sqrt{ \innerProd{\cdot}{\cdot}}$), so this is where the assumption that the norm was generated by an inner product comes into play. 
%
} In the last line, we have an equality instead of the $\leq$ inequality as $\innerProd{\dd_1}{\y} = \innerProd{\dd_0}{\y}$ $\forall \y \in \Kface_1$ and $\Kface_1 \subseteq \Kface_0$, and so we also have the $\geq$ direction. Combining the facts from~\eqref{eq:d1sf} and~\eqref{eq:d1r1}, we get in this case:
\begin{align*}
	\inf_{\substack{\x^* \in \domain\\
               \textrm{s.t. } \innerProd{\r_{\x}}{ \hat{\r}_{\x,\x^*}} > 0}}
                       \frac{\innerProd{\r_{\x}}{ \s_f(\x) - \vv_f(\x)}}{\innerProd{\r_{\x}}{ \hat{\r}_{\x,\x^*}}} 
                       &\geq  \innerProd{\dd_1}{\s(\Kface_1, \dd_1)+\x - \vv(\x, \dd_1)} 	\left( \max_{\substack{\y \in \Kface_1 \\ 
                       						 \innerProd{\dd_1}{\y} > 0}} \innerProd{\dd_1}{ \frac{\y}{\norm{\y}}} \right)^{-1} 
\end{align*}

We are now back to a similar situation as before, but with $\Kface_1$ instead of $\Kface_0$ as the reference polytope. Note that by the third line of~\eqref{eq:d1r1}, we have $\innerProd{\dd_1}{\r_1} \geq \innerProd{\dd_0}{\r_0} > 0$ and thus $\dd_1 \neq \0$ (which is crucial to avoid a trivial lower bound of zero). So again, we consider whether $\dd_1 \in \text{cone}(\Kface_1)$. If $\dd_1 \in \text{cone}(\Kface_1)$, we stop here with $\dd = \dd_1$ and $\Kface = \Kface_1$. By Cauchy-Schwartz, we again have $\displaystyle 	\max_{\substack{\y \in \Kface \\ \innerProd{\dd}{\y} > 0}} \innerProd{\dd}{ \frac{\y}{\norm{\y}}} \leq \norm{\dd}_*$, and so we conclude
\begin{align} \label{eq:muCSlower}
	\inf_{\substack{\x^* \in \domain\\
               \textrm{s.t. } \innerProd{\r_{\x}}{ \hat{\r}_{\x,\x^*}} > 0}}
                       \frac{\innerProd{\r_{\x}}{ \s_f(\x) - \vv_f(\x)}}{\innerProd{\r_{\x}}{ \hat{\r}_{\x,\x^*}}} 
                       &\geq  \innerProd{\frac{\dd}{\norm{\dd}_*}}{\s(\Kface+\x, \dd) - \vv(\x, \dd)} 	
\end{align}
where $\dd \in \text{cone}(\Kface) \setminus \{\0\}$.

If $\dd_1 \notin \text{cone}(\Kface_1)$, then we continue our iterative process: we get that $\dd_1$ exposes a facet $\Kface_2$ of $\Kface_1$. We thus project $\dd_1$ on $\Kface_2$ to get $\dd_2 = \proj_2 \dd_1$. We can repeat exactly the same argument as before to get~\eqref{eq:d1sf} and~\eqref{eq:d1r1} with $\dd_2$ and $\Kface_2$ in place of $\dd_1$ and $\Kface_1$ (and $\dd_2 \neq \0$). If $\dd_2 \in \text{cone}(\Kface_2)$, then we stop with $\dd = \dd_2$ and $\Kface = \Kface_2$ and we again get the inequality~\eqref{eq:muCSlower}. Otherwise, we get an exposed facet $\Kface_3$, and repeat the process with $\dd_3 = \proj_3 \dd_2$. This process must stop at some point $l$: at the latest, we will reach $\Kface_l = \Kface_{\x}-\x$, the minimal dimensional face containing $\0$. In this case we must have $\dd_l \in \text{cone}(\Kface_l)$ as $\0$ is in the relative interior of $\Kface_l$ for a minimal face and so all directions are feasible.
We also note that $\dd_l \neq \0$ by the argument in~\eqref{eq:d1r1} that implies $\innerProd{\dd_l}{s(\Kface_l, \dd_l)} > 0$  (this condition is crucial to avoid having a lower bound of zero!). The latter also implies that the dimensionality of $\Kface_l$ must at least be 1. Letting again $\dd = \dd_l$ and $\Kface = \Kface_l$, we get inequality~\eqref{eq:muCSlower} with $\dd \in \text{cone}(\Kface) \setminus \{\0\}$.

From this argument, we can see that by considering all the possible faces of $\tilde{\domain}$ of dimension at least one which includes $\0$, and any feasible directions for these faces, we are sure to include the $\dd$ and~$\Kface$ that appears in \eqref{eq:muCSlower}. Translating back to the affine space $\domain$ (i.e. we use $\Kface+\x$ as the face of $\domain$ which contains $\x$), we can start to vary $\x$ again. We thus obtain the following lower bound:
\begin{align*}
	\inf_{\x \notin \X^*} \inf_{\substack{\x^* \in \domain\\
               \textrm{s.t. } \innerProd{\r_{\x}}{ \hat{\r}_{\x,\x^*}} > 0}}
                       \frac{\innerProd{\r_{\x}}{ \s_f(\x) - \vv_f(\x)}}{\innerProd{\r_{\x}}{ \hat{\r}_{\x,\x^*}}} 
          &\geq \inf_{\x \notin \X^*} \inf_{\substack{\Kface \in \textrm{faces}(\domain) \\
          												  \Kface \ni \x \\
          												  \dd \in \text{cone}(\Kface-\x) \setminus \{\0\} }}       												  
           \innerProd{\frac{\dd}{\dualnorm{\dd}}}{ \s(\Kface,\dd) - \vv(\x, \dd)} \\
          &\geq  \inf_{\substack{\Kface \in \textrm{faces}(\domain) \\
                    								   \x \in \Kface \\
                    								 \dd \in \text{cone}(\Kface-\x) \setminus \{\0\} }}
                    \PdirW(\Kface,\dd, \x) 
          = \PdirW(\domain) .
\end{align*}
For the last inequality, we used~\eqref{eq:PdirWproperty} from Remark~\ref{thm:PdirWproperty}. Combining this statement with~\eqref{eq:mufInitial} concludes the proof. 
\end{proof}


%
\subsection{Linear Convergence Proof}\label{sec:MFWconv}

\paragraph{Curvature Constants.}
Because of the additional possibility of the away step in Algorithm~\ref{alg:MFW}, we need to define the following slightly modified additional curvature constant, which will be needed for the linear convergence analysis of the algorithm\footnote{This can be avoided if the algorithm uses the step-size that minimizes a quadratic upper bound (see the proof for Theorem~\ref{thm:linear_convergenceMFW}; we can actually use $\stepsize_k := \min\{1,\stepmax, \frac{g_k}{2\Cf}\}$); but then one needs to compute an upper bound on $\Cf$ to run the algorithm (which is not always easy). Moreover, this algorithm might have less chance to get the `best case' behavior by being less adaptive.}:
\begin{equation}\label{eq:CfA}
  \CfA := \sup_{\substack{\x,\s\in \domain, \\
                      \stepsize\in[0,1],\\
                      \y = \x+\stepsize(\x-\s)}}
           \frac{2}{\stepsize^2}\big( f(\y)-f(\x)-\langle \nabla f(\x), \y-\x \rangle \big) \ .
\end{equation}
By comparing with $\Cf$~\eqref{eq:Cf}, we see that the modification is that $\y$ is defined with the \emph{away} direction $\x -\s$ instead of a standard FW direction $\s - \x$. This might yield some $\y$'s which are outside of the domain $\domain$ (in fact, $\y \in \domain^\away := \domain + (\domain - \domain)$ in the Minkowski sense). On the other hand, by re-using a similar argument as in \cite[Lemma 7]{Jaggi:2013wg}, we can obtain the same bound~\eqref{eq:CfBound} for $\CfA$, with the only difference that the Lipschitz constant $L$ for the gradient function has to be valid on $\domain^\away$ instead of just $\domain$.
Finally, the curvature constant for Algorithm~\ref{alg:MFW} is simply the worst-case possibility between the standard FW steps and the away steps:
\begin{equation}\label{eq:CfMFW}
  \CfMFW := \max\{\Cf, \CfA\}.
\end{equation}

%
\begin{remark}\label{rem:mfMFWsmallerThanCf}
For all pairs of functions $f$ and domains $\domain$, it holds that $\strongConvMFW \le \Cf$ (and $\Cf \le \CfMFW$).
\end{remark}\vspace{-2mm}
\begin{proof}
Choose $\x^* := \s_f(\x)$ for an $\x$ that is an away corner (i.e. $\x = \vv_f(\x)$) in~\eqref{eq:muf}. Then $\stepsize^\away(\x,\x^*) = 1$ and so we have $\y := \x^* = \x + \stepsize (\x^*-\x)$ with $\stepsize = 1$ which can also be used in the definition of $\Cf$. Thus, we have $\strongConvMFW \leq f(\y)-f(\x) - \langle \nabla f(\x), \y-\x \rangle \leq \Cf$.
\end{proof}

\begin{reptheorem}{thm:linear_convergenceMFW}
Suppose that $f$ has smoothness constant $\CfMFW$ as defined in~\eqref{eq:CfMFW}, as well as geometric strong convexity constant~$\strongConvMFW$ as defined in~(\ref{eq:muf}).
%
Then the error of the iterates of the FW algorithm with away-steps\footnote{%
In the algorithm, one can either use line-search or set the step-size as the feasible one that minimizes the quadratic upper bound given by the curvature $\Cf$%
, i.e. $\stepsize_k := \min\{1,\stepmax, \stepbound_k\}$ where $\stepbound_k := \frac{g_k}{2\CfMFW}$ and\vspace{-2mm} $g_k :=  \langle  -\nabla f(\x^{(k)}), \s_k - \vv_k \rangle$.
}
%
(Algorithm~\ref{alg:MFW})
decreases geometrically at each step that is not a drop step (i.e. when $\stepsize_k < \stepmax$), that is\vspace{-1mm}
\[
h_{k+1} \leq \left(1-\rho_f^\away\right) h_k \ ,\vspace{-1mm}
\]
where $\rho_f^\away := \frac{\strongConvMFW}{4\CfMFW}$. 
%
Moreover, the number of drop steps up to iteration $k$ is bounded by $k/2$. %
This yields the global linear convergence rate of $h_k \leq h_0 \exp(-\frac12 \rho_f^\away k)$.
%
\end{reptheorem}
\begin{proof}
The general idea of the proof is to use the definition of the geometric strong convexity constant to upper bound $h_k$, while using the definition of the curvature constant $\CfMFW$ to lower bound the decrease in primal suboptimality $h_k - h_{k+1}$ for the `good steps' of Algorithm~\ref{alg:MFW}. Then we upper bound the number of `bad steps' (the drop steps).

\emph{Upper bounding $h_k$.} In the whole proof, we assume that $\x^{(k)}$ is not already optimal, i.e. that $h_k > 0$. If $h_k = 0$, then because line-search is used, we will have $h_{k+1} \leq h_k = 0$ and so the geometric rate of decrease is trivially true in this case.\footnote{If the fixed schedule step-size is used, $h_k=0$ implies that $g_k=0$ and so $\stepsize_k = 0$ and thus $h_{k+1} = h_k$.} Let $\x^*$ be an optimum point (which is not necessarily unique). As $h_k > 0$, we have that $\innerProd{\nabla f(\x^{(k)})}{\x^*-\x^{(k)}} < 0$. We can thus apply the geometric strong convexity bound~\eqref{eq:muf} at the current iterate $\x:=\x^{(k)}$ using $\x^*$ as an optimum reference point to get (with $\overline{\stepsize} := \stepsize^\away(\x^{(k)}, \x^*)$):
\[
\begin{array}{rll}
\frac{{\overline{\stepsize}}^2}{2} \strongConvMFW \  \leq
&f(\x^*)-f(\x^{(k)}) -\left\langle \nabla f(\x^{(k)}) , \x^*-\x^{(k)} \right\rangle \\
=& -h_k - \overline{\stepsize} \left\langle  \nabla f(\x^{(k)}), \s_f(\x^{(k)}) - \vv_f(\x^{(k)})\right\rangle \\
\le &-h_k + \overline{\stepsize} \left\langle  \nabla f(\x^{(k)}), \s_k - \vv_k \right\rangle  \\
=&  -h_k + \overline{\stepsize}  g_k \ ,
\end{array}
\]
where we define $g_k :=  \left\langle  -\nabla f(\x^{(k)}), \s_k - \vv_k \right\rangle$ (note that $h_k \leq g_k$ and so $g_k$ also gives a primal suboptimality certificate). %
For the third line, we have used the definition of $\vv_f(\x)$ which implies $\left\langle  \nabla f(\x^{(k)}), \vv_f(\x^{(k)})\right\rangle \leq \left\langle  \nabla f(\x^{(k)}),\vv_k \right\rangle$. %
Therefore $h_k \le -\frac{{\overline{\stepsize}}^2}{2} \strongConvMFW + \overline{\stepsize} g_k$,
which is always upper bounded\footnote{Here we have used the trivial inequality $0 \le %
a^2-2ab+b^2$ for the choice of numbers $a:=\frac{g_k}{\strongConvMFW}$ and $b:=\overline{\stepsize}$.o} by $\frac{{g_k}^2}{2\strongConvMFW}$:\vspace{-2mm}
\begin{equation} \label{eq:hUpperBound}
h_k \leq  \frac{{g_k}^2}{2\strongConvMFW}.
\end{equation}

\emph{Lower bounding progress $h_k-h_{k+1}$.} A key aspect of the proof is to use the following observation: because of the way the direction $\dd_k$ is chosen in Algorithm~\ref{alg:MFW}, we have 
\begin{equation} \label{eq:gapDirection}
	\left\langle  -\nabla f(\x^{(k)}), \dd_k \right\rangle \geq g_k/2 , %
\end{equation}
and thus $g_k$ characterizes the quality of the direction $\dd_k$. To see this, note that $2 \left\langle  \nabla f(\x^{(k)}), \dd_k \right\rangle \leq  \left\langle \nabla f(\x^{(k)}), \dd_k^\FW\right\rangle  + \left\langle \nabla f(\x^{(k)}), \dd_k^\away\right\rangle = \left\langle \nabla f(\x^{(k)}), \dd_k^\FW+\dd_k^\away\right\rangle = -g_k$.

We first consider the case $\stepsize_\textrm{max} \geq 1$. Let $\x_\stepsize :=  \x^{(k)} + \stepsize \dd_k$ be the point obtained by moving with step-size $\stepsize$ in direction $\dd_k$, where $\dd_k$ is the one chosen by Algorithm~\ref{alg:MFW}. By using $\s := \x^{(k)} + \dd_k$ (a feasible point as $\stepsize_\textrm{max} \geq 1$)%
, $\x := \x^{(k)}$ and $\y := \x_\stepsize$ in the definition of the curvature constant~$\Cf$~\eqref{eq:Cf}, and solving for $f(\x_\stepsize)$, we get $f(\x_\stepsize) \leq f(\x^{(k)}) + \stepsize \left\langle  \nabla f(\x^{(k)}), \dd_k \right\rangle + \frac{\stepsize^2}{2} \Cf$, valid $\forall \stepsize \in [0,1]$. As~$\stepsize_k$ is obtained by line search and that $[0,1] \subseteq [0,\stepsize_\textrm{max}]$, we also have that $f(\x^{(k+1)}) = f(\x_{\stepsize_k}) \leq  f(\x_\stepsize)$ $\forall \stepsize \in [0,1]$. Combining these two inequalities, subtracting $f(\x^*)$ on both sides, and using $\Cf \leq \CfMFW$ to simplify the possibilities yields $h_{k+1} \le h_k + \stepsize \left\langle  \nabla f(\x^{(k)}), \dd_k \right\rangle + \frac{\stepsize^2}{2} \CfMFW$.

Using the crucial gap inequality~\eqref{eq:gapDirection}, we get $h_{k+1} \le h_k - \stepsize \frac{g_k}{2} + \frac{\stepsize^2}{2} \CfMFW$, and so:
\begin{equation} \label{eq:hProgress}
h_k - h_{k+1} \ge \stepsize \frac{g_k}{2} - \frac{\stepsize^2}{2} \CfMFW \quad \forall \stepsize \in [0,1].  
\end{equation}
We can minimize the bound~\eqref{eq:hProgress} on the right hand side by letting $\stepsize = \stepbound_k := \frac{g_k}{2\CfMFW}$ -- supposing that $\stepbound_k \leq 1$, we then get $h_k - h_{k+1} \geq  \frac{g_k^2}{8\CfMFW}$ (we cover the case $\stepbound_k > 1$ later).  By combining this inequality with the one from geometric strong convexity~\eqref{eq:hUpperBound}, we get 
\begin{equation} \label{eq:mainGeometric}
\frac{h_k - h_{k+1} }{h_k} \ge \frac{\strongConvMFW}{4\CfMFW} \vspace{-1mm}
\end{equation}
implying that we have a geometric rate of decrease $h_{k+1} \leq \Big(1-\frac{\strongConvMFW}{4\CfMFW}\Big) h_k$ (this is a `good step').

\emph{Boundary cases.} We now consider the case $\stepbound_k > 1$ (with $\stepsize_\textrm{max} \geq 1$ still). The condition $\stepbound_k > 1$ then translates to $g_k \geq 2 \CfMFW$, which we can use in~\eqref{eq:hProgress} with $\stepsize = 1$ to get $h_k - h_{k+1} \geq \frac{g_k}{2} - \frac{g_k}{4} = \frac{g_k}{4}$. Combining this inequality with $h_k \leq g_k$ gives the geometric decrease $h_{k+1} \leq \left(1-\frac{1}{4}\right) h_k$ (also a `good step'). $\rho_f^\away$ is obtained by considering the worst-case of the constants obtained from $\stepbound_k > 1$ and $\stepbound_k \leq 1$. (Note that always $\strongConvMFW \leq \CfMFW$ by definition, as discussed in Remark~\ref{rem:mfMFWsmallerThanCf}).

Finally, we are left with the case that $\stepsize_\textrm{max} < 1$. This is thus an away step and so $\dd_k = \dd_k^\away = \x^{(k)} - \vv_k$. Here, we use the away version $\CfA$ of the definition for $\CfMFW$: by letting $\s := \vv_k$, $\x := \x^{(k)}$ and $\y := \x_\stepsize$ in~\eqref{eq:CfA}, we also get the bound $f(\x_\stepsize) \leq f(\x^{(k)}) + \stepsize \left\langle  \nabla f(\x^{(k)}), \dd_k \right\rangle + \frac{\stepsize^2}{2} \CfMFW$, valid $\forall \stepsize \in [0,1]$ (but note here that the points $\x_\stepsize$ are not feasible for $\stepsize > \stepsize_\textrm{max}$ -- the bound considers some points outside of $\domain$). We now have two options: either $\stepsize_k = \stepsize_\textrm{max}$ (a drop step) or $\stepsize_k < \stepsize_\textrm{max}$. In the case $\stepsize_k < \stepsize_\textrm{max}$ (the line-search yields a solution in the interior of $[0,\stepsize_\textrm{max}]$), then because $f(\x_\stepsize)$ is convex in $\stepsize$, we know that $\min_{\stepsize \in [0,\stepsize_\textrm{max}]} f(\x_\stepsize) = \min_{\stepsize \geq 0} f(\x_\stepsize)$ and thus $\min_{\stepsize \in [0,\stepsize_\textrm{max}]} f(\x_\stepsize) = f(\x^{(k+1)}) \leq f(\x_\stepsize)$ $\forall \stepsize \in [0,1]$. We can then re-use the same argument above equation~\eqref{eq:hProgress} to get the inequality~\eqref{eq:hProgress}, and again considering both the case $\stepbound_k \leq 1$ (which yields inequality~\eqref{eq:mainGeometric}) and the case $\stepbound_k > 1$ (which yields $(1-\frac{1}{4})$ as the geometric rate constant), we get a `good step' with $1-\rho_f^\away$ as the worst-case geometric rate constant.

Finally, we can easily bound the number of drop steps possible up to iteration $k$ with the following argument (the drop steps are the `bad steps' for which we cannot show good progress). Let $A_k$ be the number of steps that added a vertex in the expansion (only standard FW steps can do this) and let $D_k$ be the number of drop steps. We have that $|\Coreset^{(k)}| = |\Coreset^{(0)}| + A_k - D_k$. Moreover, we have that $A_k+D_k \leq k$. We thus have $1 \leq |\Coreset^{(k)}| \leq |\Coreset^{(0)}| + k - 2D_k$, implying that $D_k \leq \frac{1}{2}( |\Coreset^{(0)}|-1+k)=\frac{k}{2}$, as stated in the theorem.
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
\end{proof}


\remove{
%
\section{Linear Convergence of FW under Strong Convexity}

We can re-use the proof techniques for the linear convergence of MFW to get an affine invariant analysis of the linear convergence of the standard FW algorithm when $f$ is strongly convex and the solution $\x^*$ lies in the relative interior of $\domain$ (an improvement over~\cite{Guelat:1986fq}).
The standard FW algorithm is like algorithm~\ref{alg:FW} where only the standard FW steps are used. As only the direction $\dd_k^\FW = \s_k - \x^{(k)}$ is used during the algorithm, we need to replace the anchor direction $\s_f(\x) - \vv_f(\x)$ in~\eqref{eq:muf} with $\s_f(\x)-\x$. Unfortunately, trying to be agnostic about the position of the optimum $\x^*$ in $\domain$ by taking the infimum over all possible $\x^*$ as was done in~\eqref{eq:muf} will yield zero in this case. We thus define the constant $\strongConvFW$ as a function of the position of the optimum $\x^*$ to get the following affine invariant quantity:
\begin{equation}\label{eq:mufFW-again}
  \strongConvFW :=  \inf_{\x\in \domain \setminus \{\x^*\}} 
             2 \left( \frac{\left\langle \nabla f(\x), \s_f(\x) - \x) \right\rangle }{\left\langle \nabla f(\x), \x^*-\x \right\rangle}  \right)^2 
             \big( f(\x^*)-f(\x)-\left\langle \nabla f(\x),  \x^*-\x \right\rangle \big) \ .
\end{equation}
Note that $\x^*$ is assumed to be an optimal solution and is unique by strong convexity, thus $\left\langle \nabla f(\x), \x^*-\x \right\rangle < 0$ for all $\x\in \domain \setminus \{\x^*\}$.
%

\begin{lemma}
Let $f$ be a convex differentiable function and suppose $f$ is \emph{strongly convex} w.r.t. some \note{inner product} norm $\norm{.}$ over the domain $\domain$ with strong-convexity constant $\mu>0$.

Furthermore, suppose that the (unique) optimum $x^*$ lies in the relative interior of $\domain$, i.e. $\distToBoundary := \inf_{\s\in\partial \domain} \norm{\s-\x^*} > 0$.
Then\vspace{-3mm}
\[
\strongConvFW \geq \mu \cdot \distToBoundary^2  \ .
\] %
\end{lemma}
\begin{proof}
Using the strong convexity bound~\eqref{eq:strongConv} with $\y := \x^*$ on the right hand side of equation~\eqref{eq:mufFW} (and using the shorthand $\r_{\x} := -\nabla f(\x)$ ), we get:
\begin{align} \label{eq:strongConvFW}
\strongConvFW \geq&   \inf_{\x\in \domain \setminus \{\x^*\}}
                      \mu \left(  \frac{\innerProd{\r_{\x}}{ \s_f(\x) - \x}}{\innerProd{\r_{\x}}{\x^*-\x}} \norm{\x^*-\x} \right)^2 \nonumber \\
		\geq&  \inf_{\x\in \domain \setminus \{\x^*\}}
		                      \mu \left(  \innerProd{\frac{\r_{\x}}{\norm{\r_{\x}}}}{ \s_f(\x) - \x} \right)^2 , \nonumber 
\end{align}
where in the second line, we have used the Cauchy-Schwartz inequality: $\innerProd{\r_{\x}}{\frac{\x^*-\x}{\norm{\x^*-\x}}} \leq \norm{\r_{\x}} \cdot 1$. Now, by definition of $\s_f(\x)$, we have $\innerProd{\r_{\x}}{ \s_f(\x) - \x} \geq \innerProd{\r_{\x}}{ \y - \x}$ for all $\y \in \domain$. By using the particular $y := \x + \distToBoundary \frac{\r_{\x}}{\norm{\r_{\x}}}$ which is feasible by definition of $\distToBoundary$ 
%
as then $\norm{\y-\x}=\distToBoundary$, then we get $\innerProd{\r_{\x}}{ \s_f(\x) - \x} \geq  \innerProd{\r_{\x}}{ \y - \x} = \distToBoundary \norm{\r_{\x}}$, and so we have:
\begin{align*}
\strongConvMFW \geq&   \inf_{\x\in \domain \setminus \{\x^*\}} 
                      \mu \left(  \distToBoundary \right)^2 = \mu \left(  \distToBoundary \right)^2,
\end{align*}
which is the claimed bound.
\end{proof}


\paragraph{Linear Convergence Proof}

\begin{theorem}[Linear Convergence of Frank-Wolfe for Strongly Convex Functions]\label{thm:linear_convergence_FW}
Suppose that $f$ has smoothness constant $\Cf$ as defined in~(\ref{eq:Cf}),
as well as ``interior'' strong convexity constant~$\strongConvFW > 0$ as defined in~\eqref{eq:mufFW}.
%

Then the error of the iterates of the standard Frank-Wolfe algorithm with step-size $\stepsize := \min\{1, \frac{g_k}{\Cf} \}$ (or using line-search) decreases geometrically, that is
\[
h_{k+1} \leq \left(1-\rho_f^\FW\right) h_k \ , 
\]
where $\rho_f^\FW := \min \{\frac{1}{2}, \frac{\strongConvFW}{\Cf} \}$.
Here in each iteration, $h_k := f(\x^{(k)}) - f(\x^*)$ denotes the primal error, and $g_k :=  g(\x^{(k)}) = \displaystyle\max_{\s \in \domain} \,\big\langle \x^{(k)} - \s, \nabla f(\x^{(k)}) \big\rangle$ is the duality gap as defined in (\ref{eq:defGap}).
\end{theorem}
\begin{proof}
We follow a very similar argument as in the proof of Theorem~\ref{thm:linear_convergenceMFW} (excluding the away steps). In particular, we can redo the same argument to upper bound $h_k$ but using the different $\overline\stepsize := \frac{\innerProd{\r_{\x}}{\x^*-\x} }{\innerProd{\r_{\x}}{\s_f(\x) - \x} }$ to get analogously to~\eqref{eq:hUpperBound} $h_k \leq  \frac{{g_k}^2}{2\strongConvFW}$, but with  $g_k :=  \innerProd{-\nabla f(\x^{(k)})}{\s_k - x}$. To lower bound the progress $h_k - h_{k+1}$, we use here that $\innerProd{-\nabla f(\x^{(k)})}{\dd_k} = g_k$ (instead of~\eqref{eq:gapDirection}) and the argument after~\eqref{eq:gapDirection} to conclude the following analog to~\eqref{eq:hProgress}:
\begin{equation} \label{eq:hProgressFW}
h_k - h_{k+1} \ge \stepsize g_k - \frac{\stepsize^2}{2} \Cf \quad \forall \stepsize \in [0,1].  
\end{equation}
We again minimize this bound on the right hand size by letting $\stepsize = \stepbound_k := \frac{g_k}{\Cf}$ (supposing that $\stepbound_k \leq 1$), getting $h_k - h_{k+1} \geq  \frac{g_k^2}{2\Cf}$. By combining this inequality with the upper bound on $h_k$, we get 
$$
\frac{h_k - h_{k+1} }{h_k} \ge \frac{\strongConvFW}{\Cf}
$$
implying that we have a geometric rate of decrease $h_{k+1} \leq \left(1-\frac{\strongConvFW}{\Cf}\right) h_k$.

If $\stepbound_k > 1$, we get $g_k > \Cf$ and so using $\stepsize = 1$ in~\eqref{eq:hProgressFW} yields $h_k - h_{k+1} \geq g_k/2$. Combining with $h_k \leq g_k$ yields $h_{k+1} \leq (1-\frac{1}{2}) h_k$, giving the other part of the definition of $\rho_f^\FW$ and completing the proof.
\end{proof}

} %


\remove{
\todo{Fix the MDM part!}
%
\section{[Draft] Analysing The pairwise/MDM Algorithm}
\paragraph{The MDM Algorithm.}
The classical MDM algorithm \cite{Mitchell:1974uy} was proposed for the polytope distance problem (or minimum norm problem).
\cite{Lopez:2012ha} has recently proven a linear convergence rate for that case (with some caveats though, only for the distance problem and only asymptotically).
The MDM algorithm can simply be generalized to the general setting of constrained optimization~(\ref{eq:optGenConvex}), as follows:

\paragraph{The Case of Simplex Domain.}
Updates: $\x^{(k+1)} := \x^{(k)} + \stepsize (\unit_{good}-\unit_{bad})$, with the step-size~$\stepsize$ being best for the available (non-zero) weight $\x_{bad}$.

\subsection{Linear Convergence of the MDM Algorithm}
\paragraph{Directional Width.}
The directional width of a set $\domain$ with respect to a direction $\vv$ is defined as $\dirW(\domain,\vv) := \max_{\x\in\domain} \left\langle\x,\frac{\vv}{\norm{\vv}}\right\rangle-\min_{\x\in\domain} \left\langle\x,\frac{\vv}{\norm{\vv}}\right\rangle$.
We define $\dirW(\domain) := \inf_{\vv} \dirW(\domain,\vv)$.

\paragraph{A larger Definition of a Duality Gap.}
$g_k :=  g(\x^{(k)}) = \displaystyle\max_{\underline\s,\overline\s \in \domain} \,\big\langle \overline\s - \underline\s, \nabla f(\x^{(k)}) \big\rangle$
(recall that the old definition was $g_k^{old} := \displaystyle\max_{\s \in \domain} \,\big\langle \x^{(k)} - \s, \nabla f(\x^{(k)}) \big\rangle$) so that $g_k \ge g_k^{old}$ always holds.

\begin{theorem}[Linear Convergence of MDM for Strongly Convex Functions]\label{thm:linear_convergence_MDM}
Let the objective $f$ be $L$-smooth and $\mu$-strongly convex, and define $\Cf := L \diam(\domain)^2$ as well as $\strongConvMFW := \mu \dirW(\domain)^2$.

Then the error of the iterates of the MDM algorithm (with line-search) decreases geometrically, that is
\[
h_{k+1} \leq \left(1-\frac{\strongConvMFW}{\Cf}\right) h_k \ .
\]
\end{theorem}
\begin{proof}
Let us define the update-direction pair $\dd := \unit_{good}-\unit_{bad}$, as given by the linear problem defined by the current gradient $\nabla := \nabla f(\x^{(k)})$.

\textbf{Improvement bound from the descent:}
\[
\begin{array}{rl}
h_{k+1} \le& h_k + \stepsize \langle \dd,\nabla \rangle + \stepsize^2 L \norm{\dd}^2 \\
=& h_k - \stepsize g_k + \frac{\stepsize^2}{2} \Cf \ .
\end{array}
\]
%

\textbf{Error-bound from strong convexity:}
\[
\begin{array}{rl}
\frac{\mu}{2} \norm{\overline\stepsize \overline\dd}^2
\le&
f(\x^*)-f(\x) - \overline\stepsize \left\langle \overline\dd,\nabla \right\rangle \\
\le &-h_k + \overline\stepsize  g_k \ .
\end{array}
\]
where the vector $\overline \dd := \frac{1}{\overline \stepsize} (\x^*-\x)$ with its rescaling value $\overline \stepsize>0$ is defined such that $\overline \dd$ attains the directional width $\dirW(\domain,\x^*-\x)$.
%

In the last inequality we have used the small Lemma $\left\langle \overline \dd,-\nabla \right\rangle \le g_k$ (\textbf{TODO: verify}).\\

On the left hand side we can now lower-bound by the directional width, $\norm{\overline \dd}^2 \ge \dirW(\domain)^2$. 

Using the notation $\strongConvMFW$, the final inequality therefore reads as $h_k \le -\frac{\overline\stepsize^2}{2} \strongConvMFW
+ \overline\stepsize  g_k$, which is upper bounded by $\frac{{g_k}^2}{2\strongConvMFW}$.
\\
{\small(Here we have used the trivial inequality $0 \le %
a^2-2ab+b^2$ for the choice of numbers $a:=\frac{g_k}{\strongConvMFW}$ and $b:=\overline\stepsize$)}
%

\textbf{Geometric decrease:} Combining the above two derived bounds,
$h_{k+1} \le h_k - \stepsize g_k + \frac{\stepsize^2}{2} \Cf$%
, so $h_k - h_{k+1} \ge \stepsize g_k - \frac{\stepsize^2}{2} \Cf$, 
(and using the choice of step-size $\stepsize:=\frac{g_k}{\Cf}$ so that this is $= \frac{g_k^2}{2\Cf}$) %
we therefore get
\[
\frac{h_k - h_{k+1} }{h_k} \ge \frac{\strongConvMFW}{\Cf}
\]
implying that we have a geometric rate of decrease $h_{k+1} \leq \left(1-\frac{\strongConvMFW}{\Cf}\right) h_k$.
\end{proof}

\paragraph{Some TODOs:}
\begin{itemize}
\item Proof (or fix) the gap lemma (and define gap only w.r.t. non-zero weight bad vertices!)
\item Deal with small weights $\x_{bad}$.
\item Generalize everything from simplex to arbitrary convex sets $\domain$ (one easy way could be by using barycentric representation)
\item We could probably get the convergence of the old SMO algorithm for SVMs as a corollary (there are some papers already that tried to analyze it in some ad-hoc way).
\end{itemize}
}

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%



%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%

\end{document}\documentclass{article} %
\usepackage{nips15submit_e,times}

\usepackage{graphicx, amsmath, amssymb,amsthm}
\usepackage[breaklinks]{hyperref} %
\usepackage{url}
\usepackage{bm,color}
\usepackage{mathdots}

%
%
\usepackage[numbers]{natbib} %
\usepackage{multibib} %
\newcites{sup}{Supplementary References}

\usepackage{wrapfig}
\usepackage{multicol}

\usepackage{algcompatible} %
\usepackage{algorithm}
%

%
\newlength{\Sfloatsep}
\setlength{\Sfloatsep}{\floatsep}
\newlength{\Stextfloatsep}
\setlength{\Stextfloatsep}{\textfloatsep}
\newlength{\Sintextsep}
\setlength{\Sintextsep}{\intextsep}

\setlength{\textfloatsep}{0.1cm}
\setlength{\floatsep}{0.1cm}



\definecolor{darkgreen}{rgb}{0,.4,.2}
\definecolor{darkblue}{rgb}{.1,.2,.6}
\definecolor{brightblue}{rgb}{0,0.6,0.8}
\hypersetup{
  colorlinks=true,
  linkcolor=darkblue,
  citecolor=darkgreen,
  filecolor=darkblue,
  urlcolor=darkbluea
}

%
%

%
%
%
%
%
%
%
%
%
%

%
\newtheorem*{rep@theorem}{\rep@title}
\newcommand{\newreptheorem}[2]{%
\newenvironment{rep#1}[1]{%
 \def\rep@title{#2 \ref{##1}}%
 \begin{rep@theorem}}%
 {\end{rep@theorem}}}

\newreptheorem{lemma}{Lemma'}
\newreptheorem{proposition}{Proposition'}
\newreptheorem{theorem}{Theorem'}
\newreptheorem{remark}{Remark'}

%
%
%

\newtheorem{definition}{Definition}
\newtheorem{theorem}[definition]{Theorem}
\newtheorem{proposition}[definition]{Proposition}
\newtheorem{lemma}[definition]{Lemma}
\newtheorem{corollary}[definition]{Corollary}
\newtheorem{observation}[definition]{Observation}
\newtheorem{conjecture}[definition]{Conjecture}
\newtheorem{remark}[definition]{Remark}
\newtheorem{problem}{Problem}
\newtheorem{claim}{Claim}
\newtheorem{open}{Open Problem}
\newtheorem*{example}{Example}

%
\DeclareMathOperator{\st}{\textrm{s.t.}}
\DeclareMathOperator*{\conv}{conv}
\DeclareMathOperator{\diam}{diam}
\DeclareMathOperator*{\E}{\rm E}
\DeclareMathOperator*{\argmin}{\arg\min}
\DeclareMathOperator*{\argmax}{\arg\max}
\DeclareMathOperator{\lmax}{\lambda_{\max}}
\DeclareMathOperator{\lmin}{\lambda_{\min}}
\DeclareMathOperator*{\supp}{supp}
\DeclareMathOperator*{\diag}{diag}
\DeclareMathOperator*{\sign}{sign}
\DeclareMathOperator*{\tr}{Tr}
\DeclareMathOperator*{\rk}{Rk}
\DeclareMathOperator*{\card}{card}
\DeclareMathOperator*{\relint}{\mathop{relint}}

%
\providecommand{\abs}[1]{\left\lvert#1\right\rvert}
\providecommand{\norm}[1]{\left\lVert#1\right\rVert}
\providecommand{\dualnorm}[1]{\norm{#1}_*}
\providecommand{\frobnorm}[1]{\norm{#1}_{Fro}}
\providecommand{\tracenorm}[1]{\norm{#1}_{tr}}
\providecommand{\opnorm}[1]{\norm{#1}_{op}}
\providecommand{\maxnorm}[1]{\norm{#1}_{\max}}
\providecommand{\latGrNorm}[1]{\norm{#1}_{\groups}}

\newcommand{\ball}{\mathcal{B}}
%

\newcommand{\bigO}{O}
\newcommand{\R}{\mathbb{R}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Sym}{\mathbb{S}}

%
\newcommand{\domain}{\mathcal{M}} %
\newcommand{\stepsize}{\gamma}
\newcommand{\stepmax}{\stepsize_{\textnormal{\scriptsize max}}} %
\newcommand{\stepbound}{\stepsize^\textnormal{\scriptsize B}} %
\newcommand{\FW}{{\hspace{0.05em}\textnormal{FW}}}
\newcommand{\PFW}{{\hspace{0.05em}\textnormal{PFW}}}
\newcommand{\away}{{\hspace{0.06em}\textnormal{\scriptsize A}}}
\newcommand{\Cf}{C_{\hspace{-0.08em}f}}
\newcommand{\CfA}{C_{\hspace{-0.08em}f}^-}
\newcommand{\CfAFW}{C_{\hspace{-0.08em}f}^\away}
\newcommand{\strongConvFW}{\mu_{\hspace{-0.08em}f}^\FW}
\newcommand{\strongConvAFW}{\mu_{\hspace{-0.08em}f}^\away}
\newcommand{\distToBoundary}{{\delta_{\x^*\!,\domain}}}
\newcommand{\dirW}{\mathop{dirW}}
\newcommand{\x}{\bm{x}}
\newcommand{\y}{\bm{y}}
\newcommand{\z}{\bm{z}}
\newcommand{\s}{\bm{s}}
\newcommand{\dd}{\bm{d}}
\newcommand{\uu}{\bm{u}}
\newcommand{\vv}{\bm{v}} %
\newcommand{\atoms}{\mathcal A}
\newcommand{\groups}{\mathcal G}
\newcommand{\row}{\text{row}}
\newcommand{\col}{\text{col}}
\newcommand{\lft}{\text{left}}
\newcommand{\rgt}{\text{right}}
\newcommand{\mapprox}{\nu} %
\newcommand{\CfTotal}{C_f^{\text{\tiny{prod}}}}
\DeclareMathOperator*{\lmo}{LMO_{\!\Vertices}}

%
\newcommand{\VVertices}{\mathcal{V}}
\newcommand{\Vertices}{\mathcal{A}} %
\newcommand{\Coreset}{\mathcal{S}}
\renewcommand{\S}{\mathcal{S}}
\renewcommand{\aa}{\bm{\alpha}}
\renewcommand{\r}{\bm{r}}
\newcommand{\PdirW}{\mathop{PdirW}}
\newcommand{\PWidth}{\mathop{PW\!idth}}
\newcommand{\innerProd}[2]{\left\langle #1 , #2 \right\rangle}
\newcommand{\innerProdCompressed}[2]{\langle #1 , #2 \rangle}
\newcommand{\C}{\mathcal{C}}
\newcommand{\proj}{\bm{P}}
\newcommand{\K}{\bm{K}}
\newcommand{\Kface}{\mathcal{K}}

%
\newcommand{\A}{\bm{A}}
\newcommand{\B}{\bm{B}}
\newcommand{\bv}{\bm{b}}
\newcommand{\err}{\bm{e}} %

\newcommand{\xdim}{d}  %
\newcommand{\ydim}{p}  %

\newcommand{\strongConvGeneralized}{\tilde{\mu}_{\hspace{-0.08em}f}}

\DeclareMathOperator{\ddim}{dim}

%
\newcommand{\Spectahedron}{\mathcal{S}}
\newcommand{\SpectahedronLeOne}{\mathcal{S}_{\le 1}} %
\newcommand{\SpectahedronLeT}{\mathcal{S}_{t}} %
\newcommand{\MaxCutPolytope}{\boxplus}
\newcommand{\FourPointPSD}{\Spectahedron^4_{\text{sparse}}}
\newcommand{\FourPointPSDplus}{\Spectahedron^{4+}_{\text{sparse}}}
\newcommand{\FourPointPSDminus}{\Spectahedron^{4-}_{\text{sparse}}}
\newcommand{\LOneBall}{\diamondsuit}
\newcommand{\signVec}{\mathbf{s}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\id}{\mathbf{I}} %
\newcommand{\ind}{\mathbf{1}} %
\newcommand{\0}{\mathbf{0}} %
\newcommand{\unit}{\mathbf{e}} %
\newcommand{\one}{\mathbf{1}} %
\newcommand{\zero}{\mathbf{0}}
\newcommand\SetOf[2]{\left\{#1\vphantom{#2}\right.\left|\vphantom{#1}\,#2\right\}}
\newcommand{\ignore}[1]{}%


\newcommand{\todo}[1]{\marginpar[\hspace*{4.5em}\textbf{TODO}\hspace*{-4.5em}]{\textbf{TODO}}\textbf{TODO:} #1}
\newcommand{\note}[1]{{\textbf{\color{red}#1}}}
\newcommand{\remove}[1]{} %



%


%
%
\title{On the Global Linear Convergence \\ of Frank-Wolfe Optimization Variants}
%

\author{
Simon Lacoste-Julien \\
INRIA - SIERRA project-team\\
{\'E}cole Normale Sup{\'e}rieure, Paris, France \\
\And
Martin Jaggi \\
Dept. of Computer Science \\
ETH Z{\"u}rich, Switzerland \\
}

%
%
%
%
%
%
%

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\nipsfinalcopy %

\begin{document}


\maketitle
\vspace{-2mm}

\begin{abstract}\vspace{-2mm}
The Frank-Wolfe (FW) optimization algorithm has lately re-gained popularity
thanks in particular to its ability to nicely handle the structured
constraints appearing in machine learning applications. However, its
convergence rate is known to be slow (sublinear) when the solution lies at
the boundary. A simple less-known fix is to add the possibility to take `away
steps' during optimization, an operation that importantly \emph{does not}
require a feasibility oracle. %
%
In this paper, we highlight and clarify several variants of the Frank-Wolfe
optimization algorithm that have been successfully applied in practice: 
away-steps FW, pairwise FW, fully-corrective FW and Wolfe's minimum norm
point algorithm, and prove for the first time that they all enjoy global
linear convergence, under a weaker condition than strong convexity of the objective.
The constant in the convergence rate has an elegant interpretation as the product
of the (classical) condition number of the function with a novel geometric
quantity that plays the role of a `condition number' of the constraint set. 
We provide pointers to where these algorithms have made a difference in
practice, in particular with the flow polytope, the
marginal polytope and the base polytope for submodular optimization.

%
%
%
%
%
%
%
\end{abstract}

%

The Frank-Wolfe algorithm \citep{Frank:1956vp} (also known as
\emph{conditional gradient}) is one of the earliest existing methods for
constrained convex optimization, and has seen an impressive revival recently
due to its nice properties compared to projected or proximal gradient
methods, in particular for sparse optimization and machine learning
applications.

On the other hand, the classical projected gradient and proximal methods have
been known to exhibit a very nice adaptive acceleration property, 
namely that the the convergence rate becomes linear for strongly convex
objective, i.e. that the optimization error of the same algorithm after $t$
iterations will decrease geometrically with $O((1-\rho)^{t})$ instead of the
usual $O(1/t)$ for general convex objective functions.
It has become an active research topic recently whether such an acceleration
is also possible for Frank-Wolfe type methods.
%
%
%

\vspace{-2mm}
\paragraph{Contributions.}
We clarify several variants of the Frank-Wolfe algorithm and show that they all converge linearly for any strongly convex function optimized over a polytope domain, with a constant bounded away from zero that only depends on the geometry of the polytope. 
%
%
%
%
%
%
%
%
%
%
%
%
Our analysis does \emph{not} depend on the location of the true optimum with
respect to the domain, which was a disadvantage of earlier existing results
such as \citep{Wolfe:1970wy,Guelat:1986fq,Beck:2004jm}, and the newer work of \citep{Pena:2015ta},
as well as the line of work of 
\citep{Ahipasaoglu:2008il,Kumar:2010ku,Nanculef:2014bj} which rely on
Robinson's condition \citep{Robinson:1982ii}.
Our analysis yields a weaker
sufficient condition than Robinson's condition; in particular we can have
linear convergence even in some cases when the function has more than one
global minima, and is not globally strongly convex. The constant also naturally separates as the product of the condition number of the function with a novel notion of condition number of a polytope, which might have applications in complexity theory.
%
%

\vspace{-2mm}
\paragraph{Related Work.}
For the classical Frank-Wolfe algorithm, 
\citep{Beck:2004jm} showed a linear rate for the special case of quadratic
objectives when the optimum is in the strict interior of the domain, 
%
a result already subsumed by the more general~\citep{Guelat:1986fq}.
The early work of~\citep{Levitin:1966gf} %
showed linear convergence for \emph{strongly convex constraint sets},
under the strong requirement that the gradient norm is not too small
(see~\citep{Garber:2015vq} for a discussion).
%
%
%
%
The away-steps variant of the Frank-Wolfe algorithm, that can also remove
weight from `bad' atoms in the current active set, was proposed in
\citep{Wolfe:1970wy}, and later also analyzed in~\citep{Guelat:1986fq}.
The precise method is stated below in Algorithm~\ref{alg:AFW}.
\citep{Guelat:1986fq} showed a (local) linear convergence rate on polytopes,
but the constant unfortunately depends on the
distance between the solution and its relative boundary, a
quantity that can be arbitrarily small.
%
%
More recently, \citep{Ahipasaoglu:2008il,Kumar:2010ku,Nanculef:2014bj} have
obtained linear convergence results in the case that the optimum solution
satisfies Robinson's condition \citep{Robinson:1982ii}.
In a different recent line of work, \citep{Garber:2013vl,%
Lan:2013um} have
studied a variation of FW %
%
that repeatedly moves mass from the worst vertices to the standard FW vertex until
a specific condition is satisfied, yielding a linear rate on strongly convex functions. 
Their algorithm requires the knowledge of several constants though, and
moreover is not adaptive to the best-case scenario, unlike the Frank-Wolfe
algorithm with away steps and line-search. 
None of these previous works was shown to be affine invariant, and most
require additional knowledge about problem specific parameters. %
%

\vspace{-2mm}
\paragraph{Setup.}
We consider general constrained convex optimization problems of the form:
\begin{equation}\label{eq:optGenConvex}
	\min_{\x \in \domain} \, f(\x) \ , \quad\quad  \domain = \conv({\Vertices}), 
	\qquad \text{with only access to: } \,\, \lmo(\r) \in  \argmin_{\x\in \Vertices} \innerProdCompressed{\r}{\x}  ,\vspace{-2mm}
\end{equation}
where $\Vertices \subseteq \R^{\xdim}$ is a \emph{finite} set of vectors that
we call \emph{atoms}.\footnote{The atoms \emph{do not} have to be extreme
points (vertices) of $\domain$.} 
We assume that the function~$f$ is
$\mu$-strongly convex with $L$-Lipschitz continuous gradient over~$\domain$. We also
consider weaker conditions than strong convexity for~$f$ in
Section~\ref{sec:nonStronglyConvex}.
As~$\Vertices$ is finite, $\domain$ is a
(convex and bounded) polytope. The methods that we consider in this paper
only require access to a \emph{linear minimization oracle} $\lmo(.)$
associated with the domain~$\domain$ through a generating set
of atoms~$\Vertices$.
This oracle is defined as to return a minimizer of a linear subproblem
over~$\domain=\conv(\Vertices)$,
for any given direction $\r \in \R^\xdim$.\footnote{%
All our convergence results can be carefully extended to approximate linear
minimization oracles with multiplicative approximation guarantees; we state
them for exact oracles in this paper for simplicity.%
}
\vspace{-3mm}

\paragraph{Examples.} Optimization problems of the form
\eqref{eq:optGenConvex} appear widely in machine learning and signal
processing applications. The set of atoms $\Vertices$ can represent
combinatorial objects of arbitrary type. 
Efficient linear minimization oracles often exist in the form of dynamic
programs or other combinatorial optimization approaches. As an example from
tracking in computer vision, $\Vertices$ could be the set of integer flows on a
graph~\citep{Joulin:2014uw,Chari:2015vo}, where $\lmo$ can be efficiently implemented by a
minimum cost network flow algorithm. In this case, $\domain$ can also be
described with a polynomial number of linear inequalities. But in other
examples, $\domain$ might not have a polynomial description in terms of
linear inequalities, and testing membership in $\domain$ might be much more
expensive than running the linear oracle. This is the case when optimizing
over the \emph{base polytope}, an object appearing in submodular function
optimization~\citep{Bach:2013et}. There, the $\lmo$ oracle is a simple greedy
algorithm. Another example is when $\Vertices$ represents the possible
consistent value assignments on cliques of a Markov random field (MRF);
$\domain$ is the \emph{marginal
polytope}~\citep{wainwright:2008:variational}, where testing membership is
NP-hard in general, though efficient linear oracles exist for some special
cases~\citep{Kolmogorov:2004:graphCut}. Optimization %
over the marginal polytope appears for example in structured SVM
learning~\citep{LacosteJulien:2013ue} and variational %
inference~\citep{Krishnan:2015ws}.\vspace{-2mm}


\paragraph{The Original Frank-Wolfe Algorithm.}
The Frank-Wolfe (FW) optimization algorithm~\citep{Frank:1956vp}, also known
as \emph{conditional gradient}~\citep{Levitin:1966gf}, is particularly suited
for the setup~\eqref{eq:optGenConvex} where $\domain$ is only accessed
through the linear minimization oracle.
It works as follows:
%
%
%
%
%
%
%
%
%
At a current iterate~$\x^{(t)}$, the algorithm finds a feasible search atom
$\s_t$ to move towards by minimizing the linearization of the objective
function $f$ over $\domain$ (line~3 in Algorithm~\ref{alg:AFW}) -- this is
where the linear minimization oracle $\lmo$ is used.
The next iterate $\x^{(t+1)}$ is then obtained by doing a line-search on $f$
between $\x^{(t)}$ and $\s_t$ (line~11 in Algorithm~\ref{alg:AFW}). One
reason for the recent increased popularity of Frank-Wolfe-type algorithms is
the sparsity of their iterates: in iteration $t$ of the algorithm, the
iterate can be represented as a sparse convex combination of at most $t+1$
atoms $\Coreset^{(t)}\subseteq \Vertices$ of the domain~$\domain$, which we
write as $\x^{(t)}= \sum_{\vv \in \Coreset^{(t)}} \alpha^{(t)}_{\vv} \vv\vspace{-0.3mm}$. 
%
We write $\Coreset^{(t)}$ for the \emph{active set}, containing the
previously discovered search atoms $\s_r$ for $r < t$ that have non-zero
\emph{weight} $\alpha^{(t)}_{\s_r} > 0$ in the expansion (potentially also
including the starting point~$\x^{(0)}$).
While tracking the active set $\Coreset^{(t)}$ is not necessary for the
original FW algorithm, the improved variants of FW that we discuss will require that $\Coreset^{(t)}$ is maintained.
%
%

\vspace{-2mm}
\paragraph{Zig-Zagging Phenomenon.} %
When the optimal solution lies at the boundary of $\domain$, the convergence rate of
the iterates is slow, i.e. sublinear: $f(\x^{(t)}) - f(\x^*) \le O\big(1/t\big)$, for
$\x^*$ being an optimal
solution~\citep{Frank:1956vp,Canon:1968du,Dunn:1979da,Jaggi:2013wg}. %
This is because the iterates of the classical FW algorithm start to
zig-zag between the vertices defining the face containing the
solution $\x^*$ (see left of
Figure~\ref{fig:FWzigzag}). In fact, the $1/t$ rate is tight for a large class of functions: \citet{Canon:1968du,Wolfe:1970wy}
showed (roughly) that
$f(\x^{(t)}) - f(\x^*) \geq \Omega\big(1/t^{1+\delta}\big)$ for any $\delta
>0$ when $\x^*$ lies on a face of $\domain$ with some additional regularity
assumptions.
%
Note that this lower bound is different than the $\Omega\big(1/t\big)$ one
presented in \citep[Lemma 3]{Jaggi:2013wg} which holds for all one-atom-per-step algorithms but assumes high dimensionality $d \geq t$.
%
%
%
%
%
%
%
%

\begin{figure}[t]
  \vspace{-2.5mm}
  \centering
  \includegraphics[width = 0.32\textwidth]{./figures/fw} 
  \includegraphics[width = 0.32\textwidth]{./figures/fw-away} 
  \includegraphics[width = 0.32\textwidth]{./figures/fw-pair} 
  \vspace{-2mm}
  \caption{\small (left) The FW algorithm zig-zags when the solution $\x^*$ lies on the boundary. (middle) Adding the possibility of an \emph{away step} attenuates this problem. (right) As an alternative, a pairwise FW step. 
  %
  }
  \label{fig:FWzigzag} \vspace{1mm}
\end{figure}


\vspace{-2mm}
%
\section{Improved Variants of the Frank-Wolfe Algorithm}%
\label{sec:variants}\vspace{-2mm}

%
\setlength{\intextsep}{2mm}

%
\begin{algorithm}
	\caption{Away-steps Frank-Wolfe algorithm: \textbf{AFW}$(\x^{(0)}, \Vertices, \epsilon)$}
	\label{alg:AFW}
	\begin{algorithmic}[1]
%
	%
	\STATE Let $\x^{(0)} \in \Vertices$, and $\Coreset^{(0)} := \{\x^{(0)}\}$
	   \qquad \emph{\small(so that $\alpha^{(0)}_{\vv} = 1$ for $\vv=\x^{(0)}$ and $0$ otherwise)}
	\FOR{$t=0\dots T$}
		\STATE Let $\s_t := \lmo \!\left(\nabla f(\x^{(t)})\right)$ %
		   and $\dd_t^\FW := \s_t - \x^{(t)}$ \qquad~~ \emph{\small(the FW direction)}
		\STATE Let $\vv_t \in \displaystyle\argmax_{\vv \in \Coreset^{(t)} } \textstyle\left\langle \nabla f(\x^{(t)}), \vv \right\rangle$ and $\dd_t^\away := \x^{(t)} - \vv_t$ \qquad \emph{\small(the away direction)}
		\STATE \textbf{if} $g_t^\FW  := \left\langle -\nabla f(\x^{(t)}), \dd_t^\FW\right\rangle  \leq \epsilon$ \textbf{then} \textbf{return} $\x^{(t)}$ \hspace{12mm}\emph{\small(FW gap is small enough, so return)}
		  %
		  \IF{$\left\langle -\nabla f(\x^{(t)}), \dd_t^\FW\right\rangle  \geq \left\langle -\nabla f(\x^{(t)}), \dd_t^\away\right\rangle$ }
		  \STATE $\dd_t :=  \dd_t^\FW$, and $\stepmax := 1$  
			     \hspace{22mm}\emph{\small(choose the FW direction)}
		  \ELSE
		  \STATE $\dd_t :=  \dd_t^\away$, and $\stepmax := \alpha_{\vv_t} / (1- \alpha_{\vv_t})$
		  	\hspace{5mm}\emph{\small(choose away direction; maximum feasible step-size)}
		  \ENDIF	
		  \STATE Line-search: $\stepsize_t \in \displaystyle\argmin_{\stepsize \in [0,\stepmax]} \textstyle f\left(\x^{(t)} + \stepsize \dd_t\right)$ 
		  \STATE Update $\x^{(t+1)} := \x^{(t)} + \stepsize_t \dd_t$  
		  	\hspace{2cm}\emph{\small(and accordingly for the weights $\aa^{(t+1)}$, see text)}
		  \STATE Update $\Coreset^{(t+1)} := \{\vv \in \Vertices \,\: \mathrm{ s.t. } \,\: \alpha^{(t+1)}_{\vv} > 0\}$
	\ENDFOR
	%
	%
	\end{algorithmic}
\end{algorithm}
\begin{algorithm}
	\caption{Pairwise Frank-Wolfe algorithm: \textbf{PFW}$(\x^{(0)}, \Vertices, \epsilon)$}
	\label{alg:PFW}
	\begin{algorithmic}[1]
	\STATE $\ldots$ as in Algorithm~\ref{alg:AFW}, except replacing lines~6 to~10 by:  $\,\, \dd_t = \dd^\PFW_t \!\!\!:= \s_t - \vv_t$, and $\stepmax := \alpha_{\vv_t}$.
	\end{algorithmic}
\end{algorithm}

\setlength{\intextsep}{\Sintextsep}

\vspace{-1.5mm}
\paragraph{Away-Steps Frank-Wolfe.}
To address the zig-zagging problem of FW, \citet{Wolfe:1970wy} proposed to
add the possibility to move \emph{away} from an active atom in
$\Coreset^{(t)}$ (see middle of Figure~\ref{fig:FWzigzag}); this simple
modification is sufficient to make the algorithm linearly convergent for
strongly convex functions. We describe the away-steps variant of Frank-Wolfe
in Algorithm~\ref{alg:AFW}.\footnote{%
The original algorithm presented in~\citep{Wolfe:1970wy} was not convergent;
this was corrected by~\citet{Guelat:1986fq}, %
assuming a tractable representation of~$\domain$ with linear inequalities and
called it the modified Frank-Wolfe (MFW) algorithm. Our description in
Algorithm~\ref{alg:AFW} extends it to the more general setup
of~\eqref{eq:optGenConvex}.
}
The \emph{away} direction $\dd_t^\away$ is defined in line~4 by finding the
atom $\vv_t$ in~$\Coreset^{(t)}$ that maximizes the potential of descent
given by $g_t^\away := \innerProd{-\nabla f(\x^{(t)})}{\x^{(t)} - \vv_t}$. 
Note that this search is over the (typically small) active set $\Coreset^{(t)}$, and is
fundamentally easier than the linear oracle $\lmo$. %
The maximum step-size $\stepmax$ as defined on line~9 ensures that the new
iterate $\x^{(t)} + \stepsize \dd_t^\away$ stays in $\domain$. %
In fact, this guarantees that the convex representation is maintained, and we 
stay inside $\conv(\Coreset^{(t)}) \subseteq \domain$.
%
When $\domain$ is a simplex, then the barycentric coordinates are unique and
$\x^{(t)} + \stepmax \dd_t^\away$ truly lies on the boundary of $\domain$. 
On the other hand, if $|\Vertices| > \dim(\domain)+1$ (e.g. for the cube), 
then it could hypothetically be possible to have a step-size bigger than $\stepmax$ which is still 
feasible. %
%
Computing the true
maximum feasible step-size would require the ability to know when we cross
the boundary of~$\domain$ along a specific line, which is not possible for
general~$\domain$. Using the conservative maximum step-size of line~9
ensures that we do not need this more powerful oracle. This is why
Algorithm~\ref{alg:AFW} requires to maintain $\Coreset^{(t)}$ (unlike
standard FW). %
%
Finally, as in classical FW, the FW gap $g_t^\FW$ is an upper bound on the unknown
suboptimality, and can be used as a stopping criterion: \vspace{-1mm}
\begin{equation*}
g_t^\FW := \innerProd{-\nabla f(\x^{(t)})}{\dd_t^\FW} \geq \innerProd{-\nabla
f(\x^{(t)})}{\x^* - \x^{(t)}} \geq f(\x^{(t)}) - f(\x^*) \quad \text{ (by
convexity)}.\vspace{-3mm}
\end{equation*}

If $\stepsize_t = \stepmax$, then we call this step a \emph{drop step}, as it
fully removes the atom $\vv_t$ from the currently active set of atoms
$\Coreset^{(t)}$ (by settings its weight to zero).
%
The weight updates for lines~12 and~13 are of the following form:
For a FW step, we have $\Coreset^{(t+1)} = \{\s_t\}$ if $\stepsize_t = 1$;
otherwise $\Coreset^{(t+1)} = \Coreset^{(t)} \cup \{\s_t\}$. Also, we have
$\alpha^{(t+1)}_{\s_t} := (1-\stepsize_t) \alpha^{(t)}_{\s_t} + \stepsize_t$
and $\alpha^{(t+1)}_{\vv} := (1-\stepsize_t) \alpha^{(t)}_{\vv}$ for $\vv \in
\Coreset^{(t)} \setminus  \{\s_t\}$. 
For an away step, we have $\Coreset^{(t+1)} = \Coreset^{(t)} \setminus
\{\vv_t\}$ if  $\stepsize_t = \stepmax$ (a \emph{drop step}); 
otherwise $\Coreset^{(t+1)} = \Coreset^{(t)}$.  Also, we have
$\alpha^{(t+1)}_{\vv_t} := (1+\stepsize_t) \alpha^{(t)}_{\vv_t} -
\stepsize_t$ and $\alpha^{(t+1)}_{\vv} := (1+\stepsize_t) \alpha^{(t)}_{\vv}$
for $\vv \in \Coreset^{(t)} \setminus  \{\vv_t\}$. \vspace{-2mm}

\paragraph{Pairwise Frank-Wolfe.}
The next variant that we present is inspired by an early algorithm
by~\citet{Mitchell:1974uy}, called the MDM %
algorithm, originally invented for the polytope distance problem. Here the
idea is to only move weight mass between two atoms in each step. More
precisely, the generalized method as presented in Algorithm~\ref{alg:PFW}
moves weight from the away atom~$\vv_t$ to the FW atom~$\s_t$, and keeps all
other $\alpha$ weights un-changed. 
We call such a swap of mass between the two atoms a \emph{pairwise FW} step,
i.e. $\alpha_{\vv_t}^{(t+1)} = \alpha_{\vv_t}^{(t)} - \stepsize$ and
$\alpha_{\s_t}^{(t+1)} = \alpha_{\s_t}^{(t)} + \stepsize$ for some step-size
$\stepsize \leq \stepmax := \alpha_{\vv_t}^{(t)}$. 
In contrast, classical FW shrinks all active weights at every iteration.

The pairwise FW direction will also be central to our proof technique to
provide the first global linear convergence rate for away-steps FW, as well
as the fully-corrective variant and Wolfe's min-norm-point algorithm. 

As we will see in Section~\ref{sec:theorems}, the rate guarantee for the
pairwise FW variant is more loose than for the other variants, because we cannot provide a satisfactory bound on the number of the problematic \emph{swap steps} (defined just before Theorem~\ref{thm:megaConvergenceTheorem}).
Nevertheless, the algorithm seems to perform quite well in practice, often
outperforming away-steps FW, especially in the important case of
sparse solutions, that is if the optimal solution
$\x^*$ lies on a low-dimensional face of $\domain$ (and thus one wants to
keep the active set $\Coreset^{(t)}$ small). The pairwise FW step is
arguably more efficient at pruning the coordinates in $\Coreset^{(t)}\!$. In
contrast to the away step which moves the mass back \emph{uniformly} onto
all other active elements $\Coreset^{(t)}$ (and might require more
corrections later), the pairwise FW step only moves the mass onto the (good) FW
atom $\s_t$.
A slightly different version than Algorithm~\ref{alg:PFW} was also proposed
by~\citet{Nanculef:2014bj}, though their convergence proofs were incomplete
(see~Appendix~\ref{sec:PFWdetails}).
%
%
%
%
The algorithm is related to classical working set algorithms, such as
the SMO algorithm used to train SVMs~\citep{Platt:1999wl}. We refer
to~\citep{Nanculef:2014bj} for an empirical comparison for SVMs, as
well as their Section~5 for more related work. See also Appendix~\ref{sec:PFWdetails} 
for a link between pairwise FW and~\citep{Garber:2013vl}.
\vspace{-2mm}

\paragraph{Fully-Corrective Frank-Wolfe, and Wolfe's Min-Norm Point
Algorithm.}

When the linear oracle is expensive, it might be worthwhile to do more work
to optimize over the active set $\Coreset^{(t)}$ in between each call to the
linear oracle, rather than just performing an away or pairwise step. We give
in Algorithm~\ref{alg:FCFW} the fully-corrective Frank-Wolfe (FCFW) variant,
that maintains a correction polytope defined by a set of atoms
$\Vertices^{(t)}$ (potentially larger than the active set $\Coreset^{(t)}$).
Rather than obtaining the next iterate by line-search, $\x^{(t+1)}$ is
obtained by re-optimizing $f$ over $\conv(\Vertices^{(t)})$. Depending on how
the correction is implemented, and how the correction atoms $\Vertices^{(t)}$
are maintained, several variants can be obtained. 
%
These variants are known
under many names, such as the extended FW method
by~\citet{Holloway:1974:FCFW} or the simplicial decomposition
method~\citep{Hohenbalken:1977:simplicial,Hearn:1987:simplicial}. 
Wolfe's min-norm point (MNP) algorithm~\citep{Wolfe:1976:MNP} for polytope distance problems is often confused with 
FCFW for quadratic objectives.  The major difference is that standard FCFW optimizes $f$ over $\conv(\Vertices^{(t)})$, whereas MNP implements the correction as a sequence of affine projections that potentially yield a different update, but can be computed more efficiently in several practical applications~\citep{Wolfe:1976:MNP}.
We describe precisely in Appendix~\ref{sec:MNPdetails} a generalization of the MNP algorithm as a specific case of the correction subroutine from step~7 of the generic Algorithm~\ref{alg:FCFW}.\vspace{-1mm}

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%

\begin{algorithm}
	\caption{Fully-corrective Frank-Wolfe with approximate correction: \textbf{FCFW}$(\x^{(0)}, \Vertices, \epsilon)$}
	\label{alg:FCFW}
	\begin{algorithmic}[1]
	\STATE \textbf{Input:} Set of atoms $\Vertices$, active set $\Coreset^{(0)}$, starting point $\x^{(0)}= \displaystyle \sum_{\vv \in \Coreset^{(0)}} \alpha^{(0)}_{\vv} \vv$, stopping criterion $\epsilon$.
	\STATE Let $\Vertices^{(0)} := \Coreset^{(0)}$ \quad (optionally, a bigger $\Vertices^{(0)}$ could be passed as argument for a warm start)
	\FOR{$t=0\dots T$}
		\STATE Let $\s_t := \lmo \!\left(\nabla f(\x^{(t)})\right)$%
			\hspace{4cm} \emph{\small(the FW atom)}
		\STATE Let $\dd_t^\FW := \s_t - \x^{(t)}$ and $g_t^\FW = \innerProd{-\nabla f(\x^{(t)})}{\dd_t^\FW}$ \qquad \emph{\small(FW gap)}
		\STATE \textbf{if} $g_t^\FW \leq \epsilon$ \textbf{then} \textbf{return} $\x^{(t)}$
			%
		%
			%
			%
			\STATE $(\x^{(t+1)}, \Vertices^{(t+1)}) := \textbf{\textrm{Correction}}(\x^{(t)}, \Vertices^{(t)}, \s_t, \epsilon)$ \hspace{1.2cm} \emph{\small (approximate correction step)}
		 %
	\ENDFOR
	\end{algorithmic}
\end{algorithm}

%

\begin{algorithm}
	\caption{Approximate correction: \textbf{Correction}$(\x^{(t)}, \Vertices^{(t)}, \s_t, \epsilon)$}
	\label{alg:correction}
	\begin{algorithmic}[1]
	\STATE Return $(\x^{(t+1)},  \Vertices^{(t+1)})$ with the following properties:
	\STATE \quad $\Coreset^{(t+1)}$ is the active set for $\x^{(t+1)}$ and $ \Vertices^{(t+1)} \supseteq \Coreset^{(t+1)}$.
	\STATE \quad $f(\x^{(t+1)}) \leq \displaystyle\min_{\stepsize \in [0,1]} \textstyle f\left(\x^{(t)} + \stepsize (\s_t - \x^{(t)})\right)$ \qquad \emph{\small (make at least as much progress as a FW step)}
	\STATE \quad $g_{t+1}^\away := \displaystyle\max_{\vv \in \Coreset^{(t+1)} } \textstyle\left\langle -\nabla f(\x^{(t+1)}), \x^{(t+1)} - \vv \right\rangle \leq \epsilon$ \quad \emph{\small (the away gap is small enough)}
	\end{algorithmic}
\end{algorithm}

The original convergence analysis of the FCFW
algorithm~\citep{Holloway:1974:FCFW}  (and also MNP
algorithm~\citep{Wolfe:1976:MNP}) only showed that they were finitely
convergent, with a bound on the number of iterations in terms of the
cardinality of $\Vertices$ (unfortunately an exponential %
number in general). \citet{Holloway:1974:FCFW} also argued that FCFW had an
asymptotic linear convergence based on the flawed argument
of~\citet{Wolfe:1970wy}. As far as we know, our work is the first to provide
global linear convergence rates for FCFW and MNP for general strongly convex
functions. Moreover, the proof of convergence for FCFW does not require
an exact solution to the correction step; instead, we show that the weaker 
properties stated for the approximate correction procedure in Algorithm~\ref{alg:correction} 
are sufficient 
for a global linear convergence rate (this correction could be implemented
using away-steps FW, as done for example in~\cite{Krishnan:2015ws}).
\vspace{-2mm}
%
%
%
%
%
%
%
%


%
%
%
%
%
%
%

\section{Global Linear Convergence Analysis}\vspace{-2mm}

\subsection{Intuition for the Convergence Proofs}\vspace{-2mm}

We first give the general intuition for the linear convergence proof of the
different FW variants, starting from the work of~\citet{Guelat:1986fq}.
We assume that the objective function $f$ is smooth over a compact set $\domain$, i.e. its gradient is Lipschitz continuous with constant $L$. Also let $M := \diam(\domain)$. Let
$\dd_t$ be the direction in which the line-search is executed by the
algorithm (Line~11 in Algorithm~\ref{alg:AFW}). By the standard descent
lemma~\citep[see e.g. (1.2.5) in][]{Nesterov:2004:lectures}, we have:\vspace{-0.5mm}
\begin{equation} \label{eq:descentLemma}
f(\x^{(t+1)}) \leq f(\x^{(t)}+\stepsize \dd_t) \leq f(\x^{(t)}) + \stepsize
\innerProd{\nabla f(\x^{(t)})}{\dd_t} + \frac{\stepsize^2}{2} L \| \dd_t \|^2
\quad \forall \stepsize \in [0, \stepmax].
\end{equation}
We let $\r_t := - \nabla f(\x^{(t)})$ and let $h_t := f(\x^{(t)}) - f(\x^*)$
be the suboptimality error. Supposing for now that $\stepmax \geq
\stepsize_t^* :=  \innerProd{\r_t}{\dd_t} / (L \|\dd_t\|^2$). We can set
$\stepsize = \stepsize_t^*$ to minimize the RHS of~\eqref{eq:descentLemma},
subtract $f(\x^*)$ on both sides, and re-organize to get a lower bound on the
progress:\vspace{-0.5mm}
\begin{equation} \label{eq:lowerProgress}
h_t- h_{t+1} \geq \frac{\innerProdCompressed{\r_t}{\dd_t}^2}{2 L
\|\dd_t\|^2} 
= \frac{1}{2L} \innerProdCompressed{\r_t}{\hat{\dd}_t}^2 \, , \vspace{-0.5mm}
%
\end{equation}
where we use the `hat' notation to denote normalized vectors: $\hat{\dd}_t := \dd_t / \|\dd_t \|$. 
Let $\err_t := \x^* - \x^{(t)}$ be the error vector. By $\mu$-strong
convexity of $f$, we have:\vspace{-1mm}
\begin{equation} \label{eq:strongLower}
f(\x^{(t)} + \stepsize \err_t) \geq f(\x^{(t)}) + \stepsize \innerProd{\nabla
f(\x^{(t)})}{\err_t} + \frac{\stepsize^2}{2} \mu \| \err_t \|^2 \quad \forall
\stepsize \in [0,1].\vspace{-0.5mm}
\end{equation}
The RHS is lower bounded by its minimum as a function of $\stepsize$
(unconstrained), achieved using $\stepsize := \innerProdCompressed{\r_t}{\err_t} / (\mu \| \err_t \|^2)$.
We are then free to use any
value of $\stepsize$ on the LHS and maintain a valid bound. In particular, we
use $\stepsize = 1$ to obtain $f(\x^*)$. Again re-arranging, we get:\vspace{-0.5mm}
\begin{equation} \label{eq:linearProgressBound}
h_t \leq \frac{\innerProd{\r_t}{\hat{\err}_t}^2}{2 \mu} \, , \quad \text{ and
combining with \eqref{eq:lowerProgress}, we obtain: } \quad h_{t} - h_{t+1}
\geq \frac{\mu}{L}
\frac{\innerProdCompressed{\r_t}{\hat{\dd}_t}^2}{\innerProdCompressed{\r_t}{\hat{\err}_t}^2} \, h_t .
\end{equation}
%
The inequality~\eqref{eq:linearProgressBound} is
fairly general and valid for any line-search method in direction $\dd_t$. To
get a linear convergence rate, we need to lower bound (by a positive
constant) the term in front of $h_t$ on the RHS, which depends on the angle
between the update direction $\dd_t$ and the negative gradient~$\r_t$. If we
assume that the solution $\x^*$ lies in the relative interior of $\domain$
with a distance of at least $\delta > 0$ from the boundary, then
$\innerProdCompressed{\r_t}{\dd_t} \geq \delta \|\r_t\|$ for the FW direction
$\dd_t^\FW$, and by combining with $\|\dd_t\| \leq M$, we get a linear rate with constant
$1-\frac{\mu}{L}(\frac{\delta}{M})^2$ (this was the result
from~\citep{Guelat:1986fq}). On the other hand, if~$\x^*$ lies
on the boundary, then $\innerProdCompressed{\hat{\r}_t}{\hat{\dd}_t}$ %
gets
arbitrary close to zero for standard FW (the zig-zagging phenomenon) and the
convergence is sublinear.

\paragraph{Proof Sketch for AFW.} The key insight to prove the global linear
convergence for AFW is to relate $\innerProdCompressed{\r_t}{\dd_t}$ with the
\emph{pairwise FW} direction $\dd_t^\PFW := \s_t - \vv_t$. By the way the
direction $\dd_t$ is chosen on lines~6 to~10 of Algorithm~\ref{alg:AFW}, we have:
\begin{equation} \label{eq:AFWgapInequality}
2 \innerProdCompressed{\r_t}{\dd_t} \geq
\innerProdCompressed{\r_t}{\dd_t^\FW} + \innerProdCompressed{\r_t}{\dd_t^A} =
\innerProdCompressed{\r_t}{\dd_t^\FW+\dd_t^A} = 
\innerProdCompressed{\r_t}{\dd_t^\PFW} .  
\end{equation}
We thus have $\innerProdCompressed{\r_t}{\dd_t} \geq
\innerProdCompressed{\r_t}{\dd_t^\PFW} / 2$. Now the crucial property of the
pairwise FW direction is that for any potential negative gradient direction
$\r_t$, the worst case inner product $\innerProdCompressed{\hat{\r}_t}{\dd_t^\PFW}$
\begin{wrapfigure}{r}{3.5cm}\vspace{-3mm}\hspace{-3mm}
%
\includegraphics[width=1.06\linewidth]{figures/p-width-triangle}\vspace{-0.5em}
\label{fig:pwidth}
%
\vspace{-1.5em}
\end{wrapfigure}
%
%
%
can be lower bounded away from
zero by a quantity depending only on the geometry of $\domain$ (unless we are at the optimum).
We call this quantity the \emph{pyramidal width} of $\Vertices$. 
The figure on the right shows the six possible pairwise FW directions $\dd_t^\PFW$ for a triangle domain, depending on which colored area the $\r_t$ direction falls into. We will see that the pyramidal width is
related to the smallest width of pyramids that we can construct from
$\Vertices$ in a specific way related to the choice of the away and towards atoms $\vv_t$ and $\s_t$. See~\eqref{eq:Pwidth} and our main Theorem~\ref{thm:muFdirWinterpretation} in Section~\ref{sec:Pwidth}.

This gives the main argument for the linear convergence of AFW for steps
where $\gamma_t^* \leq \stepmax$. When~$\stepmax$ is too small, AFW will perform a
\emph{drop step}, as the line-search will truncate the step-size to
$\stepsize_t = \stepmax$. We cannot guarantee sufficient progress in this
case, but the drop step decreases the active set size by one, and thus they
cannot happen too often (not more than half the time). These are the main
elements for the global linear convergence proof for AFW. The rest is to
carefully consider various boundary cases.
We can re-use the same 
techniques to prove the convergence for pairwise FW, though unfortunately the
latter also has the possibility of problematic \emph{swap steps}. While their
number can be bounded, so far we only found the extremely loose bound quoted in Theorem~\ref{thm:megaConvergenceTheorem}.\vspace{-2mm}

\paragraph{Proof Sketch for FCFW.} For FCFW, by line~4 of the
%
correction Algorithm~\ref{alg:correction}, 
the away gap satisfies $g_t^\away \leq \epsilon$
at the beginning of a new iteration.
Supposing that the algorithm does not exit at line~6 of
Algorithm~\ref{alg:FCFW}, we have $g_t^\FW > \epsilon$ and therefore $2
\innerProdCompressed{\r_t}{\dd_t^\FW} \geq
\innerProdCompressed{\r_t}{\dd_t^\PFW}$ using a similar argument as
in~\eqref{eq:AFWgapInequality}. Finally, by line~3 of Algorithm~\ref{alg:correction}, the correction is
guaranteed to make at least as much progress as a line-search in direction
$\dd_t^\FW$, and so the progress bound~\eqref{eq:linearProgressBound} applies also to FCFW.\vspace{-2mm}

%
\subsection{Convergence Results}\label{sec:theorems}\vspace{-2mm}
We now give the global linear convergence rates for the four variants of the
FW algorithm: away-steps FW (AFW Alg.~\ref{alg:AFW}); pairwise FW
(PFW Alg.~\ref{alg:PFW}); fully-corrective FW (FCFW
Alg.~\ref{alg:FCFW} with approximate correction Alg.~\ref{alg:correction}); and Wolfe's
min-norm point algorithm (Alg.~\ref{alg:FCFW} with
MNP-correction as Alg.~\ref{alg:MNP} in Appendix~\ref{sec:MNPdetails}). For the AFW, MNP  and PFW algorithms, we call
a \emph{drop step} when the active set shrinks $|S^{(t+1)}| < |S^{(t)}|$. For
the PFW algorithm, we also have the possibility of a \emph{swap step} where
$\stepsize_t = \stepmax$ but $|S^{(t+1)}| = |S^{(t)}|$ (i.e. the mass was
fully swapped from the away atom to the FW atom). 
A nice property of FCFW is that it does not have any drop step (it executes both FW steps and away steps simultaneously while guaranteeing enough progress at
every iteration).

\begin{theorem}\label{thm:megaConvergenceTheorem}
Suppose that $f$ has $L$-Lipschitz gradient\footnote{For AFW and PFW, we
actually require that $\nabla f$ is $L$-Lipschitz over the larger
domain $\domain + \domain - \domain$.} and is $\mu$-strongly convex over
$\domain=\conv(\Vertices)$. Let $M=\diam(\domain)$ and $\delta =
\PWidth(\Vertices)$ as defined by~\eqref{eq:Pwidth}.
%
Then the suboptimality~$h_t$ of the iterates of all the four variants of the FW algorithm %
%
%
%
%
%
decreases geometrically at each step that is not a drop step nor a swap step
(i.e. when $\stepsize_t < \stepmax$, called a `good step'), that is\vspace{-1mm}
\[
h_{t+1} \leq \left(1-\rho\right) h_t \ , \quad\quad \text{ where } \rho :=
\frac{\mu}{4L}\left(\frac{\delta}{M}\right)^2. \vspace{-3mm}
\]

%
%
Let $k(t)$ be the number of `good steps' up to iteration $t$.
We have $k(t) = t$ for FCFW; $k(t) \geq t/2$ for
MNP and AFW; and $k(t) \geq t/(3|\Vertices|!+1)$ for PFW (because of the swap steps).
This yields a global linear convergence rate of $h_t \leq h_0 \exp(-
\rho \, k(t))$ for all variants. If $\mu=0$ (general convex),
then $h_t = O(1/k(t))$ instead. See Theorem~\ref{thm:megaConvergenceTheorem2} in Appendix~\ref{sec:conv} for an affine invariant version and proof.
\vspace{-1mm}
\end{theorem}

Note that to our knowledge, none of the existing linear convergence results showed that the duality gap was also linearly convergent. The result for the gap follows directly from the simple manipulation of~\eqref{eq:descentLemma}; putting the FW gap to the LHS
and optimizing the RHS for $\stepsize \in [0,1]$.

\begin{theorem}\label{thm:gapTheorem}
Suppose that $f$ has $L$-Lipschitz gradient over $\domain$ with $M :=
\diam(\domain)$. Then the FW gap $g_t^\FW$ for \emph{any} algorithm is upper
bounded by the primal error $h_t$ as follows:
\begin{equation}
g_t^\FW \leq h_t + LM^2/2 \,\textnormal{ when }\, h_t > LM^2/2, 
\quad\quad\quad\quad  g_t^\FW \leq M \sqrt{2 h_t L} \,\,\textnormal{
otherwise }.
%
\end{equation}
\end{theorem}
%


%
%
%


\section{Pyramidal Width} \label{sec:Pwidth}\vspace{-3mm}

We now describe the claimed lower bound on the angle between
the negative gradient and the pairwise FW direction, which depends only on
the geometric properties of~$\domain$. According to our argument about the
progress bound~\eqref{eq:linearProgressBound} and the PFW
gap~\eqref{eq:AFWgapInequality}, our goal is to find a lower bound on
$\innerProdCompressed{\r_t}{\dd_t^\PFW}/\innerProdCompressed{\r_t}{\hat{\err}
_t}$. First note that $\innerProdCompressed{\r_t}{\dd_t^\PFW}\! = \!\innerProdCompressed{\r_t}{\s_t\! -\! \vv_t} \! = \hspace{-5mm}\displaystyle
\max_{\s \in \domain, \vv \in \Coreset^{(t)}}\hspace{-3mm} \innerProdCompressed{\r_t}{\s -
\vv}$ where $\Coreset^{(t)}$ is a possible active set for~$\x^{(t)}$. This
looks like the \emph{directional width} of a pyramid with base~$\Coreset^{(t)}$ 
and summit $\s_t$. To be conservative, we consider the worst
case possible active set for~$\x^{(t)}$; this is what we will call the
\emph{pyramid directional width}~$\PdirW(\Vertices, \r_t, \x^{(t)})$. We 
start with the following definitions.

\vspace{-3mm}
\paragraph{Directional Width.}
The directional width of a set $\Vertices$ with respect to a direction $\r$
is defined as $\dirW(\Vertices,\r) := \max_{\s, \vv \in\Vertices}
\big\langle \frac{\r}{\norm{\r}}, \s - \vv \big\rangle$. The \emph{width} of~$\Vertices$
is the minimum directional width over all possible directions in its affine
hull.

\vspace{-3mm}
\paragraph{Pyramidal Directional Width.} We define the pyramidal directional
width of a set $\Vertices$ with respect to a direction $\r$ and a base point
$\x \in \domain$ to be\vspace{-0.5mm}
\begin{equation} \label{eq:TruePdirW}
\PdirW(\Vertices,\r, \x) := \min_{\S \in \S_{\x}} \dirW( \S \cup
\{\s(\Vertices,\r) \} , \; \r) = \min_{\S \in \S_{\x}} \max_{\s \in
\Vertices, \vv \in \S} \textstyle \big\langle \frac{\r}{\norm{\r}}, \s - \vv \big\rangle
,
\end{equation}
where $\S_{\x} := \{ \S \, | \, \S \subseteq \Vertices$ such that $\x$ is a
proper\footnote{By \emph{proper} convex combination, we mean that all
coefficients are non-zero in the convex combination.} convex combination of
all the elements in $\S\}$, and $\s(\Vertices,\r) := \argmax_{\vv \in
\Vertices} \innerProdCompressed{\r}{\vv}$ is the FW atom used as a summit. 

\vspace{-2mm}
\paragraph{Pyramidal Width.}
To define the pyramidal width of a set, we take the minimum over the cone of
possible \emph{feasible} directions~$\r$ (in order to avoid the problem of zero width).\\
%
A direction~$\r$ is \emph{feasible} for $\Vertices$ from $\x$ if it points inwards $\conv(\Vertices)$, 
%
(i.e. $\r \in \text{cone}(\Vertices-\x)$).\\
We define the \emph{pyramidal width} of a set $\Vertices$ to be the smallest pyramidal width of all its faces, i.e.\vspace{-0.5mm}
\begin{equation} \label{eq:Pwidth}
\PWidth(\Vertices) := \displaystyle \min_{\substack{\Kface \in \textrm{faces}(\conv(\Vertices)) \\
												  \x \in \Kface \\
												  \r \in \text{cone}(\Kface-\x) \setminus \{\0\}} 
                                   } \PdirW(\Kface \cap \Vertices,\r, \x)                   .    \vspace{-1mm}                             
\end{equation}
%

\begin{theorem} \label{thm:muFdirWinterpretation}
Let $\x \in \domain=\conv(\Vertices)$ be a suboptimal point and $\S$ be an
active set for $\x$. Let~$\x^*$ be an optimal point and corresponding error
direction $\hat{\err} = (\x^*-\x)/\norm{\x^*-\x}$, and negative gradient $\r
:= -\nabla f(\x)$  (and so $\innerProdCompressed{\r}{\hat{\err}} > 0$). Let
$\dd = \s \!- \!\vv$ be the pairwise FW direction obtained over $\Vertices$ and $\S$ with negative
gradient $\r$. Then \vspace{-2mm}
\begin{equation} \label{eq:PWidthIsBound}
\frac{\innerProdCompressed{\r}{\dd}}{\innerProdCompressed{\r}{\hat{\err}}}
\geq \PWidth(\Vertices) .\vspace{-3mm}
\end{equation}   
\end{theorem}

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


\subsection{Properties of Pyramidal Width and Consequences}\vspace{-2mm}

\paragraph{Examples of Values.} The pyramidal width of a set $\Vertices$ is
lower bounded by the minimal width over all subsets of atoms, and thus is
strictly greater than zero if the number of atoms is finite.
On the other hand, this lower bound is often too loose to be useful, as in
particular, vertex subsets of the unit cube in dimension $d$ can have exponentially small width $O(d^{-\frac{d}{2}})$~\citep[see
Corollary 27 in][]{Ziegler:1999:01polytope}.
On the other hand, as we show here, the pyramidal width of the unit cube is actually
$1/\sqrt{d}$, justifying why we kept the tighter but more involved
definition~\eqref{eq:Pwidth}. See Appendix~\ref{app:cubeWidth} for the proof.

%
\begin{lemma}\label{lem:cubeWidth}
The pyramidal width of the unit cube in $\R^d$ is $1/\sqrt{d}$.\vspace{-2mm}
\end{lemma}
For the probability simplex with $d$ vertices, the pyramidal width is
actually the same as its width, which is $2/\sqrt{d}$ when $d$ is
even, and $2/\sqrt{d\!-\!1/d}$ when $d$ is odd~\citep{Alexander:1977:simplex} (see Appendix~\ref{app:cubeWidth}). 
In contrast, the pyramidal width of an infinite set can be zero. For
example, for a curved domain, the set of active atoms $\S$ can contain
vertices %
forming a very narrow pyramid, yielding a zero width in the limit. \vspace{-2mm}

\paragraph{Condition Number of a Set.} The inverse of the rate constant
$\rho$ appearing in Theorem~\ref{thm:megaConvergenceTheorem} is the product
of two terms: $L/\mu$ is the standard \emph{condition number} of the objective function appearing in
the rates of gradient methods in convex optimization. The second quantity
$(M/\delta)^2$ (diameter over pyramidal width) can be interpreted as
a \emph{condition number} of the domain $\domain$, or its
\emph{eccentricity}. %
The more eccentric the constraint set (large diameter
compared to its pyramidal width), the slower the convergence. 
The best condition number of a function is when its level sets are spherical; the
analog in term of  the constraint sets is actually the regular simplex, which has the
maximum width-to-diameter ratio amongst all simplices~\citep[see Corollary 1
in][]{Alexander:1977:simplex}. Its eccentricity is (at most) $d/2$. In contrast, the
eccentricity of the unit cube is  $d^2$, which is much worse.

We conjecture that the pyramidal width of a set of \emph{vertices} (i.e. extrema of their convex hull) is \emph{non-increasing} when another
vertex is added (assuming that all previous points remain vertices). 
For example, the unit cube can be obtained by iteratively adding vertices to the
regular probability simplex, and the pyramidal width thereby decreases from
$2/\sqrt{d}$ to $1/\sqrt{d}$.
This property could provide lower bounds for the pyramidal width of more complicated polytopes, such 
as $1/\sqrt{d}$ for the $d$-dimensional marginal polytope, 
as it can be obtained by removing vertices from the unit cube. 
%
%
%
%
%
\vspace{-3mm}
\paragraph{Complexity Lower Bounds.}  %
Combining the convergence Theorem~\ref{thm:megaConvergenceTheorem}
and the condition number of the unit simplex, we get a complexity of $O(d
\frac{L}{\mu} \log(\frac{1}{\epsilon}))$ to reach $\epsilon$-accuracy 
when optimizing a strongly convex function over the unit simplex. Here the
linear dependence on $d$ should not come as a surprise, 
in view of the known lower bound of $1/t$ for $t \leq d$ for Frank-Wolfe type
methods~\citep{Jaggi:2013wg}.
%

\vspace{-3mm}
\paragraph{Applications to Submodular Minimization.}
See~Appendix~\ref{app:submodular} for a consequence of our linear rate for the popular MNP algorithm for submodular function optimization (over the base polytope).
%
%
%

%
%
%
%
%
%


%
%
%
%
%
%
%
%
%
%
%
%


\vspace{-4mm}
\section{Non-Strongly Convex Generalization} \label{sec:nonStronglyConvex}
\vspace{-2mm}
Building on the work of~\citet{Beck:2015vo}
and~\citet{Wang:2014:nonStronglyConvex}, we can generalize our global linear
convergence results for all Frank-Wolfe variants for the more general case
where $f(\x) := g(\A \x) + \innerProd{\bv}{\x}$, for $\A \in \R^{\ydim \times
\xdim}$, $\bv \in \R^{\xdim}$ and where $g$ is $\mu_g$-strongly convex and
continuously differentiable over $\A \domain$. We note that for a general
matrix $\A$, $f$ is convex but not necessarily \emph{strongly} convex. In
this case, the linear convergence still holds but with the constant $\mu$
appearing in the rate of Theorem~\ref{thm:megaConvergenceTheorem} replaced
with the generalized constant $\tilde{\mu}$ appearing in
Lemma~\ref{lem:generalizedStrongConvexity} in
Appendix~\ref{app:NonStronglyConvex}.



\begin{wrapfigure}{r}{4.8cm}\vspace{-1.5em}
%
  \vspace{-8mm}
  \begin{center}
  \includegraphics[width=0.95\linewidth]{figures/lasso-plot-s50-n500-d200-er01.pdf}
  \\%\hspace{1cm}
  \ \includegraphics[width=0.95\linewidth]{figures/FW_vs_MFW_vs_PairFW-2000.pdf}
  \vspace{-1em}
  \caption{\small Duality gap $g_t^\FW$ vs iterations on the Lasso problem (top), and video co-localization (bottom). 
  Code is available from the authors' website.
  %
  %
  }
  \label{fig:experiments}
  \end{center}  \vspace{-1em}
%
\end{wrapfigure}

%
\vspace{-4mm}
\section{Illustrative Experiments}
\vspace{-2mm}

We illustrate the performance of the presented algorithm variants in two
numerical experiments, shown in Figure~\ref{fig:experiments}.
%
The first example is a constrained Lasso problem ($\ell_1$-regularized least
squares regression), that is $\min_{\x \in \domain} \, f(\x) =
\norm{\A\x-\bv}^2$, with $\domain=20\cdot L_1$ a scaled $L_1$-ball. We used a
random Gaussian matrix $\A\in \R^{200 \times 500}$, and a noisy measurement
$\bv=\A\x^*$ with $\x^*$ being a sparse vector with 50 entries $\pm1$, and
$10\%$ of additive noise. For the $L_1$-ball, the linear minimization oracle
$\lmo$ just selects the column of $\A$ of best inner product with the residual vector.
%
The second application comes from video co-localization. The approach used
by~\citep{Joulin:2014uw} is formulated as a quadratic program (QP) over a
flow polytope, the convex hull of paths in a network. In this application,
the linear minimization oracle is equivalent to finding a shortest path in
the network, which can be done easily by dynamic
programming. %
For the $\lmo$, we re-use the code provided
by~\citep{Joulin:2014uw} %
and their included \textsf{\small aeroplane} dataset resulting in a QP over
660 variables.
%
In both experiments, we see that the modified FW variants (away-steps and
pairwise) outperform the original FW algorithm, and exhibit a linear convergence.
%
In addition, the constant in the convergence rate of Theorem~\ref{thm:megaConvergenceTheorem}
can also be empirically shown to be fairly tight for AFW and PFW by running them
on an increasingly obtuse triangle (see Appendix~\ref{app:triangle}).

\vspace{-2mm}
\paragraph{Discussion.}
Building on a preliminary version of our work~\citep{lacoste2013affine},
\citet{Beck:2015vo} also proved a linear rate for away-steps FW, but
with a simpler lower bound for the LHS of~\eqref{eq:PWidthIsBound} using
linear duality arguments. However, their lower bound
\citep[see e.g. Lemma 3.1 in][]{Beck:2015vo} is looser: they
get a $d^2$ constant for the eccentricity of the regular simplex instead
of the tighter $d$ that we proved. 
%
%
%
%
%
%
%
Finally, the recently proposed 
generic scheme for \emph{accelerating} first-order optimization 
methods in the sense of Nesterov from~\cite{lin2015catalyst}
applies directly to the FW variants given their global 
linear convergence rate that we proved.
This gives for the first time
first-order methods that \emph{only use linear oracles} 
and obtain the ``near-optimal'' $\tilde{O}(1/k^2)$ rate for smooth convex
functions, or the accelerated $\tilde{O}(\sqrt{L/\mu})$ constant in the
linear rate for strongly convex functions. Given that
the constants also depend on the dimensionality, 
it remains an open question whether this acceleration is 
practically useful.

\vspace{-3mm}
{\small \paragraph{Acknowledgements.}
We thank J.B. Alayrac, E. Hazan, A. Hubard, A. Osokin and P. Marcotte
for helpful discussions. This work was partially supported by the MSR-Inria Joint Center
and a Google Research Award.}
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%





%
\bibliographystyle{abbrvnat} 
{\small
\bibliography{references}
}%
%
%



%

\newpage
%
%
%
\appendix

%
\setlength{\floatsep}{\Sfloatsep}
\setlength{\textfloatsep}{\Stextfloatsep}
\setlength{\intextsep}{\Sintextsep}

\rule{\textwidth}{1pt}
~\\
{\huge Appendix}


\paragraph{Outline.}
The appendix is organized as follows: In Appendix \ref{sec:FWvariants}, we discuss some of the Frank-Wolfe algorithm variants in more details and related work (including the MNP algorithm in Appendix~\ref{sec:MNPdetails} and its application to submodular minimization in Appendix \ref{app:submodular}).

In Appendix \ref{sec:pyramidal}, we discuss the pyramidal width for some particular cases of sets (such as the probability simplex and the unit cube), and then provide the proof of the main Theorem~\ref{thm:muFdirWinterpretation} relating the pyramidal width to the progress quantity essential for the linear convergence rate.
Section~\ref{sec:invariance} presents an affine invariant version of the complexity constants.

In the following Section~\ref{sec:conv}, we show the main linear convergence result for the four variants of the FW algorithm, and also discuss the sublinear rates for general convex functions.
Section~\ref{app:triangle} presents a simple experiment demonstrating the empirical tightness of the theoretical linear convergence rate constant.
Finally, in Appendix~\ref{app:NonStronglyConvex} we discuss the generalization of the linear convergence to some cases of non-strongly convex functions in more details.



%
\section{More on Frank-Wolfe Algorithm Variants}\label{sec:FWvariants}

\subsection{Wolfe's Min-Norm Point (MNP) algorithm}\label{sec:MNPdetails}

%
A generalization of Wolfe's min-norm point (MNP) algorithm~\citep{Wolfe:1976:MNP} for general convex functions is to 
run Algorithm~\ref{alg:FCFW} with the correction subroutine in step~7
implemented as presented below in Algorithm~\ref{alg:MNP}.
In Wolfe's paper~\citep{Wolfe:1976:MNP}, the correction step is called the minor cycle; whereas the FW outer loop is called the major cycle.

As we have mentioned in Section \ref{sec:variants}, MNP for polytope distance is often confused with fully-corrective FW as presented in Algorithm~\ref{alg:FCFW}, for quadratic
objectives. In fact, standard FCFW optimizes $f$ over
$\conv(\Vertices^{(t)})$, whereas MNP implements the correction as a
sequence of \emph{affine} projections on the active set that potentially yield a different update. 


\begin{algorithm}
	\caption{Generalized version of Wolfe's MNP correction: \textbf{MNP-Correction}$(\x^{(t)}, \Vertices^{(t)}, \s_t)$} %
	\label{alg:MNP}
	\begin{algorithmic}[1]
	\STATE Let $\S^{(0)} := \Vertices^{(t)} \bigcup \{\s_t\}$, and $\z_0 := \x^{(t)}$. Note that $\x^{(t)} = \sum_{\vv \in \Vertices^{(t)}} \alpha_{\vv} \, \vv$ and we assume that the elements 
	of $\Vertices^{(t)}$ are \emph{affinely independent}.
	\FOR{$k=1\dots |\S^{(0)}|$}
		\STATE Let $\y_k$ be the minimizer of $f$ on the affine hull of $\S^{(k-1)}$
		\IF{$\y_k$ is in the relative interior of $\conv(\S^{(k-1)})$}
			\STATE \textbf{return} $(\y_k, \S^{(k-1)}) \quad\quad$ \emph{($\S^{(k-1)}$ is active set for $\y_k$)} 
		\ELSE
			\STATE Let $\z_k$ be the solution of doing line-search from $\z_{k-1}$ to $\y_k$. {\citepsup[step 2(d)(iii) of Alg.~1 in][]{Chakrabarty:2014:MNP}}
			\STATE \emph{\small (Note that $\z_k$ now lies on the boundary of $\conv(\S^{(k-1)})$, and so some atoms were removed)} 
			\STATE Let $\S^{(k)}$ be the (affinely independent) active atoms in the expansion of $\z_k$.
		 \ENDIF	
	\ENDFOR
	\end{algorithmic}
\end{algorithm}


There are two main differences between FCFW and the
MNP algorithm. First, after a correction step, MNP guarantees that $\x^{(t+1)}$ is \emph{both} the minimizer of
$f$ over the \emph{affine hull} of $\Vertices^{(t+1)}$ and also $\conv(\Vertices^{(t+1)})$ (where $\Vertices^{(t+1)}$ might
be much smaller than $\Vertices^{(t)} \cup
\{\s_t\}$), whereas FCFW guarantees that
$\x^{(t+1)}$ is the minimizer of $f$ over $\conv(\Vertices^{(t)} \cup
\{\s_t\})$ -- this is usually not the case for MNP unless at most one atom
was dropped from the correction polytope, as is apparent from our convergence proof. 
Secondly, the correction atoms $\Vertices^{(t)}$ are always
affinely independent for MNP and are identical to the active set
$\Coreset^{(t)}$, whereas FCFW can use both redundant as well as inactive atoms. 
The advantage of the MNP
implementation using affine hull projections is that the correction can be
efficiently implemented when $f$ is the Euclidean norm, especially when a
triangular array representation of the active set is maintained (see the
careful implementation details in Wolfe's original paper~\citep{Wolfe:1976:MNP}).

The MNP variant indeed only makes sense when the minimization of $f$ over the affine hull of $\domain$ is well-defined (and is efficient). Note though that the line-search in step 7 does not require any new information about $\domain$, as it is made only with respect to $\conv(\S^{(k-1)})$, for which we have an explicit list of vertices. This line-search can be efficiently computed in $O(|\S^{(k-1)}|)$, and is well described for example in step 2(d)(iii) of Algorithm 1 of \citetsup{Chakrabarty:2014:MNP}.


\subsection{Applications to Submodular Minimization} \label{app:submodular}
An interesting consequence of our global linear convergence result for FW algorithm variants here is the potential to reduce the gap between the known
theoretical rates and the impressive empirical performance of MNP for submodular
function minimization (over the base polytope). 
While \citet{Bach:2013et} already showed convergence of FW in this case, \citetsup{Chakrabarty:2014:MNP} later gave a weaker convergence rate for Wolfe's MNP variant.
For exact submodular function optimization, the overall complexity by \citepsup{Chakrabarty:2014:MNP} was $O(d^5 F^2)$ 
(with some corrections\footnote{\citetsup{Chakrabarty:2014:MNP} quoted a complexity of $O(d^7 F^2)$ for MNP.
However, this fell short of the earlier result of \citet{Bach:2013et} for
classic FW in the submodular minimization case, which was better by two~$O(d)$ factors. \citepsup{Chakrabarty:2014:MNP} counted $O(d^3)$ per iteration
of the MNP algorithm whereas Wolfe had provided a $O(d^2)$ implementation;
and they missed that there were at least $t/2$ good cycles (`non drop steps')
after $t$ iterations, rather than $O(t/d)$ as they have used.
}), where
$F$ is the maximum absolute value of the integer-valued submodular
function.
This is in contrast to $O(d^5 \log(d \, F))$ for the fastest
algorithms~\citepsup{Iwata:2002:subraey}. 
Using our linear convergence, the $F$ factor can be put back in the $\log$ term for 
MNP,\footnote{This is assuming that the eccentricity of the base polytope
does not depend on~$F$, which remains to be proven.}
matching their empirical observations that the MNP algorithm was not too 
sensitive to $F$. The same follows for AFW and FCFW, which is novel.
%
%


\subsection{Pairwise Frank-Wolfe}\label{sec:PFWdetails}
Our new analysis of the pairwise Frank-Wolfe variant as introduced in Section
\ref{sec:variants} is motivated by the work of~\citet{Garber:2013vl}, who
provided the first variant of Frank-Wolfe with a global linear convergence
rate with explicit constants that do not depend on the location of the
optimum $\x^*$, for a more complex extension of such a pairwise algorithm.
An important contribution of the work of~\citet{Garber:2013vl} was to define
the concept of \emph{local linear oracle}, which (approximately) minimizes a
linear function on the intersection of $\domain$ and a small ball around
$\x^{(t)}$ (hence the name \emph{local}). They showed that if such a local
linear oracle was available, then one could replace the step that moves
towards $\s_t$ in the standard FW procedure with a constant step-size move
towards the point returned by the local linear oracle to obtain a globally
linearly convergent algorithm. They then demonstrated how to implement such a
local linear oracle by using only one call to the linear oracle (to get
$\s_t$), as well as sorting the atoms in $\Coreset^{(t)}$ in decreasing order
of their inner product with $\nabla f(\x^{(t)})$ (note that the first element
then is the away atom $\vv_t$ from Algorithm~\ref{alg:AFW}). The procedure
implementing the local linear oracle amounts to iteratively swapping the mass
from the away atom~$\vv_t$ to the FW atom~$\s_t$ until enough mass has been
moved (given by some precomputed constants). If the amount of mass to move is
bigger than $\alpha_{\vv_t}^{(t)}$, then one sets $\alpha_{\vv_t}^{(t)}$  to zero and
start moving mass from the \emph{second} away atom, and so on, until enough
mass has been moved (which is why the sorting is needed). We call such a swap
of mass between the away atom and the FW atom a \emph{pairwise FW} step, i.e.
$\alpha_{\vv_t}^{(t+1)} = \alpha_{\vv_t}^{(t)} - \stepsize$ and
$\alpha_{\s_t}^{(t+1)} = \alpha_{\s_t}^{(t)} + \stepsize$ for some step-size
$\stepsize \leq \stepmax := \alpha_{\vv_t}^{(t)}$. The local linear oracle is
implemented as a sequence of pairwise FW steps, always keeping the same FW
atom~$\s_t$ as the target, but updating the away atom to move from as we set their
coordinates to zero.

A major disadvantage of the algorithm presented by~\citet{Garber:2013vl} is
that their algorithm is \emph{not adaptive}: it requires the computation of
several (loose) constants to determine the step-sizes, which means that the
behavior of the algorithm is stuck in its worst-case analysis. 
The pairwise Frank-Wolfe variant is obtained by simply doing one line-search
in the pairwise Frank-Wolfe direction $\dd^\PFW_t := \s_t - \vv_t$ (see
Algorithm~\ref{alg:PFW}). This gives a fully adaptive algorithm, and it turns
out that this is sufficient to yield a global linear convergent rate.

\paragraph{Notes on Convergence Proofs in~\cite{Nanculef:2014bj}.}
We here point out some corrections to the convergence proofs given in~\cite{Nanculef:2014bj}
for a variant of pairwise FW that chooses between a standard FW step
and a pairwise FW step by picking the one which makes the most progress
on the objective after a line-search. \cite[Proposition~$1$]{Nanculef:2014bj}
states the global convergence of their algorithm by arguing that
$\innerProdCompressed{-\nabla f(\x^{(t)})}{\dd_t^\PFW}  \geq \innerProdCompressed{-\nabla f(\x^{(t)})}{\dd_t^\FW}$ and then stating that they can re-use the same pattern 
as the standard FW
convergence proof but with the direction $\dd_t^\PFW$. But this
is forgetting the fact that the maximal step-size $\stepmax = \alpha_{\vv_t}$ for
a pairwise FW step can be too small to make sufficient progress.
Their global convergence statement is still correct as every step
of their algorithm
makes more progress than a FW step, 
which already has a global convergence result, 
but this is not the argument they made. Similarly, 
they state a global linear convergence result in their Proposition~4,
citing a proof from~\citepsup{allende2013PFW}.
On the other hand, the relevant used Proposition~3 in~\citepsup{allende2013PFW}
forgot to consider the possibility of problematic \emph{swap steps} that 
we had to painfully bound in our convergence 
Theorem~\ref{thm:megaConvergenceTheorem2}; they only
considered drop steps or `good steps', thereby missing a bound on the number of swap steps to
get a valid global bound.


\subsection{Other Related Work} \label{app:PenaDiscussion}
Very recently, following the earlier workshop version of our article~\citep{lacoste2013affine}, \citet{Pena:2015ta} presented an alternative geometric quantity measuring the linear convergence speed of the AFW algorithm variant. Their approach is motivated by a special case of the Frank-Wolfe method, the von Neumann algorithm.
Their complexity constant -- called the restricted width -- is also bounded away from zero, but its value does depend on the location of the optimal solution, which is a disadvantage shared with the earlier existing results of~\citep{Wolfe:1970wy,Guelat:1986fq,Beck:2004jm}, as well as the line of work 
of~ \citep{Ahipasaoglu:2008il,Kumar:2010ku,Nanculef:2014bj} that relies on
Robinson's condition \citep{Robinson:1982ii}.
More precisely, the bound on the constant given in~\citep[Theorem 4]{Pena:2015ta}  applies to the translated atoms $\tilde A$ relative to the optimum point. The  constant is not affine-invariant, whereas the constants $\strongConvAFW$~\eqref{eq:muf} and $\CfAFW$~\eqref{eq:CfAFW} in our setting are so, see the discussion in Section~\ref{sec:invariance}. It would still be interesting to compare
the value of our respective constants on standard polytopes.




%
\section{Pyramidal Width}\label{sec:pyramidal}

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


\subsection{Pyramidal Width of the Cube and Probability Simplex} \label{app:cubeWidth}
\begin{replemma}{lem:cubeWidth}
The pyramidal width of the unit cube in $\R^d$ is $1/\sqrt{d}$.
\end{replemma}
\begin{proof}[Proof of Lemma \ref{lem:cubeWidth}]
First consider a point $\x$ in the interior of the cube, and let $\r$ be the
unit length direction achieving the smallest pyramidal width for $\x$. Let $\s
= \s(\Vertices, \r)$ (the FW atom in direction~$\r$). Without loss of generality, 
by symmetry,\footnote{We thank Jean-Baptiste Alayrac for inspiring us
to use symmetry in the proof.} we can rotate
the cube so that $\s$ lies at the origin. This implies that each coordinate
of $\r$ is non-positive. Represent a vertex $\vv$ of the cube  as its set of
indices for which $v_i = 1$. Then $\innerProd{\r}{\s - \vv} = \sum_{i \in
\vv} -r_i \geq \max_{i \in \vv} |r_i|$. Consider any possible active set
$\S$; as $\x$ has all its coordinate strictly positive, for each dimension
$i$, there must exist an element of $\S$ with its $i$ coordinate equals to 1.
This means that $\max_{\vv \in \S} \innerProdCompressed{\r}{\s-\vv} \geq
\|\r\|_{\infty}$. But as~$\r$ has unit Euclidean norm, then
$\|\r\|_{\infty} \geq 1/\sqrt{d}$. Now consider $\x$ to lie on a facet of
the cube (i.e. the active set $\S$ is lower dimensional); and let $I := \{i :
r_i < 0 \}$. Since $\r$ has to be feasible from $\x$, for each $i \in I$, we
cannot have $x_i = 0$ and thus there exists an element of the active set with
its $i^{\text{th}}$ coordinate equal to 1. We thus have that $\max_{\vv \in
\S} \innerProdCompressed{\r}{\s-\vv} \geq \|\r\|_{\infty} \geq 1/\sqrt{|I|}
\geq 1/\sqrt{d}$. Using the same argument on a lower dimensional $\Kface$
give a lower bound of $1/\sqrt{\dim(\Kface)}$ which is bigger. These cover
all the possibilities appearing in the definition of the pyramidal width, and
thus the lower bound is correct. It is achieved by choosing an $\x$ in the
interior, the canonical basis as the active set $\S$, and the direction
defined by $r_i = -1/\sqrt{d}$ for each $i$. 
\end{proof}

We note that both the active set definition $\S$ and the feasibility condition on $\r$
were crucially used in the above proof to obtain such a large value
for the pyramidal width of the unit cube, thus justifying the somewhat
involved definition appearing in~\eqref{eq:Pwidth}. On the other hand, 
the astute reader might have noticed that the important quantity to lower
bound for the linear convergence rate of the different FW variants is
$\frac{\innerProdCompressed{\r_t}{\hat{\dd}_t}}{\innerProdCompressed{\r_t}{\hat{\err}_t}}$
(as in~\eqref{eq:linearProgressBound}),
rather than the looser value $\frac{1}{M} \frac{\innerProdCompressed{\r_t}{\dd_t}}{\innerProdCompressed{\r_t}{\hat{\err}_t}}$ 
that we used to handle the proof of the difficult Theorem~\ref{thm:muFdirWinterpretation}
(where we recall that $M$ is the diameter of $\domain$).
One could thus hope to get a tighter measure for the condition number of a set
by considering $\|\s -\vv\|$ (with $\s$ and $\vv$ the minimizing witnesses for
the pyramidal width) instead of the diameter $M$ in the ratio diameter / pyramidal width.
This might give a tighter constant for general sets, but in the case of the cube, it does not
change the general $\Omega(d^2)$ dependence for its condition number. To see this, suppose that $d$ is
even and let $k = d/2$. Consider the direction $\r$ with $r_i := -1$ for $1 \leq i \leq k$, and $r_i := -\epsilon$ for $(k\!+\!1) \leq i \leq d$. We thus have that the FW atom $\s(\Vertices,\r)$
is the origin as before. Consider $\x$ such that $x_i := 1/k$ for $1 \leq i \leq k$, 
and $x_i := 1$ for $(k\!+\!1) \leq i \leq d$, that is, $\x$ is the uniform convex combination
of the $k$ vertices which has only one non-zero in the first $k$ coordinates, and the last $k$
coordinates all equal to $1$. We have that $\r$ is a feasible direction from $\x$, and
that all vertices in the active set for $\x$ have the same inner product with $\r$:
$\max_{\vv \in \S} \innerProdCompressed{\r}{\s-\vv} = 1 + k \epsilon$. We thus have:
$$
\innerProd{\frac{\r}{\|\r\|}}{\frac{\s - \vv}{\|\s - \vv\|}} = \frac{1 + k \epsilon}{(\sqrt{k}\sqrt{1 + \epsilon^2}) (\sqrt{k+1})} \leq \frac{1}{k} \quad \text{for $\epsilon$ small enough}.
$$
Squaring the inverse, we thus get that the condition number of the cube is at least $k^2 = d^2/4$
even using this tighter definition, thus not changing the $\Omega(d^2)$ dependence.

\paragraph{Pyramidal Width for the Probability Simplex.} For any $\x$ in the relative
interior of the probability simplex on $d$ vertices, we have that $\S = \Vertices$,
and thus the pyramidal directional width~\eqref{eq:TruePdirW} in the feasible direction $\r$
with base point $\x$ is the same as the standard directional width.
Moreover, any face of the probability simplex is just a probability simplex in lower
dimensions (with bigger width). This is why the pyramidal width of the
probability simplex is the same as its standard width. The width of a regular simplex
was implicitly given in~\citep{Alexander:1977:simplex}; we provide more
details here on this citation. \citet{Alexander:1977:simplex} considers a regular
simplex with $k$ vertices and side length $\Delta$. For any partition of the $k$ points
into a set of $r$ and $k-r$ points (for $r \leq \left\lfloor{k/2}\right \rfloor$),
one can compute the distance $c(r,\Delta)$ between the flats (affine hulls) of the two
sets (which also corresponds to a specific directional width). 
Alexander gives a formula on the third line of p.~91 
in~\citep{Alexander:1977:simplex} for the square of this distance:
\begin{equation} \label{eq:flatDistance}
\left( c(r,\Delta) \right)^2 = \Delta^2 \frac{k}{2r (k-r)} . 
\end{equation}
The width of the regular simplex is obtained by taking the minimum of~\eqref{eq:flatDistance} with
respect to $r  \leq \left\lfloor{k/2}\right \rfloor$. As~\eqref{eq:flatDistance} is
a decreasing function up to $r = k/2$, we obtain its minimum 
by substituting $r = \left\lfloor{k/2}\right \rfloor$. By using $\Delta = \sqrt{2}$, $k=d$
and $r = \left\lfloor{d/2}\right \rfloor$ in~\eqref{eq:flatDistance},
we get that the width for the probability simplex on $d$ vertices is $2/\sqrt{d}$ when
$d$ is even and the slightly bigger $2/\sqrt{d - 1/d}$ when $d$ is odd.

From the pyramidal width perspective, one can obtain these numbers by 
considering any relative interior point of the probability simplex as $\x$
and considering the following feasible $\r$. For $d$ even,
we let $r_i := 1$ for $1 \leq i \leq d/2$ and $r_i := -1$ for $i > d/2$.
Note that $\sum_i r_i = 0$ and thus $\r$ is a feasible direction
for a point in the relative interior of the probability simplex.
Then $\max_{\s, \vv \in \Vertices} \innerProdCompressed{\frac{\r}{\| \r\|}}{\s - \vv} = \frac{1}{\sqrt{d}} (1+1) = \frac{2}{\sqrt{d}}$ as claimed. For $d$ odd, we can choose
$r_i := \frac{2}{d-1}$ for $1 \leq i \leq \frac{d-1}{2}$, and $r_i := \frac{2}{d+1}$
for $i \geq \frac{d+1}{2}$. Then $\| \r \| = \sqrt{\frac{4d}{d^2 -1}}$ and
$\max_{\s, \vv \in \Vertices} \innerProdCompressed{\frac{\r}{\| \r\|}}{\s - \vv} = \sqrt{\frac{4d}{d^2-1}}
= 2/\sqrt{d - 1/d}$ as claimed. Showing that these obtained values
were the minimum possible ones is non-trivial though, which is
why we appealed to the width of the regular simplex computed 
from~\citep{Alexander:1977:simplex}.


%
\subsection{Proof of Theorem~\ref{thm:muFdirWinterpretation} on the Pyramidal Width} \label{app:ProofWidth} 

In this section, we prove the main technical result in our paper: a geometric lower bound
for the crucial quantity appearing in the linear convergence rate for the FW 
optimization variants.

\begin{reptheorem}{thm:muFdirWinterpretation}
Let $\x \in \domain=\conv(\Vertices)$ be a suboptimal point and $\S$ be an
active set for $\x$. Let $\x^*$ be an optimal point and corresponding error
direction $\hat{\err} = (\x^*-\x)/\norm{\x^*-\x}$, and negative gradient $\r
:= -\nabla f(\x)$  (and so $\innerProdCompressed{\r}{\hat{\err}} > 0$). Let
$\dd^\PFW$ be the pairwise FW direction obtained over $\Vertices$ and $\S$ with negative
gradient $\r$. Then we have:
\begin{equation} \label{eq:PWidthIsBound2}
\frac{\innerProdCompressed{\r}{\dd^\PFW}}{\innerProdCompressed{\r}{\hat{\err}}}
\geq \PWidth(\Vertices) .
\end{equation}   
\end{reptheorem}

We first give a proof sketch, and then give the full proof.

Recall that a direction~$\r$ is \emph{feasible} for $\Vertices$ from $\x$ if it points inwards $\conv(\Vertices)$, 
%
i.e. $\r \in \text{cone}(\Vertices-\x)$.

\begin{proof}[Warm-Up Proof Sketch]
By Cauchy-Schwarz, the denominator of~\eqref{eq:PWidthIsBound2} is at most
$\norm{\r}$. If $\r$ is a feasible direction from $\x$ for $\Vertices$, then the LHS is lower bounded by $\PdirW(\Vertices, \r,
\x)$ as $\dd^\PFW$ is included as a possible $\s - \vv$ direction considered in the definition of $\PdirW$~\eqref{eq:TruePdirW}.
If $\r$ is not feasible from $\x$, this means that
$\x$ lies on the boundary of $\domain$. One can then show that the
potential $\x^*$ that can maximize $\innerProdCompressed{\r}{\hat{\err}}$ has
also to lie on a facet~$\Kface$ of~$\domain$ containing $\x$ (see Lemma~\ref{lem:minAngle} below). 
The idea is then to
project~$\r$ onto~$\Kface$, and re-do the argument with $\Kface$ replacing
$\domain$ and show that the inequality is in the right direction. This
explains why all the subfaces of $\domain$ are considered in the definition of
the pyramidal width~\eqref{eq:Pwidth}, and that only \emph{feasible}
directions are considered.
%
\end{proof}

\begin{lemma}[Minimizing angle is on a facet] \label{lem:minAngle}
Let $\x$ be at the origin, inside a polytope $\Kface$ and suppose that $\r \in \text{span}(\Kface)$ is not a feasible direction for $\Kface$ from $\x$ (i.e. $\r \notin \text{cone}(\Kface)$). Then a feasible direction in $\Kface$ minimizing the angle with $\r$ lies on a facet\footnote{As a reminder, we define a \emph{k-face} of
$\domain$ (a $k$-dimensional face of $\domain$) a set $\Kface$ such that
$\Kface = \domain \cap \{ \y : \innerProd{\r}{\y - \x} = \0 \}$ for some
normal vector $\r$ and fixed reference point $\x \in \Kface$ with the
additional property that $\domain$ lies on one side of the given half-space
determined by $\r$ i.e. $ \innerProd{\r}{\y - \x} \leq \0$ $\forall \y \in
\domain$. $k$ is the dimensionality of the affine hull of $\Kface$. We call a
$k$-face of dimensions $k = 0$, $1$, $\textrm{dim}(\domain)-2$ and
$\textrm{dim}(\domain)-1$ a \emph{vertex}, \emph{edge}, \emph{ridge} and
\emph{facet} respectively. $\domain$ is a $k$-face of itself with $k =
\textrm{dim}(\domain)$. See Definition 2.1 in the book of~\citetsup{Ziegler:1995td}, which we also recommend for more background material on polytopes.}%
~$\Kface'$ of $\Kface$ that includes the origin $\x$. That is:
\begin{equation} \label{eq:angleRelation}
\max_{\err \in \Kface} \innerProd{\r}{\frac{\err}{\|\err\|}} = \max_{\err \in \Kface'} \innerProd{\r}{\frac{\err}{\|\err\|}} =  \max_{\err \in \Kface'} \innerProd{\r'}{\frac{\err}{\|\err\|}}
\end{equation}
where $\Kface'$ contains $\x$, $\|\cdot \|$ is the Euclidean norm and $\r'$ is defined
as the orthogonal projection of~$\r$ on~$\text{span}(\Kface')$.
\end{lemma}

\begin{figure}[t]
\begin{center}
\includegraphics[width=0.7\linewidth]{figures/angleFigure}
\end{center}
\vspace{-4mm}
\caption{Depiction of the quantities in the proof of Lemma~\ref{lem:minAngle}. 
If $\r \notin \text{cone}(\Kface)$, but $\r \in \text{span}(\Kface)$, then the unit vector
direction~$\hat{\err}^*$ minimizing the angle with~$\r$ is generated
by a point~$\x^*$ lying on a facet~$\Kface'$ of the polytope~$\Kface$
that contains~$\x$. \vspace{5mm}} \label{fig:rFig}
\end{figure}


\begin{proof}
This seems like an obvious geometric fact (see Figure~\ref{fig:rFig}), but we prove it formally, as sometimes high dimensional geometry is tricky (for example, the result is false without the assumption that $\r \in \text{span}(\Kface)$ or if $\|\cdot\|$ is not the Euclidean norm). Rewrite the optimization variable on the LHS of~\eqref{eq:angleRelation} as $\hat{\err} = \frac{\err}{\|\err\|}$. The optimization domain for $\hat{\err}$ is thus the intersection between the unit sphere and $\text{cone}(\Kface)$.
We now show that any maximizer $\hat{\err}^*$ cannot lie in the relative interior of $\text{cone}(\Kface)$, and thus it has to lie on a \emph{facet} of $\text{cone}(\Kface)$, implying then that a corresponding maximizer $\err^*$ is lying on a facet of $\Kface$ containing $\x$, concluding the proof for the first equality in~\eqref{eq:angleRelation}.

First, as $\r \in \text{span}(\Kface)$, we can consider without loss of generality that $\text{cone}(\Kface)$ is full dimensional by projecting on its affine hull if needed. We want to solve $\max_{\hat{\err}} \innerProd{\r}{\hat{\err}}$ s.t. $\| \hat{\err}\|^2=1$ and $\hat{\err} \in \text{cone}(\Kface)$. By contradiction, we suppose that $\hat{\err}^*$ lies in the interior of $\text{cone}(\Kface)$, and so we can remove the polyhedral cone constraint. The gradient of the objective is the constant $\r$ and the gradient of the equality constraint is $2\hat{\err}$. By the Karush-Kuhn-Tucker (KKT) necessary conditions for a stationary point to the problem with the only equality constraint $\| \hat{\err}\|^2=1$ (see e.g.~\citepsup[Proposition 3.3.1 in][]{bertsekas1999nonlinear}), 
%
then the gradient of the objective is collinear to the gradient of the equality constraint, i.e. we have $\hat{\err}^* = \pm \hat{\r}$. Since $\hat{\r}$ is not feasible, then $\hat{\err}^* = -\hat{\r}$, which is actually a local \emph{minimum} of the inner product by Cauchy-Schwarz. We thus conclude that the maximizing $\hat{\err}^*$ lies on the boundary of $\text{cone}(\Kface)$, concluding the proof for the first equality in~\eqref{eq:angleRelation}.

For the second equality in~\eqref{eq:angleRelation}, we simply use the fact that $\r - \r'$ is orthogonal
to the elements of~$\Kface'$ by the definition of the orthogonal projection.
\end{proof}

\begin{proof}[Proof of Theorem~\ref{thm:muFdirWinterpretation}]
Let $\hat{\err}(\x^*) := \frac{\x^*-\x}{\norm{\x^*-\x}}$ be the normalized error vector. 
We consider the worst-case possibility for $\x^*$. As $\x$ is not optimal, we require that $\innerProd{\r}{ \hat{\err}(\x^*)} > 0$.
We recall that by definition of the pairwise FW direction:
%
\begin{equation} \label{eq:simplePDirWidthBound}
\innerProd{\frac{\r}{\|\r\|}}{\dd^\PFW} = \max_{\s \in \Vertices, \vv \in \S} \innerProd{\frac{\r}{\|\r\|}}{\s - \vv} 
\geq \min_{\S' \in \S_{\x}} \max_{\s \in \Vertices, \vv \in \S'} \innerProd{\frac{\r}{\|\r\|}}{\s - \vv} = \PdirW(\Vertices, \r, \x). 
\end{equation}
%
%
%
%
By Cauchy-Schwarz, we always have $\innerProd{\r}{\hat{\err}(\x^*)} \leq \|\r\|$. 
If $\r$ is a feasible direction from~$\x$ in~$\conv(\Vertices)$, then $\r$ appears in the set of directions
considered in the definition of the pyramidal width~\eqref{eq:Pwidth} for $\Vertices$ and so from~\eqref{eq:simplePDirWidthBound}, we have that the inequality~\eqref{eq:PWidthIsBound2} holds.

If $\r$ is not a feasible direction, then we iteratively project it on the faces of $\domain$ until we get a feasible direction $\r'$, obtaining a term $\PdirW(\Vertices \cap \Kface, \r', \x)$ for some 
face $\Kface$ of $\domain$ as appearing in the definition of the pyramidal width~\eqref{eq:Pwidth}. 
The rest of the proof formalizes this process. As $\x$ is fixed, 
we work on the centered polytope at $\x$ to simplify the statements, i.e. 
let $\tilde{\domain} := \domain - \x$. We have
the following worst case lower bound for~\eqref{eq:PWidthIsBound2}:
\begin{equation} \label{eq:MainLowerBound}
\frac{\innerProdCompressed{\r}{\dd^\PFW}}{\innerProdCompressed{\r}{\hat{\err}}} \geq
	\bigg( \max_{\s \in \Vertices, \vv \in \S} \innerProdCompressed{\r}{\s - \vv} \bigg)
	\left( \max_{\err \in \tilde{\domain}} \, \innerProdCompressed{\r}{\frac{\err}{\|\err\|}} \right)^{-1} . 
\end{equation}
The first term on the RHS of~\eqref{eq:MainLowerBound} just comes from the definition of
$\dd^\PFW$ (with equality), whereas the second term is considering the worst case
possibility for $\x^*$ to lower bound the LHS. Note also that the second term has to
be strictly greater to zero since $\x$ is not optimal.

Without loss of generality, we can assume that $\r \in \text{span}(\tilde{\domain})$ (otherwise, just project it),
as any orthogonal component would not change the inner products appearing in~\eqref{eq:MainLowerBound}.
If (this projected) $\r$ is feasible from $\x$, then $\max_{\err \in \tilde{\domain}} \, \innerProdCompressed{\r}{\frac{\err}{\|\err\|}} = \| \r \|$, and we again
have the lower bound~\eqref{eq:simplePDirWidthBound} arising in the definition
of the pyramidal width.

We thus now suppose that $\r$ is not feasible. By the Lemma~\ref{lem:minAngle},
we have the existence of a facet~$\Kface'$ of~$\tilde{\domain}$
that includes the origin $\x$ such that:
\begin{equation} \label{eq:angleEquality}
\max_{\err \in \tilde{\domain}} \innerProd{\r}{\frac{\err}{\|\err\|}} = \max_{\err \in \Kface'} \innerProd{\r}{\frac{\err}{\|\err\|}} = \max_{\err \in \Kface'} \innerProd{\r'}{\frac{\err}{\|\err\|}},
\end{equation}
where~$\r'$ is the result of the orthogonal projection of~$\r$ on~$\text{span}(\Kface')$.
We now look at how the numerator of~\eqref{eq:MainLowerBound} transforms
when considering $\r'$ and $\Kface'$:
\begin{align}
	\max_{\s \in \Vertices, \vv \in \S} \innerProdCompressed{\r}{\s - \vv}   &= 
	 \max_{\s \in \domain} \innerProd{\r}{ \s - \x} +
     \max_{\vv \in \S} \innerProd{-\r}{ \vv - \x} 
\nonumber \\
	 &\ge \max_{\s \in (\Kface'+\x)} \innerProd{\r}{  \s - \x } +
 \max_{\vv \in \S \cap (\Kface' + \x)}
\innerProd{-\r}{ \vv - \x} \nonumber \\
	 &=  \max_{\s \in (\Kface'+\x)} \innerProd{\r'}{  \s - \x } +
	 	  \max_{\vv \in \S } \innerProd{-\r'}{
\vv - \x} \nonumber \\
     &= \max_{\s \in \Vertices \cap (\Kface' + \x), \vv \in \S} \innerProd{\r'}{\s - \vv} .
     \label{eq:d1sf}
\end{align}
To go from the first to the second
line, we use the fact that the first term yields an inequality as $(\Kface'+\x)
\subseteq (\tilde{\domain}+ \x) = \domain$. Also, since 
$\x$ is in the relative interior of $\conv(\S)$ (as $\x$ is a \emph{proper} 
convex combination of elements of $\S$ by definition), we have that
$(\S - \x) \subseteq \Kface$ for any face~$\Kface$ of~$\tilde{\domain}$
containing the origin $\x$. Thus $\S = \S\cap (\Kface'+\x)$, and the
second term on the first line actually yields an equality for the second line. 
The third line uses the
fact that $\r - \r'$ is orthogonal to members of $\Kface'$,
as $\r'$ is obtained by orthogonal projection.

Plugging~\eqref{eq:angleEquality} and~\eqref{eq:d1sf} into the inequality~\eqref{eq:MainLowerBound},
we get:
\begin{equation} \label{eq:MainLowerBound2}
\frac{\innerProdCompressed{\r}{\dd^\PFW}}{\innerProdCompressed{\r}{\hat{\err}}} \geq
	\bigg( \max_{\substack{\s \in \Vertices \cap (\Kface' + \x), \\ \vv \in \S}} \innerProdCompressed{\r'}{\s - \vv} \bigg)
	\left( \max_{\err \in \Kface'} \, \innerProdCompressed{\r'}{\frac{\err}{\|\err\|}} \right)^{-1} .
\end{equation}
We are back to a similar situation to~\eqref{eq:MainLowerBound}, with the lower dimensional~$\Kface'$ playing the role of the polytope~$\tilde{\domain}$, and $\r' \in \text{span}(\Kface')$ playing the role of $\r$.
If $\r'$ is feasible from $\x$ in $\Kface'$, then re-using the previous argument,
we get $\PdirW(\Vertices \cap (\Kface' + \x), \r', \x)$ as the lower bound, which is part
of the definition of the pyramidal width of $\Vertices$ (note that we have $(\Kface'\!+\!\x)$
as $\Kface'$ is a face of the \emph{centered} polytope $\tilde{\domain}$). 
Otherwise (if $\r \not \in \text{cone}(\Kface')$),
then we use Lemma~\ref{lem:minAngle} again to get a facet~$\Kface''$ of~$\Kface'$ as
well as a new direction~$\r''$ which is the orthogonal projection of~$\r'$ on~$\text{span}(\Kface'')$
such that we can re-do the manipulations for~\eqref{eq:angleEquality} and~\eqref{eq:MainLowerBound2},
yielding $\PdirW(\Vertices \cap (\Kface'' + \x), \r'', \x)$ as a lower bound
if~$\r''$ is feasible from~$\x$ in~$\Kface''$. As long as we do not obtain a feasible
direction, we keep re-using Lemma~\ref{lem:minAngle} to project the direction
on a lower dimensional face of
$\tilde{\domain}$ that contains $\x$. This process must stop at some point; ultimately,
we will reach the lowest dimensional face~$\Kface_{\x}$ that contains~$\x$.
As~$\x$ lies in the relative interior of~$\Kface_{\x}$, then all 
directions in~$\text{span}(\Kface_{\x})$ are feasible, and so the projected
$\r$ will have to be feasible.
Moreover, by stringing together the equalities of the type~\eqref{eq:angleEquality}
for all the projected directions, we know that $\max_{\err \in \Kface_{\x}} \innerProdCompressed{\r_{\mathrm{final}}}{\frac{\err}{\|\err\|}} > 0$ (as we originally
had $\innerProdCompressed{\r}{\hat{\err}} > 0$), and thus $\Kface_{\x}$ is 
at least one-dimensional and we also have $\r_{\mathrm{final}} \neq \0$
(this last condition is crucial to avoid having a lower bound of zero!).
This concludes the proof, and also explains why in the definition
of the pyramidal width~\eqref{eq:Pwidth}, we consider
the pyramidal directional width for
all the faces of $\conv(\Vertices)$ and respective non-zero 
feasible direction $\r$.
\end{proof}


%
\section{Affine Invariant Formulation}\label{sec:invariance}

Here we provide linear convergence proofs in terms of affine invariant quantities, since all the Frank-Wolfe algorithm variants presented in this paper are affine invariant. The statements presented in the main paper above are special cases of the following more general theorems, by using the bounds~\eqref{eq:CfBound} for the curvature constant $\Cf$, and Theorem~\ref{thm:muFdirWinterpretation2} for the affine invariant strong convexity~$\strongConvAFW$.

An optimization method is called \emph{affine invariant} if it is invariant
under affine transformations of the input problem: If one chooses any
re-parameterization of the domain~$\domain$ by a \emph{surjective} linear or
affine map $\A:\hat\domain\rightarrow\domain$, then the ``old'' and ``new''
optimization problems $\min_{\x\in\domain}f(\x)$ and
$\min_{\hat\x\in\hat\domain}\hat f(\hat\x)$ for $\hat f(\hat\x):=f(\A\hat\x)$
look completely the same to the algorithm.

More precisely, every ``new'' iterate must remain exactly the transform of
the corresponding old iterate; an affine invariant analysis should thus yield
the convergence rate and constants unchanged by the transformation. It is
well known that Newton's method is affine invariant under invertible~$\A$,
and the Frank-Wolfe algorithm and all the variants presented here are affine
invariant in the even stronger sense under arbitrary surjective
$\A$~\citep{Jaggi:2013wg}. (This is directly implied if the algorithm and all
constants appearing in the analysis only depend on inner products with the
gradient, which are preserved since $\nabla \hat f = \A^T\nabla f$.)

Note however that the property of being an extremum point (vertex) of~$\domain$ is
\emph{not} affine invariant (see~\citep[Section~3.1]{Beck:2015vo} for an
example). This explains why we presented all algorithms here as
working with atoms $\Vertices$ rather than vertices of the domain, thus maintaining the affine invariance of the algorithms as well as their convergence analysis.
%
%
%
%
%
%
%
%
%
%
%
%
%

%

%
%
%
%






\paragraph{Affine Invariant Measures of Smoothness.}
The affine invariant convergence analysis of the standard Frank-Wolfe
algorithm by \citep{Jaggi:2013wg} crucially relies on the following measure
of non-linearity of the objective function $f$ over the domain $\domain$. The
(upper) \emph{curvature constant} $C_{f}$ of a convex and differentiable
function $f:\R^d\rightarrow\R$, with respect to a compact domain $\domain$ is defined as
\begin{equation}\label{eq:Cf}
  \Cf := \sup_{\substack{\x,\s\in \domain,  ~\stepsize\in[0,1],\\
                      \y = \x+\stepsize(\s-\x)}} \textstyle
           \frac{2}{\stepsize^2}\big( f(\y)-f(\x)-\innerProd{\nabla f(\x)}{\y-\x}\big) \ .
\end{equation}

%
%
%
%

The definition of $\Cf$ closely mimics the fundamental descent
lemma~\eqref{eq:descentLemma}. 
The assumption of bounded curvature $\Cf$ closely corresponds to a Lipschitz
assumption on the gradient of~$f$. %
More precisely, if~$\nabla f$ is $L$-Lipschitz continuous on $\domain$ with
respect to some arbitrary chosen norm $\norm{.}$ in dual pairing, i.e.
$\norm{\nabla f(\x) - \nabla f(\y)}_* \leq L \norm{\x-\y}$, then
\begin{equation} \label{eq:CfBound}
\Cf \le L \diam_{\norm{.}}(\domain)^2  \ ,
\end{equation}
where $\diam_{\norm{.}}(.)$ denotes the $\norm{.}$-diameter, see \citep[Lemma
7]{Jaggi:2013wg}.
%
While the early papers \citep{Frank:1956vp,Dunn:1979da} on the Frank-Wolfe
algorithm relied on such Lipschitz constants with respect to a norm, 
the curvature constant $\Cf$ here is affine invariant, does not depend on any
norm, and gives tighter convergence rates. 
The quantity $\Cf$ combines the complexity of the domain $\domain$ and the curvature of the objective function $f$ into a
single quantity. The advantage of this combination is well illustrated in~\citep[Lemma A.1]{LacosteJulien:2013ue}, where Frank-Wolfe was used to
optimize a quadratic function over product of probability simplices with an
exponential number %
of dimensions. In this case, the Lipschitz constant could
be exponentially worse than the curvature constant which does take the simplex 
geometry of $\domain$ into account.
%
%
%
%
%

\paragraph{An Affine Invariant Notion of Strong Convexity which Depends on the
Geometry of $\domain$.} We now present the affine invariant analog of the
strong convexity bound~\eqref{eq:strongLower}, which could be interpreted as
the \emph{lower} curvature $\strongConvAFW$ analog of $\Cf$. The role of
$\stepsize$ in the definition~\eqref{eq:Cf} of~$\Cf$ was to define an affine
invariant scale by only looking at proportions over lines (as line segments
between $\x$ and~$\s$ in this case). The trick here is to use anchor points
in~$\Vertices$ in order to define standard lengths (by looking at proportions
on lines). These anchor points ($\s_f(\x)$ and $\vv_f(\x)$ defined below) are
motivated directly from the FW atom and the away atom appearing in the away-steps FW 
algorithm. Specifically, let $\x^*$ be a potential optimal point
and $\x$ a non-optimal point; thus we have $\innerProd{-\nabla f(\x)}{\x^*-\x} > 0$ 
(i.e. $\x^*\!-\!\x$ is a strict descent direction from $\x$ for $f$). 
We then define the positive step-size quantity:
\begin{equation}\label{eq:gammaAway}
\stepsize^\away(\x,\x^*) := \frac{\innerProd{-\nabla f(\x)}{\x^*-\x}
}{\innerProd{-\nabla f(\x)}{\s_f(\x) - \vv_f(\x)} } \ . 
\end{equation}
This quantity is
motivated from both~\eqref{eq:AFWgapInequality} and the linear rate
inequality~\eqref{eq:linearProgressBound}, and enables to transfer lengths
from the error $\err_t = \x^* - \x_t$ to the pairwise FW direction $\dd_t^\PFW = \s_f(\x_t) - \vv_f(\x_t)$.
More precisely, $\s_f(\x) :=$%
$~\argmin_{\vv \in \Vertices} \left\langle \nabla f(\x), \vv \right\rangle$ is the
standard FW atom. To define the away-atom, we consider all possible expansions of 
$\x$ as a convex combination of atoms.\footnote{As we are working with general polytopes,
the expansion of a point as a convex combination of atoms is not 
necessarily unique.}
%
We recall that set of possible active sets is $\S_{\x} := \{ \S \, | \, \S \subseteq \Vertices$ such that $\x$ is a
proper convex combination of
all the elements in $\S\}$.
For a given set~$\S$, we write $\vv_{\S}(\x) := \argmax_{\vv \in \S }
\left\langle \nabla f(\x), \vv \right\rangle$ for the away atom in the
algorithm supposing that the current set of active atoms is $\S$.
Finally, we define $\vv_f(\x) := \hspace{-3mm}\displaystyle\argmin_{\{\vv = \vv_{\S}(\x)
\,|\, \S \in \S_{\x} \}} \textstyle \hspace{-3mm}\left\langle \nabla f(\x), \vv
\right\rangle$ to be the worst-case away atom (that is, the atom which would
yield the smallest away descent).

%

We then define the \emph{geometric strong convexity} constant
$\strongConvAFW$ which depends \emph{both} on the function~$f$ and the
domain~$\domain=\conv(\Vertices)$:
\begin{equation}\label{eq:muf}
  \strongConvAFW \ :=\  \inf_{\x\in \domain} \inf_{\substack{\x^* \in \domain\\
                        \textrm{s.t. } \left\langle \nabla f(\x), \x^*-\x \right\rangle < 0 }}
           \frac{2}{{\stepsize^\away(\x,\x^*)}^2}
           %
           \big( f(\x^*)-f(\x)-\left\langle \nabla f(\x),  \x^*-\x \right\rangle \big) \ .
\end{equation}


%

%

%
%
%
%
%

%




%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


%
\subsection{Lower Bound for the Geometric Strong Convexity Constant $\strongConvAFW$}
The geometric strong convexity constant $\strongConvAFW$, as defined in
(\ref{eq:muf}), is affine invariant, since it only depends on the inner
products of feasible points with the gradient. Also, it combines both the
complexity of the function~$f$ and the geometry of the domain $\domain$.
Theorem~\ref{thm:muFdirWinterpretation2} allows us to lower bound the constant
$\strongConvAFW$ in terms of the strong convexity of the objective function,
combined with a purely geometric complexity measure of the domain $\domain$
(its pyramidal width $\PWidth(\Vertices)$~\eqref{eq:Pwidth}).
In the following Section~\ref{sec:conv} below, we will show the linear
convergence of the four variants of the FW algorithm presented in this paper under the assumption that
$\strongConvAFW > 0$. 

In view of the following Theorem~\ref{thm:muFdirWinterpretation2}, we have 
that the condition $\strongConvAFW > 0$ is slightly weaker
 than the strong 
convexity of the objective function\footnote{As an example
of function that is not strongly convex but can still have $\strongConvAFW > 0$,
consider $f(\x) := g(\A \x)$ where $g$ is $\mu_g$-strongly convex, but the
matrix $\A$ is rank deficient. Then by using the affine invariance
of the definition of~$\strongConvAFW$ and 
using Theorem~\eqref{thm:muFdirWinterpretation2} applied
on the equivalent problem on~$g$ with domain~$\conv(\A \Vertices)$,
we get $\strongConvAFW \geq \mu_g \cdot (\PWidth(\A \Vertices))^2 > 0$.
} over a polytope domain 
(it is implied by strong convexity).

\begin{theorem}\label{thm:muFdirWinterpretation2}
Let $f$ be a convex differentiable function and suppose that $f$ is
$\mu$-\emph{strongly convex} w.r.t. to the Euclidean norm
$\norm{\cdot}$ over the domain $\domain=\conv(\Vertices)$ with strong-convexity constant $\mu
\geq 0$. Then
\begin{equation} \label{eq:muDirWidthBound}
\strongConvAFW \geq \mu \cdot \left( \PWidth(\Vertices) \right)^2 \ .
\end{equation}
\end{theorem}
%
\begin{proof}
By definition of strong convexity with respect to a norm, we have that for any $\x,\y\in\domain$,
\begin{equation} \label{eq:strongConv}
f(\y)- f(\x)- \langle\nabla f(\x), \y-\x \rangle
\geq \textstyle\frac{\mu}{2} \norm{\y-\x}^2 \ .
\end{equation}
Using the strong convexity bound~\eqref{eq:strongConv} with $\y := \x^*$ on the right hand side of equation~\eqref{eq:muf} (and using the shorthand $\r_{\x} := -\nabla f(\x)$ ), we thus get:
\begin{align}
\strongConvAFW \geq&  \inf_{\substack{\x, \x^* \in \domain\\
                                   \textrm{s.t. } \innerProd{\r_{\x}}{\x^*-\x} > 0}}
                      \mu \left(  \frac{\innerProd{\r_{\x}}{ \s_f(\x) - \vv_f(\x)}}{\innerProd{\r_{\x}}{\x^*-\x}} \norm{{\x^*-\x}} \right)^2 \nonumber \\
		&=  \mu \inf_{\substack{\x \neq \x^* \in \domain \\
                        \textrm{s.t. } \innerProd{\r_{\x}}{ \hat{\err}} > 0}}
           \left(  \frac{\innerProd{\r_{\x}}{ \s_f(\x) - \vv_f(\x)}}{\innerProd{\r_{\x}}{ \hat{\err}(\x^*,\x)}} \right)^2  , \label{eq:mufInitial}
\end{align}
where $\hat{\err}(\x^*,\x) := \frac{\x^*-\x}{\norm{\x^*-\x}}$ is the unit length feasible
direction from $\x$ to $\x^*$. We are thus taking an infimum over all
possible feasible directions starting from $\x$ (i.e. which moves within
$\domain$) with the additional constraint that it makes a positive inner
product with the negative gradient $\r_{\x}$, i.e. it is a strict descent
direction. This is only possible if $\x$ is not already optimal, i.e. $\x \in
\domain \setminus \X^*$ where $\X^* := \{\x^* \in \domain :
\innerProd{\r_{\x^*}}{\x-\x^*} \leq 0 \,\, \forall \x \in \domain \}$ is the
set of optimal points. 

We note that $\s_f(\x) - \vv_f(\x)$ is a valid pairwise FW direction for a specific active set $\S$ for $\x$, and so we can re-use~\eqref{eq:PWidthIsBound2} from Theorem~\ref{thm:muFdirWinterpretation} for the right hand side of \eqref{eq:mufInitial} to conclude the proof.
\end{proof}

We now proceed to present the main linear convergence result in the next section, using only the mentioned affine invariant quantities.


%
\section{Linear Convergence Proofs}\label{sec:conv}

\paragraph{Curvature Constants.}
Because of the additional possibility of the away step in
Algorithm~\ref{alg:AFW}, we need to define the following slightly modified
additional curvature constant, which will be needed for the linear
convergence analysis of the algorithm:
\begin{equation}\label{eq:CfAFW}
  \CfAFW := \sup_{\substack{\x,\s,\vv\in \domain, \\
                      \stepsize\in[0,1],\\
                      \y = \x+\stepsize(\s-\vv)}}
           \frac{2}{\stepsize^2}\big( f(\y)-f(\x)-\stepsize \langle \nabla f(\x), \s-\vv
\rangle \big) \ .
\end{equation}
By comparing with $\Cf$~\eqref{eq:Cf}, we see that the modification is that
$\y$ is defined with any direction $\s -\vv$ instead of a standard
FW direction $\s - \x$. This allows to use the away direction or the pairwise FW direction even though these might yield some $\y$'s which are outside of the
domain $\domain$ when using $\stepsize > \stepmax$ (in fact, $\y \in \domain^\away := \domain + (\domain -
\domain)$ in the Minkowski sense). On the other hand, by re-using a similar
argument as in \citep[Lemma 7]{Jaggi:2013wg}, we can obtain the same
bound~\eqref{eq:CfBound} for $\CfAFW$, with the only difference that the
Lipschitz constant $L$ for the gradient function has to be valid on
$\domain^\away$ instead of just $\domain$.

%
\begin{remark}\label{rem:mfAFWsmallerThanCf}
For all pairs of functions $f$ and compact domains $\domain$, it holds that $\strongConvAFW \le \Cf$ (and $\Cf \le \CfAFW$).
\end{remark}\vspace{-2mm}
\begin{proof}
Let $\x$ be a vertex of $\domain$, so that $\S = \{\x\}$. Then $\x = \vv_f(\x)$. Pick $\x^* := \s_f(\x)$ and substitute in the definition for~$\strongConvAFW$~\eqref{eq:muf}. Then $\stepsize^\away(\x,\x^*) = 1$ and so we have $\y := \x^* = \x + \stepsize (\x^*-\x)$ with $\stepsize = 1$ which can also be used in the definition of~$\Cf$~\eqref{eq:Cf}. Thus, we have $\strongConvAFW \leq 2\left( f(\y)-f(\x) - \langle \nabla f(\x), \y-\x \rangle \right) \leq \Cf$.
\end{proof}

We now give the global linear convergence rates for the four variants of the
FW algorithm: away-steps FW (AFW Algorithm~\ref{alg:AFW}); pairwise FW
(PFW Algorithm~\ref{alg:PFW}); fully-corrective FW (FCFW
Algorithm~\ref{alg:FCFW} with approximate correction as per Algorithm~\ref{alg:correction}); 
and Wolfe's min-norm point algorithm (Algorithm~\ref{alg:FCFW} with
MNP-correction given in Algorithm~\ref{alg:MNP}). For the AFW, MNP  and PFW algorithms, we call
a \emph{drop step} when the active set shrinks, i.e. $|S^{(t+1)}| < |S^{(t)}|$. For
the PFW algorithm, we also have the possibility of a \emph{swap step}, where
$\stepsize_t = \stepmax$ but the size of the active set stays constant $|S^{(t+1)}| = |S^{(t)}|$ 
(i.e. the mass gets
fully swapped from the away atom to the FW atom). We note that a nice
property of the FCFW variant is that it does not have any drop steps (it executes both FW
steps and away steps simultaneously while guaranteeing enough progress at
every iteration).

\begin{theorem}\label{thm:megaConvergenceTheorem2}
Suppose that $f$ has smoothness constant $\CfAFW$ ($\Cf$ for FCFW and
MNP),
%
as well as geometric strong convexity constant~$\strongConvAFW$ as defined
in~\eqref{eq:muf}.
%
Then the suboptimality $h_t := f(\x^{(t)}) - f(\x^*)$ of the iterates of 
all the four variants of the FW algorithm %
%
%
%
%
%
decreases geometrically at each step that is not a drop step nor a swap step (i.e.
when $\stepsize_t < \stepmax$, called a `good step'\footnote{Note that any step with $\stepmax \geq 1$ can also
be considered a `good step', even if $\stepsize_t = \stepmax$, as is apparent from the proof. The problematic
steps arise only when $\stepmax \ll 1$.}), that is
\[
h_{t+1} \leq \left(1-\rho_f\right) h_t \ ,\vspace{-2mm}
\]
where:
\begin{align*}
	\rho_f &:= \frac{\strongConvAFW}{4\CfAFW} \quad \text{for the AFW algorithm,}  & 
		\rho_f &:= \min\left\{\frac{1}{2}, \frac{\strongConvAFW}{\CfAFW} \right\} \quad \text{for the PFW algorithm,}\\
	\rho_f &:= \frac{\strongConvAFW}{4\Cf} \quad \text{for the FCFW algorithm,} &
		\rho_f &:= \min\left\{\frac{1}{2}, \frac{\strongConvAFW}{\Cf} \right\} \quad \text{\parbox{0.3\linewidth}{for the MNP algorithm, or\\ FCFW with exact correction.}} 
\end{align*}
%
%
%
%
%
%
%
%

Moreover, the number of drop steps up to iteration $t$ is bounded by $t/2$.
%
This yields the global linear convergence rate of $h_t \leq h_0 \exp(-\frac12
\rho_f t)$ for the AFW and MNP variants. FCFW does not need the extra $1/2$
factor as it does not have any bad step. Finally, the PFW algorithm has at
most $3 |\Vertices| !$ swap steps between any two `good steps'.

If $\strongConvAFW = 0$ (i.e. the case of general convex objectives), then all the four variants have
a $O(1/k(t))$ convergence rate where $k(t)$ is the number of `good steps' up to 
iteration $t$. More specifically, we can summarize the suboptimality bounds
for the four variants as:
$$
h_t \leq \frac{4 C}{k(t) + 4}   \quad \text{for $k(t) \geq 1$},
$$
where $C = 2 \strongConvAFW + h_0$ for AFW; $C = 2 \Cf + h_0$ for 
FCFW with approximate correction; $C = \Cf/2$ for MNP; and $C=\CfAFW/2$
for PFW. The number of good steps is $k(t) = t$ for FCFW; it is $k(t) \geq t/2$ for
MNP and AFW; and $k(t) \geq t/(3|\Vertices|!+1)$ for PFW.
\end{theorem}

\begin{proof}
\textbf{Proof for AFW.} 
The general idea of the proof is to use the definition of the geometric
strong convexity constant to upper bound $h_t$, while using the definition of
the curvature constant $\CfAFW$ to lower bound the decrease in primal
suboptimality $h_t - h_{t+1}$ for the `good steps' of
Algorithm~\ref{alg:AFW}. Then we upper bound the number of `bad steps' (the
drop steps).

\emph{Upper bounding $h_t$.} In the whole proof, we assume that $\x^{(t)}$ is
not already optimal, i.e. that $h_t > 0$. If $h_t = 0$, then because
line-search is used, we will have $h_{t+1} \leq h_t = 0$ and so the geometric
rate of decrease is trivially true in this case.
%
%
%
Let $\x^*$ be an optimum point (which is not
necessarily unique). As $h_t > 0$, we have that $\innerProd{-\nabla
f(\x^{(t)})}{\x^*-\x^{(t)}} > 0$. We can thus apply the geometric strong
convexity bound~\eqref{eq:muf} at the current iterate $\x:=\x^{(t)}$ using
$\x^*$ as an optimum reference point to get (with $\overline{\stepsize} :=
\stepsize^\away(\x^{(t)}, \x^*)$ as defined in \eqref{eq:gammaAway}):
\begin{align}
\frac{{\overline{\stepsize}}^2}{2} \strongConvAFW  &\leq
f(\x^*)-f(\x^{(t)}) +\left\langle -\nabla f(\x^{(t)}) , \x^*-\x^{(t)} \right\rangle  \label{eq:ht_bound_trick} \\
&= -h_t + \overline{\stepsize} \left\langle - \nabla f(\x^{(t)}), \s_f(\x^{(t)}) - \vv_f(\x^{(t)})\right\rangle \nonumber\\
&\le -h_t + \overline{\stepsize} \left\langle - \nabla f(\x^{(t)}), \s_t - \vv_t \right\rangle  \nonumber\\
&=  -h_t + \overline{\stepsize}  g_t \ , \nonumber
\end{align}
where we define $g_t :=  \left\langle  -\nabla f(\x^{(t)}), \s_t - \vv_t \right\rangle$ (note that $h_t \leq g_t$ and so $g_t$ also gives a primal suboptimality certificate). %
For the third line, we have used the definition of $\vv_f(\x)$ which implies $\left\langle  \nabla f(\x^{(t)}), \vv_f(\x^{(t)})\right\rangle \leq \left\langle  \nabla f(\x^{(t)}),\vv_t \right\rangle$. %
Therefore $h_t \le -\frac{{\overline{\stepsize}}^2}{2} \strongConvAFW + \overline{\stepsize} g_t$,
which is always upper bounded\footnote{%
Here we have used the trivial inequality $0 \le %
a^2-2ab+b^2$ for the choice of numbers $a:=\frac{g_t}{\strongConvAFW}$ and $b:=\overline{\stepsize}$.
An alternative way to obtain the bound is to look at the unconstrained maximum of the RHS which is 
a concave function of $\overline{\stepsize}$ by letting $\overline{\stepsize} = g_t / \strongConvAFW$,
as we did in the main paper to obtain the upper bound on $h_t$ in~\eqref{eq:linearProgressBound}.
} %
by\vspace{-2mm}
\begin{equation} \label{eq:hUpperBound}
h_t \leq  \frac{{g_t}^2}{2\strongConvAFW}.
\end{equation}

\emph{Lower bounding progress $h_t-h_{t+1}$.} We here use
the key aspect in the proof that we had described in the main text with~\eqref{eq:AFWgapInequality}.
Because of the way the direction $\dd_t$ is chosen in the AFW
Algorithm~\ref{alg:AFW}, we have 
\begin{equation} \label{eq:gapDirection}
	\left\langle  -\nabla f(\x^{(t)}), \dd_t \right\rangle \geq g_t/2 , %
\end{equation}
and thus $g_t$ characterizes the quality of the direction $\dd_t$. To see
this, note that $2 \left\langle  \nabla f(\x^{(t)}), \dd_t \right\rangle \leq
 \left\langle \nabla f(\x^{(t)}), \dd_t^\FW\right\rangle  + \left\langle
\nabla f(\x^{(t)}), \dd_t^\away\right\rangle = \left\langle \nabla
f(\x^{(t)}), \dd_t^\FW+\dd_t^\away\right\rangle = -g_t$.

We first consider the case $\stepsize_\textrm{max} \geq 1$. Let $\x_\stepsize
:=  \x^{(t)} + \stepsize \dd_t$ be the point obtained by moving with
step-size $\stepsize$ in direction $\dd_t$, where $\dd_t$ is the one chosen
by Algorithm~\ref{alg:AFW}. By using $\s := \x^{(t)} + \dd_t$ (a feasible
point as $\stepsize_\textrm{max} \geq 1$)%
, $\x := \x^{(t)}$ and $\y := \x_\stepsize$ in the definition of the
curvature constant~$\Cf$~\eqref{eq:Cf}, and solving for $f(\x_\stepsize)$, we
get the affine invariant version of the descent lemma~\eqref{eq:descentLemma}:
\begin{equation} \label{eq:descentLemmaCf}
f(\x_\stepsize) \leq f(\x^{(t)}) + \stepsize \left\langle  \nabla
f(\x^{(t)}), \dd_t \right\rangle + \frac{\stepsize^2}{2} \Cf, \quad \text{valid $\forall
\stepsize \in [0,1]$}.
\end{equation}
As~$\stepsize_t$ is obtained by line-search and that
$[0,1] \subseteq [0,\stepsize_\textrm{max}]$, we also have that
$f(\x^{(t+1)}) = f(\x_{\stepsize_t}) \leq  f(\x_\stepsize)$ $\forall
\stepsize \in [0,1]$. Combining these two inequalities, subtracting $f(\x^*)$
on both sides, and using $\Cf \leq \CfAFW$ to simplify the possibilities
yields $h_{t+1} \le h_t + \stepsize \left\langle  \nabla f(\x^{(t)}), \dd_t
\right\rangle + \frac{\stepsize^2}{2} \CfAFW$.

Using the crucial gap inequality~\eqref{eq:gapDirection}, we get $h_{t+1} \le
h_t - \stepsize \frac{g_t}{2} + \frac{\stepsize^2}{2} \CfAFW$, and so:
\begin{equation} \label{eq:hProgress}
h_t - h_{t+1} \ge \stepsize \frac{g_t}{2} - \frac{\stepsize^2}{2} \CfAFW
\quad \forall \stepsize \in [0,1].  
\end{equation}
We can minimize the bound~\eqref{eq:hProgress} on the right hand side by
letting $\stepsize = \stepbound_t := \frac{g_t}{2\CfAFW}$. Supposing that
$\stepbound_t \leq 1$, we then get $h_t - h_{t+1} \geq 
\frac{g_t^2}{8\CfAFW}$ (we cover the case $\stepbound_t > 1$ later).  By
combining this inequality with the one from geometric strong
convexity~\eqref{eq:hUpperBound}, we get 
\begin{equation} \label{eq:mainGeometric}
h_t - h_{t+1} \ge \frac{\strongConvAFW}{4\CfAFW} \, h_t
\end{equation}
implying that we have a geometric rate of decrease $h_{t+1} \leq
\Big(1-\frac{\strongConvAFW}{4\CfAFW}\Big) h_t$ (this is a `good step').

\emph{Boundary cases.} We now consider the case $\stepbound_t > 1$ (with
$\stepsize_\textrm{max} \geq 1$ still). The condition $\stepbound_t > 1$ then
translates to $g_t \geq 2 \CfAFW$, which we can use in~\eqref{eq:hProgress}
with $\stepsize = 1$ to get $h_t - h_{t+1} \geq \frac{g_t}{2} - \frac{g_t}{4}
= \frac{g_t}{4}$. Combining this inequality with $h_t \leq g_t$ gives the
geometric decrease $h_{t+1} \leq \left(1-\frac{1}{4}\right) h_t$ (also a
`good step'). $\rho_f^\away$ is obtained by considering the worst-case of the
constants obtained from $\stepbound_t > 1$ and $\stepbound_t \leq 1$. (Note
that $\strongConvAFW \leq \CfAFW$ by Remark~\ref{rem:mfAFWsmallerThanCf},
and thus $\frac{1}{4} \geq \frac{\strongConvAFW}{4\CfAFW}$).

Finally, we are left with the case that $\stepsize_\textrm{max} < 1$. This is
thus an away step and so $\dd_t = \dd_t^\away = \x^{(t)} - \vv_t$. Here, we
use the away version $\CfAFW$: by letting $\s := \x^{(t)}$, $\vv = \vv_t$ and $\y := \x_\stepsize$ in~\eqref{eq:CfAFW}, we also
get the bound $f(\x_\stepsize) \leq f(\x^{(t)}) + \stepsize \left\langle 
\nabla f(\x^{(t)}), \dd_t \right\rangle + \frac{\stepsize^2}{2} \CfAFW$,
valid $\forall \stepsize \in [0,1]$ (but note here that the points
$\x_\stepsize$ are not feasible for $\stepsize > \stepsize_\textrm{max}$ --
the bound considers some points outside of $\domain$). We now have two
options: either $\stepsize_t = \stepsize_\textrm{max}$ (a drop step) or
$\stepsize_t < \stepsize_\textrm{max}$. In the case $\stepsize_t <
\stepsize_\textrm{max}$ (the line-search yields a solution in the interior of
$[0,\stepsize_\textrm{max}]$), then because $f(\x_\stepsize)$ is convex in
$\stepsize$, we know that $\min_{\stepsize \in [0,\stepsize_\textrm{max}]}
f(\x_\stepsize) = \min_{\stepsize \geq 0} f(\x_\stepsize)$ and thus
$\min_{\stepsize \in [0,\stepsize_\textrm{max}]} f(\x_\stepsize) =
f(\x^{(t+1)}) \leq f(\x_\stepsize)$ $\forall \stepsize \in [0,1]$. We can
then re-use the same argument above equation~\eqref{eq:hProgress} to get the
inequality~\eqref{eq:hProgress}, and again considering both the case
$\stepbound_t \leq 1$ (which yields inequality~\eqref{eq:mainGeometric}) and
the case $\stepbound_t > 1$ (which yields $(1-\frac{1}{4})$ as the geometric
rate constant), we get a `good step' with $1-\rho_f$ as the worst-case
geometric rate constant.

Finally, we can easily bound the number of drop steps possible up to
iteration $t$ with the following argument (the drop steps are the `bad steps'
for which we cannot show good progress). Let $A_t$ be the number of steps
that added a vertex in the expansion (only standard FW steps can do this) and
let $D_t$ be the number of drop steps. We have that $|\Coreset^{(t)}| =
|\Coreset^{(0)}| + A_t - D_t$. Moreover, we have that $A_t+D_t \leq t$. We
thus have $1 \leq |\Coreset^{(t)}| \leq |\Coreset^{(0)}| + t - 2D_t$,
implying that $D_t \leq \frac{1}{2}( |\Coreset^{(0)}|-1+t)=\frac{t}{2}$, as stated in the theorem.
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\paragraph{Proof for FCFW.}
In the case of FCFW, we do not need to consider away steps: by the quality of the 
approximate correction in Algorithm~\ref{alg:correction} (as specified in Line~4), we know that at the
beginning of a new iteration, the away gap $g_t^\away \leq \epsilon$.
Supposing that the algorithm does not exit at line~6 of
Algorithm~\ref{alg:FCFW}, then $g_t^\FW > \epsilon$ and thus we have that $2
\innerProdCompressed{\r_t}{\dd_t^\FW} \geq
\innerProdCompressed{\r_t}{\dd_t^\PFW}$ using a similar argument as
in~\eqref{eq:AFWgapInequality} (i.e. if one would be to run the AFW algorithm at this point, it
would take a FW step). Finally, by property of the line~3 of the approximate correction
Algorithm~\ref{alg:correction}, the correction is
guaranteed to make at least as much progress as a line-search in direction
$\dd_t^\FW$, and so the lower bound~\eqref{eq:hProgress} can be
used for FCFW as well (but using $\Cf$ as the constant instead of $\CfAFW$ given that it was a FW step).

\paragraph{Proof for MNP.}
After a correction step in the MNP algorithm, we have that the current iterate is the minimizer over the active set, and thus $g_t^\away = 0$. We thus have $\innerProdCompressed{\r_t}{\dd_t^\FW} = \innerProdCompressed{\r_t}{\dd_t^\PFW} = g_t$, which means that a standard FW step would yield a geometric decrease of error.\footnote{Moreover, as we do not have the factor of $2$ relating $\innerProdCompressed{\r_t}{\dd_t^\FW}$ and $g_t$ unlike in the AFW and approximate FCFW case,
we can remove the factor of~$\frac{1}{2}$ in front of~$g_t$ in~\eqref{eq:hProgress}, removing
the factor of~$\frac{1}{4}$ appearing in~\eqref{eq:mainGeometric},
and also giving a geometric decrease with factor $(1-\frac{1}{2})$ when $\stepbound_t > 1$.}
It thus remains to show that the MNP-correction is making as much progress as a FW line-search. Consider $\y_1$ as defined in Algorithm~\ref{alg:MNP}. If it belongs to $\conv(\VVertices^{(0)})$, then it has made more progress than a FW line-search as $\s_t$ and $\x^{(t)}$ belongs to $\conv(\VVertices^{(0)})$. 

The next possibility is the crucial step in the proof: suppose that exactly one atom was removed from the correction polytope and that $\y_1$ does not belong to $\conv(\VVertices^{(0)})$ (as this was covered in the above case). This means that $\y_2$ belongs to \emph{the relative interior} of $\conv(\VVertices^{(1)})$. Because $\y_2$ is by definition the affine minimizer of $f$ on $\conv(\VVertices^{(1)})$, the negative gradient $-\nabla f(\y_2)$ is pointing away to the polytope $\conv(\VVertices^{(1)})$ (by the optimality condition). But $\conv(\VVertices^{(1)})$ is a \emph{facet} of $\conv(\VVertices^{(0)})$, this means that $-\nabla f(\y_2)$ determines a facet of $\conv(\VVertices^{(0)})$ (i.e. $\innerProdCompressed{-\nabla f(\y_2)}{\y - \y_2} \leq 0$ for all $\y \in \conv(\VVertices^{(0)})$). This means that $\y_2$ is also the minimizer of $f$ on $\conv(\VVertices^{(0)})$ and thus has made more progress than a FW line-search.

In the case that two atoms are removed from $\conv(\VVertices^{(0)})$, we cannot make this argument anymore
(it is possible that $\y_3$ makes less progress than a FW line-search); but in this case, the size of the active set is reduced by one (we have a drop step), and thus we can use the same argument as in the AFW algorithm to bound the number of such steps.

\paragraph{Proof for PFW.}
In this case, $\innerProdCompressed{\r_t}{\dd_t} = \innerProdCompressed{\r_t}{\dd_t^\PFW}$, so we do not even need a factor of 2 to relate the gaps (with the same consequence as in MNP in getting slightly bigger constants). We can re-use the same argument as in the AFW algorithm to get a geometric progress when $\stepsize_t < \stepmax$. When $\stepsize_t = \stepmax$ we can either have a drop step if $\s_t$ was already in $\Coreset^{(t)}$, or a swap step if $\s_t$ was also added to $\Coreset^{(t)}$ and so $|\Coreset^{(t+1)}| = |\Coreset^{(t)}|$. The number of drop steps can be bounded similarly as in the AFW algorithm. On the other hand, in the worst case, there could be a very large number of swap steps. We provide here a very loose bound, though it would be interesting to use other properties of the objective to prove that this worst case scenario cannot happen.

We thus bound the maximum number of swap steps between two `good steps' (very loosely). Let $m = |\Vertices|$ be the number of possible atoms, and let $r$ be the size of the current active set $|\Coreset^{(t)}| = r \leq m$. When doing a drop step $\stepsize_t = \alpha_{\vv_t}$, there are two possibilities: either we move all the mass from $\vv_t$ to a new atom $\s_t \notin \Coreset^{(t)}$ i.e. $\alpha^{(t+1)}_{\vv_t} = 0$ and $\alpha^{(t+1)}_{\s_t} = \alpha^{(t)}_{\vv_t}$ (a swap step); or we move all the mass from $\vv_t$ to an old atom $\s_t \in \Coreset^{(t)}$ i.e. $\alpha^{(t+1)}_{\s_t} = \alpha^{(t)}_{\s_t}+ \alpha^{(t)}_{\vv_t}$ (a `full drop step'). When doing a swap step, the set of 
\emph{possible values} for the coordinates $\alpha_{\vv}$ \emph{do not change}, they are only `swapped around' amongst the $m$ possible slots. The maximum number of possible consecutive swap steps without revisiting an iterate already seen is thus bounded by the number of ways we can assign $r$ numbers in $m$ slots (supposing the $r$ coordinates were all distinct in the worst case), which is $m! / (m-r)!$. Note that because the algorithm does a line-search in a strict descent direction at each iteration, we always have $f(\x^{(t+1)}) < f(\x^{(t)})$ unless $\x^{(t)}$ is already optimal. This means that the algorithm cannot revisit the same point unless it has converged. When doing a `full drop step', the set of coordinates changes, but the size of the active set is reduced by one (thus $r$ reduced by one). In the worst case, we will do a maximum number of swap steps, followed by a full drop step, repeated so on all the way until we reach an active set of only one element (in which case there is a maximum number of $m$ swap steps). Starting with an active set of $r$ coordinates, the maximum number of swap steps $B$ without doing any `good step' (which would also change the set of coordinates), is thus upper bounded by:
\begin{equation*}
B \leq \sum_{l = 1}^{r} \frac{m!}{(m-l)!} \leq m! \sum_{l=0}^{\infty} \frac{1}{l!} = m! \, e \leq 3 m! \,\, , 
\end{equation*}
as claimed.

\paragraph{Proof of Sublinear Convergence for General Convex Objectives (i.e. when $\strongConvAFW=0$).} 
For the good steps 
of the MNP algorithm and the pairwise FW algorithm, we have the reduction of suboptimality
given by~\eqref{eq:hProgress} without the factor of $\frac{1}{2}$ in front of $g_t \geq h_t$.
This is the standard recurrence that appears in the convergence proof of Frank-Wolfe
(see for example Equation~(4) in~\citep[proof of Theorem~1]{Jaggi:2013wg}), yielding
the usual convergence:
\begin{equation} \label{eq:sublinearMNP}
h_t \leq \frac{2 C}{k(t) + 2}  \quad \text{ for $k(t) \geq 1$},
\end{equation} 
where $k(t)$ is the number of good steps up to iteration $t$, and $C = \Cf$ for MNP and $C = \CfAFW$ for PFW.
The number of good steps for MNP is $k(t) \geq t/2$, while for PFW, we have the (useless) lower bound
$k(t) \geq t/(3|\Vertices|!+1)$. For FCFW with exact correction, the rate~\eqref{eq:sublinearMNP} was 
already proven in~\citep{Jaggi:2013wg} with $k(t) = t$. On the other hand, for FCFW with approximate correction, and for AFW, the factor of $\frac{1}{2}$ in front of the gap~$g_t$ in the suboptimality bound~\eqref{eq:hProgress}
somewhat complicates the convergence proof. The recurrence we get for the
suboptimality is the same as in Equation~(20) of~\citep[proof of Theorem~C.1]{LacosteJulien:2013ue},
with $\nu = \frac{1}{2}$ and $n=1$, giving the following suboptimality bound:
\begin{equation} \label{eq:sublinearAFW}
h_t \leq \frac{4 C}{k(t) + 4}  \quad \text{ for $k(t) \geq 0$},
\end{equation} 
where $C = 2\CfAFW + h_0$ for AFW and $C = 2\Cf + h_0$ for FCFW with approximate correction.
Moreover, the number of good steps is $k(t) \geq t/2$ for AFW, and $k(t) = t$ for FCFW.
A weaker (logarithmic) dependence on the initial error~$h_0$ can also be obtained by following a tighter
analysis (see~\citep[Theorem~C.4]{LacosteJulien:2013ue} or~\citep[Lemma~D.5 and Theorem~D.6]{Krishnan:2015ws}),
though we only state the simpler result here.
\end{proof}


%
\section{Empirical Tightness of Linear Rate Constant} \label{app:triangle}

We describe here a simple experiment to test how tight the 
constant in the linear convergence rate of Theorem~\ref{thm:megaConvergenceTheorem2}
is. We test both AFW and PFW on the triangle domain
with corners at the locations
$(-1,0)$, $(0,0)$ and $(\cos(\theta),\sin(\theta))$, for
increasingly small $\theta$ (see Figure~\ref{fig:triangle}).

\begin{figure}[t]
\begin{center}
%
\includegraphics[width=0.7\linewidth]{figures/triangle}
\end{center}
\caption{Simple triangle domain to test the empirical
tightness of the constant in the convergence rate for AFW and PFW.
The width $\delta$ varies with $\theta$. We optimize $f(\x) := \frac{1}{2} \|\x-\x^*\|^2$
over this domain.} \label{fig:triangle}
\end{figure}

The pyramidal width $\delta=\sin(\frac{\theta}{2})$
becomes vanishingly small as $\theta \to 0$; the
diameter is $M=2\cos(\frac{\theta}{2})$.
We consider the optimization of the 
function $f(\x) := \frac{1}{2} \|\x-\x^*\|^2$
with $\x^*=(-0.5,1)$ on one edge of the domain.
Note that the condition number of $f$ is $\frac{L}{\mu}=1$.
The bound on the linear convergence rate $\rho$ according
to Theorem~\ref{thm:megaConvergenceTheorem2} 
(using $\CfAFW \leq L M^2$~\eqref{eq:CfBound} and $\strongConvAFW \geq \mu\, \delta^2$~\eqref{eq:muDirWidthBound})
is $\rho^\PFW = \frac{\strongConvAFW}{\CfAFW} \geq \frac{\mu}{L}\left(\frac{\delta}{M}\right)^2$
for PFW and $\rho^\away = \frac{1}{4} \rho^\PFW $ for AFW.
The theoretical constant here is thus $\rho^\PFW = \frac{1}{4} \tan^2 (\frac{\theta}{2})$.
We consider $\theta$ varying from $\pi/4$ to $1e\!-\!3$, and thus theoretical rates
varying on a wide range from $0.04$ to $1e\!-\!7$. We compare the theoretical rate $\rho$
with the empirically observed one by estimating $\hat{\rho}$ in the relationship
$h_t \approx h_0 \exp(-\rho t)$ (using linear regression on the semilogarithmic scale).
For each $\theta$, we run both AFW and PFW for 2000 iterations 
starting from 20 different random starting points\footnote{$\x_0$ is obtained by taking
a random convex combination of the corners of the domain.} 
in the interior of the triangle domain. We disregard the starting points
that yield a drop step (as then the algorithm converges in one iteration;
these happen for about $10\%$ of the starting points). Note
that as there is no drop step in our setup, we do not
need to divide by two the effective rate as is done in 
Theorem~\ref{thm:megaConvergenceTheorem2} (the number of `good steps' is $k(t) = t$).

\begin{figure}[t]
\begin{center}
%
\begin{minipage}{0.46\linewidth}
\centering
\includegraphics[width=\linewidth]{figures/empirical_vs_theory}
(a)  Ratio of empirical rate vs. theoretical one
\end{minipage}	
\hfill
\begin{minipage}{0.48\linewidth}
\centering
\includegraphics[width=0.9\linewidth]{figures/ratio_PFW_vs_AFW}
\\[3mm]
(b) Ratio of empirical rate of PFW vs. AFW
\end{minipage}
\end{center}
\caption{Empirical rate results for the triangle domain of Figure~\ref{fig:triangle}.
We plot median values over 20 random starting points; the
error bars represent the $25\%$ and $75\%$ quantiles.
The empirical rate for PFW is closely following the theoretical one.} \label{fig:triangleResults}
\end{figure}


Figure~\ref{fig:triangleResults} presents the results.
In Figure~\ref{fig:triangleResults}(a), we
plot the ratio of the estimated rate over
the theoretical rate $\frac{\hat{\rho}}{\rho}$ for both PFW 
and AFW as $\theta$ varies. Note that the ratio
is very stable for PFW (around 10), despite the rate
changing through six orders of magnitude, demonstrating
the empirical tightness of the constant for this domain. 
The ratio for AFW has more fluctuations, but also stays
within a stable range. We can also 
do a finer analysis than the pyramidal width and
consider the finite number possibilities for the worst case angles
 for $(\innerProdCompressed{\hat{\r}}{{\dd^\PFW(\r)}})^2$.
This gives the tighter constant $\rho^\PFW = \sin^2(\frac{\theta}{2})$
for our triangle domain, gaining a factor of about~4, but still
not matching yet the empirical observation for PFW.

In Figure~\ref{fig:triangleResults}(b), we compare the empirical
rate for PFW vs. the one for AFW. For bigger theoretical rates, PFW appears
to converge faster. However, AFW gets a slightly
better empirical rate for very small rates (small angles).


%
\section{Non-Strongly Convex Generalization} \label{app:NonStronglyConvex}

Here we will study the generalized setting with objective $f(\x) := g(\A \x) + \innerProdCompressed{\bv}{\x}$ where $g$ is $\mu_g$-\emph{strongly convex} w.r.t.\ the Euclidean norm over the domain $\A \domain$ with strong convexity constant $\mu_g > 0$. 

We first define a few constants: let $G := \max_{\x \in \domain} \| \nabla g
(\A \x) \|$ be the maximal norm of the gradient of $g$ over $\A \domain$; $M$
be the diameter of $\domain$ and $M_{\A}$ be the diameter of $\A \domain$.

Let $\theta$ be the Hoffman constant (see~\citep[Lemma 2.2]{Beck:2015vo})
associated with the matrix $[\A ; \bv^\top; \B] = \left(\substack{\A \\ \bv^\top \\ \B}\right)$, where the rows of $\B$ are the linear inequality constraints defining the set $\domain$.

We present here a generalization of Lemma 2.5 from~\citep{Beck:2015vo}:
\begin{lemma}\label{lem:generalizedStrongConvexity}
For any $\x \in \domain$ and $\x^*$ in the solution set $\X^*$:
\begin{equation}
f(\x^*)- f(\x)- 2 \langle\nabla f(\x), \x^*-\x \rangle \geq 2 \tilde{\mu} \, d(\x, \X^*)^2 ,
\end{equation}
where $\tilde{\mu} := 1/\left(2 \theta^2 \left( \|\bv \| M + 3 G M_{\A} + \frac{2}{\mu_g} (G^2+1) \right) \right)$ is the generalized strong convexity constant for $f$.
\end{lemma}
\begin{proof}
Let $\x^*$ be any element of the solution set $\X^*$. By the strong convexity of $g$, we have
\begin{equation} \label{eq:strongConvForG}
f(\x^*)- f(\x)- \langle\nabla f(\x), \x^*-\x \rangle
\geq \textstyle\frac{\mu_g}{2} \norm{\A \x^* - \A \x}^2 \ .
\end{equation}

Moreover, by the convexity of $f$, we have:
\begin{equation} \label{eq:convexityForG}
- \langle\nabla f(\x), \x^*-\x \rangle
\geq f(\x) - f(\x^*) .
\end{equation}

We now use inequality (2.10) in~\citep{Beck:2015vo} to get the bound:
\begin{equation} \label{eq:lowerBoundWithB}
f(\x) - f(\x^*) \geq  \frac{1}{B_1} (\innerProdCompressed{\bv}{\x} - \innerProdCompressed{\bv}{\x^*} )^2 ,
\end{equation}
where $B_1 := ( \|\bv \| M + 3 G M_{\A} + \frac{2}{\mu_g} G^2)$.

Plugging~\eqref{eq:lowerBoundWithB} into~\eqref{eq:convexityForG} and adding to~\eqref{eq:strongConvForG}, we get:
\begin{align*} 
f(\x^*)- f(\x)- 2 \langle\nabla f(\x), \x^*-\x \rangle
&\geq \frac{1}{B_2} \left( \norm{\A \x^* - \A \x}^2  + (\innerProdCompressed{\bv}{\x} - \innerProdCompressed{\bv}{\x^*} )^2 \right) \\
&\geq \frac{1}{B_2 \theta^2} d(\x, \X^*)^2 ,
\end{align*}
where $B_2 := ( \|\bv \| M + 3 G M_{\A} + \frac{2}{\mu_g} (G^2+1))$. For the last inequality, we used 
inequality~(2.1) in~\citep{Beck:2015vo} that made use of
the Hoffman's Lemma (see~\citep[Lemma 2.2]{Beck:2015vo}),
where $\theta$ is the Hoffman constant associated with the matrix $[\A ; \bv^\top ; \B]$. In this case, $\B$ is the matrix with rows containing the linear inequality constraints defining $\domain$.
\end{proof}

We now define the following generalization of the geometric strong convexity constant \eqref{eq:muf}, that we now call $\strongConvGeneralized$:
\begin{equation}\label{eq:mufGeneralized}
  \strongConvGeneralized := \inf_{\x\in \domain} \sup_{\substack{\x^* \in \X^*\\
                        \textrm{s.t. } \left\langle \nabla f(\x), \x^*-\x \right\rangle < 0 }}
           \frac{1}{{2 \stepsize^\away(\x,\x^*)}^2}
           \big( f(\x^*)-f(\x)-2 \left\langle \nabla f(\x),  \x^*-\x \right\rangle \big) \ .
\end{equation}
Notice the new inner \emph{supremum} over the solution set $\X^*$ compared to the original definition \eqref{eq:muf}, the factor of $2$ in front of the gradient, and the different 
overall scaling to have a similar form as in the previous linear convergence theorem. 
This new quantity $\strongConvGeneralized$ is still \emph{affine invariant}, but
unfortunately now depends on the location of the solution set $\X^*$.
We now present the generalization of Theorem~\ref{thm:muFdirWinterpretation2}.

\begin{theorem}\label{thm:generalizedPWidth}
Let $f(\x) := g(\A \x) + \innerProdCompressed{\bv}{\x}$ where $g$ is $\mu_g$-\emph{strongly convex} w.r.t.\ the Euclidean norm over the domain $\A \domain$ with strong convexity constant $\mu_g > 0$. Let $\tilde{\mu}$ be the corresponding generalized strong convexity constant coming from Lemma~\ref{lem:generalizedStrongConvexity}.
Then
\[
\strongConvGeneralized \geq \tilde{\mu} \cdot \left( \PWidth(\domain) \right)^2 \ .
\] 
\end{theorem}
\begin{proof}
Let $\x$ be fixed and not optimal; let $\x^*$ be its closest point in $\X^*$ i.e. $\|\x - \x^*\| = d(\x, \X^*)$. We have that $\left\langle \nabla f(\x), \x^*-\x \right\rangle < 0$ as $\x$ is not optimal.

We use the generalized strong convexity notion $\tilde{\mu}$ from Lemma~\ref{lem:generalizedStrongConvexity} for the particular reference point~$\x^*$ in the third line below to get:
\begin{multline*}
\sup_{\substack{\x' \in \X^*\\
                        \textrm{s.t. } \left\langle \nabla f(\x), \x'-\x \right\rangle < 0 }}
           \frac{1}{{2\stepsize^\away(\x,\x')}^2}
           \big( f(\x')-f(\x)-2 \left\langle \nabla f(\x),  \x'-\x \right\rangle \big)  \\
           \geq  \frac{1}{{2\stepsize^\away(\x,\x^*)}^2}
                      \big( f(\x^*)-f(\x)-2 \left\langle \nabla f(\x),  \x^*-\x \right\rangle \big) \\ 
          \geq  \frac{1}{{2\stepsize^\away(\x,\x^*)}^2} 2 \tilde{\mu}\, d(\x, \X^*)^2
          = \frac{1}{{ \stepsize^\away(\x,\x^*)}^2} \tilde{\mu} \|\x-\x^*\|^2 .                  
\end{multline*}
We can do this for each non-optimal $\x$. We thus obtain:
\begin{equation} \label{eq:muTildeInequality}
\strongConvGeneralized \geq  \inf_{\substack{\x, \x^* \in \domain\\
                                   \textrm{s.t. } \innerProd{\r_{\x}}{\x^*-\x} > 0}}
                      \tilde{\mu} \left(  \frac{\innerProd{\r_{\x}}{ \s_f(\x) - \vv_f(\x)}}{\innerProd{\r_{\x}}{\x^*-\x}} \norm{{\x^*-\x}} \right)^2 .
\end{equation}
And we are back to the same situation as in the proof of our earlier Theorem~\ref{thm:muFdirWinterpretation2}, the only change being that we now have equation~\eqref{eq:mufInitial} holding for the general strong convexity constant $\tilde{\mu}$ instead of its classical analogue~$\mu$.
\end{proof}

Having this tool at hand, the linear convergence of all Frank-Wolfe algorithm variants now holds with the earlier $\strongConvAFW$ complexity constant replaced with~$\strongConvGeneralized$. The factor of 2 in the denominator of~\eqref{eq:mufGeneralized} is to ensure the same scaling.%

Again, as we have shown in Theorem \ref{thm:generalizedPWidth}, we have that our condition $\strongConvGeneralized > 0$ leading to linear convergence is slightly weaker than generalized strong convexity in the Hoffman sense (it is implied by it).


\begin{theorem}\label{thm:megaConvergenceTheoremHoffman}
Suppose that $f$ has smoothness constant $\CfAFW$ ($\Cf$ for FCFW and
MNP),
as well as generalized geometric strong convexity constant~$\strongConvGeneralized$ as defined
in~\eqref{eq:mufGeneralized}.
%

Then the suboptimality error~$h_t$ of the iterates of all the four variants of the FW algorithm (AFW, FCFW, MNP and PFW) decreases geometrically at each step that is not a drop step nor a swap step (i.e.
when $\stepsize_t < \stepmax$), with the same constants as in Theorem~\ref{thm:megaConvergenceTheorem2}, except that $\strongConvAFW$ is replaced by~$\strongConvGeneralized$.
\end{theorem}

\begin{proof}
The proof closely follows the proof of Theorem \ref{thm:megaConvergenceTheorem2}.

We start from the above generalization~\eqref{eq:mufGeneralized} of the original geometric strong convexity constant \eqref{eq:muf}, and first replace the $\inf$ over $\x$ by considering only the choice $\x := \x^{(t)}$, giving
\begin{equation}\label{eq:mufGeneralizedAgain}
  \strongConvGeneralized \le  \sup_{\substack{\x^* \in \X^*\\
                        \textrm{s.t. } \left\langle \nabla f(\x^{(t)}), \x^*-\x^{(t)} \right\rangle < 0 }}
           \frac{1}{{2 \stepsize^\away(\x^{(t)},\x^*)}^2}
           %
           \big( f(\x^*)-f(\x^{(t)})-2 \left\langle \nabla f(\x^{(t)}),  \x^*-\x^{(t)} \right\rangle \big) \ .
\end{equation}

From here, we will now mirror our earlier derivation for an upper bound on the suboptimality 
as a function of the gap $g_t$, as given in~\eqref{eq:ht_bound_trick}.
As an optimal reference point $\x^*$ in \eqref{eq:ht_bound_trick}, we will choose a $\tilde \x^*$ attaining the supremum in \eqref{eq:mufGeneralizedAgain}, given $\x^{(t)}$.

We again employ the `step-size quantity' $\overline{\stepsize} :=
\stepsize^\away(\x^{(t)}, \tilde \x^*)$ as defined in \eqref{eq:gammaAway}. Using \eqref{eq:mufGeneralizedAgain}, we have
\begin{align}\label{eq:ht_bound_trickAgain}
   2 \overline{\stepsize}^2 %
   \,\strongConvGeneralized
    &\le  f(\x^*)-f(\x^{(t)})+2 \left\langle -\nabla f(\x^{(t)}),  \tilde \x^*-\x^{(t)} \right\rangle  \\
&= -h_t + 2\overline{\stepsize} \left\langle  -\nabla f(\x^{(t)}), \s_f(\x^{(t)}) - \vv_f(\x^{(t)})\right\rangle \nonumber\\
&\le -h_t + 2\overline{\stepsize} \left\langle  -\nabla f(\x^{(t)}), \s_t - \vv_t \right\rangle  \nonumber\\
&=  -h_t + 2\overline{\stepsize}  g_t \ , \nonumber
\end{align}

Therefore $h_t \le -\frac{{\hat{\stepsize}}^2}{2} \strongConvGeneralized + \hat{\stepsize} g_t$ when writing $\hat{\stepsize}:=2\overline{\stepsize}$,
which is always upper bounded\footnote{%
Here we have again used the trivial inequality $0 \le %
a^2-2ab+b^2$ for the choice of numbers $a:=\frac{g_t}{\strongConvGeneralized}$ and $b:=\hat{\stepsize}$.
} %
by\vspace{-2mm}
\begin{equation}
h_t \leq  \frac{{g_t}^2}{2\strongConvGeneralized}.
\end{equation}
which is exactly the bound~\eqref{eq:hUpperBound} as in the classical case, with the denominator being~$2\strongConvGeneralized$ instead of~$2\strongConvAFW$.

From here, the proof of the main convergence Theorem \ref{thm:megaConvergenceTheorem2} continues without modification, using~$\strongConvGeneralized$ instead of~$\strongConvAFW$.
\end{proof}


\bigskip
\bigskip
%
\bibliographystylesup{abbrvnat}
\bibliographysup{references}

\end{document}\documentclass{article}

%
\PassOptionsToPackage{numbers, compress}{natbib}
%
%
%
%

%

%
%
\usepackage[final]{nips_2017} %

\usepackage[utf8]{inputenc} %
\usepackage[T1]{fontenc}    %
\usepackage{hyperref}       %
\usepackage{url}            %
\usepackage{booktabs}       %
\usepackage{amsfonts}       %
\usepackage{nicefrac}       %
\usepackage{microtype}      %

%
\usepackage{times}
%
\usepackage{graphicx} %
%
%


%
\usepackage{algorithm}
\usepackage{algorithmic}


%
\usepackage{amsmath, amssymb, amsthm, bm, color}
\usepackage{enumitem}
\usepackage{notation}
\usepackage{times}
\usepackage{subcaption}
\usepackage{booktabs}       %
\usepackage{array}
\usepackage{multirow}
\usepackage[table,xcdraw]{xcolor}
\usepackage{mathtools}

\usepackage{calc} %


\definecolor{mydarkblue}{rgb}{0,0.08,0.45}

\hypersetup{ %
    pdftitle={On Structured Prediction Theory with Calibrated Convex Surrogate Losses},
    pdfauthor={Anton Osokin, Francis Bach, Simon Lacoste-Julien},
    pdfsubject={On Structured Prediction Theory with Calibrated Convex Surrogate Losses},
    pdfkeywords={structured prediction, consistency, surrogate losses, machine learning},
    pdfborder=0 0 0,
    pdfpagemode=UseNone,
    colorlinks=true,
    linkcolor=mydarkblue,
    citecolor=mydarkblue,
    filecolor=mydarkblue,
    urlcolor=mydarkblue,
    pdfview=FitH
}

%
%
%

\begin{document} 
\title{On Structured Prediction Theory with Calibrated Convex Surrogate Losses}

\author{
    Anton Osokin \\
    INRIA/ENS\thanks{DI \'{E}cole normale sup\'{e}rieure, CNRS, PSL Research University}, Paris, France\\
    HSE\thanks{National Research University Higher School of Economics}, Moscow, Russia
    \And
    Francis Bach\\
    INRIA/ENS\footnotemark[1], Paris, France
%
%
%
    \And
    Simon Lacoste-Julien\\
    MILA and DIRO \\
     Universit\'{e} de Montr\'{e}al, Canada
}

\maketitle

\begin{abstract}
We provide novel theoretical insights on structured prediction in the context of \emph{efficient} convex surrogate loss minimization with consistency guarantees.
For any task loss, we construct a convex surrogate that can be optimized via stochastic gradient descent and we prove tight bounds on the so-called ``calibration function'' relating the excess surrogate risk to the actual risk.
In contrast to prior related work, we carefully monitor the effect of the exponential number of classes in the learning guarantees as well as on the optimization complexity.
As an interesting consequence, we formalize the intuition that some task losses make learning harder than others, and that the classical 0-1 loss is ill-suited for structured prediction.
%
%
%
%
\end{abstract}

\setcounter{footnote}{0}
\section{Introduction}
\label{sec:intro}
Structured prediction is a subfield of machine learning aiming at making multiple interrelated predictions simultaneously. 
The desired outputs (labels) are typically organized in some structured object such as a sequence, a graph, an image, etc.
Tasks of this type appear in many practical domains such as computer vision~\citep{nowozin2011structured}, natural language processing~\citep{smith2011linguistic} and bioinformatics~\citep{durbin1998bio}.

The structured prediction setup has at least two typical properties differentiating it from the classical binary classification problems extensively studied in learning theory:
\begin{enumerate}[topsep=-\parskip,noitemsep,itemindent=0.45cm,leftmargin=0.0cm]
    \item Exponential number of classes: this brings both additional computational and statistical challenges.
    By \emph{exponential}, we mean exponentially large in the size of the natural dimension of output, e.g., the number of all possible sequences is exponential w.r.t. the sequence length.
    \item Cost-sensitive learning: in typical applications, prediction mistakes are \emph{not} all equally costly. The prediction error is usually measured with a highly-structured task-specific loss function, e.g., Hamming distance between sequences of multi-label variables or mean average precision for ranking.
    %
\end{enumerate}

\vspace{\parskip}Despite many algorithmic advances to tackle structured prediction problems~\citep{bakir2007predicting,nowozin2014structpred}, there have been relatively few papers devoted to its theoretical understanding. Notable recent exceptions that made significant progress include~\citet{cortes16} and~\citet{london16pac} (see references therein) which proposed data-dependent generalization error 
%
bounds in terms of popular empirical convex surrogate losses such as the structured hinge loss~\citep{taskar03,taskar2005learning,tsochantaridis05}.
A question not addressed by these works is whether their algorithms are \emph{consistent}: does minimizing their convex bounds with infinite data lead to the minimization of the task loss as well?
Alternatively, the structured probit and ramp losses are consistent~\cite{mcallester07,mcallester11}, but non-convex and thus it is hard to obtain computational guarantees for them.
In this paper, we aim at getting the property of consistency for surrogate losses that can be \emph{efficiently} minimized with guarantees, and thus we consider \emph{convex} surrogate losses.

The consistency of convex surrogates is well understood in the case of binary classification~\citep{zhang2004annals,bartlett06convexity,steinwart07} and there is significant progress in the case of multi-class 0-1 loss~\citep{zhang04,tewari07} and general multi-class loss functions~\citep{pires2013riskbounds,ramaswamy16calibrDim,williamson16}.
A large body of work specifically focuses on the related tasks of ranking~\cite{duchi10,calauzenes12,ramaswamy13rankSurrogates} and ordinal regression~\citep{pedregosa15ordivalreg}.\vspace{-0.5mm}

\textbf{Contributions.} In this paper, we study consistent convex surrogate losses specifically in the context of an exponential number of classes. 
%
We argue that even while being consistent, a convex surrogate might not allow efficient learning.
As a concrete example, \citet{ciliberto16} recently proposed a consistent approach to structured prediction, but the constant in their generalization error bound can be exponentially large as we explain in Section~\ref{sec:relatedworks}.
There are two possible sources of difficulties from the optimization perspective: to reach adequate accuracy on the \emph{task} loss, one might need to optimize a surrogate loss to exponentially small accuracy; or to reach adequate accuracy on the \emph{surrogate} loss, one might need an exponential number of algorithm steps because of exponentially large constants in the convergence rate.
We propose a theoretical framework that jointly tackles these two aspects and allows to judge the feasibility of efficient learning.
In particular, we construct a \emph{calibration function}~\cite{steinwart07}, i.e., a function setting the relationship between accuracy on the surrogate and task losses, and normalize it by the means of convergence rate of an optimization algorithm.\vspace{-0.5mm}

Aiming for the simplest possible application of our framework, we propose a family of convex surrogates that are consistent for any given task loss and can be optimized using stochastic gradient descent.
For a special case of our family (quadratic surrogate), we provide a complete analysis including general lower and upper bounds on the calibration function for any task loss, with exact values for the 0-1, block 0-1 and Hamming losses.
We observe that to have a tractable learning algorithm, one needs both a structured loss (not the 0-1 loss) and appropriate constraints on the predictor, e.g., in the form of linear constraints for the score vector functions.
Our framework also indicates that in some cases it might be beneficial to use non-consistent surrogates.
In particular, a non-consistent surrogate might allow optimization only up to specific accuracy, but exponentially faster than a consistent one.\vspace{-0.5mm}
%


%
%
%
%
%
%
%
%

We introduce the structured prediction setting suitable for studying consistency in Sections~\ref{sec:notation} and~\ref{sec:consistency}. We analyze the calibration function for the quadratic surrogate loss in Section~\ref{sec:quadrSurr}. We review the related works in Section~\ref{sec:relatedworks} and conclude in Section~\ref{sec:conclusion}. 
%


%

\vspace{-1.5mm}
\section{Structured prediction setup}
\label{sec:notation}
\vspace{-1mm}
In structured prediction, the goal is to predict
%
a structured output~$\outputvarv \in \outputdomain$
(such as a sequence, a graph, an image) given an input $\inputvarv \in \inputdomain$.
The quality of prediction is measured by a task-dependent \emph{loss function}~$\lossmatrix(\hat{\outputvarv},\outputvarv \mid \inputvarv) \geq 0$ specifying the cost for predicting~$\hat{\outputvarv}$ when the correct output is~$\outputvarv$.
In this paper, we consider the case when the number of possible predictions and the number of possible labels are both finite.
For simplicity,\footnote{Our analysis is generalizable to rectangular losses, e.g., ranking losses studied by~\citet{ramaswamy13rankSurrogates}.
%
} we also assume that the sets of possible predictions and correct outputs always coincide and do not depend on~$\inputvarv$.
We refer to this set as the set of labels~$\outputdomain$, denote its cardinality by~$\outputvarcard$, and map its elements to $1,\dots,\outputvarcard$.
In this setting, assuming that the loss function depends only on~$\hat{\outputvarv}$ and~$\outputvarv$, but not on~$\inputvarv$ directly, the loss is defined by a loss matrix~$\lossmatrix \in\R^{\outputvarcard \times \outputvarcard}$.
We assume that all the elements of the matrix~$\lossmatrix$ are non-negative and will use~$\Lmax$ to denote the maximal element.
Compared to multi-class classification, $\outputvarcard$ is typically exponentially large in the size of the natural dimension of~$\outputvarv$, e.g., contains all possible sequences of symbols from a finite alphabet.

Following standard practices in structured prediction~\citep{collins2002,taskar03},
%
we define the prediction model by a \emph{score function} $\scorefunc: \inputdomain \to \R^{\outputvarcard}$ specifying a score $\scorefunc_\outputvarv(\inputvarv)$ for each possible output~$\outputvarv \in \outputdomain$.
The final prediction is done by selecting a label with the maximal value of the score\vspace{-0.5mm}
\begin{equation}
\label{eq:predictor}
\predictor(\scorefunc(\inputvarv)) := \argmax_{\hat{\outputvarv} \in \outputdomain} \scorefunc_{\hat{\outputvarv}}(\inputvarv),
\end{equation}
with some fixed strategy to resolve ties.
To simplify the analysis, we assume that among the labels with maximal scores, the predictor always picks the one with the smallest index.

The goal of prediction-based machine learning consists in finding a predictor that works well on the unseen test set, i.e., data points coming from the same distribution~$\data$ as the one generating the training data.
One way to formalize this is to minimize the generalization error, often referred to as the actual (or population) \emph{risk} based on the loss~$\lossmatrix$,
\begin{equation}
\label{eq:risk}
\risk_\lossmatrix(\scorefunc) := \E_{(\inputvarv, \outputvarv) \sim \data} \;\lossmatrix\bigl( \predictor(\scorefunc(\inputvarv)), \outputvarv \bigr).
\end{equation}
Minimizing the actual risk~\eqref{eq:risk} is usually hard.
The standard approach is to minimize a \emph{surrogate risk}, which is a different objective easier to optimize, e.g., convex.
We define a surrogate loss as a function $\surrogateloss: \R^\outputvarcard \times \outputdomain \to \R$ depending on a score vector $\scorev = \scorefunc(\inputvarv) \in \R^\outputvarcard$ and a target label~$\outputvarv \in \outputdomain$ as input arguments.
We denote the $\outputvarv$-th component of~$\scorev$ with~$\score_\outputvarv$.
The surrogate risk (the $\surrogateloss$-risk) is defined as
\begin{equation}
\label{eq:surrogateRisk}
\risk_\surrogateloss(\scorefunc) := \E_{(\inputvarv, \outputvarv) \sim \data} \;\surrogateloss( \scorefunc(\inputvarv), \outputvarv ),
\end{equation}
where the expectation is taken w.r.t.\ the data-generating distribution~$\data$.
To make the minimization of~\eqref{eq:surrogateRisk} well-defined, we always assume that the surrogate loss~$\surrogateloss$ is bounded from below and continuous.

Examples of common surrogate losses include the structured hinge-loss~\citep{taskar03,tsochantaridis05}
%
%
$
\surrogateloss_{\text{SSVM}}( \scorev, \outputvarv )
:=
\max_{\hat{\outputvarv} \in \outputdomain} \bigl( \score_{\hat{\outputvarv}} + \lossmatrix( \hat{\outputvarv}, \outputvarv ) \bigr) - \score_\outputvarv,
$
%
the log loss (maximum likelihood learning) used, e.g., in conditional random fields~\citep{lafferty01crf},
%
%
$
\surrogateloss_{\text{log}}(\scorev, \outputvarv )
:=
 \log (\sum_{\hat{\outputvarv} \in \outputdomain} \exp \score_{\hat{\outputvarv}} ) - \score_\outputvarv,
$
%
and their hybrids~\citep{pletscher10,gimpel10,hazan10,shi2015hybrid}.

%
%
%
%

In terms of task losses, we consider the unstructured \emph{0-1 loss} $\lossmatrix_{01}(\hat{\outputvarv}, \outputvarv) := [\hat{\outputvarv} \neq \outputvarv]$,\footnote{\label{footnote:Iverson}Here we use the Iverson bracket notation, i.e., $[A] := 1$ if a logical expression~$A$ is true, and zero otherwise.} and the two following structured losses: \emph{block 0-1 loss} with $\numblocks$~equal blocks of labels $\lossmatrix_{01,\numblocks}(\hat{\outputvarv}, \outputvarv) := [\text{$\hat{\outputvarv}$ and $\outputvarv$ are not in the same block}]$; and (normalized) \emph{Hamming loss} between tuples of $\hamminglen$~binary variables~$\outputvar_\hammingindex$: $\lossmatrix_{\hamming,\hamminglen}(\hat{\outputvarv}, \outputvarv)
:=
\frac{1}{\hamminglen}\sum\nolimits_{\hammingindex=1}^\hamminglen [\hat{\outputvar}_\hammingindex \neq \outputvar_\hammingindex]
$.
To illustrate some aspects of our analysis, we also look at the \emph{mixed loss}~$\lossmatrix_{01,\numblocks,\consbreakpoint}$: a convex combination of the 0-1 and block 0-1 losses, defined as $\lossmatrix_{01,\numblocks,\consbreakpoint} := \consbreakpoint \lossmatrix_{01} + (1 - \consbreakpoint) \lossmatrix_{01,\numblocks}$ for some $\eta \in [0,1]$.

\section{Consistency for structured prediction}
\label{sec:consistency}
\subsection{Calibration function}
We now formalize the connection between the actual risk~$\risk_\lossmatrix$ and the surrogate $\surrogateloss$-risk $\risk_\surrogateloss$ via the so-called \emph{calibration function}, see Definition~\ref{def:calibrationFunc} below~\citep{bartlett06convexity,zhang04,steinwart07,duchi10,pires2013riskbounds}.
As it is standard for this kind of analysis, the setup is \emph{non-parametric}, i.e. it does not take into account the dependency of scores on input variables~$\inputvarv$.
For now, we assume that a family of score functions~$\scorefuncset_\scoresubset$ consists of all vector-valued Borel measurable functions $\scorefunc: \inputdomain \to \scoresubset$ where $\scoresubset \subseteq \R^\outputvarcard$ is a subspace of allowed score vectors, which will play an important role in our analysis.
This setting is equivalent to a pointwise analysis, i.e, looking at the different input $\inputvarv$ independently.
We bring the dependency on the input back into the analysis in Section~\ref{sec:connection} where we assume a specific family of score functions.
%

Let $\data_{\inputdomain}$ represent the marginal distribution for $\data$ on $\inputvarv$ and $\P(\cdot \mid \inputvarv)$ denote its conditional given $\inputvarv$. 
We can now rewrite the risk~$\risk_\lossmatrix$ and $\surrogateloss$-risk $\risk_\surrogateloss$ as
\begin{equation*}
\risk_\lossmatrix(\scorefunc)
 =
\E_{\inputvarv \sim \data_{\inputdomain}} \; \lossweighted(\scorefunc(\inputvarv), \P(\cdot \mid \inputvarv)),
\quad
\risk_\surrogateloss(\scorefunc)
=
\E_{\inputvarv \sim \data_{\inputdomain}}\;\surrogateweighted(\scorefunc(\inputvarv), \P(\cdot \mid \inputvarv)),
\end{equation*}
where the conditional risk~$\lossweighted$ and the conditional~$\surrogateloss$-risk~$\surrogateweighted$ depend on a vector of scores~$\scorev$ and a conditional distribution on the set of output labels~$\qv$ as
\begin{equation*}
\lossweighted(\scorev, \qv)
:=
\sum\nolimits_{c=1}^\outputvarcard q_c \lossmatrix( \predictor(\scorev), c ),
\quad
\surrogateweighted(\scorev, \qv)
:=
\sum\nolimits_{c=1}^\outputvarcard q_c \surrogateloss( \scorev, c ).
\end{equation*}
The \emph{calibration function}~$\calibrationfunc_{\surrogateloss,\lossmatrix,\scoresubset}$ between the surrogate loss~$\surrogateloss$ and the task loss~$\lossmatrix$ relates the excess surrogate risk with the actual excess risk via the \emph{excess risk bound}:
\begin{equation}
\label{eq:excessRiskBound}
\calibrationfunc_{\surrogateloss,\lossmatrix,\scoresubset}(\excess\lossweighted(\scorev, \qv)) \leq \excess\surrogateweighted(\scorev, \qv), \; \forall \scorev \in \scoresubset, \; \forall \qv \in \simplex_\outputvarcard,
\end{equation}
where 
%
$
\excess\surrogateweighted(\scorev, \qv)
=
\surrogateweighted(\scorev, \qv) - \inf_{\hat{\scorev} \in \scoresubset} \surrogateweighted(\hat{\scorev}, \qv)
$,
$
\excess\lossweighted(\scorev, \qv)
=
\lossweighted(\scorev, \qv) - \inf_{\hat{\scorev} \in \scoresubset} \lossweighted(\hat{\scorev}, \qv)
$%
\ are the excess risks and $\simplex_\outputvarcard$ denotes the probability simplex on $k$ elements.

In other words, to find a vector $\scorev$ that yields an excess risk smaller than $\eps$, we need to optimize the $\surrogateloss$-risk up to $\calibrationfunc_{\surrogateloss,\lossmatrix,\scoresubset}(\eps)$ accuracy (in the worst case).
We make this statement precise in Theorem~\ref{th:lossConnection} below, and now proceed to the formal definition of the calibration function.

\begin{definition}[Calibration function]
\label{def:calibrationFunc}
For a task loss~$\lossmatrix$, a surrogate loss~$\surrogateloss$, a set of feasible scores~$\scoresubset$, the \emph{calibration function}~$\calibrationfunc_{\surrogateloss,\lossmatrix,\scoresubset}(\eps)$ (defined for $\eps \geq 0$) equals the infimum excess of the conditional surrogate risk when the excess of the conditional actual risk is at least~$\eps$:
\begin{align}
\label{eq:calibrationfunc}
\calibrationfunc_{\surrogateloss,\lossmatrix,\scoresubset}(\eps)
:=
\;\;\;\;
\inf_{\mathclap{\scorev \in \scoresubset, \;\qv \in \simplex_\outputvarcard}} \;&\;\;\;\;\excess\surrogateweighted(\scorev, \qv) \\
\label{eq:calibrationfunc:epsConstr}
\text{\textup{s.t.}} \:&\;\;\;\;\excess\lossweighted(\scorev, \qv) \geq \eps.
%
%
%
\end{align}
%
%
%
%
%
%
%
%
%
%
We set $\calibrationfunc_{\surrogateloss,\lossmatrix,\scoresubset}(\eps)$ to $+\infty$ when the feasible set is empty.
\end{definition}
By construction, $\calibrationfunc_{\surrogateloss,\lossmatrix,\scoresubset}$ is non-decreasing on $[0, +\infty)$, $\calibrationfunc_{\surrogateloss,\lossmatrix,\scoresubset}(\eps) \geq 0$, the inequality~\eqref{eq:excessRiskBound} holds, and $\calibrationfunc_{\surrogateloss,\lossmatrix,\scoresubset}(0) = 0$.
Note that $\calibrationfunc_{\surrogateloss,\lossmatrix,\scoresubset}$ can be non-convex and even non-continuous (see examples in Figure~\ref{fig:exampleTranferFunctions}). Also, note that large values of $\calibrationfunc_{\surrogateloss,\lossmatrix,\scoresubset}(\eps)$ are better.

%
\begin{figure}
    \begin{center}
        \begin{tabular}{c@{\qquad\qquad}c}
            \includegraphics[trim = 0mm 0mm 0mm 0mm, clip, width=0.3\columnwidth]{figures/calibrationFunctions_hammingloss.pdf} 
            &
            \includegraphics[trim = 0mm 0mm 0mm 0mm, clip, width=0.3\columnwidth]{figures/calibrationFunctions_mixedloss.pdf} \\
            {\footnotesize (a): Hamming loss $\lossmatrix_{\hamming,\hamminglen}$} & {\footnotesize (b): Mixed loss $\lossmatrix_{01,\numblocks,0.4}$}\\[-1mm]
        \end{tabular}
    \end{center}
    \caption{\label{fig:exampleTranferFunctions} Calibration functions for the quadratic surrogate $\surrogatelossquad$~\eqref{eq:quadrLoss} defined in Section~\ref{sec:quadrSurr} and two different task losses.
        (a)~-- the calibration functions for the Hamming loss $\lossmatrix_{\hamming,\hamminglen}$ when used without constraints on the scores, $\scoresubset = \R^\outputvarcard$ (in red), and with the tight constraints implying consistency, $\scoresubset= \colspace(\lossmatrix_{\hamming,\hamminglen})$ (in blue).
        The red curve can grow exponentially slower than the blue one. 
        (b)~-- the calibration functions for the mixed loss $\lossmatrix_{01,\numblocks,\consbreakpoint}$ with $\consbreakpoint=0.4$ (see Section~\ref{sec:notation} for the definition) when used without constraints on the scores (red) and with tight constraints for the block 0-1 loss (blue).
        The blue curve represents level-$0.2$ consistency.
        The calibration function equals zero for~$\eps \leq \consbreakpoint/2$, but grows exponentially faster than the red curve representing a consistent approach and thus could be better for small $\consbreakpoint$.
        More details on the calibration functions in this figure are given in Section~\ref{sec:quadrSurr}.}
\end{figure}

\subsection{Notion of consistency}
We use the calibration function $\calibrationfunc_{\surrogateloss,\lossmatrix,\scoresubset}$ to set a connection between optimizing the surrogate and task losses by Theorem~\ref{th:lossConnection}, which is similar to Theorem~3 of \citet{zhang04}.
\begin{theorem}[Calibration connection]
\label{th:lossConnection}
Let $\calibrationfunc_{\surrogateloss,\lossmatrix,\scoresubset}$ be the calibration function between the surrogate loss~$\surrogateloss$ and the task loss~$\lossmatrix$ with feasible set of scores~$\scoresubset \subseteq \R^\outputvarcard$. Let $\check{\calibrationfunc}_{\surrogateloss,\lossmatrix,\scoresubset}$ be a convex non-decreasing lower bound of the calibration function.
Assume that~$\surrogateloss$ is continuous and bounded from below.
Then, for any $\eps > 0$ with finite $\check{\calibrationfunc}_{\surrogateloss,\lossmatrix,\scoresubset}(\eps)$ and any $\scorefunc \in \scorefuncset_\scoresubset$, we have
\begin{equation}
\label{eq:theoremConnection:1}
\risk_\surrogateloss(\scorefunc) < \risk_{\surrogateloss,\scoresubset}^* + \check{\calibrationfunc}_{\surrogateloss,\lossmatrix,\scoresubset}(\eps) 
\;\;
\Rightarrow
\;\;
\risk_\lossmatrix(\scorefunc) < \risk_{\lossmatrix,\scoresubset}^* + \eps,
\end{equation}
where $\risk_{\surrogateloss,\scoresubset}^* := \inf_{\scorefunc \in \scorefuncset_\scoresubset} \risk_\surrogateloss(\scorefunc)$ and $\risk_{\lossmatrix,\scoresubset}^*:= \inf_{\scorefunc \in \scorefuncset_\scoresubset} \risk_\lossmatrix (\scorefunc)$. 
\end{theorem}
\begin{proof}
    We take the expectation of~\eqref{eq:excessRiskBound} w.r.t.\ $\inputvarv$, where the second argument of $\lossweighted$ is set to the conditional distribution $\P(\cdot \mid \inputvarv)$.
    Then, we apply Jensen's inequality (since $\check{\calibrationfunc}_{\surrogateloss,\lossmatrix,\scoresubset}$ is convex) to get
    \begin{equation}
    \label{eq:theoremConnection:proof}
    \check{\calibrationfunc}_{\surrogateloss,\lossmatrix,\scoresubset}(\risk_\lossmatrix(\scorefunc) - \risk_{\lossmatrix,\scoresubset}^*)
    \leq
    \risk_\surrogateloss(\scorefunc) - \risk_{\surrogateloss,\scoresubset}^*
    <
    \check{\calibrationfunc}_{\surrogateloss,\lossmatrix,\scoresubset}(\eps),
    \end{equation}
    which implies~\eqref{eq:theoremConnection:1} by monotonicity of $\check{\calibrationfunc}_{\surrogateloss,\lossmatrix,\scoresubset}$.
\end{proof}


%
%

%

A suitable convex non-decreasing lower bound~$\check{\calibrationfunc}_{\surrogateloss,\lossmatrix,\scoresubset}(\eps)$ required by 
Theorem~\ref{th:lossConnection} always exists, e.g., the zero constant. However, in this case Theorem~\ref{th:lossConnection} is not informative, because the l.h.s.\ of~\eqref{eq:theoremConnection:1} is never true.
\citet[Proposition~25]{zhang04} claims that~$\check{\calibrationfunc}_{\surrogateloss,\lossmatrix,\scoresubset}$ defined as the lower convex envelope of the calibration function~$\calibrationfunc_{\surrogateloss,\lossmatrix,\scoresubset}$ satisfies $\check{\calibrationfunc}_{\surrogateloss,\lossmatrix,\scoresubset}(\eps) > 0$, $\forall \eps > 0$, if $\calibrationfunc_{\surrogateloss,\lossmatrix,\scoresubset}(\eps) > 0$, $\forall \eps > 0$, and, e.g., the set of labels is finite.
This statement implies that an informative $\check{\calibrationfunc}_{\surrogateloss,\lossmatrix,\scoresubset}$ always exists and allows to characterize consistency through properties of the calibration function~$\calibrationfunc_{\surrogateloss,\lossmatrix,\scoresubset}$.

%

%
%

We now define a notion of \emph{level-$\consbreakpoint$ consistency}, which is more general than consistency.
\begin{definition}[level-$\consbreakpoint$ consistency]
    \label{def:consistency}
    A surrogate loss~$\surrogateloss$ is \emph{consistent up to level~$\consbreakpoint \geq 0$} w.r.t.\ a task loss~$\lossmatrix$ and a set of scores~$\scoresubset$ if and only if the calibration function satisfies $\calibrationfunc_{\surrogateloss,\lossmatrix,\scoresubset}(\eps) > 0$ for all $\eps > \consbreakpoint$
    and there exists $\hat\eps > \consbreakpoint$ such that $\calibrationfunc_{\surrogateloss,\lossmatrix,\scoresubset}(\hat\eps)$ is finite.
\end{definition}
%
%
Looking solely at (standard level-$0$) consistency vs. inconsistency might be too coarse to capture practical properties related to optimization accuracy (see, e.g., \citep{long2013consistency}).
For example, if $\calibrationfunc_{\surrogateloss,\lossmatrix,\scoresubset}(\eps) = 0$ only for very small values of~$\eps$, then the method can still optimize the actual risk up to a certain level which might be good enough in practice, especially if it means that it can be optimized faster.
Examples of calibration functions for consistent and inconsistent surrogate losses are shown in Figure~\ref{fig:exampleTranferFunctions}.

%



%
\textbf{Other notions of consistency.}
Definition~\ref{def:consistency} with $\consbreakpoint = 0$ and $\scoresubset = \R^\outputvarcard$ results in the standard setting often appearing in the literature. In particular, in this case Theorem~\ref{th:lossConnection} implies Fisher consistency as formulated, e.g., by~\citet{pedregosa15ordivalreg} for general losses and~\citet{lin04} for binary classification.
This setting is also closely related to many definitions of consistency used in the literature.
For example, for a bounded from below and continuous surrogate, it is equivalent to infinite-sample consistency~\citep{zhang04}, classification calibration~\citep{tewari07}, edge-consistency~\citep{duchi10}, $(\lossmatrix, \R^\outputvarcard)$-calibration~\citep{ramaswamy16calibrDim}, prediction calibration~\citep{williamson16}.
See~\citep[Appendix~A]{zhang04} for the detailed discussion.

\textbf{Role of $\scoresubset$.} Let the \emph{approximation error} for the restricted set of scores $\scoresubset$ be defined as $\risk_{\lossmatrix,\scoresubset}^* - \risk_{\lossmatrix}^* := \inf_{\scorefunc \in \scorefuncset_\scoresubset} \risk_\lossmatrix (\scorefunc) - \inf_{\scorefunc} \risk_\lossmatrix (\scorefunc)$. %
For any conditional distribution~$\qv$, the score vector $\scorev := -L\qv$ will yield an optimal prediction. Thus the condition $\colspace(\lossmatrix) \subseteq \scoresubset$ is sufficient for $\scoresubset$ to have zero approximation error for any distribution~$\data$, and for our $0$-consistency condition to imply the standard Fisher consistency with respect to $\lossmatrix$. In the following, we will see that a restricted $\scoresubset$ can both play a role for computational efficiency as well as statistical efficiency (thus losses with smaller $\colspace(\lossmatrix)$ might be easier to work with). 

%
%
%
%
%
%
%
%
%
%
%



%


\subsection{Connection to optimization accuracy and statistical efficiency}
\label{sec:connection}
The scale of a calibration function is not intrinsically well-defined: we could multiply the surrogate function by a scalar and it would multiply the calibration function by the same scalar, without changing the optimization problem.
Intuitively, we would like the surrogate loss to be of order~$1$.
If with this scale the calibration function is exponentially small (has a~$\nicefrac{1}{\outputvarcard}$ factor), then we have strong evidence that the stochastic optimization will be difficult (and thus learning will be slow).

To formalize this intuition, we add to the picture the \emph{complexity} of optimizing the surrogate loss with a \emph{stochastic approximation} algorithm.
By using a scale-invariant convergence rate, we provide a natural normalization of the calibration function.
%
The following two observations are central to the theoretical insights provided in our work:
\begin{enumerate}[itemindent=0.45cm,leftmargin=0.0cm,topsep=\parsep]
    \item \textbf{Scale.} For a properly scaled surrogate loss, the \emph{scale} of the calibration function is a good indication of whether a stochastic approximation algorithm will take a large number of iterations (in the worst case) to obtain guarantees of small excess of the actual risk (and vice-versa, a large coefficient indicates a small number of iterations). The actual verification requires computing the normalization quantities given in Theorem~\ref{th:calibrationSGD:kernels} below.
    \item \textbf{Statistics.} The bound on the number of iterations directly relates to the number of training examples that would be needed to learn, if we see each iteration of the stochastic approximation algorithm as using one training example to optimize the expected surrogate.
    %
\end{enumerate}
%
%

To analyze the statistical convergence of surrogate risk optimization, we have to specify the set of score functions that we work with.
We assume that the structure on input~$\inputvarv \in \inputdomain$ is defined by a positive definite kernel~$\kernelmatrix: \inputdomain \times \inputdomain \to \R$.
We denote the corresponding reproducing kernel Hilbert space (RKHS) by~$\hilbertspace$ and its explicit feature map by~$\featuremap(\inputvarv) \in \hilbertspace$. By the reproducing property, we have $\langle\score, \featuremap(\inputvarv) \rangle_{\hilbertspace} = \score(\inputvarv)$ for all $\inputvarv \in \inputdomain$, $\score \in \hilbertspace$, where $\langle\cdot, \cdot \rangle_{\hilbertspace}$ is the inner product in the RKHS.
We define the subspace of allowed scores~$\scoresubset \subseteq \R^\outputvarcard$ via the span of the columns of a matrix $\scorematrix \in \R^{\outputvarcard \times \scoresubspacedim}$.
The matrix~$\scorematrix$ explicitly defines the structure of the score function.
With this notation, we will assume that the score function is of the form $\scorefunc(\inputvarv) = \scorematrix \parammatrix \featuremap(\inputvarv)$, where $\parammatrix: \hilbertspace \to \R^\scoresubspacedim$ is a linear operator to be learned (a matrix if $\hilbertspace$ is of finite dimension) that represents a collection of~$\scoresubspacedim$ elements in~$\hilbertspace$, transforming $\featuremap(\inputvarv)$ to a vector in~$\R^\scoresubspacedim$ by applying the RKHS inner product~$\scoresubspacedim$ times.\footnote{Note that if $\rank(F) =\scoresubspacedim$, our setup is equivalent to assuming a \emph{joint kernel}~\citep{tsochantaridis05} in the product form: $K_{\text{joint}}((\inputvarv,c),(\inputvarv',c')) := K(\inputvarv,\inputvarv') F(c,:) F(c',:)^\transpose$, where $F(c,:)$ is the row $c$ for matrix $F$.}
Note that for structured losses, we usually have $\scoresubspacedim \ll \outputvarcard$.
The set of all score functions is thus obtained by varying~$\parammatrix$ in this definition and is denoted by~$\scorefuncset_{\scorematrix, \hilbertspace}$.
As a concrete example of a score family~$\scorefuncset_{\scorematrix, \hilbertspace}$ for structured prediction, consider the standard sequence model with unary and pairwise potentials.
In this case, the dimension~$\scoresubspacedim$ equals~$\hamminglen s + (\hamminglen-1) s^2$, where~$\hamminglen$ is the sequence length and $s$ is the number of labels of each variable.
The columns of the matrix~$\scorematrix$ consist of $2\hamminglen - 1$ groups (one for each unary and pairwise potential).
Each row of~$\scorematrix$ has exactly one entry equal to one in each column group (with zeros elsewhere).

%

In this setting, we use the online projected averaged stochastic subgradient descent ASGD\footnote{See, e.g., \cite{orabona2014simultaneous} for the formal setup of kernel ASGD.} (stochastic w.r.t.\ data $(\inputvarv^{(\iter)}, \outputvarv^{(\iter)}) \sim \data$) to minimize the surrogate risk directly~\cite{bousquet2008tradeoffs}. The $\iter$-th update consists in
\begin{equation}
\label{eq:rkhsSgdStep}
\parammatrix^{(\iter)} := \proj_{D} \bigl[\parammatrix^{(\iter-1)} - \gamma^{(\iter)} \scorematrix^\transpose \gradient \surrogateloss \featuremap(\inputvarv^{(\iter)})^\transpose  \bigr],
\end{equation}
where $\scorematrix^\transpose \gradient \surrogateloss \featuremap(\inputvarv^{(\iter)})^\transpose : \hilbertspace \to \R^\scoresubspacedim$ is the stochastic functional gradient, $\gamma^{(\iter)}$ is the step size and $\proj_{D}$ is the projection on the ball of radius~$D$ w.r.t.\ the Hilbert–Schmidt norm\footnote{The Hilbert–Schmidt norm of a linear operator $A$ is defined as~$\|A\|_{HS} = \sqrt{\trace A^\ddagger A }$ where $A^\ddagger$ is the adjoint operator. In the case of finite dimension, the Hilbert–Schmidt norm coincides with the Frobenius matrix norm.}\!\!. The vector $\gradient \surrogateloss \in \R^\outputvarcard$ is a regular gradient of the sampled surrogate $\surrogateloss( \scorefunc(\inputvarv^{(\iter)}), \outputvarv^{(\iter)} )$ w.r.t.\ the scores, $\gradient \surrogateloss = \gradient_\scorev \surrogateloss( \scorev, \outputvarv^{(\iter)} )|_{\scorev = \scorefunc(\inputvarv^{(\iter)})}$.
We wrote the above update using an explicit feature map $\featuremap$ for notational simplicity, but kernel ASGD can also be implemented without it by using the kernel trick.
%
The convergence properties of ASGD in RKHS are analogous to the finite-dimensional ASGD because they rely on dimension-free quantities.
To use a simple convergence analysis, we follow~\citet{ciliberto16} and make the following simplifying assumption:
\begin{assumption}[Well-specified optimization w.r.t. the function class~$\scorefuncset_{\scorematrix, \hilbertspace}$]
    \label{th:well_specification}
    The distribution~$\data$ is such that $\risk_{\surrogateloss,\scoresubset}^* := \inf_{\scorefunc \in \scorefuncset_\scoresubset} \risk_\surrogateloss(\scorefunc)$ has some global minimum~$\scorefunc^*$ that also belongs to~$\scorefuncset_{\scorematrix, \hilbertspace}$.
\end{assumption}
Assumption~\ref{th:well_specification} simply means that each row of
$\parammatrix^*$ defining~$\scorefunc^*$ belongs to the RKHS~$\hilbertspace$ implying a finite norm $\| \parammatrix^* \|_{HS}$.
Assumption~\ref{th:well_specification} can be relaxed if the kernel~$K$ is universal, but then the convergence analysis becomes much more complicated~\cite{orabona2014simultaneous}.

\begin{theorem}[Convergence rate]
\label{th:rkhsSgdConvergence}
Under Assumption~\ref{th:well_specification} and assuming that (i) the functions $\surrogateloss( \scorev, \outputvarv )$ are  bounded from below and convex w.r.t.\ $\scorev \in \R^{\outputvarcard}$ for all $\outputvarv \in \outputdomain$; (ii) the expected square of the norm of the stochastic gradient is bounded, $\E_{(\inputvarv, \outputvarv) \sim \data} \| \scorematrix^\transpose \gradient \surrogateloss \featuremap(\inputvarv)^\transpose\|_{HS}^2 \leq M^2$ and (iii) $\| \parammatrix^* \|_{HS} \leq D$, then running the ASGD algorithm~\eqref{eq:rkhsSgdStep} with the constant step-size~$\gamma := \frac{2D}{M \sqrt{\maxiter}}$ for $\maxiter$~steps admits the following expected suboptimality for the averaged iterate $\bar{\scorefunc}^{(\maxiter)}$:
\begin{equation}
\label{eq:rkhsSgdRate}
\E[\risk_\surrogateloss(\bar{\scorefunc}^{(\maxiter)})] -  \risk_{\surrogateloss,\scoresubset}^*
\leq
\frac{2D M}{\sqrt{\maxiter}}
\quad
\text{where}\;\;
\bar{\scorefunc}^{(\maxiter)} := \frac{1}{\maxiter} \sum\nolimits_{\iter=1}^{\maxiter} \!\scorematrix\parammatrix^{(\iter)} \featuremap(\inputvarv^{(\iter)})^\transpose.
\end{equation}
\end{theorem}
Theorem~\ref{th:rkhsSgdConvergence} is a straight-forward extension of classical results~\cite{nemirovski09,orabona2014simultaneous}.

By combining the convergence rate of Theorem~\ref{th:rkhsSgdConvergence} with Theorem~\ref{th:lossConnection} that connects the surrogate and actual risks, we get Theorem~\ref{th:calibrationSGD:kernels} which explicitly gives the number of iterations required to achieve $\eps$~accuracy on the expected \emph{population risk} (see App.~\ref{sec:Thm6Proof} for the proof).
Note that since ASGD is applied in an online fashion, Theorem~\ref{th:calibrationSGD:kernels} also serves as the sample complexity bound, i.e., says how many samples are needed to achieve $\eps$ target accuracy (compared to the best prediction rule if $\scoresubset$ has zero approximation error).
\begin{theorem}[Learning complexity]
    \label{th:calibrationSGD:kernels}
    Under the assumptions of Theorem~\ref{th:rkhsSgdConvergence}, for any~$\eps > 0$, the random (w.r.t.\ the observed training set) output~$\bar{\scorefunc}^{(\maxiter)} \in \scorefuncset_{\scorematrix, \hilbertspace}$ of the ASGD algorithm after
    \begin{equation}
    \label{eq:theoremSgd:2}
    \maxiter > \maxiter^* := \frac{4 D^2 M^2}{\vphantom{(\big(\bigr)}\check{\calibrationfunc}_{\surrogateloss,\lossmatrix,\scoresubset}^2(\eps) }
    \end{equation}
    iterations has the expected excess risk bounded with~$\eps$, i.e.,
    %
    %
    $
    \E[\risk_\lossmatrix(\bar{\scorefunc}^{(\maxiter)})] < \risk_{\lossmatrix,\scoresubset}^* + \eps.
    $
    %
\end{theorem}

\section{Calibration function analysis for quadratic surrogate}
\label{sec:quadrSurr}
A major challenge to applying Theorem~\ref{th:calibrationSGD:kernels} is the computation of the calibration function 
$\calibrationfunc_{\surrogateloss,\lossmatrix,\scoresubset}$.
In App.~\ref{sec:specialLosses}, we present a generalization to arbitrary multi-class losses of a surrogate loss class from~\citet[Section 4.4.2]{zhang04} that is consistent for any task loss~$\lossmatrix$. Here, we consider the simplest example of this family, called the \emph{quadratic surrogate}~$\surrogatelossquad$, which has the advantage that we can bound or even compute exactly its calibration function.
%
We define the quadratic surrogate as
\begin{equation}
\label{eq:quadrLoss}
\surrogatelossquad( \scorev, \outputvarv )
:=
\frac{1}{2\outputvarcard} \| \scorev + \lossmatrix(:, \outputvarv)\|_2^2 = \frac{1}{2k} \sum_{c=1}^k (f_c^2 + 2f_c L(c,\outputvarv)  + L(c,\outputvarv)^2).
%
%
\end{equation}
One simple sufficient condition for the surrogate~\eqref{eq:quadrLoss} to be consistent and also to have zero approximation error is that $\scoresubset$ fully contains~$\colspace(\lossmatrix)$.
To make the dependence on the score subspace explicit, we parameterize it with a matrix~$\scorematrix \in \R^{\outputvarcard \times \scoresubspacedim}$ with the number of columns~$\scoresubspacedim$ typically being much smaller than the number of labels~$\outputvarcard$.
With this notation, we have $\scoresubset = \colspace(\scorematrix) = \{\scorematrix\scoreparamv \mid \scoreparamv\in \R^\scoresubspacedim\}$, and the dimensionality of~$\scoresubset$ equals the rank of~$\scorematrix$, which is at most~$\scoresubspacedim$.\footnote{Evaluating $\surrogatelossquad$ requires computing $\scorematrix^\transpose \scorematrix$ and $\scorematrix^\transpose \lossmatrix(:, \outputvarv)$ for which direct computation is intractable when $\outputvarcard$ is exponential, but which can be done in closed form for the structured losses we consider (the Hamming and block 0-1 loss).
More generally, these operations require suitable inference algorithms. See also App.~\ref{sec:tranferAndSgd}.}
%

For the quadratic surrogate~\eqref{eq:quadrLoss}, the excess of the expected surrogate takes a simple form:
\begin{equation}
\label{eq:quadSurrogateExcessSimple}
\excess\surrogateweightedquad(\scorematrix\scoreparamv, \qv) = \frac{1}{2\outputvarcard}\| \scorematrix\scoreparamv + \lossmatrix \qv \|_2^2.
\end{equation}
%
Equation~\eqref{eq:quadSurrogateExcessSimple} holds under the assumption that the subspace~$\scoresubset$ contains the column space of the loss matrix~$\colspace(\lossmatrix)$, which also means that the set $\scoresubset$ contains the optimal prediction for any~$\qv$ (see Lemma~\ref{th:quadSurrogateExcess} in App.~\ref{sec:techLemmas} for the proof).
Importantly, the function~$\excess\surrogateweightedquad(\scorematrix\scoreparamv, \qv)$ is jointly convex in the conditional probability~$\qv$ and parameters~$\scoreparamv$, which simplifies its analysis.


%
%
\textbf{Lower bound on the calibration function.}
We now present our main technical result: a lower bound on the calibration function for the surrogate loss $\surrogatelossquad$~\eqref{eq:quadrLoss}. This lower bound characterizes the easiness of learning with this surrogate given the scaling intuition mentioned in Section~\ref{sec:connection}.
The proof of Theorem~\ref{th:lowerBoundcalibrationFunction} is given in App.~\ref{sec:boundscalibrationFunction:lower}.

%
%
%

%
%
%

\begin{theorem}[Lower bound on $\calibrationfunc_{\surrogatelossquad}$]
    \label{th:lowerBoundcalibrationFunction}
    For any task loss~$\lossmatrix$, its quadratic surrogate~$\surrogatelossquad$, and a score subspace~$\scoresubset$ containing the column space of $\lossmatrix$, the calibration function can be lower bounded:
    \begin{equation}
    \label{eq:lowerBoundcalibrationFunction}
    \calibrationfunc_{\surrogatelossquad,\lossmatrix,\scoresubset}(\eps)
    \geq
    \frac{\eps^2}{2\outputvarcard \max_{i\neq j}\|\proj_{\scoresubset} \Delta_{ij}\|_2^2}
    \geq 
    \frac{\eps^2}{4\outputvarcard},
    \end{equation}
    where $\proj_{\scoresubset}$ is the orthogonal projection on the subspace~$\scoresubset$ and $\Delta_{ij} = \unit_i - \unit_j \in \R^k$ with $\unit_c$ being the $c$-th basis vector of the standard basis in~$\R^\outputvarcard$.
\end{theorem}

\textbf{Lower bound for specific losses.} We now discuss the meaning of the bound~\eqref{eq:lowerBoundcalibrationFunction} for some specific losses (the detailed derivations are given in App.~\ref{sec:bounds:interpretations}).
For the 0-1, block 0-1 and Hamming losses ($\lossmatrix_{01}$, $\lossmatrix_{01,\numblocks}$ and $\lossmatrix_{\hamming,\hamminglen}$, respectively) with the smallest possible score subspaces~$\scoresubset$, the bound~\eqref{eq:lowerBoundcalibrationFunction} gives ~$\frac{\eps^2}{4\outputvarcard}$, $\frac{\eps^2}{4\numblocks}$ and $\frac{\epsilon^2}{8\hamminglen}$, respectively.
All these bounds are tight (see App.~\ref{sec:exactTranferProofs}).
However, if~$\scoresubset = \R^\outputvarcard$ the bound~\eqref{eq:lowerBoundcalibrationFunction} is not tight for the block 0-1 and mixed losses (see also App.~\ref{sec:exactTranferProofs}).
In particular, the bound~\eqref{eq:lowerBoundcalibrationFunction} cannot detect level-$\consbreakpoint$ consistency for $\consbreakpoint > 0$ (see Def.~\ref{def:consistency}) and does not change when the loss changes, but the score subspace stays the same.


\textbf{Upper bound on the calibration function.}
Theorem~\ref{th:upperBoundcalibrationFunction} below gives an upper bound on the calibration function holding for unconstrained scores, i.e, $\scoresubset = \R^{\outputvarcard}$ (see the proof in App.~\ref{sec:boundscalibrationFunction:upper}).
This result shows that without some appropriate constraints on the scores, efficient learning is not guaranteed (in the worst case) because of the~$\nicefrac{1}{\outputvarcard}$ scaling of the calibration function.
\begin{theorem}[Upper bound on $\calibrationfunc_{\surrogatelossquad}$]
    \label{th:upperBoundcalibrationFunction}
    If a loss matrix~$\lossmatrix$ with $\Lmax > 0$ defines a pseudometric\footnote{\label{footnote:pseudometric}A pseudometric is a function $d(a,b)$ satisfying the following axioms: $d(x, y) \geq 0$, $d(x, x) = 0$ (but possibly $d(x,y)=0$ for some $x\neq y$), $d(x, y) = d(y, x)$, $d(x, z) \leq d(x, y) + d(y, z)$.} on labels and there are no constraints on the scores, i.e., $\scoresubset = \R^{\outputvarcard}$, then the calibration function for the quadratic surrogate~$\surrogatelossquad$ can be upper bounded:
%
    %
    $
    \calibrationfunc_{\surrogatelossquad,\lossmatrix,\R^{\outputvarcard}}(\eps)
    \leq  \frac{\eps^2}{2\outputvarcard}, \quad 0 \leq \eps \leq \Lmax.
    $
%
\end{theorem}
%
From our lower bound in Theorem~\ref{th:lowerBoundcalibrationFunction} (which guarantees consistency), the natural constraint on the score is $\scoresubset = \colspace(\lossmatrix)$, with the dimension of this space giving an indication of the intrinsic ``difficulty'' of a loss.
Computations for the lower bounds in some specific cases (see App.~\ref{sec:bounds:interpretations} for details) show that the 0-1 loss is ``hard'' while the block 0-1 loss and the Hamming loss are ``easy''.
Note that in all these cases the lower bound~\eqref{eq:lowerBoundcalibrationFunction} is tight, see the discussion below.


%
%
\textbf{Exact calibration functions.}
%
Note that the bounds proven in Theorems~\ref{th:lowerBoundcalibrationFunction} and~\ref{th:upperBoundcalibrationFunction} imply that, in the case of no constraints on the scores~$\scoresubset = \R^\outputvarcard$, for the 0-1, block 0-1 and Hamming losses, we have
\begin{equation}
\label{eq:lowerUpperBounds}
\frac{\eps^2}{4\outputvarcard} \leq \calibrationfunc_{\surrogatelossquad,\lossmatrix,\R^\outputvarcard}(\eps) \leq \frac{\eps^2}{2\outputvarcard},
\end{equation}
where $\lossmatrix$ is the matrix defining a loss.
For completeness, in App.~\ref{sec:exactTranferProofs}, we compute the exact calibration functions for the 0-1 and block 0-1 losses.
Note that the calibration function for the \textbf{0-1 loss} equals the lower bound, illustrating the worst-case scenario. To get some intuition, an example of a conditional distribution~$\qv$ that gives the (worst case) value to the calibration function (for several losses) is $\q_i = \frac{1}{2} + \frac{\eps}{2}$, $\q_j = \frac{1}{2} - \frac{\eps}{2}$ and $\q_c=0$ for $c \in \neq \{i,j\}$. See the proof of Proposition~\ref{th:calibrationFunction:01loss} in App.~\ref{sec:exactTranferProofs:01loss}.

In what follows, we provide the calibration functions in the cases with constraints on the scores.
For the \textbf{block 0-1 loss} with~$\numblocks$ equal blocks and under constraints that the scores within blocks are equal, the calibration function equals (see Proposition~\ref{th:calibrationFunction:block01loss:hardConstr} of  App.~\ref{sec:exactTranferProofs:block01loss})
\begin{equation}
\label{eq:calibrationFunction:block01loss:hardConstr}
\calibrationfunc_{\surrogatelossquad,\lossmatrix_{01,\numblocks},\scoresubset_{01,\numblocks}}(\eps)
=
\frac{\eps^2}{4\numblocks},
\quad 0 \leq \eps \leq 1.
\end{equation}

For the \textbf{Hamming loss} defined over $\hamminglen$~binary variables and under constraints implying separable scores, the calibration function equals (see Proposition~\ref{th:calibrationFunction:hammingLoss:hardConstr} in App.~\ref{sec:exactTranferProofs:hamming})
\begin{equation}
\label{eq:calibrationFunction:hammingloss:hardConstr}
\calibrationfunc_{\surrogatelossquad,\lossmatrix_{\hamming,\hamminglen},\scoresubset_{\hamming,\hamminglen}}(\eps)
=
\frac{\eps^2}{8\hamminglen},
\; 0 \leq \eps \leq 1.
\end{equation}

The calibration functions~\eqref{eq:calibrationFunction:block01loss:hardConstr} and~\eqref{eq:calibrationFunction:hammingloss:hardConstr} depend on the quantities representing the actual complexities of the loss (the number of blocks~$\numblocks$ and the length of the sequence~$\hamminglen$) and can be exponentially larger than the upper bound for the unconstrained case.

In the case of \textbf{mixed 0-1 and block 0-1 loss},
if the scores~$\scorev$ are constrained to be equal inside the blocks, i.e., belong to the subspace~$\scoresubset_{01,\numblocks} = \colspace(\lossmatrix_{01,\numblocks}) \subsetneq \R^k$, then the calibration function is equal to~$0$ for $\eps \leq \frac{\consbreakpoint}{2}$, implying inconsistency (and also note that the approximation error can be as big as~$\consbreakpoint$ for $\scoresubset_{01,\numblocks}$).
However, for $\eps > \frac{\consbreakpoint}{2}$, the calibration function is of the order
$
\frac{1}{\numblocks} (\eps-\frac{\consbreakpoint}{2})^2.
$
See Figure~\ref{fig:exampleTranferFunctions}b for the illustration of this calibration function and Proposition~\ref{th:calibrationFunction:mixedLoss:hardConstr} of App.~\ref{sec:exactTranferProofs:mixedloss} for the exact formulation and the proof.
Note that while the calibration function for the constrained case is inconsistent, its value can be exponentially larger than the one for the unconstrained case for $\eps$ big enough and when the blocks are exponentially large (see Proposition~\ref{th:calibrationFunction:mixedLoss} of App.~\ref{sec:exactTranferProofs:mixedloss}).

\textbf{Computation of the SGD constants.}
Applying the learning complexity Theorem~\ref{th:calibrationSGD:kernels} requires to compute the quantity $DM$ where $D$ bounds the norm of the optimal solution and $M$ bounds the expected square of the norm of the stochastic gradient.
In App.~\ref{sec:tranferAndSgd}, we provide a way to bound this quantity for our quadratic surrogate~\eqref{eq:quadrLoss} under the simplifying assumption that each conditional~$\q_c(\inputvarv)$ (seen as function of $\inputvarv$) belongs to the RKHS~$\hilbertspace$ (which implies Assumption~\ref{th:well_specification}).
In particular, we get
\begin{equation}
\label{eq:sgdConstant}
DM
=
\Lmax^2 \xi(\condnum(\scorematrix) \sqrt{\scoresubspacedim} \rkhsbound  \qbound), \quad \xi(z) = z^2 + z,
\end{equation}
where $\condnum(\scorematrix)$ is the condition number of the matrix~$\scorematrix$, $\rkhsbound$ is an upper bound on the RKHS norm of object feature maps $\|\featuremap(\inputvarv)\|_\hilbertspace$. We define $\qbound$ as an upper bound on $\sum_{c=1}^{\outputvarcard} \|\q_c\|_\hilbertspace$ (can be seen as the generalization of the inequality~$\sum_{c=1}^{\outputvarcard} \q_c \leq 1$ for probabilities).
The constants $\rkhsbound$ and $\qbound$ depend on the data, the constant~$\Lmax$ depends on the loss, $\scoresubspacedim$ and $\condnum(\scorematrix)$ depend on the choice of matrix~$\scorematrix$.

We compute the constant $DM$ for the specific losses that we considered in App.~\ref{sec:tranferAndSgd:constants}.
For the 0-1, block 0-1 and Hamming losses, we have $DM = \bigO(\outputvarcard)$, $DM = \bigO(\numblocks)$ and $DM = \bigO(\log_2^3 \outputvarcard)$, respectively.
These computations indicate that the quadratic surrogate allows efficient learning for structured block 0-1 and Hamming losses, but that the convergence could be slow in the worst case for the 0-1 loss.


\section{Related works}
\label{sec:relatedworks}
\textbf{Consistency for multi-class problems.}
Building on significant progress for the case of binary classification, see, e.g.~\cite{bartlett06convexity}, there has been a lot of interest in the multi-class case.
\citet{zhang04} and \citet{tewari07} analyze the consistency of many existing surrogates for the 0-1 loss.
\citet{gao2011multilabel} focus on  multi-label classification.
\citet{narasimhan15} provide a consistent algorithm for arbitrary multi-class loss defined by a function of the confusion matrix.
Recently, \citet{ramaswamy16calibrDim} introduce the notion of convex calibrated dimension, as the minimal dimensionality of the score vector that is required for consistency.
In particular, they showed that for the Hamming loss on~$\hamminglen$ binary variables, this dimension is at most~$\hamminglen$.
In our analysis, we use scores of rank~$(\hamminglen+1)$, see~\eqref{eq:hammingLoss} in App.~\ref{sec:bounds:interpretations}, yielding a similar result.

The task of ranking has attracted a lot of attention and \cite{duchi10,buffoni2011learning,calauzenes12,ramaswamy13rankSurrogates} analyze different families of surrogate and task losses proving their (in-)consistency.
In this line of work, \citet{ramaswamy13rankSurrogates} propose a quadratic surrogate for an arbitrary low rank loss which is related to our quadratic surrogate~\eqref{eq:quadrLoss}.
They also prove that several important ranking losses, i.e.,
precision@q, 
expected rank utility, 
mean average precision and
pairwise disagreement, are of low-rank.
We conjecture that our approach is compatible with these losses and leave precise connections as future work.


%


\textbf{Structured SVM (SSVM) and friends.}
SSVM~\citep{taskar03,taskar2005learning,tsochantaridis05} is one of the most used convex surrogates for tasks with structured outputs, thus, its consistency has been a question of great interest.
It is known that Crammer-Singer multi-class SVM~\citep{crammer01}, which SSVM is built on, is not consistent for 0-1 loss unless there is a majority class with probability at least $\frac12$~\citep{zhang04,mcallester07}.
However, it is consistent for the ``abstain'' and ordinal losses in the case of~$3$ classes~\cite{ramaswamy16calibrDim}.
Structured ramp loss and probit surrogates are closely related to SSVM and are consistent~\citep{mcallester07,chapelle2009tighter,mcallester11,keshet14}, but not convex.
%
%

%
%

Recently, \citet{dogan2016svm} categorized different versions of multi-class SVM and analyzed them from Fisher and universal consistency point of views.
In particular, they highlight differences between Fisher and universal consistency and give examples of surrogates that are Fisher consistent, but not universally consistent and vice versa.
They also highlight that the Crammer-Singer SVM is neither Fisher, not universally consistent even with a careful choice of regularizer.

\textbf{Quadratic surrogates for structured prediction.}
\citet{ciliberto16} and \citet{brouard2016input} consider minimizing~$\sum_{i=1}^n \|g(\inputvarv_i) - \featuremap_o(\outputvarv_i)\|_\hilbertspace^2$ aiming to match the RKHS embedding of inputs~$g: \inputdomain \to  \hilbertspace$ to the feature maps of outputs~$\featuremap_o: \outputdomain \to \hilbertspace$.
In their frameworks, the task loss is not considered at the learning stage, but only at the prediction stage.
Our quadratic surrogate~\eqref{eq:quadrLoss} depends on the loss directly.
The empirical risk defined by both their and our objectives can be minimized analytically with the help of the kernel trick and, moreover, the resulting predictors are identical.
However, performing such computation in the case of large dataset can be intractable and the generalization properties have to be taken care of, e.g., by the means of regularization.
In the large-scale scenario, it is more natural to apply stochastic optimization (e.g., kernel ASGD) that directly minimizes the population risk and has better dependency on the dataset size.
When combined with stochastic optimization, the two approaches lead to different behavior.
In our framework, we need to estimate~$\scoresubspacedim = \rank(\lossmatrix)$ scalar functions, but the alternative needs to estimate~$\outputvarcard$ functions (if, e.g., $\featuremap_o(\outputvarv) = \unit_\outputvarv \in \R^\outputvarcard$), which results in significant differences for low-rank losses, such as block 0-1 and Hamming.

\textbf{Calibration functions.}
\citet{bartlett06convexity} and \citet{steinwart07} provide calibration functions for most existing surrogates for binary classification.
All these functions differ in term of shape, but are roughly similar in terms of constants.
\citet{pedregosa15ordivalreg} generalize these results to the case of ordinal regression.
However, their calibration functions have at best a $\nicefrac{1}{\outputvarcard}$ factor if the surrogate is normalized w.r.t.\ the number of classes.
The task of ranking has been of significant interest.
However, most of the literature \citep[e.g.,][]{clemencon2008ranking,cossock2008ranking,kotlowski2011bipartite,agarwal2014regretbounds},
only focuses on calibration functions (in the form of regret bounds) for bipartite ranking, which is more akin to cost-sensitive binary classification.

\citet{pires2013riskbounds} generalize the theoretical framework developed by~\citet{steinwart07}  and present results for the multi-class SVM of~\citet{lee2004multicategory} (the score vectors are constrained to sum to zero) that can be built for any task loss of interest.
Their surrogate $\surrogateloss$ is of the form $\sum_{c \in \outputdomain} \lossmatrix(c, \outputvarv) a( \score_c )$ where $\sum_{c \in \outputdomain} \score_c = 0$ and $a(\score)$ is some convex function with all subgradients at zero being positive.
The recent work by~\citet{pires2016calfuncs} refines the results, but specifically for the case of 0-1 loss.
In this line of work, the surrogate is typically not normalized by~$\outputvarcard$, and if normalized the calibration functions have the constant $\nicefrac{1}{\outputvarcard}$ appearing.

Finally, \citet{ciliberto16} provide the calibration function for their quadratic surrogate.
Assuming that the loss can be represented as
$
\lossmatrix(\hat{\outputvarv}, \outputvarv) = \langle V \featuremap_o(\hat{\outputvarv}), \featuremap_o(\outputvarv) \rangle_{\mathcal{H}_\outputdomain}
$, $\hat{\outputvarv}, \outputvarv \in \outputdomain$
(this assumption can always be satisfied in the case of a finite number of labels, by taking $V$ as the loss matrix~$\lossmatrix$ and $\featuremap_o(\outputvarv) := \unit_\outputvarv \in \R^\outputvarcard$ where $\unit_\outputvarv$ is the $\outputvarv$-th vector of the standard basis in~$\R^{\outputvarcard}$).
In their Theorem~2, they provide an excess risk bound leading to a lower bound on the corresponding calibration function
$
\calibrationfunc_{\surrogateloss,\lossmatrix,\R^{\outputvarcard}}(\eps) \geq \frac{\eps^2}{c_\Delta^2}
$
where a constant $c_\Delta = \|V\|_2 \max_{\outputvarv \in \outputdomain} \|\featuremap_o(\outputvarv)\|$ simply equals the spectral norm of the loss matrix for the finite-dimensional construction provided above.
However, the spectral norm of the loss matrix is exponentially large even for highly structured losses such as the block 0-1 and Hamming losses, i.e.,
$\|\lossmatrix_{01,\numblocks}\|_2 = \outputvarcard - \frac{\outputvarcard}{\numblocks}$,
$\|\lossmatrix_{\hamming,\hamminglen}\|_2 = \frac{\outputvarcard}{2}$.
This conclusion puts the objective of~\citet{ciliberto16} in line with ours when no constraints are put on the scores.

\section{Conclusion}
\label{sec:conclusion}
In this paper, we studied the consistency of convex surrogate losses specifically in the context of structured prediction.
We analyzed calibration functions and proposed an optimization-based normalization aiming to connect consistency with the existence of efficient learning algorithms.
Finally, we instantiated all components of our framework for several losses by computing the calibration functions and the constants coming from the normalization.
By carefully monitoring exponential constants, we highlighted the difference between tractable and intractable task losses.

These were first steps in advancing our theoretical understanding of consistent structured prediction. Further steps include analyzing more losses such as the low-rank ranking losses studied by~\citet{ramaswamy13rankSurrogates} and, instead of considering constraints on the scores, one could instead put constraints on the set of distributions to investigate the effect on the calibration function.  
%

\subsubsection*{Acknowledgements}
We would like to thank Pascal Germain for useful discussions.
This work was partly supported by the ERC grant Activia (no. 307574), the NSERC Discovery Grant RGPIN-2017-06936 and the MSR-INRIA Joint Center.
%
%
%

%
%
%
%

\bibliographystyle{icml2017}
%
{ %
\bibliography{references}}


%
\clearpage
\appendix

%
%
%

\newcommand{\toptitlebar}{
    \hrule height 4pt
    \vskip 0.25in
    \vskip -\parskip%
}
\newcommand{\bottomtitlebar}{
    \vskip 0.29in
    \vskip -\parskip
    \hrule height 1pt
    \vskip 0.09in%
}
\toptitlebar
\begin{center}
{\LARGE\bf Supplementary Material (Appendix)}
\\[4mm]
{\LARGE\bf On Structured Prediction Theory with Calibrated }
\\
{\LARGE\bf Convex Surrogate Losses }
\end{center}
\bottomtitlebar

\section*{Outline} 
\begin{description}[itemsep=0mm,topsep=0cm,leftmargin=*,font=\normalfont]
    \renewcommand\labelitemi{--}
    \item[Section~\ref{sec:Thm6Proof}:] Proof of learning complexity Theorem~\ref{th:calibrationSGD:kernels}.
    \item[Section~\ref{sec:techLemmas}:] Technical lemmas useful for the proofs.
    \item[Section~\ref{sec:specialLosses}:] Discussion and consistency results on a family of surrogate losses.
    \item[Section~\ref{sec:boundscalibrationFunction}:] Bounds on the calibration functions.
    \begin{description}[itemsep=0mm,topsep=-0.05cm,itemindent=0.0cm,leftmargin=0.6cm,font=\normalfont]
    \item[Section~\ref{sec:boundscalibrationFunction:lower}:] Theorem~\ref{th:lowerBoundcalibrationFunction}~-- a lower bound.
    \item[Section~\ref{sec:boundscalibrationFunction:upper}:]  Theorem~\ref{th:upperBoundcalibrationFunction}~-- an upper bound.
    \item[Section~\ref{sec:bounds:interpretations}:] Computation of the bounds for specific task losses.
    \end{description}
    \item[Section~\ref{sec:exactTranferProofs}:] Computations of the exact calibration functions for the quadratic surrogate.
    \begin{description}[itemsep=0mm,topsep=-0.05cm,itemindent=0.0cm,leftmargin=0.6cm,font=\normalfont]
        \item[Section~\ref{sec:exactTranferProofs:01loss}:] 0-1 loss.
        \item[Section~\ref{sec:exactTranferProofs:block01loss}:] Block 0-1 loss.
        \item[Section~\ref{sec:exactTranferProofs:hamming}:] Hamming loss.
        \item[Section~\ref{sec:exactTranferProofs:mixedloss}:] Mixed 0-1 and block 0-1 loss.
    \end{description}
    \item[Section~\ref{sec:tranferAndSgd}:] Computing constants appearing in the SGD rate.
    \item[Section~\ref{sec:hammingLossProps}:] Properties of the basis of the Hamming loss.
\end{description}

\section{Learning complexity theorem} \label{sec:Thm6Proof}

\begin{reptheorem}{th:calibrationSGD:kernels}[Learning complexity]
    \label{th:rep:calibrationSGD:kernels}
 Under the assumptions of Theorem~\ref{th:rkhsSgdConvergence}, for any~$\eps > 0$, the random (w.r.t.\ the observed training set) output~$\bar{\scorefunc}^{(\maxiter)} \in \scorefuncset_{\scorematrix, \hilbertspace}$ of the ASGD algorithm after
    \begin{equation}
    %
    \maxiter > \maxiter^* := \frac{4 D^2 M^2}{\vphantom{(\big(\bigr)}\check{\calibrationfunc}_{\surrogateloss,\lossmatrix,\scoresubset}^2(\eps) }
    \end{equation}
    iterations has the expected excess risk bounded with~$\eps$, i.e.,
    %
    %
    $
    \E[\risk_\lossmatrix(\bar{\scorefunc}^{(\maxiter)})] < \risk_{\lossmatrix,\scoresubset}^* + \eps.
    $
    %
\end{reptheorem}
\begin{proof}
    %
	By~\eqref{eq:rkhsSgdRate} from Theorem~\ref{th:rkhsSgdConvergence}, $\maxiter$ steps of the algorithm, in expectation, result in~$\check{\calibrationfunc}_{\surrogateloss,\lossmatrix,\scoresubset}(\eps)$ accuracy on the surrogate risk, i.e., $\E[\risk_\surrogateloss(\bar{\scorefunc}^{(\maxiter)})] -  \risk_{\surrogateloss,\scoresubset}^* < \check{\calibrationfunc}_{\surrogateloss,\lossmatrix,\scoresubset}(\eps)$.
    We now generalize the proof of Theorem~\ref{th:lossConnection} to the case of expectation w.r.t.~$\bar{\scorefunc}^{(\maxiter)}$ depending on the random samples used by the ASGD algorithm.
    We take the expectation of~\eqref{eq:excessRiskBound} w.r.t.\ $\bar{\scorefunc}^{(\maxiter)}$ substituted as~$\scorefunc$ and use Jensen's inequality (by convexity of~$\check{\calibrationfunc}_{\surrogateloss,\lossmatrix,\scoresubset}$) to get
    $\E[\risk_\surrogateloss(\bar{\scorefunc}^{(\maxiter)})] -  \risk_{\surrogateloss,\scoresubset}^* \geq \E[\check{\calibrationfunc}_{\surrogateloss,\lossmatrix,\scoresubset}(\risk_\lossmatrix(\bar{\scorefunc}^{(\maxiter)}) - \risk_{\lossmatrix,\scoresubset}^*)] \geq \check{\calibrationfunc}_{\surrogateloss,\lossmatrix,\scoresubset}(\E[\risk_\lossmatrix(\bar{\scorefunc}^{(\maxiter)})] - \risk_{\lossmatrix,\scoresubset}^*)$.
    Finally, monotonicity of~$\check{\calibrationfunc}_{\surrogateloss,\lossmatrix,\scoresubset}$ implies~$\E[\risk_\lossmatrix(\bar{\scorefunc}^{(\maxiter)})] - \risk_{\lossmatrix,\scoresubset}^* < \eps$.
\end{proof}

\section{Technical lemmas}
\label{sec:techLemmas}
In this section, we prove two technical lemmas that simplify the proofs of the main theoretical claims of the paper.

Lemma~\ref{th:quadSurrogateExcess} computes the excess of the weighted surrogate risk~$\excess\surrogateweighted$ for the quadratic loss~$\surrogatelossquad$~\eqref{eq:quadrLoss}, which is central to our analysis presented in Section~\ref{sec:quadrSurr}.
The key property of this result is that the excess~$\excess\surrogateweighted$ is jointly convex w.r.t.\ the parameters~$\scoreparamv$ and conditional distribution~$\qv$, which simplifies further analysis.

Lemma~\ref{th:breakingSymmetries} allows to cope with the combinatorial aspect of the computation of the calibration function.
In particular, when the excess of the weighted surrogate risk is convex, Lemma~\ref{th:breakingSymmetries} reduces the computation of the calibration function to a set of convex optimization problems, which often can be solved analytically.
For symmetric losses, such as the 0-1, block 0-1 and Hamming losses, Lemma~\ref{th:breakingSymmetries} also provides ``symmetry breaking'', meaning that many of the obtained convex optimization problems are identical up to a permutation of labels.
\begin{lemma}
    \label{th:quadSurrogateExcess}
    Consider the quadratic surrogate $\surrogatelossquad$~\eqref{eq:quadrLoss} defined for a task loss~$\lossmatrix$.
    Let a subspace of scores~$\scoresubset \subseteq \R^\outputvarcard$ be parametrized by $\scoreparamv\in \R^\scoresubspacedim$, i.e., $\scorev = \scorematrix\scoreparamv \in \scoresubset$ with $\scorematrix \in \R^{\outputvarcard \times \scoresubspacedim}$, and assume that $\colspace(\lossmatrix) \subseteq \scoresubset$.
    Then, the excess of the weighted surrogate loss can be expressed as
    \begin{equation*}
%
    \excess\surrogateweightedquad(\scorematrix\scoreparamv, \qv) := 
    \surrogateweightedquad(\scorematrix\scoreparamv, \qv)-\inf_{\scoreparamv' \in \R^\scoresubspacedim} \surrogateweightedquad(\scorematrix\scoreparamv', \qv) =
    \frac{1}{2\outputvarcard}\| \scorematrix\scoreparamv + \lossmatrix \qv \|_2^2.
    \end{equation*}
\end{lemma}
\begin{proof}
    By using the definition of the quadratic surrogate~$\surrogatelossquad$~\eqref{eq:quadrLoss}, we have
    \begin{align*}
    \surrogateweighted(\scorev(\scoreparamv), \qv) &= \frac{1}{2\outputvarcard}(\scoreparamv^\transpose \scorematrix^\transpose \scorematrix \scoreparamv + 2 \scoreparamv^\transpose \scorematrix^\transpose \lossmatrix \qv) + r(\qv), \\
    \scoreparamv^* &:= \argmin\nolimits_\scoreparamv \surrogateweighted(\scorev(\scoreparamv), \qv) =  -(\scorematrix^\transpose \scorematrix)^\pinv \scorematrix^\transpose \lossmatrix \qv, \\
    \excess\surrogateweighted(\scorev(\scoreparamv), \qv) &= \frac{1}{2\outputvarcard}(\scoreparamv^\transpose \scorematrix^\transpose \scorematrix \scoreparamv + 2 \scoreparamv^\transpose \scorematrix^\transpose \lossmatrix \qv \\
    &\qquad\qquad\qquad+ \qv^\transpose \lossmatrix^\transpose \scorematrix (\scorematrix^\transpose \scorematrix)^\pinv \scorematrix^\transpose \lossmatrix \qv),
    \end{align*}
    where $r(\qv)$ denotes the quantity independent of parameters~$\scoreparamv$.
    Note that $\proj_{\scorematrix} := \scorematrix (\scorematrix^\transpose \scorematrix)^\pinv \scorematrix^\transpose$ is the orthogonal projection on the subspace~$\colspace(\scorematrix)$, so if $\colspace(\lossmatrix) \subseteq \colspace(\scorematrix)$ we have 
    $
    \proj_{\scorematrix} \lossmatrix = \lossmatrix,
    $
    which finishes the proof.
\end{proof}


\begin{lemma}
    \label{th:breakingSymmetries}
    In the case of a finite number~$\outputvarcard$ of labels, for any task loss~$\lossmatrix$, a surrogate loss~$\surrogateloss$ that is continuous and bounded from below, and a set of scores~$\scoresubset$, the calibration function can be written as 
    \begin{equation}
    \label{eq:calibrationfunc:decomposition}
    \calibrationfunc_{\surrogateloss,\lossmatrix,\scoresubset}(\eps) = \min_{\substack{i, j \in \predictor(\scoresubset) \\ i\neq j}} \calibrationfunc_{ij}(\eps),
    \end{equation}
    where the set $\predictor(\scoresubset) \subseteq \outputdomain$ is defined as the set of labels that the predictor can predict for some feasible scores and $\calibrationfunc_{ij}$ is defined via minimization of the same objective as~\eqref{eq:calibrationfunc}, but w.r.t.\ a smaller domain:
    \begin{align}
    \label{eq:calibrationfunc:generalBreakingSymmetries}
    \calibrationfunc_{ij}(\eps)
    =
    \inf_{\scorev, \qv} \;& \excess\surrogateweighted(\scorev, \qv), \\
    \notag %
    \text{s.t.} \;& \lossweighted_i(\qv) \leq\lossweighted_j(\qv) - \eps,\\
    \notag %
    &\lossweighted_i(\qv) \leq \lossweighted_c(\qv),\;\; \forall c \in \predictor(\scoresubset), \\
    \notag %
    &\score_j \geq \score_c,\;\:\: \forall c \in \predictor(\scoresubset), \\
    \notag %
    & \scorev \in \scoresubset,\\
    \notag
    & \qv \in \simplex_\outputvarcard.
    \end{align}
    Here $\lossweighted_c(\qv) := (\lossmatrix \qv)_c$ is the expected loss if predicting label~$c$.
    Index~$i$ represents a label with the smallest expected loss while index~$j$ represents a label with the largest score.
\end{lemma}
\begin{proof}
    We use the notation $\scoresubset_j$ to define the set of score vectors~$\scorev$ where the predictor $\predictor(\scorev)$ takes a value~$j$, i.e., $\scoresubset_j := \{\scorev \in \scoresubset \mid \predictor(\scorev) = j\}$.
    The union of the sets $\scoresubset_j$, $j \in \predictor(\scoresubset)$, equals the whole set~$\scoresubset$.
    It is possible that sets $\scoresubset_j$ do not fully contain their boundary because of the usage of a particular tie-breaking strategy, but their closure can be expressed as $\overline{\scoresubset}_j := \{\scorev \in \scoresubset \mid \score_j \geq  \score_c, \forall c \in \predictor(\scoresubset)\}$.
    
    If $\scorev \in \scoresubset_j$, i.e. $j=\predictor(\scorev)$, then the feasible set of  probability vectors~$\qv$ for which a label~$i$ is one of the best possible predictions (i.e. $\excess\lossweighted(\scorev, \qv) = \ell_j(\qv) - \ell_i(\qv)  \geq \eps$) is $$\simplex_{\outputvarcard,i,j,\eps} := \{ \qv \in \simplex_{\outputvarcard} \mid \lossweighted_i(\qv) \leq \lossweighted_c(\qv), \forall c \in \predictor(\scoresubset); \ell_j(\qv) - \ell_i(\qv) \geq \eps\},$$ because $\inf_{\scorev' \in \scoresubset} \lossweighted(\scorev', \qv) = \min_{c \in \predictor(\scoresubset)} \lossweighted_c(\qv)$.
    
    The union of the sets~$\{\scoresubset_j \times \simplex_{\outputvarcard,i,j,\eps}\}_{i, j \in \predictor(\scoresubset)}$ thus exactly equals the feasibility set of the optimization problem~\eqref{eq:calibrationfunc}-\eqref{eq:calibrationfunc:epsConstr} (note that this is not true for the union of the sets~$\{\overline{\scoresubset}_j \times \simplex_{\outputvarcard,i,j,\eps}\}_{i, j \in \predictor(\scoresubset)}$, which can be strictly larger), thus we can rewrite the definition of the calibration function as follows:
    \begin{equation}
    \label{eq:calibrationfunc:proof}
    \calibrationfunc_{\surrogateloss,\lossmatrix,\scoresubset}(\eps) = \min_{\substack{i, j \in \predictor(\scoresubset) \\ i\neq j}} \inf_{\substack{\scorev \in \scoresubset_j,\\ \qv \in \simplex_{\outputvarcard,i,j,\eps}}} \excess\surrogateweighted(\scorev, \qv).
    \end{equation}
    To finish the proof, we use Lemma~27 of~\citep{zhang04} claiming that the function~$\excess\surrogateweighted(\scorev, \qv)$ is continuous w.r.t.\ both~$\qv$ and~$\scorev$, which allows us to substitute sets $\scoresubset_j$ in~\eqref{eq:calibrationfunc:proof} with their closures~$\overline{\scoresubset}_j$ without changing the value of the infimum.
%
%
%
%
%
%
%
%
%
%
\end{proof}


\section{Consistent surrogate losses}
\label{sec:specialLosses}
An ideal surrogate should not only be consistent, but also allow efficient optimization, by, e.g., being convex and allowing fast computation of stochastic gradients.
In this paper, we study a generalization to arbitrary multi-class losses of a surrogate loss class from~\citet[Section~4.4.2]{zhang04}\footnote{\citet{zhang04} refers to this surrogate as ``decoupled unconstrained background discriminative surrogate''. Note the~$\nicefrac{1}{\outputvarcard}$ scaling to make $\surrogateloss_{a,b}$ of order~$1$.}
that satisfies these requirements:
\begin{equation}
\label{eq:abLoss}
\surrogateloss_{a,b}( \scorev, \outputvarv  )
:=
\frac{1}{\outputvarcard}\sum\nolimits_{c = 1}^\outputvarcard \bigl( \lossmatrix(c, \outputvarv) a( \score_c ) + b( \score_c ) \bigr),
\end{equation}
where~$a, b : \R \to \R$ are convex functions.
A generic method to minimize this surrogate is to use any version of the SGD algorithm, while computing the stochastic gradient by sampling~$\outputvarv$ from the data generating distribution and a label~$c$ uniformly.
In the case of the quadratic surrogate~$\surrogatelossquad$, we proposed instead in the main paper to compute the sum over~$c$ analytically instead of sampling~$c$.

Extending the argument from~\citet{zhang04}, we show that the surrogates of the form~\eqref{eq:abLoss} are consistent w.r.t.\ a task loss~$\lossmatrix$ under some sufficient assumptions formalized in Theorem~\ref{th:abConsist}.
\begin{theorem}[Sufficient conditions for consistency]
    \label{th:abConsist}
    The surrogate loss~$\surrogateloss_{a,b}$ is consistent w.r.t.\ a task loss~$\lossmatrix$, i.e., $\calibrationfunc_{\surrogateloss_{a,b},\lossmatrix,\R^\outputvarcard}(\eps) > 0$ for any $\eps > 0$, under the following conditions on the functions~$a(\score)$ and $b(\score)$:
    \begin{enumerate}[noitemsep,topsep=-2mm]
        \item\label{item:thCond:smooth} The functions~$a$ and $b$ are convex and differentiable.
        \item\label{item:thCond:bounded} The function $ca(f) + b(f)$ is bounded from below and has a unique global minimizer (finite or infinite) for all $c \in [0, \Lmax]$.
        %
        \item\label{item:thCond:second}  The functions~$a(f)$ and~$\frac{b'(f)}{a'(f)}$ are strictly increasing.
    \end{enumerate}
\end{theorem}
\begin{proof}
    Consider an arbitrary conditional probability vector~$\qv \in \simplex_k$.
    Assumption~\ref{item:thCond:bounded} then implies that the global minimizer~$\scorev^*$ of the conditional surrogate risk~$\surrogateweighted(\scorev, \qv)$ w.r.t.\ $\scorev$ is unique.
    Assumption~\ref{item:thCond:smooth} allows us to set the derivatives to zero and obtain~$\frac{b'(f_c^*)}{a'(f_c^*)} = - \lossweighted_c(\qv)$ where $\lossweighted_c(\qv) := (\lossmatrix \qv)_c$.
    Assumption~\ref{item:thCond:second} then implies that~$f_j^* \geq f_i^*$ holds if and only if $\lossweighted_j(\qv) \leq \lossweighted_i(\qv)$.
    
    Now, we will prove by contradiction that $\calibrationfunc(\eps) := \calibrationfunc_{\surrogateloss_{a,b},\lossmatrix,\R^\outputvarcard}(\eps) > 0$ for any $\eps > 0$.
    Assume that for some~$\eps > 0$ we have $\calibrationfunc(\eps) = 0$.
    Lemma~\ref{th:breakingSymmetries} then implies that for some $i, j \in \outputdomain$, $i \neq j$, we have $\calibrationfunc_{ij}(\eps) = 0$.
    Note that the domain of~\eqref{eq:calibrationfunc:generalBreakingSymmetries} defining $\calibrationfunc_{ij}$ is separable w.r.t.\ $\qv$ and $\scorev$.
    We can now rewrite~\eqref{eq:calibrationfunc:generalBreakingSymmetries} as
    \[
    \calibrationfunc_{ij}(\eps)
    =\!\!\!
    \inf_{\qv \in \simplex_{\outputvarcard,i,j,\eps}}\!\!\! \excess\surrogateweighted^*(\qv), \;
    \text{where}\; \excess\surrogateweighted^*(\qv) := \!\inf_{\scorev \in \overline{\scoresubset}_j}\! \excess\surrogateweighted(\scorev,\qv),
    \]
    where $\simplex_{\outputvarcard,i,j,\eps}$ and $\overline{\scoresubset}_j$ are defined in the proof of Lemma~\ref{th:breakingSymmetries}.
    Lemma~27 of~\citep{zhang04} implies that the function~$\excess\surrogateweighted^*(\qv)$ is a continuous function of~$\qv$.
    Given that~$\simplex_{\outputvarcard,i,j,\eps}$ is a compact set, the infimum is achieved at some point~$\qv^* \in \simplex_{\outputvarcard,i,j,\eps}$.
    For this~$\qv^*$, the global minimum w.r.t.\ $\scorev$ exists (Assumption~\ref{item:thCond:bounded}).
    The uniqueness of the global minimum implies that we have~$\score_{j}^* = \max_{c \in \outputdomain} \score_c^*$.
    The argument at the beginning of this proof then implies $\lossweighted_j(\qv^*) \leq \lossweighted_i(\qv^*)$ which contradicts the inequality $\lossweighted_i(\qv^*) \leq \lossweighted_j(\qv^*) - \eps$ in the definition of~$\simplex_{\outputvarcard,i,j,\eps}$.
\end{proof}
%
%
%
%
%
%
%
%

Note that Theorem~\ref{th:abConsist} actually proves that the surrogate~$\surrogateloss_{a,b}$ is order-preserving~\citep{zhang04}, which is a stronger property than consistency.


Below, we give several examples of possible functions $a(\score)$, $b(\score)$ that satisfy the conditions in Theorem~\ref{th:abConsist} and their corresponding~$\score^*(\lossweighted):=\argmin_{\scorev \in \R^\outputvarcard}  \surrogateweighted(\scorev, \qv)$ when $\lossweighted := \lossmatrix \qv$:
\begin{enumerate} %
    \item If $a(\score) = \score$, $b(\score) = \frac{\score^2}{2}$ then~$\score^*(\lossweighted) = -\lossweighted$, leading to our quadratic surrogate~\eqref{eq:quadrLoss}.
    \item If $a(\score) = \frac{1}{\Lmax}(\exp(\score) - \exp(-\score))$, $b(\score) = \exp(-\score)$ then~$\score^*(\lossweighted) = \frac12\log(1-\frac{1}{\Lmax}\lossweighted) - \frac12\log(\frac{1}{\Lmax}\lossweighted)$.
    \item If $a(\score) = \frac{1}{\Lmax}\score$, $b(\score) = \log(1 + \exp(-\score))$ then~$\score^*(\lossweighted) = \log(1-\frac{1}{\Lmax}\lossweighted) - \log(\frac{1}{\Lmax}\lossweighted)$.
\end{enumerate}

In the case of binary classification, these surrogates reduce to $\ltwonorm$-, exponential, and logistic losses, respectively.

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%


\section{Bounds on the calibration function}
\label{sec:boundscalibrationFunction}

\subsection{Lower bound}
\label{sec:boundscalibrationFunction:lower}
\begin{reptheorem}{th:lowerBoundcalibrationFunction}[Lower bound on $\calibrationfunc_{\surrogatelossquad}$]
    \label{th:rep:lowerBoundcalibrationFunction}
     For any task loss~$\lossmatrix$, its quadratic surrogate~$\surrogatelossquad$, and a score subspace~$\scoresubset$ containing the column space of $\lossmatrix$, the calibration function can be lower bounded:
     \begin{equation*}
%
     \calibrationfunc_{\surrogatelossquad,\lossmatrix,\scoresubset}(\eps)
     \geq
     \frac{\eps^2}{2\outputvarcard \max_{i\neq j}\|\proj_{\scoresubset} \Delta_{ij}\|_2^2}
     \geq 
     \frac{\eps^2}{4\outputvarcard},
     \end{equation*}
     where $\proj_{\scoresubset}$ is the orthogonal projection on the subspace~$\scoresubset$ and $\Delta_{ij} = \unit_i - \unit_j \in \R^k$ with $\unit_c$ being the $c$-th basis vector of the standard basis in~$\R^\outputvarcard$.
\end{reptheorem}
\begin{proof}
    First, let us assume that the score subspace $\scoresubset$ is defined as the column space of a matrix~$\scorematrix \in \R^{\outputvarcard \times \scoresubspacedim}$, i.e., $\scorev(\scoreparamv)=\scorematrix\scoreparamv$.
    Lemma~\ref{th:quadSurrogateExcess} gives us expression~\eqref{eq:quadSurrogateExcessSimple} for~$\excess\surrogateweightedquad(\scorematrix\scoreparamv, \qv)$, which is jointly convex w.r.t.\ a conditional probability vector~$\qv$ and parameters~$\scoreparamv$.
    
    The optimization problem~\eqref{eq:calibrationfunc}-\eqref{eq:calibrationfunc:epsConstr} is non-convex because the constraint~\eqref{eq:calibrationfunc:epsConstr} on the excess risk depends of the predictor function~$\predictor(\scorev)$, see Eq.~\eqref{eq:predictor}, containing the~$\argmax$ operation.
    However, if we constrain the predictor to output label $j$, i.e., $\score_j \geq \score_c$, $\forall c$, and the label delivering the smallest possible expected loss to be $i$, i.e., $(\lossmatrix \qv)_i \leq (\lossmatrix \qv)_c$, $\forall c$, the problem becomes convex because all the constraints are linear and the objective is convex.
     Lemma~\ref{th:breakingSymmetries} in App.~\ref{sec:techLemmas} allows to bound the calibration function with the minimization w.r.t.\ selected labels $i$ and $j$, $\calibrationfunc_{\surrogatelossquad,\lossmatrix,\scoresubset}(\eps) \geq \min\limits_{i \neq j} \calibrationfunc_{ij}(\eps)$,\footnote{To simplify the statement of Theorem~\ref{th:lowerBoundcalibrationFunction}, we removed the constraints~$i,j \in \predictor(\scoresubset)$ from Lemma~\ref{th:breakingSymmetries} which said that we should consider only the labels that can be predicted with some feasible scores. A potentially tighter lower bound can be obtained by keeping the~$i,j \in \predictor(\scoresubset)$ constraint.} where~$\calibrationfunc_{ij}(\eps)$ is defined as follows:
    \begin{align}
    \label{eq:calibrationfunc:breakingSymmetries}
    \calibrationfunc_{ij}(\eps)
    =
    \min_{\scoreparamv, \qv} \;& \frac{1}{2 \outputvarcard}\|\scorematrix \scoreparamv + \lossmatrix \qv\|_2^2, \\
    \notag %
    \text{s.t.} \;& (\lossmatrix \qv)_i \leq (\lossmatrix \qv)_j - \eps,\\
    \notag %
    &(\lossmatrix \qv)_i \leq (\lossmatrix \qv)_c,\;\; \forall c \in \predictor(\scoresubset) \\
    \notag %
    &(\scorematrix \scoreparamv)_j \geq (\scorematrix \scoreparamv)_c,\; \forall c \in \predictor(\scoresubset) \\
    \notag %
    & \qv \in \simplex_\outputvarcard.
    \end{align}
    
%
%
%
%
%
%
%
%
%
%
    
    To obtain a lower bound, we relax~\eqref{eq:calibrationfunc:breakingSymmetries} by removing some of the constraints and arrive at
    \begin{align}
    \label{eq:calibrationfunc:breakingSymmetries:stripped}
    \calibrationfunc_{ij}(\eps)
    \geq
    \min_{\scoreparamv, \qv} \;& \frac{1}{2 \outputvarcard}\|\scorematrix \scoreparamv + \lossmatrix \qv\|_2^2, \\
    \label{eq:calibrationfunc:breakingSymmetries:stripped:epsConstr}
    \text{s.t.} \;& \Delta_{ij}^\transpose \lossmatrix \qv \leq - \eps,\\
    \label{eq:calibrationfunc:breakingSymmetries:fConstr}
    &\Delta_{ij}^\transpose \scorematrix \scoreparamv \leq 0,
    \end{align}
    where 
    $\Delta_{ij}^\transpose \lossmatrix \qv = (\lossmatrix \qv)_i - (\lossmatrix \qv)_j$,
    $\Delta_{ij}^\transpose \scorematrix \scoreparamv = (\scorematrix \scoreparamv)_i - (\scorematrix \scoreparamv)_j$, 
    and $\Delta_{ij} = \unit_i - \unit_j \in \R^k$ with $\unit_c \in \R^k$ being a vector of all zeros with $1$ at position~$c$.
%
    
    The constraint~\eqref{eq:calibrationfunc:breakingSymmetries:stripped:epsConstr} can be readily substituted with equality
    \begin{equation}
    \label{eq:calibrationfunc:breakingSymmetries:stripped:epsConstr:equality}
    \Delta_{ij}^\transpose \lossmatrix \qv = - \eps,
    \end{equation}
    without changing the minimum because multiplication of both~$\qv$ and~$\scoreparamv$ by the constant $\frac{-\eps}{\Delta_{ij}^\transpose \lossmatrix \qv} \in (0,1]$ preserves feasibility and can only decrease the objective~\eqref{eq:calibrationfunc:breakingSymmetries:stripped}.
    
    We now explicitly solve the resulting constraint optimization problem via the KKT optimality conditions.
    The stationarity constraints give us
    \begin{align}
    \label{eq:calibrationfunc:proof:KKT:dtheta}
    \frac{1}{\outputvarcard} \scorematrix^\transpose (\scorematrix \scoreparamv + \lossmatrix \qv) + \mu \scorematrix^\transpose \Delta_{ij} = 0,\\
    \label{eq:calibrationfunc:proof:KKT:dq}
    \frac{1}{\outputvarcard} \lossmatrix^\transpose (\scorematrix \scoreparamv + \lossmatrix \qv) + \nu \lossmatrix^\transpose \Delta_{ij} = 0;
    \end{align}
    the complementary slackness gives $\mu \Delta_{ij}^\transpose \scorematrix \scoreparamv = 0$ and the feasibility constraints give~\eqref{eq:calibrationfunc:breakingSymmetries:stripped:epsConstr:equality}, \eqref{eq:calibrationfunc:breakingSymmetries:fConstr}, and $\mu \geq 0$.
    
    Equation~\eqref{eq:calibrationfunc:proof:KKT:dtheta} allows to compute
    \begin{equation}
    \label{eq:calibrationfunc:proof:theta}
    \scoreparamv = -(\scorematrix^\transpose \scorematrix)^\pinv(\outputvarcard \mu \scorematrix^\transpose \Delta_{ij} + \scorematrix^\transpose \lossmatrix \qv).
    \end{equation}
    By substituting \eqref{eq:calibrationfunc:proof:theta} into~\eqref{eq:calibrationfunc:proof:KKT:dq} and by using the identity (because $\lossmatrix \in \colspace(\scorematrix)$):
    \begin{equation}
    \label{eq:ProjFL}
    \proj_{\scorematrix} \lossmatrix = \scorematrix (\scorematrix^\transpose \scorematrix)^\pinv \scorematrix^\transpose \lossmatrix = \lossmatrix,
    \end{equation}
    we get
    $
    (\mu -\nu) \lossmatrix^\transpose \Delta_{ij} = 0.
    $
    If $\lossmatrix^\transpose \Delta_{ij} = 0$, the problem~\eqref{eq:calibrationfunc:breakingSymmetries:stripped}, \eqref{eq:calibrationfunc:breakingSymmetries:fConstr}, \eqref{eq:calibrationfunc:breakingSymmetries:stripped:epsConstr:equality} is infeasible for $\eps > 0$ implying $\calibrationfunc_{ij}(\eps) = +\infty$. Otherwise, we have $\mu = \nu$.
    
    From~\eqref{eq:calibrationfunc:proof:theta} and~\eqref{eq:ProjFL}, we also have that:
    \begin{equation}
    \label{eq:LowerBoundObjectiveVector}
    \scorematrix \scoreparamv + \lossmatrix \qv = -\outputvarcard \mu \proj_\scorematrix \Delta_{ij} .
    \end{equation}
    
    By plugging~\eqref{eq:calibrationfunc:proof:theta} into the complementary slackness condition and combining with~\eqref{eq:calibrationfunc:breakingSymmetries:stripped:epsConstr:equality}, we get
    \[
    \mu^2 \outputvarcard \| \proj_\scorematrix \Delta_{ij} \|_2^2 = \mu \eps
    \]
    implying that either $\mu = 0$ or $\mu \outputvarcard \| \proj_\scorematrix \Delta_{ij} \|_2^2 = \eps$.
    In the first case, Eq.~\eqref{eq:LowerBoundObjectiveVector} implies $\scorematrix \scoreparamv = -\lossmatrix \qv$ making satisfying both \eqref{eq:calibrationfunc:breakingSymmetries:stripped:epsConstr:equality} and~\eqref{eq:calibrationfunc:breakingSymmetries:fConstr} impossible. Thus, the later is satisfied implying that the objective~\eqref{eq:calibrationfunc:breakingSymmetries:stripped} is equal to\footnote{The possibility $\proj_\scorematrix \Delta_{ij}=0$ is also covered by this equation with the convention that $1/0=\infty$ (in this case, $\mu^*=\infty$).}
    \[
    \frac{1}{2 \outputvarcard}\|\scorematrix \scoreparamv + \lossmatrix \qv\|_2^2
    =
    \frac{\eps^2}{2 \outputvarcard \| \proj_\scorematrix \Delta_{ij} \|_2^2}.
    \]
    Finally, orthogonal projections contract the $\ltwonorm$-norm, thus $\|\proj_\scorematrix \Delta_{ij}\|_2^2 \leq 2$, which gives the second lower bound in the statement of the theorem and finishes the proof.
\end{proof}
 
\subsection{Upper bound}
\label{sec:boundscalibrationFunction:upper}
\begin{reptheorem}{th:upperBoundcalibrationFunction}[Upper bound on $\calibrationfunc_{\surrogatelossquad}$]
    \label{th:rep:upperBoundcalibrationFunction}
    If a loss matrix~$\lossmatrix$ with $\Lmax > 0$ defines a pseudometric\footnoteref{footnote:pseudometric} on labels and there are no constraints on the scores, i.e., $\scoresubset = \R^{\outputvarcard}$, then the calibration function for the quadratic surrogate~$\surrogatelossquad$ can be upper bounded:
    \begin{equation*}
    %
    \calibrationfunc_{\surrogatelossquad,\lossmatrix,\scoresubset}(\eps)
    \leq  \frac{\eps^2}{2\outputvarcard}, \quad 0 \leq \eps \leq \Lmax.
    \end{equation*}
\end{reptheorem}
\begin{proof}
    After applying Lemmas~\ref{th:quadSurrogateExcess} and~\ref{th:breakingSymmetries}, we arrive at
    \begin{align}
    \label{eq:upperBoundcalibrationFunction:breakingSymmetries}
    \calibrationfunc_{ij}(\eps)
    =
    \inf_{\scorev, \qv} \;& \frac{1}{2\outputvarcard} \| \scorev + \lossmatrix\qv \|^2, \\
    \notag %
    \text{s.t.} \;& \lossweighted_i(\qv) \leq\lossweighted_j(\qv) - \eps,\\
    \notag %
    &\lossweighted_i(\qv) \leq \lossweighted_c(\qv),\;\; \forall c \in \outputdomain, \\
    \notag %
    &\score_j \geq \score_c,\;\:\: \forall c \in \outputdomain, \\
    \notag %
    & \scorev \in \R^\outputvarcard, \quad \qv \in \simplex_\outputvarcard.
    \end{align}
    We now consider labels $i$ and $j$ such that $\lossmatrix_{ij}=\Lmax > 0$ and the point $\q_i = \frac12 + \frac{\eps}{2\lossmatrix_{ij}}$, $\q_j = \frac12 - \frac{\eps}{2\lossmatrix_{ij}}$ (non-negative for $\eps \leq \Lmax$). We let $\q_c = 0$ for $c \not\in \{i,j\}$, $\score_j = \score_i = -\lossweighted_i(\qv)$ and $\score_c = -\lossweighted_c(\qv)$ for $c \not\in \{i,j\}$. We now show that this assignment is feasible. 
    
    We have $\lossweighted_j(\qv) = \q_i \lossmatrix_{ji} + \q_j \lossmatrix_{jj}=\q_i \lossmatrix_{ji} = \q_i \lossmatrix_{ij}$ by symmetry of $\lossmatrix$. Similarly, $\lossweighted_i(\qv) = \q_j \lossmatrix_{ij}$ and thus
    \[
    \lossweighted_j(\qv) - \lossweighted_i(\qv) = \lossmatrix_{ij} \frac{\eps}{\lossmatrix_{ij}} = \eps.
    \]
    We also have 
    \begin{equation*}
    \lossweighted_c(\qv) - \lossweighted_i(\qv) = \q_i \lossmatrix_{ci} + \q_j \lossmatrix_{cj} - \q_j \lossmatrix_{ij} \geq  \q_j (\lossmatrix_{ic} + \lossmatrix_{cj} - \lossmatrix_{ij}) \geq 0.
    \end{equation*}
    The first inequality uses  $\q_i \geq \q_j$ and the second inequality uses the fact that $\lossmatrix$ satisfies the triangle inequality (as a pseudometric).
    Finally, $\score_j - \score_c = - \lossweighted_i(\qv) + \lossweighted_c(\qv) \geq 0$.
    
    We thus have shown that the defined point is feasible, so we compute its objective value.
    We have
    \[
    \frac{1}{2\outputvarcard} \| \scorev + \lossmatrix\qv \|^2 = \frac{1}{2\outputvarcard}(\lossweighted_j(\qv) - \lossweighted_i(\qv))^2 = \frac{\eps^2}{2\outputvarcard},
    \]
    which completes the proof.
\end{proof}

\subsection{Computation of the lower bounds for specific task losses}
\label{sec:bounds:interpretations}
\textbf{0-1 loss.}
Let $\lossmatrix_{01}$ denote the loss matrix of the 0-1 loss, i.e., $\lossmatrix_{01}(i, j) := [i \neq j]$.\footnoteref{footnote:Iverson}
It is convenient to rewrite it with a matrix notation $\lossmatrix_{01} = \one_\outputvarcard \one_\outputvarcard^\transpose - \id_\outputvarcard$, where $\one_\outputvarcard \in \R^\outputvarcard$ is the vector of all ones and $\id_\outputvarcard \in \R^{\outputvarcard\times\outputvarcard}$ is the identity matrix.
We have $\rank(\lossmatrix_{01}) = \outputvarcard$ (for $\outputvarcard \geq 2$), thus $\colspace(\lossmatrix) = \R^\outputvarcard$.
By putting no constraints on the scores, we can easily apply Theorem~\ref{th:lowerBoundcalibrationFunction} and obtain the lower bound of~$\frac{\eps^2}{4\outputvarcard}$, which is shown to be tight in Proposition~\ref{th:calibrationFunction:01loss} of Section~\ref{sec:exactTranferProofs:01loss}.

\textbf{Block 0-1 loss.}
We use the symbol $\lossmatrix_{01,\numblocks}$ to denote the loss matrix of the block 0-1 loss with $\numblocks$~blocks, i.e., $\lossmatrix_{01,\numblocks}(i, j) := [\text{$i$ and $j$ are not in the same block}]$.
We use $\blocksize_v$ to denote the size of block~$v$, $v=1,...,\numblocks$, and then $\blocksize_1 +\dots + \blocksize_\numblocks = \outputvarcard$.
In the case when all the blocks are of equal sizes, we denote their size by~$\blocksize$ and have~$\outputvarcard = \numblocks \blocksize$.

With a matrix notation, we have $\lossmatrix_{01,\numblocks} = \one_\outputvarcard \one_\outputvarcard^\transpose - UU^\transpose$ where the columns of the matrix $U\in \R^{\outputvarcard \times \numblocks}$ are indicators of the blocks.
We have $\rank(\lossmatrix_{01,\numblocks}) = \numblocks$ and can simply define $\scoresubset_{01,\numblocks} := \colspace(\scorematrix_{01,\numblocks})$ with  $\scorematrix_{01,\numblocks} := U$.
If we assume that all the blocks have equal size, then we have $U^\transpose U = \blocksize \id_\numblocks$ and $\|\proj_{\scoresubset_{01,\numblocks}}\Delta_{ij}\|_2^2=\frac{2}{\blocksize}$ if labels~$i$ and~$j$ belong to different blocks, while $\proj_{\scoresubset_{01,\numblocks}}\Delta_{ij}=0$ if $i$ and~$j$ belong to the same block.  This leads to the lower bound~$\frac{\eps^2}{4\numblocks}$, which is shown to be tight in Proposition~\ref{th:calibrationFunction:block01loss:hardConstr} of Section~\ref{sec:exactTranferProofs:block01loss}.

\textbf{Hamming loss.}
Consider the (normalized) Hamming loss between tuples of $\hamminglen$~binary variables, where~$\hat{\outputvar}_\hammingindex$ and~$\outputvar_\hammingindex$ are the $\hammingindex$-th variables of a prediction~$\hat{\outputvarv}$ and a correct label~$\outputvarv$, respectively:
\begin{align}
\label{eq:hammingLoss}
\lossmatrix_{\hamming,\hamminglen}(\hat{\outputvarv}, \outputvarv)
&:=
\frac{1}{\hamminglen}\sum\nolimits_{\hammingindex=1}^\hamminglen [\hat{\outputvar}_\hammingindex \neq \outputvar_\hammingindex]
\\
\notag &=
\frac{1}{\hamminglen}\sum\nolimits_{\hammingindex=1}^\hamminglen ([\hat{\outputvar}_\hammingindex = 0] [\outputvar_\hammingindex = 1] + [\hat{\outputvar}_\hammingindex = 1] [\outputvar_\hammingindex = 0])
\\
\notag &=
\frac{1}{\hamminglen}\sum\nolimits_{\hammingindex=1}^\hamminglen (1-[\hat{\outputvar}_\hammingindex = 1]) [\outputvar_\hammingindex = 1] + [\hat{\outputvar}_\hammingindex = 1] [\outputvar_\hammingindex = 0])
\\
\notag &=
\frac{1}{\hamminglen}\sum\nolimits_{\hammingindex=1}^\hamminglen  [\outputvar_\hammingindex = 1] +  
\frac{1}{\hamminglen}\sum\nolimits_{\hammingindex=1}^\hamminglen   \left([\outputvar_\hammingindex = 0]-[\outputvar_\hammingindex = 1]\right) \,\, [\hat{\outputvar}_\hammingindex = 1]
\\
\notag &=
\alpha_0(\outputvarv) + \sum\nolimits_{\hammingindex=1}^\hamminglen \alpha_\hammingindex(\outputvarv) [\hat{\outputvar}_\hammingindex = 1],
\end{align}
The vectors $\alphav_\hammingindex(\cdot)$ depend only on the column index of the loss matrix.
The decomposition~\eqref{eq:hammingLoss} implies that 
$
\scoresubset_{\hamming,\hamminglen}
:=
\colspace(\scorematrix_{\hamming,\hamminglen})
$
equals to
$
\colspace(\lossmatrix_{\hamming,\hamminglen})
$
for
$
\scorematrix_{\hamming,\hamminglen} := [ \frac12 \one_{2^\hamminglen}, \hv^{(1)}, \dots, \hv^{(\hamminglen)} ],
$
$
(\hv^{(\hammingindex)})_{\hat{\outputvarv}} := [\hat{\outputvar}_\hammingindex = 1],
$
$\hammingindex = 1,\dots,\hamminglen$.
We also have that $\rank(\lossmatrix_{\hamming,\hamminglen}) = \rank(\scorematrix_{\hamming,\hamminglen}) = \hamminglen + 1$.

In Section~\ref{sec:hammingLossProps}, we show that $\max_{i\neq j}\|\proj_{\scoresubset_{\hamming,\hamminglen}} \Delta_{ij}\|_2^2 = \frac{4\hamminglen}{2^\hamminglen}$.
By plugging this identity into the lower bound~$\eqref{eq:lowerBoundcalibrationFunction}$, we get $\calibrationfunc_{\surrogatelossquad,\lossmatrix_{\hamming,\hamminglen},\scoresubset_{\hamming,\hamminglen}} \geq \frac{\epsilon^2}{8\hamminglen}$, which appears to be tight according to  Proposition~\ref{th:calibrationFunction:hammingLoss:hardConstr} of Section~\ref{sec:exactTranferProofs:hamming}.

\textbf{Non-tight cases.}
In the cases of the block 0-1 loss and the mixed 0-1 and block 0-1 loss   (Propositions~\ref{th:calibrationFunction:block01loss} and~\ref{th:calibrationFunction:mixedLoss}, respectively), we observe gaps between the lower bound~$\eqref{eq:lowerBoundcalibrationFunction}$ and the exact calibration functions, which shows the limitations of the bound.
In particular, it cannot detect level-$\consbreakpoint$ consistency for $\consbreakpoint > 0$ (see Definition~\ref{def:consistency}) and does not change when the loss changes, but the score subspace stays the same.


%
%


\section{Exact calibration functions for quadratic surrogate}
\label{sec:exactTranferProofs}
This section presents our derivations for the exact values of the calibration functions for different losses.
While doing these derivations, we have used numerical simulations and symbolic derivations to check for correctness.
Our numerical and symbolic tools are available online.\footnote{\url{https://github.com/aosokin/consistentSurrogates_derivations}}

\subsection{0-1 loss}
\label{sec:exactTranferProofs:01loss}
\begin{proposition}
    \label{th:calibrationFunction:01loss}
    Let $\lossmatrix_{01}$ be the 0-1 loss, i.e., $\lossmatrix_{01}(i, j) = [i \neq j]$.
    Then, the calibration function equals the following quadratic function w.r.t.\ $\eps$:
    \begin{equation}
    \label{eq:calibrationFunction:01loss}
    \calibrationfunc_{\surrogatelossquad,\lossmatrix_{01},\R^k}(\eps) = \frac{\eps^2}{4\outputvarcard},
    \quad 0 \leq \eps \leq 1.
    \end{equation}
\end{proposition}
Note that in the case of binary classification, the function~\eqref{eq:calibrationFunction:01loss} is equal to the calibration function for the least squares and truncated least squares surrogates~\citep{bartlett06convexity,steinwart07}.

\begin{proof}
First, Lemma~\ref{th:quadSurrogateExcess} with $\scoresubset = \R^{\outputvarcard}$ and $\scorematrix = \id_\outputvarcard$ gives us the expression
\begin{equation}
\label{eq:quadSurrogateExcessSimple:noConstrainst}
\excess\surrogateweightedquad(\scorematrix\scoreparamv, \qv) = \frac{1}{2\outputvarcard}\|\scorev + \lossmatrix \qv \|_2^2,
\end{equation}
with $\scorev = \scoreparamv \in \R^{\outputvarcard}$.

We now reduce the optimization problem~\eqref{eq:calibrationfunc}-\eqref{eq:calibrationfunc:epsConstr} to a convex one by using Lemma~\ref{th:breakingSymmetries} and by writing 
$
\calibrationfunc_{\surrogatelossquad,\lossmatrix_{01},\R^k}(\eps) = \min_{i \neq j \in \outputdomain}\calibrationfunc_{ij}(\eps)
$, which holds because $\predictor(\R^k) = \outputdomain$.
%
%
Because of the symmetries of the 0-1 loss, all the choices of~$i$ and~$j$ give the same (up to a permutation of labels) optimization problem to compute~$\calibrationfunc_{ij}(\eps)$.
The definition of the 0-1 loss implies $(\lossmatrix \qv)_c = 1-q_c$, which simplifies the excess of the expected task loss appearing in~\eqref{eq:calibrationfunc:epsConstr} to
$
%
%
\excess\lossweighted(\scorev, \qv) = (\lossmatrix \qv)_j - (\lossmatrix \qv)_i = \q_i - \q_j.
%
$
After putting all these together, we get
\begin{align}
\label{eq:calibrationfunc:01loss:breakingSymmetries}
\calibrationfunc_{ij}(\eps)
=
\min_{\scorev, \qv} \;& \frac{1}{2 \outputvarcard} \sum_{c=1}^{\outputvarcard} (\score_c + 1 - \q_c)^2, \\
\notag %
\text{s.t.} \;& \q_i \geq \q_j + \eps,\\
\notag %
&\q_i \geq \q_c,\;\; c = 1,\dots,\outputvarcard, \\
\notag %
&\score_j \geq \score_c,\; c = 1,\dots,\outputvarcard, \\
\notag %
& \sum_{c=1}^{\outputvarcard} \q_c = 1, \quad \q_c \geq 0.
\end{align}
We claim that there exists an optimal point of~\eqref{eq:calibrationfunc:01loss:breakingSymmetries}, $\scorev^*$, $\qv^*$, such that $\q^*_c=0$, $c\not\in\{i, j\}$, $\q^*_i = \frac12 + \frac{\eps}{2}$,  $\q^*_j = \frac12 - \frac{\eps}{2}$; $\score^*_c = -1$, $c\not\in\{i, j\}$, $\score^*:= \score^*_i = \score^*_j$.
Note that apart from the specific value of $\score^*$, this is the same point used to prove the upper bound of Theorem~\ref{th:upperBoundcalibrationFunction}.
After proving this, we will minimize the objective w.r.t.\ remaining scores at this point.\footnote{Note that just showing the feasibility of the assigned values~$\qv^*$ and~$\scorev^*$ give us an upper bound on the calibration function.
In the case of the 0-1 loss, it appears that this upper bound matches the lower bound provided by Theorem~\ref{th:lowerBoundcalibrationFunction}, so we do not need to prove optimality explicitly.
However, we still give this proof as a simple illustration of the proof technique as its structure will be re-used also for the cases when the bound of Theorem~\ref{th:lowerBoundcalibrationFunction} is not tight.}

First, if any $\q^*_c=\delta > 0$, $c\not\in\{i, j\}$, we can safely move this probability mass to $\q_i$ and $\q_j$ with the operation
\begin{align*}
&\q^*_c := \q^*_c -\delta = 0, \; &&\q^*_i := \q^*_i +\frac{\delta}{2}, \; &&\q^*_j := \q^*_j +\frac{\delta}{2},\\
&\score^*_c := \score^*_c -\delta, \; &&\score^*_i := \score^*_i +\frac{\delta}{2}, \; &&\score^*_j := \score^*_j +\frac{\delta}{2},
\end{align*}
which keeps all the constraints of~\eqref{eq:calibrationfunc:01loss:breakingSymmetries} feasible and does not change the objective value.

Second, all the scores~$\score^*_c$ have to belong to the segment~$[-1, 0]$ otherwise clipping them will decrease the objective.
With this, setting $\score^*_c := -1$, $c\not\in\{i, j\}$ can only decrease the objective and will not violate the constraints.

We now show that the equality $\q^*_i = \q^*_j + \eps$ can hold at the optimum.
Indeed, if $\q^*_i - \q^*_j = \delta' >  \eps$, the operation
\begin{align}
\label{eq:tfproofs:truncatingToEps}
\q_i^* &:= \q_i^* - \frac{\delta'-\eps}{2}, &
\q_{j}^* &:= \q_{j}^* + \frac{\delta'-\eps}{2}, \\
\notag\score_i^* &:= \score_i^* - \frac{\delta'-\eps}{2}, &
\score_j^* &:= \score_j^* + \frac{\delta'-\eps}{2}.
\end{align}
keeps the objective the same and maintains the feasibility constraints. So combining with $\q^*_i + \q^*_j = 1$, we can now conclude that $\q^*_i = \frac12 + \frac{\eps}{2}$,  $\q^*_j = \frac12 - \frac{\eps}{2}$ is an optimal point.

We now show that the equality $\score^*_i = \score^*_j$ can hold at the optimum. First, we know that the values $\score^*_i$ and $\score^*_j$ belong to the segment $[\q^*_j-1,\q^*_i-1]$, otherwise we can always truncate the values to the borders of the segment and get an improvement of the objective. Finally, since the inequality $\score^*_j \geq \score^*_i$ must hold, we conclude that $\score^*_i = \score^*_j := \score^*$ so that $\score^*_i$ is closest to its target $\q^*_i-1$ to minimize the objective. 

At the optimal point defined above, it remains to find the value~$\score^*$ delivering the minimum of the objective.
We can achieve this by computing 
\[
\calibrationfunc_{ij}(\eps) = \frac{1}{2\outputvarcard} \min_{\score \in [-1, 0]}  (f +\frac12 -\frac{\eps}{2})^2 + (f +\frac12 +\frac{\eps}{2})^2,
\]
which implies $f^* = -0.5$ and $\calibrationfunc_{\surrogatelossquad,\lossmatrix_{01},\R^k}(\eps) = \frac{\eps^2}{4\outputvarcard}$.
\end{proof}

\textbf{Remark.} We note that the conditional distribution used in the proof above, $\q_i = \frac12 + \frac{\eps}{2}$,  $\q_j = \frac12 - \frac{\eps}{2}$, $\q_c = 0$, $c\not\in\{i, j\}$, is somewhat unsatisfying from the perspective of explaining why learning the 0-1 loss might be difficult. Indeed, it looks like a gradient based learning algorithm that would start with all values $\score_c = -1$ would at the end only optimize over $\score_i$ and $\score_j$ as the gradient with respect to $\score_c$ for $c \notin \{i,j\}$ would stay at zero in~$\surrogatelossquad( \scorev, \outputvarv )$~\eqref{eq:quadrLoss} given that only $i$ or $j$ could appear in $\outputvarv$. 
From this observation, one could think that the calibration function perspective is misleading as SGD could have faster convergence rate than predicted by the worst case for this situation.
Fortunately, one can easily check that the point $\q_i = \frac13 + \frac{\eps}{2}$,  $\q_j = \frac13 - \frac{\eps}{2}$, $\q_c = \frac{1}{3(k-2)}$ for  $c\not\in\{i, j\}$, $\score_i = \score_j = \frac{1}{3}$ and $\score_c = -\lossweighted_c(\qv)$ for $c\not\in\{i, j\}$ is feasible for~\eqref{eq:calibrationfunc:01loss:breakingSymmetries} and yields the same optimal value of $\frac{\eps^2}{4\outputvarcard}$ for the objective, thus providing another example where the exponential multiclass nature is more readily apparent and cannot be fixed by some ``natural initialization'' of the learning algorithm.

\subsection{Block 0-1 loss}
\label{sec:exactTranferProofs:block01loss}
Recall that $\lossmatrix_{01,\numblocks}$ is the block 0-1 loss, i.e., $\lossmatrix_{01,\numblocks}(i, j) = [\text{$i$ and $j$ are not in the same block}]$.
We use $\numblocks$ to denote the total number of blocks and $\blocksize_v$ to denote the size of block~$v$, $v=1,...,\numblocks$.
In this section, we compute the calibration functions for the case of unconstrained scores (Proposition~\ref{th:calibrationFunction:block01loss}) and for the case of the scores belonging to the column span of the loss matrix (Proposition~\ref{th:calibrationFunction:block01loss:hardConstr}).
\begin{proposition}
    \label{th:calibrationFunction:block01loss}
    Without constraints on the scores, the calibration function for the block 0-1 loss equals the following quadratic function w.r.t.\ $\eps$:
    \[
    \calibrationfunc_{\surrogatelossquad,\lossmatrix_{01,\numblocks},\R^k}(\eps) = \frac{\eps^2}{4\outputvarcard} \min_{v=1,...,\numblocks}\frac{2 \blocksize_v}{\blocksize_v + 1} \leq \frac{\eps^2}{2\outputvarcard},
    \quad 0 \leq \eps \leq 1.
    \]
\end{proposition}
Note that when $\blocksize_v = 1$ for some $v$, we have  $\calibrationfunc_{\surrogatelossquad,\lossmatrix_{01,\numblocks},\R^k}(\eps)$ matching to the $\frac{\eps^2}{4\outputvarcard}$ lower bound of Theorem~\ref{th:lowerBoundcalibrationFunction}.
When $\blocksize_v \to \infty$ for all blocks, we have $\calibrationfunc_{\surrogatelossquad,\lossmatrix_{01,\numblocks},\R^k}(\eps)$ matching to the $\frac{\eps^2}{2\outputvarcard}$ upper bound of Theorem~\ref{th:upperBoundcalibrationFunction}.


\begin{proof}
    This proof is of the same structure as the proof of Proposition~\ref{th:calibrationFunction:01loss} above.
    
    We use $\blockOfLabel{i} \in 1,\dots,\numblocks$ to denote the block to which label~$i$ belongs and $\labelsOfBlock{v}$ to denote the set of labels that belong to block~$v$.
    We also use $\Q_v$, $v\in 1,\dots, \numblocks$, as a shortcut to $\sum_{i \in \labelsOfBlock{v}} \q_i$, which is the total probability mass on block~$v$.
    
    We start by noting that the $i$-th component of the vector $(\lossmatrix_{01,\numblocks}) \qv$ equals $1-\Q_{\blockOfLabel{i}}$.
    By applying Lemmas~\ref{th:quadSurrogateExcess},~\ref{th:breakingSymmetries}, we get
    \begin{align}
    \label{eq:calibrationfunc:block01loss:breakingSymmetries}
    \calibrationfunc_{ij} (\eps) = \min_{\scorev, \qv} \quad & \frac{1}{2\outputvarcard} \sum_{v=1}^\numblocks \sum_{c \in \labelsOfBlock{v}} (\score_c + 1 - \Q_{\blockOfLabel{c}})^2, \\
    \label{eq:calibrationfunc:block01loss:breakingSymmetries:epsConstr}
    & \Q_{\blockOfLabel{i}} - \Q_{\blockOfLabel{j}} \geq \eps, \\
    \notag & \Q_{\blockOfLabel{i}} \geq \Q_u, \;u=1,\dots,\numblocks, \\
    \notag & \score_j \geq f_c, \;c=1,\dots,\outputvarcard, \\
    \notag & \sum\nolimits_{c=1}^{\outputvarcard} \q_c = 1, \quad \q_c \geq 0.
    \end{align}
    Analogously to Proposition~\ref{th:calibrationFunction:01loss}, we claim that there exists an optimal point of~\eqref{eq:calibrationfunc:block01loss:breakingSymmetries} such that $\q_c=0$, $c\not\in\{i,j\}$; $q_i = 0.5 + \frac{\eps}{2} = \Q_{\blockOfLabel{i}}$;  $\q_{j} = 0.5 - \frac{\eps}{2} = \Q_{\blockOfLabel{j}}$; $\score_c = -1$, $c \not\in \labelsOfBlock{ij} := \labelsOfBlock{\blockOfLabel{i}} \cup \labelsOfBlock{\blockOfLabel{j}}$.

    At first, note that if $\blockOfLabel{i} = \blockOfLabel{j}$, then the constraint~\eqref{eq:calibrationfunc:block01loss:breakingSymmetries:epsConstr} is never feasible, so we'll assume that $\blockOfLabel{i} \neq \blockOfLabel{j}$.
    
    We will now show that we can consider only configurations with all the probability mass on the two selected blocks.
    Consider some optimal point $\scorev^*$, $\qv^*$ and denote with $\delta = \sum_{c\in \outputdomain \setminus \labelsOfBlock{ij}} \q_c^*$ the probability mass on the unselected blocks.
    The operation
    \begin{align*}
    \score_c^* &:= \score_c^* + \frac{\delta}{2}, \; c \in \labelsOfBlock{ij},& 
    \score_c^* &:= -1, \; c\not\in \labelsOfBlock{ij} \\
    \q_i^* &:= \q_i^* + \frac{\delta}{2}, 
    \q_{j}^* := \q_{j}^* + \frac{\delta}{2}, &
    \q_c^* &:= 0, \; c\not\in \labelsOfBlock{ij} 
    \end{align*}
    can only decrease the objective of~\eqref{eq:calibrationfunc:block01loss:breakingSymmetries} because the summands corresponding to the unselected blocks are set to zero.
    All the constraints stay feasible and the summands corresponding to the selected blocks keep their values.
    
    The probability mass within the block~$\blockOfLabel{i}$ can be safely moved to $\q_i^*$ without changing the objective or violating any constraints.
    Analogously, the probability mass within the block~$\blockOfLabel{j}$ can be safely moved to $\q_j^*$.
    By reusing the operation~\eqref{eq:tfproofs:truncatingToEps}, we can now ensure that $\q^*_i = \q^*_j + \eps$ and thus that $\q^*_i = \frac{1}{2} + \frac{\eps}{2}$ and $\q^*_{j} = \frac{1}{2} - \frac{\eps}{2}$.

    At the point defined above, we now minimize the objective~\eqref{eq:calibrationfunc:block01loss:breakingSymmetries} w.r.t.\ $\score_c$, $c \in \labelsOfBlock{ij}$.
    At an optimal point, all values~$\score_c^*$, $c \in \labelsOfBlock{ij}$, belong to the segment $[\Q_{\blockOfLabel{j}}^* - 1, \Q_{\blockOfLabel{i}}^* - 1]$, otherwise we can always truncate the values to the borders of the segment and get an improvement of the objective.
    For all the scores $\score_c^*$, $c \neq j$, the following identity holds
    \begin{equation}
    \label{eq:calibrationfunc:block01loss:scoreInSelBlocks}
    \score_c^* = \begin{cases}
    \Q_{\blockOfLabel{c}}^* - 1, \; \text{if $\Q_{\blockOfLabel{c}}^* - 1 < \score_{j}^*$}, \\
    \score_{j}^*.
    \end{cases}
    \end{equation}
    Combining with the segment constraint, it implies that in the block of the label~$i$, we have $\score_c^* = \score_{j}^*$, $c \in \labelsOfBlock{\blockOfLabel{i}}$, and, in the block of the label~$j$, we have $\score_c^* = \Q_{\blockOfLabel{j}}^* - 1$, $c \in \labelsOfBlock{\blockOfLabel{j}} \setminus j$.
    
    By plugging the obtained values of $\q_c^*$ and $\score_c^*$ into~\eqref{eq:calibrationfunc:block01loss:breakingSymmetries} and denoting the value $\score_{j}^* + 0.5$ with $\tilde{\score}$, we get
    \begin{align}
    \label{eq:block01loss:proofSquare:obj2}
    \calibrationfunc_{ij}(\eps) = \min_{\tilde{\score}} \; &\frac{1}{2\outputvarcard}  \left( \blocksize_{\blockOfLabel{i}} (\tilde{\score} - \frac{\eps}{2})^2 + (\tilde{\score} + \frac{\eps}{2})^2 \right),\\
    \notag
    \mbox{s.t.}\; & \tilde{\score} \in [-\frac{\eps}{2}, \frac{\eps}{2}].
    \end{align}
    By setting the derivative of the objective~\eqref{eq:block01loss:proofSquare:obj2} to zero, we get
    $$
    \tilde{\score} = \frac{\eps}{2} \frac{\blocksize_{\blockOfLabel{i}} - 1}{\blocksize_{\blockOfLabel{i}} + 1},
    $$
    which belongs to the segment $[-\frac{\eps}{2}, \frac{\eps}{2}]$.
    We compute the function value at this point:
    $$
    \calibrationfunc_{ij} (\eps) = \frac{\eps^2}{4\outputvarcard} \frac{2\blocksize_{\blockOfLabel{i}}}{\blocksize_{\blockOfLabel{i}}+1},
    $$
    which finishes the proof.
\end{proof}

\begin{proposition}
    \label{th:calibrationFunction:block01loss:hardConstr}
    Let the scores~$\scorev$ be piecewise constant on the blocks of the loss, i.e.\ belong to the subspace~$\scoresubset_{01,\numblocks} = \colspace(\lossmatrix_{01,\numblocks}) \subseteq \R^k$.
    Then, the calibration function equals the following quadratic function w.r.t.\ $\eps$:
    \[
    \calibrationfunc_{\surrogatelossquad,\lossmatrix_{01,\numblocks},\scoresubset_{01,\numblocks}}(\eps)
    =
    \frac{\eps^2}{4\outputvarcard} \min_{v \neq u} \frac{2 \blocksize_v \blocksize_u}{\blocksize_v + \blocksize_u},
    \quad 0 \leq \eps \leq 1.
    \]
    If all the blocks are of the same size, we have
    $
    \calibrationfunc_{\surrogatelossquad,\lossmatrix_{01,\numblocks},\scoresubset_{01,\numblocks}}(\eps)
    =
    \frac{\eps^2}{4\numblocks}
    $
    where $\numblocks$ is the number of blocks.
\end{proposition}
\begin{proof}
    The constraints on scores $\scorev \in \scoresubset_{01,\numblocks}$ simply imply that the scores within all the blocks are equal.
    Having this in mind, the proof exactly matches the proof of Proposition~\ref{th:calibrationFunction:block01loss} until the argument around Eq.~\eqref{eq:calibrationfunc:block01loss:scoreInSelBlocks}.
    Now we cannot set the scores of the block~$\blockOfLabel{j}$ to different values, and, thus they are all equal to $\score^*$.
    
    By plugging the obtained values of $\q_c^*$ and $\score_c^*$ into~\eqref{eq:calibrationfunc:block01loss:breakingSymmetries} and denoting the value $\score_{j}^* + 0.5$ with $\tilde{\score}$, we get
    \begin{align}
    \notag
    \calibrationfunc_{ij}(\eps) = \min_{\tilde{\score}} \; &\frac{1}{2\outputvarcard}  \left( \blocksize_{\blockOfLabel{i}} (\tilde{\score} - \frac{\eps}{2})^2 + \blocksize_{\blockOfLabel{j}} (\tilde{\score} + \frac{\eps}{2})^2 \right),\\
    \label{eq:block01loss:hardConstr:proofSquare:obj2}
    \mbox{s.t.}\; & \tilde{\score} \in [-\frac{\eps}{2}, \frac{\eps}{2}].
    \end{align}
    By setting the derivative of the objective~\eqref{eq:block01loss:hardConstr:proofSquare:obj2} to zero, we get
    $$
    \tilde{\score} = \frac{\eps}{2} \frac{\blocksize_{\blockOfLabel{i}} - \blocksize_{\blockOfLabel{j}}}{\blocksize_{\blockOfLabel{i}} + \blocksize_{\blockOfLabel{j}}},
    $$
    which belongs to the segment $[-\frac{\eps}{2}, \frac{\eps}{2}]$.
    We now compute the function value at this point:
    $$
    \calibrationfunc_{ij} (\eps) = \frac{\eps^2}{4\outputvarcard} \frac{2\blocksize_{\blockOfLabel{i}}\blocksize_{\blockOfLabel{j}}}{\blocksize_{\blockOfLabel{i}}+\blocksize_{\blockOfLabel{j}}},
    $$
    which finishes the proof.
\end{proof}


\subsection{Hamming loss}
\label{sec:exactTranferProofs:hamming}
Recall that $\lossmatrix_{\hamming,\hamminglen}$ is the Hamming loss defined over $\hamminglen$ binary variables (see Eq.~\eqref{eq:hammingLoss} for the precise definition).
In this section, we compute the calibration function for the case of the scores belonging to the column span of the loss matrix (Proposition~\ref{th:calibrationFunction:hammingLoss:hardConstr}).

%

%
%
%
%
%
%
%
%
%
%
%
%
%
%

\begin{proposition}
    \label{th:calibrationFunction:hammingLoss:hardConstr}
    Assume that the scores~$\scorev$ always belong to the column span of the Hamming loss matrix~$\lossmatrix_{\hamming,\hamminglen}$, i.e., $\scoresubset_{\hamming,\hamminglen} = \colspace(\lossmatrix_{\hamming,\hamminglen}) \subseteq \R^k$.
    Then, the calibration function can be computed as follows:
    \[
    \calibrationfunc_{\surrogatelossquad,\lossmatrix_{\hamming,\hamminglen},\scoresubset_{\hamming,\hamminglen}}(\eps)
    =
    \frac{\eps^2}{8\hamminglen}, \quad 0 \leq \eps \leq 1.
    \]
\end{proposition}
\begin{proof}
    We start the proof by applying Lemma~\ref{th:breakingSymmetries} and by studying the vector of the expected losses~$(\lossmatrix_{\hamming,\hamminglen}) \qv$.
    We note that the $\hat{\outputvarv}$-th element~$\lossweighted_{\hat{\outputvarv}}(\qv)$, $\hat{\outputvarv} = (\hat{\outputvar}_t)_{t=1}^\hamminglen$, $\hat{\outputvar}_t \in \{0,1\}$, has a simple form of
    \[
    \lossweighted_{\hat{\outputvarv}}(\qv)
    =
    \sum_{\outputvarv \in \outputdomain} \frac{\q_\outputvarv}{\hamminglen}\sum_{t=1}^\hamminglen [\hat{\outputvar}_t \!\neq\! \outputvar_t]
    =
    1 - \frac{1}{\hamminglen} \sum_{t=1}^\hamminglen  \sum_{\outputvarv \in \outputdomain} \q_\outputvarv [\hat{\outputvar}_t \!=\! \outputvar_t].
    \]
    The quantity~$\sum_{\outputvarv \in \outputdomain} \q_\outputvarv [\hat{\outputvar}_t \!=\! \outputvar_t]$ corresponds to the marginal probability of a variable~$t$ taking a label~$\hat{\outputvar}_t$.
    Note that the expected loss~$\lossweighted_{\hat{\outputvarv}}(\qv)$ only depends on~$\qv$ through marginal probabilities, thus two distributions~$\qv_1$ and $\qv_2$ with the same marginals would be indistinguishable when plugged in the optimization problem for $\calibrationfunc_{ij}(\eps)$~\eqref{eq:calibrationfunc:generalBreakingSymmetries}, given that both the constraints and the objective (by Lemma~\ref{th:quadSurrogateExcess}) only depend on $\qv$ through the expected loss~$\lossweighted_{\hat{\outputvarv}}(\qv)$. 
    Having this in mind, we can consider only separable distributions, i.e., $\q_\outputvarv = \prod_{t=1}^\hamminglen \bigl(q_{t}[\outputvar_t\!=\!1] + (1-q_{t})[\outputvar_t\!=\!0]\bigr)$, where $q_{t} \in [0,1]$, $t=1,\dots,\hamminglen$, are the parameters defining the distribution.
    
    By combining the notation above with Lemmas~\ref{th:quadSurrogateExcess} and~\ref{th:breakingSymmetries}, we arrive at the following optimization problem:
    \begin{align}
    \label{eq:calibrationfunc:hammingloss:breakingSymmetries}
    \calibrationfunc_{\tilde{\outputvarv}\hat{\outputvarv}}(\eps)
    =
    \min_{\scorev, \qv} \;& \frac{1}{2 \outputvarcard}\! \sum_{\outputvarv \in \outputdomain}^{\outputvarcard} \Bigl( \score_\outputvarv \!+\! 1 \!-\! \frac{1}{\hamminglen} \!\!\sum\nolimits_{t=1}^\hamminglen \!\q_{t, \outputvar_t}\Bigr)^2, \\
    \label{eq:calibrationfunc:hammingloss:breakingSymmetries:epsConstr}
    \text{s.t.} \;& \frac{1}{\hamminglen}\!\!\sum\nolimits_{t=1}^{\hamminglen} (\q_{t, \tilde{\outputvar}_t} \!-\! \q_{t, \hat{\outputvar}_t}) \geq \eps,\\
    \label{eq:calibrationfunc:hammingloss:breakingSymmetries:qConstr}
    & \frac{1}{\hamminglen}\!\!\sum\nolimits_{t=1}^{\hamminglen} (\q_{t, \tilde{\outputvar}_t} \!-\! \q_{t, \outputvar_t}) \geq 0,\: \forall \outputvarv \in \outputdomain, \\
    \label{eq:calibrationfunc:hammingloss:breakingSymmetries:fConstr}
    &\score_{\hat{\outputvarv}} \geq \score_\outputvarv,\; \forall \outputvarv \in \outputdomain, \\
    \label{eq:calibrationfunc:hammingloss:breakingSymmetries:simplexConstr}
    & 0\leq \q_t \leq 1, \; t = 1,\dots, \hamminglen,\\
    \label{eq:calibrationfunc:breakingSymmetries:hammingloss:fSet}
    & \scorev \in \scoresubset,
    \end{align}
    where~$q_{t, \outputvar_t}$ is a shortcut to $q_{t}[\outputvar_t\!=\!1] + (1-q_{t})[\outputvar_t\!=\!0]$ and labels $\tilde{\outputvarv}$ and $\hat{\outputvarv}$ serve as the selected labels~$i$ and $j$, respectively.
    
    The calibration function $\calibrationfunc_{\surrogatelossquad,\lossmatrix_{\hamming,\hamminglen},\scoresubset_{\hamming,\hamminglen}}(\eps)=
    \frac{\eps^2}{8\hamminglen}$ in the formulation of this proposition matches the lower bound provided by Theorem~\ref{th:lowerBoundcalibrationFunction} in Section~\ref{sec:bounds:interpretations}.
    Thus, it suffices to construct a feasible w.r.t.~\eqref{eq:calibrationfunc:hammingloss:breakingSymmetries:epsConstr}-\eqref{eq:calibrationfunc:breakingSymmetries:hammingloss:fSet} assignment of variables $\scorev$, $\qv$ and labels~$\tilde{\outputvarv}$, $\hat{\outputvarv}$ such that the objective equals the lower bound.
    
    It suffices to simply set $\tilde{\outputvarv}$ to all zeros and $\hat{\outputvarv}$ to all ones.
    In this case, the constraints~\eqref{eq:calibrationfunc:hammingloss:breakingSymmetries:epsConstr} and~\eqref{eq:calibrationfunc:hammingloss:breakingSymmetries:qConstr} take the simplified form:
    \begin{align}
    \label{eq:calibrationfunc:hammingloss:breakingSymmetries:epsConstr:2}
    \frac{1}{\hamminglen}\!\!\sum\nolimits_{t=1}^{\hamminglen} (1 - 2\q_t) \geq \eps,\\
    \label{eq:calibrationfunc:hammingloss:breakingSymmetries:qConstr:2}
    \q_t \leq \frac12, \; t = 1,\dots, \hamminglen.
    \end{align}
    
    We now set~$\q_t := \frac12 - \frac{\eps}{2}$, $t = 1, \dots, \hamminglen$, and $\scorev := -\frac12\one_{\outputvarcard}$.
    This point is clearly feasible when $0\leq\epsilon\leq 1$, so it remains to compute the value of the objective.
    We complete the proof by writing (let~$w$ be the count of ones in an assignment $\outputvarv$):
    \begin{align*}
    &\frac{1}{2 \outputvarcard}\! \sum_{\outputvarv \in \outputdomain}^{\outputvarcard} \Bigl( \score_\outputvarv \!+\! 1 \!-\! \frac{1}{\hamminglen} \!\!\sum\nolimits_{t=1}^\hamminglen \!\q_{t, \outputvar_t}\Bigr)^2
    =\\
    &\frac{1}{2 \outputvarcard} \sum_{w=0}^{\hamminglen} \tbinom{\hamminglen}{w} \bigl(\frac12 - \frac{1}{\hamminglen} (w(\frac12 - \frac{\eps}{2}) + (\hamminglen - w)(\frac12 + \frac{\eps}{2})) \bigr)^2 =
    \\
    & \frac{1}{2 \outputvarcard} \sum_{w=0}^{\hamminglen} \tbinom{\hamminglen}{w} (\frac{\eps}{2} - \frac{w\eps}{\hamminglen})^2 = \frac{\eps^2}{2 \outputvarcard} \sum_{w=0}^{\hamminglen} \tbinom{\hamminglen}{w} (\frac14 - \frac{w}{\hamminglen} + \frac{w^2}{\hamminglen^2}) =\\
    &\frac{\eps^2}{2 \outputvarcard} (\frac14 2^\hamminglen - \frac{1}{\hamminglen} \hamminglen 2^{\hamminglen-1} + \frac{1}{\hamminglen^2}\hamminglen(\hamminglen+1)2^{\hamminglen-2}) = \frac{\eps^2}{8\hamminglen},
    \end{align*}
    where we use the equality~$\outputvarcard = 2^\hamminglen$ and the identities $\sum_{t=0}^\hamminglen \binom{\hamminglen}{t} = 2^\hamminglen$, $\sum_{t=0}^\hamminglen t \binom{\hamminglen}{t} = \hamminglen 2^{\hamminglen-1}$, $\sum_{t=0}^\hamminglen t^2 \binom{\hamminglen}{t} = \hamminglen (\hamminglen+1) 2^{\hamminglen-2}$.
\end{proof}

\subsection{Mixed 0-1 and block 0-1 loss}
\label{sec:exactTranferProofs:mixedloss}
Recall that $\lossmatrix_{01,\numblocks,\consbreakpoint}$ is the convex combination of the 0-1 loss and the block 0-1 loss with $\numblocks$~blocks, i.e., $\lossmatrix_{01,\numblocks,\consbreakpoint} = \consbreakpoint \lossmatrix_{01} + (1 - \consbreakpoint) \lossmatrix_{01,\numblocks}$, $0 \leq \consbreakpoint \leq 1$.
Let all the blocks be of the same size $\blocksize = \frac{\outputvarcard}{\numblocks} \geq 2$.
In this section, we compute the calibration functions for the case of unconstrained scores (Proposition~\ref{th:calibrationFunction:mixedLoss}) and for the case when scores belong to the column span of the loss matrix (Proposition~\ref{th:calibrationFunction:mixedLoss:hardConstr}).

\begin{proposition}
    \label{th:calibrationFunction:mixedLoss}
    If there are no constraints on scores~$\scorev$ then the calibration function
    \[
    \calibrationfunc_{\surrogatelossquad,\lossmatrix_{01,\numblocks,\consbreakpoint},\R^\outputvarcard}(\eps)
    =
    \begin{cases}
    \frac{\eps^2}{4\outputvarcard}, \qquad \text{$\eps \leq \frac{\consbreakpoint}{1-\consbreakpoint}$}, \\
    \frac{\eps^2 \blocksize}{2\outputvarcard(\blocksize+1)} \!-\! \frac{\consbreakpoint(\eps+1)(\blocksize-1)}{4\outputvarcard(\blocksize+1)}(2\eps \!-\! \eps \consbreakpoint \!-\! \consbreakpoint)
    	\quad \text{$\frac{\consbreakpoint}{1-\consbreakpoint} \leq \eps \leq 1$}
    \end{cases}
    \]
    shows that the surrogate is consistent.
\end{proposition}
Note that when $\consbreakpoint=0$, we have $\calibrationfunc(\eps) = \frac{\eps^2}{4\outputvarcard}\frac{2\blocksize}{\blocksize+1}$ as in Proposition~\ref{th:calibrationFunction:block01loss}.
When $\consbreakpoint \geq 0.5$ we have $\calibrationfunc(\eps) = \frac{\eps^2}{4\outputvarcard}$, which matches Proposition~\ref{th:calibrationFunction:01loss}.

\begin{proof}
    This proof is very similar to the proof of Proposition~\ref{th:calibrationFunction:block01loss}, but technically more involved.
    
    We start by noting that the $i$-th element of the vector~$(\lossmatrix_{01,\numblocks,\consbreakpoint}) \qv$ equals
    \begin{equation}
    \label{eq:calibrationFunction:mixedLoss:expectedLoss}
    \sum_{j:\;\blockOfLabel{j}\neq \blockOfLabel{i}} \!\!\!\!\!\! (1-\consbreakpoint)\q_j\; + \sum_{j:\:j\neq i}\!\! \consbreakpoint \q_j  
    = 
    \consbreakpoint(1-\q_i) + (1-\consbreakpoint)(1-\Q_{\blockOfLabel{i}}),
    \end{equation}
    where for $\blockOfLabel{i}$ and $\Q_v$ we reuse the notation defined in the proof of Proposition~\ref{th:calibrationFunction:block01loss}.
    By combining this with Lemmas~\ref{th:quadSurrogateExcess} and~\ref{th:breakingSymmetries}, we get
    \begin{align}
    \label{eq:calibrationFunction:mixedLoss:proofSquare:obj1}
    \calibrationfunc_{ij}(\eps) \!\!=\!\! \min_{\scorev, \qv} \; & \frac{1}{2\outputvarcard} \sum_{v=1}^\numblocks \;\sum_{c \in\labelsOfBlock{v}} (\score_c + 1 - \consbreakpoint \q_c - (1-\consbreakpoint)\Q_{\blockOfLabel{c}})^2, \\
    \notag \mbox{s.t.} \;& \consbreakpoint (\q_i - \q_j) + (1-\consbreakpoint)(\Q_{\blockOfLabel{i}} - \Q_{\blockOfLabel{j}}) \geq \eps, \\
    \notag & \consbreakpoint (\q_i - \q_c) + (1-\consbreakpoint)(\Q_{\blockOfLabel{i}} - \Q_{\blockOfLabel{c}}) \geq 0, \forall c \\
    \notag & \score_{j} \geq \score_c, \; \forall c, \\
    \notag & \sum_{c=1}^\outputvarcard \q_c = 1, \quad \q_c \geq 0, \; \forall c.
    \end{align}
    
    The blocks are all of the same size so we need to consider just the two cases: 1) the selected labels belong to the same block, i.e., $\blockOfLabel{i} = \blockOfLabel{j}$; 2) the selected labels belong to the two different blocks, i.e., $\blockOfLabel{i} \neq \blockOfLabel{j}$.
    
    The first case can be proven by a straight forward generalization of the proof of Proposition~\ref{th:calibrationFunction:01loss}.
    Given that the loss value is bounded by~$1$, the maximal possible value of~$\eps$ when the constraints can be feasible equals~$\consbreakpoint$.
    Thus, we have $\calibrationfunc_{ij}(\eps) = \frac{\eps^2}{4\outputvarcard}$ for $\eps \leq \consbreakpoint$ and $+\infty$ otherwise.
    
    We will now proceed to the second case $\blockOfLabel{i} \neq \blockOfLabel{j}$.
    We show that
    \[
    \calibrationfunc_{ij}(\eps) \!\!=\!\! \begin{cases}
    \frac{\eps^2}{4\outputvarcard}, \quad \text{for $\eps \leq \frac{\consbreakpoint}{1-\consbreakpoint}$}, \\
    \frac{\eps^2 \blocksize}{2\outputvarcard(\blocksize+1)} - \frac{\consbreakpoint(\eps+1)(\blocksize-1)}{4\outputvarcard(\blocksize+1)}(2\eps - \eps \consbreakpoint - \consbreakpoint), \text{otherwise.} \\
    \end{cases}
    \]
    
    Similarly to the arguments used in Propositions~\ref{th:calibrationFunction:01loss} and~\ref{th:calibrationFunction:block01loss}, we claim that there is an optimal point of~\eqref{eq:calibrationFunction:mixedLoss:proofSquare:obj1} such that $\q^*_c=0$, $c \not\in \{i,j\}$; $\q^*_i = 0.5 + \frac{\eps}{2}$;  $\q^*_{j} = 0.5 - \frac{\eps}{2}$; and $\score^*_c = -1$ for $c \not\in \labelsOfBlock{ij} := \labelsOfBlock{\blockOfLabel{i}} \cup \labelsOfBlock{\blockOfLabel{j}}$.
    
    First, we will show that we can consider only configurations with all the probability mass on the two selected blocks $\blockOfLabel{i}$ and $\blockOfLabel{j}$.
    Given any optimal point $\scorev^*$ and $\qv^*$, the operation (with $\delta = \sum_{c \not\in \labelsOfBlock{ij}} \q_c^*$)
    \begin{align*}
    \score_i^* &:= \score_i^* + \frac{\delta}{2}, &
    &\q_i^* := \q_i^* + \frac{\delta}{2}, \\
    \score_j^* &:= \score_j^* + \frac{\delta}{2}, &
    &\q_{j}^* := \q_{j}^* + \frac{\delta}{2},\\
    \score_c^* &:= -1, c \not\in\labelsOfBlock{ij} &
    &\q_c^* := 0, c \not\in\labelsOfBlock{ij} \\
    \score_c^* &:= \score_c^* + (1-\consbreakpoint)\frac{\delta}{2}, \mathrlap{\;\;c \in \;\labelsOfBlock{ij} \setminus \{i,j\}}
    \end{align*}
    can only decrease the objective of~\eqref{eq:calibrationFunction:mixedLoss:proofSquare:obj1} because the summands corresponding to the unselected $\numblocks-2$ blocks are set to zero.
    All the constraints stay feasible and the values corresponding to the blocks $\blockOfLabel{i}$ and $\blockOfLabel{j}$ do not change.
    The last operation is required, because the values~$\Q_{\blockOfLabel{i}}$, $\Q_{\blockOfLabel{j}}$ change when we change $\q_i$ and $\q_j$.
    Adding $(1-\consbreakpoint)\frac{\delta}{2}$ to some scores compensates this and cannot violate the constraints because $\score_j^*$ goes up by $\frac{\delta}{2} \geq (1-\consbreakpoint)\frac{\delta}{2}$.
    
    Now we will show that it is possible to move all the mass to the two selected labels $i$ and $j$.
    We cannot simply move the mass within one block, but need to create some overflow and move it to another block in a specific way.
    Consider $\delta:=q_a^*$, which is some non-zero mass on a non-selected label of the block~$\blockOfLabel{i}$.
    Then, the operation
    \begin{align*}
    \score_i^* &:= \score_i^* + \delta\frac{\consbreakpoint}{2}, &
    \q_i^* &:= \q_i^* + \delta(1-\frac{\consbreakpoint}{2}), \\
    \score_j^* &:= \score_j^* + \delta\frac{\consbreakpoint}{2}, &
    \q_{j}^* &:= \q_{j}^* + \delta\frac{\consbreakpoint}{2},\\
    \score_a^* &:= \score_a^* + \delta\frac{\consbreakpoint}{2}(\consbreakpoint - 3), &
    \q_{a}^* &:= \q_{a}^* - \delta = 0,\\
    \score_c^* &:= \score_c^* - \delta\frac{\consbreakpoint}{2} (1-\consbreakpoint), \mathrlap{\;\;c \in \;\labelsOfBlock{i} \setminus \{i,a\}} &&\\
    \score_c^* &:= \score_c^* +  \delta\frac{\consbreakpoint}{2}(1-\consbreakpoint), \mathrlap{\;\;c \in \;\labelsOfBlock{j} \setminus \{j\}} &&
    \end{align*}
    does no change the objective value of~\eqref{eq:calibrationFunction:mixedLoss:proofSquare:obj1} because the quantities $\score_c + 1 -  \consbreakpoint \q_c - (1-\consbreakpoint)\Q_{\blockOfLabel{c}}$, $c \in \labelsOfBlock{ij}$, stay constant and all the constraints of~\eqref{eq:calibrationFunction:mixedLoss:proofSquare:obj1} stay feasible.
    We repeat this operation for all $a \in \labelsOfBlock{\blockOfLabel{i}}\setminus \{i\}$ and, thus, move all the probability mass within the block~$\blockOfLabel{i}$ to the label~$i$.
    In the block~$\blockOfLabel{j}$, an analogous operation can move all the mass to the label~$j$.
    
    It remains to show that $\q_i^* - \q_j^* = \eps$.
    Indeed, if $\q^*_i - \q^*_j = \delta' >  \eps$, the operation analogous to~\eqref{eq:tfproofs:truncatingToEps}
    \begin{align*}
    \score_i^* &:= \score_i^* - \frac{\delta'-\eps}{2}, &
    \q_i^* &:= \q_i^* - \frac{\delta'-\eps}{2}, \\
    \score_j^* &:= \score_j^* + \frac{\delta'-\eps}{2},&
    \q_{j}^* &:= \q_{j}^* + \frac{\delta'-\eps}{2}, \\
    \score_c^* &:= \score_c^* - (1-\consbreakpoint)\frac{\delta'-\eps}{2}, \mathrlap{c \in  \labelsOfBlock{\blockOfLabel{i}}\setminus \{i\},} \\
    \score_c^* &:= \score_c^* + (1-\consbreakpoint)\frac{\delta'-\eps}{2}, \mathrlap{c \in \labelsOfBlock{\blockOfLabel{j}}\setminus \{j\}}
    \end{align*}
    can always set  $\q_i^* - \q_j^* = \eps$, and thus  $\q^*_i = 0.5 + \frac{\eps}{2}$ and $\q^*_{j} = 0.5 - \frac{\eps}{2}$.
    After this operation, all the scores of the block~$\blockOfLabel{i}$ go down and all the scores of the block~$\blockOfLabel{j}$ go up at most as much as~$\score_j^*$, so the constraints $\score_{j} \geq \score_c$ cannot get violated.
    
    We now proceed with the computation of $\calibrationfunc_{ij}(\eps)$.
    First, we note that convexity and symmetries of~\eqref{eq:calibrationFunction:mixedLoss:proofSquare:obj1} implies that all the non-selected scores within each block are equal.\footnote{If these optimal scores are not equal, by symmetry, one can obtain the same objective and feasibility by permuting their corresponding values. By taking a uniform convex combination on all permutations, we obtain a point where all the scores are equal, and by convexity, would yield a lower objective value.}
    Denote the scores of the non-selected labels of the block~$\blockOfLabel{i}$ by~$\score_i'$, and the scores of the non-selected labels of the block~$\blockOfLabel{j}$ by~$\score_j'$.
    
    Analogous to all the previous propositions, the truncation argument gives us that all the values $\score_c^*$ belong to the segment~$[-1, -0.5 + \frac{\eps}{2}]$.
    For all the optimal values $\score_c^*$, $c \neq j$, the following identity holds:
    \[
    \score_c^* = \begin{cases}
    \score_j^*, \quad \text{if\;\; $\consbreakpoint \q_c^* + (1-\consbreakpoint)\Q_{\blockOfLabel{c}}^*-1 \geq \score_j^*$},\\
    \consbreakpoint \q_c^* + (1-\consbreakpoint)\Q_{\blockOfLabel{c}}^*-1, \quad \text{otherwise}.
    \end{cases}
    \]
    Given that $\score_i^*$ wants to equal the maximal possible value $-0.5 + \frac{\eps}{2}$, it implies that $\score_i^* = \score_j^*$.
    Denote this value by~$\score$.
    
    By, plugging the values of~$\qv^*$ and $\scorev^*$ provided above into the objective of~\eqref{eq:calibrationFunction:mixedLoss:proofSquare:obj1}, we get
    \begin{align}
    \notag
    \frac{1}{2\outputvarcard}\Bigl(  
    &(\score \!+\! 0.5 \!-\! \frac{\eps}{2})^2
    \!+\!
    (\blocksize\!-\!1)(\score_i' \!+\! 1\!-\!(1\!-\!\consbreakpoint)(0.5\!+\!\frac{\eps}{2}))^2
    + \\
    \label{eq:calibrationFunction:mixedLoss:proofSquare:case2:obj2}
    &(\score \!+\! 0.5 \!+\! \frac{\eps}{2})^2
    \!+\!
    (\blocksize\!-\!1)(\score_j' \!+\! 1\!-\!(1\!-\!\consbreakpoint)(0.5\!-\!\frac{\eps}{2}))^2
    \Bigr).
    \end{align}
    By minimizing~\eqref{eq:calibrationFunction:mixedLoss:proofSquare:case2:obj2} without constraints, we get $\score^*=-0.5$, $\score_i'^*=\frac12(1+\eps)(1-\consbreakpoint)-1$, $\score_j'^*=\frac12(1-\eps)(1-\consbreakpoint)-1$.
    We now need to compare $\score_i'^*$ and $\score_j'^*$ with $\score^*$ to satisfy the constraints $\score^* \geq \score_i'^*$ and $\score^* \geq \score_j'^*$.
    First, we have that
    \[
    \score^*\!-\!\score_j'^* = \frac12(\consbreakpoint+\eps-\consbreakpoint\eps) \geq 0,\; \text{for $0\!\leq\!\eps\!\leq\!1$ and $0\!\leq\!\consbreakpoint\!\leq\!1$}.
    \]
    Second, we have 
    \[
    \score^*-\! \score_i'^* \!=\! \frac12(\consbreakpoint-\eps+\consbreakpoint\eps) \geq 0,\; \text{for $0\!\leq\!\eps\!\leq\!\frac{\consbreakpoint}{1-\consbreakpoint}$ and $0\!\leq\!\consbreakpoint\!\leq\! 1$}.
    \]
    We can now conclude that when $\eps\leq\frac{\consbreakpoint}{1-\consbreakpoint}$ we have both $\score_i'$ and $\score_j'$ equal to their unconstrained minimum points leading to $\calibrationfunc_{ij}(\eps)=\frac{\eps^2}{4\outputvarcard}$.
    
    Now, consider the case $\eps>\frac{\consbreakpoint}{1-\consbreakpoint}$.
    We have the constraint $\score \geq \score_i'$ violated, so at the minimum we have $\score_i'= \score$. The new unconstrained minimum w.r.t.~$\score$ equals $\score^* = \frac{1}{\blocksize+1}(-1-(\blocksize-1)(1- \frac12(1-\consbreakpoint)(1-\eps)))$.
    We now show that the inequality $\score^* \geq \score_j'^*$ still holds. 
    We have
    \[
    \score^* - \score_j'^* = \frac{\consbreakpoint+\eps \blocksize - \consbreakpoint \eps \blocksize}{\blocksize+1} \geq 0, \; \text{for \:$0\leq\eps\leq 1$ \:and \:$0\leq \consbreakpoint \leq 1$}.
    \]
    Substitution of $\score_i'^*=\score^*$ and $\score_j'^*$ into~\eqref{eq:calibrationFunction:mixedLoss:proofSquare:case2:obj2} gives us 
    \[
    \frac{1}{\outputvarcard} \Bigl( \frac{\eps^2 \blocksize}{2(\blocksize+1)} - \frac{\consbreakpoint(\eps+1)(\blocksize-1)}{4(\blocksize+1)}(2\eps - \eps \consbreakpoint - \consbreakpoint)\Bigr),
    \]
    which equals $\calibrationfunc_{ij}(\eps)$ for $1\geq \eps>\frac{\consbreakpoint}{1-\consbreakpoint}$.
    
    Comparing cases~1 and~2, we observe that $\calibrationfunc_{ij}(\eps)$ from case~2 is never larger than the one of case~1, thus case~2 provides the overall calibration function~$\calibrationfunc_{ij}(\eps)$.
\end{proof}

\begin{proposition}
    \label{th:calibrationFunction:mixedLoss:hardConstr}
    If the scores~$\scorev$ are constrained to be equal inside the blocks, i.e.\ belong to the subspace~$\scoresubset_{01,\numblocks} = \colspace(\lossmatrix_{01,\numblocks}) \subseteq \R^k$, then the calibration function
    \[
    \calibrationfunc_{\surrogatelossquad,\lossmatrix_{01,\numblocks,\consbreakpoint},\scoresubset_{01,\numblocks}}(\eps)
    =
    \begin{cases}
    \frac{(\eps-\frac{\consbreakpoint}{2})^2}{4\numblocks}\frac{(\frac{\consbreakpoint \numblocks}{\outputvarcard}+1-\consbreakpoint)^2}{(1-\frac{\consbreakpoint}{2})^2}, \;&\text{$\frac{\consbreakpoint}{2} \leq \eps \leq 1$}, \\
    0,\;&\text{$0\leq \eps \leq \frac{\consbreakpoint}{2}$}
    \end{cases}
    \]
    shows that the surrogate is consistent up to level~$\frac{\consbreakpoint}{2}$.
\end{proposition}
When $\consbreakpoint=0$, we have $H(\eps) = \frac{\eps^2}{4 \numblocks}$ as in Proposition~\ref{th:calibrationFunction:block01loss:hardConstr}.
When $\consbreakpoint = 1$ we have $H(\eps) = 0$, which corresponds to the case of fully inconsistent surrogate (0-1 loss and constrained scores).
\begin{proof}
    This proof combines ideas from Proposition~\ref{th:calibrationFunction:mixedLoss} and Proposition~\ref{th:calibrationFunction:block01loss:hardConstr}.
    
    Note that contrary to all the previous results, Lemma~\ref{th:quadSurrogateExcess} is not applicable, because, for $\numblocks < \outputvarcard$, we have that $\colspace\lossmatrix_{01,\numblocks,\consbreakpoint} = \R^\outputvarcard \not\subset \scoresubset_{01,\numblocks} = \colspace(\lossmatrix_{01,\numblocks})$.
    
    We now derive an analog of Lemma~\ref{th:quadSurrogateExcess} for this specific case.
    We define the subspace of scores~$\scoresubset_{01,\numblocks} = \{ \scorematrix \scoreparamv \mid \scoreparamv \in \R^{\numblocks} \}$ with a matrix $\scorematrix : = \scorematrix_{01,\numblocks} \in \R^{\outputvarcard \times \numblocks}$ with columns containing the indicator vectors of the blocks.
    We have $\scorematrix^\transpose \scorematrix = \blocksize \id_{\numblocks}$ and thus $(\scorematrix^\transpose \scorematrix)^{-1} = \frac{1}{\blocksize} \id_{\numblocks}$.
    We shortcut the loss matrix $\lossmatrix_{01,\numblocks,\consbreakpoint}$ to $\lossmatrix$ and rewrite it as
    \[
    \lossmatrix = \consbreakpoint \lossmatrix_{01} + (1-\consbreakpoint)\lossmatrix_{01,\numblocks} =
    \one_\outputvarcard \one_\outputvarcard^\transpose - \consbreakpoint \id_\outputvarcard - (1-\consbreakpoint)\scorematrix\scorematrix^\transpose.
    \]
    By redoing the derivation of Lemma~\ref{th:quadSurrogateExcess}, we arrive at a different excess surrogate:
    \begin{align*}
    \surrogateweighted(\scorev(\scoreparamv), \qv) &= \frac{1}{2\outputvarcard}(\blocksize \scoreparamv^\transpose \scoreparamv + 2 \scoreparamv^\transpose \scorematrix^\transpose \lossmatrix \qv) + r(\qv), \\
    \scoreparamv^* &:= \argmin\nolimits_\scoreparamv \surrogateweighted(\scorev(\scoreparamv), \qv) =  -\frac{1}{\blocksize} \scorematrix^\transpose \lossmatrix \qv, \\
    \excess\surrogateweighted(\scorev(\scoreparamv), \qv) &= \frac{1}{2\outputvarcard}(\blocksize \scoreparamv^\transpose \scoreparamv + 2 \scoreparamv^\transpose \scorematrix^\transpose \lossmatrix \qv + \frac{1}{\blocksize} \qv^\transpose \lossmatrix^\transpose \scorematrix \scorematrix^\transpose \lossmatrix \qv)\\
    &=
    \frac{\blocksize}{2\outputvarcard} \| \scoreparamv +  \frac{1}{\blocksize} \scorematrix^\transpose \lossmatrix \qv\|_2^2\\
    &= 
    \frac{\blocksize}{2\outputvarcard} \sum_{v=1}^\blocksize
    (\scoreparam_v + 1 - (1-\consbreakpoint) \Q_v - \frac{\consbreakpoint}{\blocksize} \Q_v)^2,
    \end{align*}
    where $\Q_v = \sum_{c \in \labelsOfBlock{v}} \q_c$ is the total probability mass on block~$v$ and $\labelsOfBlock{v} \subset \outputdomain$ denotes the set of labels of block~$v$.
    
    Analogously to Proposition~\ref{th:calibrationFunction:mixedLoss} we can now apply Lemma~\ref{th:breakingSymmetries} and obtain $\calibrationfunc_{ij}(\eps)$.
    \begin{align}
    \label{eq:calibrationFunction:mixedLoss:hardConstr:proofSquare:obj1}
    \calibrationfunc_{ij}(\eps) \!\!=\!\! \min_{\scoreparamv, \qv} \; & \frac{\blocksize}{2\outputvarcard} \sum_{v=1}^\numblocks (\scoreparam_v + 1 - (1\!-\!\consbreakpoint) \Q_v - \frac{\consbreakpoint}{\blocksize} \Q_v)^2, \\
    \notag \mbox{s.t.} \;& \consbreakpoint (\q_i - \q_j) + (1-\consbreakpoint)(\Q_{\blockOfLabel{i}} - \Q_{\blockOfLabel{j}}) \geq \eps, \\
    \notag & \consbreakpoint (\q_i - \q_c) + (1-\consbreakpoint)(\Q_{\blockOfLabel{i}} - \Q_{\blockOfLabel{c}}) \geq 0, \forall c \\
    \notag & \scoreparam_{\blockOfLabel{j}} \geq \scoreparam_u, \; \forall u = 1,\dots,\numblocks, \\
    \notag & \sum_{c=1}^\outputvarcard \q_c = 1, \quad \q_c \geq 0, \; \forall c.
    \end{align}
    The main difference to~\eqref{eq:calibrationFunction:mixedLoss:proofSquare:obj1} consists in the fact that we now minimize w.r.t.\ $\scoreparamv$ instead of~$\scorev$.
    
    Note that because of the way the predictor~$\predictor(\scorev(\scoreparamv))$ resolves ties (among the labels with maximal scores it always picks the label with the smallest index), not all labels can be predicted.
    Specifically, only one label from each block can be picked.
    This argument allows us to assume that $\blockOfLabel{i} \neq \blockOfLabel{j}$ in the remainder of this proof.
    
    First, let us prove the case for $\eps \leq \frac{\consbreakpoint}{2}$.
    We explicitly provide a feasible assignment of variables where the objective equals zero.
    We set $\q_i = \frac12$ and $\q_c = \frac{1}{2(\blocksize-1)}$, $c \in \labelsOfBlock{\blockOfLabel{j}} \setminus \{j\}$.
    All the other labels (including~$j$ and the unselected labels of the block~$\blockOfLabel{i}$) receive zero probability mass.
    This assignment of~$\qv$ implies~$\Q_{\blockOfLabel{i}} = \Q_{\blockOfLabel{j}} = \frac12$ and the zero mass on the other blocks.
    We also set $\scoreparam_{\blockOfLabel{i}}$ and $\scoreparam_{\blockOfLabel{j}}$ to $(1\!-\!\consbreakpoint) \frac12 + \frac{\consbreakpoint}{\blocksize} \frac12 - 1$ to ensure zero objective value.
    Verifying other feasibility constraints we have 
    $\consbreakpoint (\q_i - \q_j) + (1-\consbreakpoint)(\Q_{\blockOfLabel{i}} - \Q_{\blockOfLabel{j}}) = \frac{\consbreakpoint}{2} \geq \eps$ and $\consbreakpoint (\q_i - \q_c) + (1-\consbreakpoint)(\Q_{\blockOfLabel{i}} - \Q_{\blockOfLabel{c}}) = \consbreakpoint(\frac12-\frac{1}{2(\blocksize-1)}) \geq 0$, $c \in \labelsOfBlock{\blockOfLabel{j}} \setminus \{j\}$.
    Other constraints are trivially satisfied.
    
    Now, consider the case of $\eps > \frac{\consbreakpoint}{2}$.
    As usual, we claim  the following values of the variables~$\scorev$ and~$\qv$ result in an optimal point. We have $\q_c^*=0$, $c\not\in \labelsOfBlock{ij}$; $\scoreparam_v^* = -1$, $v \not\in\{\blockOfLabel{i}, \blockOfLabel{j}\}$; and $\q_i^* =\Q_{\blockOfLabel{i}}^* = \frac{1+\eps-\consbreakpoint}{2-\consbreakpoint}$; $\q_c^*=0$, $c \in \labelsOfBlock{\blockOfLabel{i}} \setminus \{i\}$ (other labels in the block~$\blockOfLabel{i}$);  $\q_{j}^* = 0$, $\q_c^* = \frac{1-\eps}{(2-\consbreakpoint)(\blocksize-1)}$, $c \in \labelsOfBlock{\blockOfLabel{j}} \setminus \{j\}$ (other labels in the block~$\blockOfLabel{j}$).
    
    First, we will show that we can consider only configurations with all the probability mass on the two selected blocks~$\blockOfLabel{i}$ and $\blockOfLabel{j}$.
    Given some optimal variables $\scorev^*$ and $\qv^*$, the operation (with $\delta = \sum_{c \in \outputdomain \setminus \labelsOfBlock{ij}} \q_c^*$)
    \begin{align*}
    \q_c^* &:= 0, \; c \in \outputdomain \setminus \labelsOfBlock{ij}, &
    \q_i^* &:= \q_i^* + \frac{\delta}{2}, &
    \q_{j}^* &:= \q_{j}^* + \frac{\delta}{2}, \\
    \scoreparam_v^* &:= -1, \; \mathrlap{v \notin \{\blockOfLabel{i}, \blockOfLabel{j}\},} \\
    \scoreparam_{\blockOfLabel{i}}^* &:= \scoreparam_{\blockOfLabel{i}}^* + \mathrlap{\frac{\delta}{2}(1-\consbreakpoint+\frac{\consbreakpoint}{\blocksize}),} \\
    \scoreparam_{\blockOfLabel{j}}^* & := \scoreparam_{\blockOfLabel{j}}^* + \mathrlap{\frac{\delta}{2}(1-\consbreakpoint+\frac{\consbreakpoint}{\blocksize})}
    \end{align*}
    can only decrease the objective of~\eqref{eq:calibrationFunction:mixedLoss:hardConstr:proofSquare:obj1} because the summands corresponding to the unselected $\numblocks - 2$ blocks are set to zero.
    All the constraints stay feasible and the values corresponding to the blocks $\blockOfLabel{i}$ and $\blockOfLabel{j}$ do not change.
    
    Now, we move the mass within the two selected blocks.
    To start with, moving the mass within one block does not change the objective, because it depends only on $\Q_{\blockOfLabel{c}}$ and not on $\qv$ directly.
    In the block~$\blockOfLabel{i}$, it is safe to increase $\q_i$ and decrease the mass on the other labels, because $\q_i$ enters the constraints with the positive sign and while the others enter with the negative sign. So we let $\q_c = 0$ for $c \in \labelsOfBlock{\blockOfLabel{i}}  / \{i \}$ and $ \Q_{\blockOfLabel{i}}=\q_i$. We also have $\Q_{\blockOfLabel{j}}=1-\q_i$ as the mass on all other blocks is zero.
    
    Moving mass within the block~$\blockOfLabel{j}$ is more complicated, as moving mass to some label~$c$ of this block might violate the constraints of \eqref{eq:calibrationFunction:mixedLoss:hardConstr:proofSquare:obj1} 
    on $\q_i$. We start by considering the first constraint in~$\eqref{eq:calibrationFunction:mixedLoss:hardConstr:proofSquare:obj1}$, using $\Q_{\blockOfLabel{j}}=1-\q_i$, we get:
    \begin{equation}
    \label{eq:mixedLoss:firstConstraint}
    \q_i \geq \eps + \consbreakpoint \q_j + (1-\consbreakpoint)(1-\q_i).
    \end{equation}
    By using $\q_j \geq 0$ and $\eps \geq \frac{\consbreakpoint}{2}$, the inequality~\eqref{eq:mixedLoss:firstConstraint} implies that $\q_i \geq \frac{1}{2}$ and thus that 
    \begin{equation}
    \label{eq:mixedLossQcProperty}
    \q_c \leq \Q_{\blockOfLabel{j}} \leq \frac{1}{2} \quad \forall c \in  \labelsOfBlock{\blockOfLabel{j}} \, .
    \end{equation}
    Now the second constraint of~\eqref{eq:calibrationFunction:mixedLoss:hardConstr:proofSquare:obj1} that we want to satisfy is:
    \begin{equation}
        \label{eq:mixedLoss:secondConstraint}
        \q_i \geq \consbreakpoint \q_c + (1-\consbreakpoint)\Q_{\blockOfLabel{j}} \quad \forall c \in  \labelsOfBlock{\blockOfLabel{j}} \, .
    \end{equation}
    Using~\eqref{eq:mixedLossQcProperty}, we have that the RHS of~\eqref{eq:mixedLoss:secondConstraint} is $\leq 1/2$, and so since $\q_i \geq 1/2$, we have that~\eqref{eq:mixedLoss:secondConstraint} is satisfied for any valid mass distribution on block~$\blockOfLabel{j}$ (i.e. such that $\Q_{\blockOfLabel{j}} \leq 1/2$). Using $\q_j = 0$ gives the most possibilities for the value of~$\q_i$ in the constraint~\eqref{eq:mixedLoss:firstConstraint}. Moreover, the constraint~\eqref{eq:mixedLoss:firstConstraint} is more stringent than the constraint~\eqref{eq:mixedLoss:secondConstraint}, i.e. if it is satisfied, the second one is also satisfied; so we focus only on the first constraint.
    
    As in the proof of all other propositions, we can make the constraint~\eqref{eq:mixedLoss:firstConstraint} an equality for the optimum by generalizing the transformation of~\eqref{eq:tfproofs:truncatingToEps} which makes the constraint tight without changing the objective and maintaining feasibility. So~\eqref{eq:mixedLoss:firstConstraint} as an equality with $\q_j = 0$ yields the value
    $$
    \q_i^* = \frac{1+\eps-\consbreakpoint}{2-\consbreakpoint}.
    $$
    So to summarize at this point, we have $\q_j^* = 0$; $\q_c^* = 0$, $c \in \labelsOfBlock{\blockOfLabel{i}} \setminus \{i\}$; $\q_c^* = 0$, $\labelsOfBlock{\blockOfLabel{i}} \not\in \{\blockOfLabel{i}, \blockOfLabel{j}\}$.  $\q_i^* = \frac{1+\eps-\consbreakpoint}{2-\consbreakpoint}$ and $\Q_{\blockOfLabel{j}}=1-\q_i^*$. The precise distribution of mass for $c \in \labelsOfBlock{\blockOfLabel{j}}/\{j\}$ does not matter (any distribution is feasible and does not influence the objective, only the total mass matters), but for concreteness, we can choose them to all have the same mass yielding $\q_c^* = \frac{1-\eps}{(2-\consbreakpoint)(\blocksize-1)}$, $c \in \labelsOfBlock{\blockOfLabel{j}} \setminus \{j\}$.
    
    We now finish the computation of $\calibrationfunc_{ij}(\eps)$.
    First, we note that, due to the truncation argument similar to the one mentioned in the paragraph after~\eqref{eq:tfproofs:truncatingToEps}, we have  both $\scoreparam_i^*$ and $\scoreparam_j^*$ in the segment $[(1-\consbreakpoint)\Q_{\blockOfLabel{j}}^* + \frac{\consbreakpoint}{\blocksize} \Q_{\blockOfLabel{j}}^*-1, (1-\consbreakpoint)\Q_{\blockOfLabel{i}}^* + \frac{\consbreakpoint}{\blocksize} \Q_{\blockOfLabel{i}}^*-1]$ and since $\scoreparam_j^* \geq \scoreparam_i^*$, we have $\scoreparam_j^* = \scoreparam_i^* =: \scoreparam$ at the optimum.
    
    Substituting the values~$\Q_{\blockOfLabel{i}}^*$ and $\Q_{\blockOfLabel{j}}^*$ provided above into the objective of~\eqref{eq:calibrationFunction:mixedLoss:hardConstr:proofSquare:obj1} and performing unconstrained minimization w.r.t. $\scoreparam$ (we use the help of MATLAB symbolic toolbox to set the derivative to zero) we get
    \[
    \scoreparam^* = -\frac{\blocksize-\consbreakpoint+\consbreakpoint \blocksize}{2\blocksize}
    \]
    and, consequently, 
    \[
    \calibrationfunc_{ij}(\eps) =
    \frac{\blocksize(\eps-\frac{\consbreakpoint}{2})^2(\frac{\consbreakpoint}{\blocksize}+1-\consbreakpoint)^2}{4\outputvarcard(1-\frac{\consbreakpoint}{2})^2},
    \]
    which finishes the proof.
\end{proof}



\section{Constants in the SGD rate}
\label{sec:tranferAndSgd}
To formalize the learning difficulty by bounding the required number of iterations to get a good value of the risk (Theorem~\ref{th:calibrationSGD:kernels}), we need to bound the constants~$D$ and~$M$.
In this section, we provide a way to bound these constants for the quadratic surrogate~$\surrogatelossquad$~\eqref{eq:quadrLoss} under a simplifying assumption slightly stronger than the well-specified model Assumption~\ref{th:well_specification}.

%
%

%
%
Consider the family of score functions~$\scorefuncset_{\scorematrix, \hilbertspace}$ defined via an explicit feature map~$\featuremap(\inputvarv) \in \hilbertspace$, i.e., $\scorefunc_\parammatrix(\inputvarv) = \scorematrix \parammatrix \featuremap(\inputvarv)$, where a matrix $\scorematrix \in \R^{\outputvarcard \times \scoresubspacedim}$ defines the structure and an operator (which we think of as a matrix with one dimension being infinite) $\parammatrix : \hilbertspace \to \R^{\scoresubspacedim}$ contains the learnable parameters.
Then the surrogate risk can be written as
\[
\risk_\surrogateloss(\scorefunc_\parammatrix)
=
\E_{(\inputvarv, \outputvarv) \sim \data} \frac{1}{2\outputvarcard} \| \scorematrix \parammatrix \featuremap(\inputvarv) + \lossmatrix(:, \outputvarv) \|^2_{\R^\outputvarcard}
\]
and its stochastic w.r.t. $(\inputvarv, \outputvarv)$ gradient as
\begin{equation}
\gv_{\inputvarv, \outputvarv}(\parammatrix) = \frac{1}{\outputvarcard} \scorematrix^\transpose (\scorematrix \parammatrix \featuremap(\inputvarv) + \lossmatrix(:, \outputvarv)) \featuremap(\inputvarv)^\transpose
\end{equation}
where $\lossmatrix(:, \outputvarv)$ denotes the column of the loss matrix corresponding to the correct label~$\outputvarv$.
Note that computing the stochastic gradient requires performing products $\scorematrix^\transpose \scorematrix$ and $\scorematrix^\transpose \lossmatrix(:, \outputvarv)$ for which direct computation is intractable when $\outputvarcard$ is exponential, but which can be done in closed form for the structured losses we consider (the Hamming and block 0-1 loss).
More generally, these operations require suitable inference algorithms.

To derive the constants, we use a simplifying assumption stronger than Assumption~\ref{th:well_specification} in the case of quadratic surrogate: we assume that the conditional~$\q_c(\inputvarv)$, seen as a function of $\inputvarv$, belongs to the RKHS~$\hilbertspace$, which by the reproducing property implies that for each~$c=1,\dots,\outputvarcard$, there exists~$v_c \in \hilbertspace$ such that $\q_c(\inputvarv) = \langle v_c, \featuremap(\inputvarv) \rangle_\hilbertspace$ for all~$\inputvarv\in\inputdomain$.
Concatenating all~$v_c$, we get an operator $V: \hilbertspace \to \R^\outputvarcard$.
To derive the bound, we also assume that $\sum_{c=1}^{\outputvarcard} \|v_c\|_\hilbertspace \leq \qbound$ and $\|\featuremap(\inputvarv)\|_{\hilbertspace} \leq \rkhsbound$ for all~$\inputvarv \in \inputdomain$. In the following, we use the notation $\qv_\inputvarv$ to denote the vector in $\R^\outputvarcard$ with components $\q_c(\inputvarv)$, $c=1,\dots,\outputvarcard$, for a fixed $\inputvarv$, and thus $\qv_\inputvarv = V\featuremap(\inputvarv)$.

%

Under these assumptions, we can write the theoretical minimum of the surrogate risk.
The gradient of the surrogate risk gives
\begin{align*}
\outputvarcard \gradient_{\parammatrix} \risk_\surrogateloss(\scorefunc_\parammatrix)
%
%
&=
\scorematrix^\transpose \scorematrix \parammatrix \E_{\inputvarv \sim \data_{\inputdomain}}(\featuremap(\inputvarv) \featuremap(\inputvarv)^\transpose) + \scorematrix^\transpose \lossmatrix \E_{\inputvarv \sim \data_{\inputdomain}}(\qv_\inputvarv \featuremap(\inputvarv)^\transpose ) \\
&=
\scorematrix^\transpose \scorematrix \parammatrix \E_{\inputvarv \sim \data_{\inputdomain}}(\featuremap(\inputvarv) \featuremap(\inputvarv)^\transpose) + \scorematrix^\transpose \lossmatrix V \E_{\inputvarv \sim \data_{\inputdomain}}(\featuremap(\inputvarv) \featuremap(\inputvarv)^\transpose) \\
&= 
\left( \scorematrix^\transpose \scorematrix \parammatrix + \scorematrix^\transpose \lossmatrix V \right)  \E_{\inputvarv \sim \data_{\inputdomain}}(\featuremap(\inputvarv) \featuremap(\inputvarv)^\transpose) .
\end{align*}
Setting the content of the parenthesis to zero gives that $\parammatrix^* = -(\scorematrix^\transpose \scorematrix)^\pinv \scorematrix^\transpose \lossmatrix V$ is a solution to the stationary condition equation $\gradient_{\parammatrix} \risk_\surrogateloss(\scorefunc_\parammatrix) = 0$. 

We can now bound the Hilbert-Schmidt norm of this choice of optimal parameters~$\parammatrix^*$ as
\begin{align*}
\|\parammatrix^*\|_{HS}
&=
\| (\scorematrix^\transpose \scorematrix)^\pinv \scorematrix^\transpose \lossmatrix V \|_{HS}
&&
\\
%
%
&\leq
\| (\scorematrix^\transpose \scorematrix)^\pinv \scorematrix^\transpose \|_{HS} \|  \lossmatrix V \|_{HS} 
&&
\quad\text{//submultiplicativity of $\|\cdot\|_{HS}$}
\\
&\leq
\sqrt{\scoresubspacedim} \| (\scorematrix^\transpose \scorematrix)^\pinv \scorematrix^\transpose \|_2 \|  \lossmatrix V \|_{HS}
&&
\quad\text{//connection of $\|\cdot\|_{HS}$ and $\|\cdot\|_2$ via $\scoresubspacedim = \rank(\scorematrix)$}
\\
&=
\sqrt{\scoresubspacedim} \sigmamin^{-1}(\scorematrix) \|  \lossmatrix V \|_{HS}
&&
\quad\text{//rotation invariance of $\|\cdot\|_2$}
\\
&\leq 
\sqrt{\scoresubspacedim} \sigmamin^{-1}(\scorematrix) \sqrt{\outputvarcard} \Lmax \qbound
=: D &&
\quad\text{//the definition of $\|\cdot\|_{HS}$ and triangular inequality}
\end{align*}
where $\|\cdot\|_{HS}$ and $\| \cdot \|_2$ denote the Hilbert-Schmidt and spectral norms, respectively, and $\sigmamin^{-1}(\scorematrix)$ stands for the smallest singular value of the matrix~$\scorematrix$.
The last inequality follows from the definition of the Hilbert-Schmidt norm $\|  \lossmatrix V \|_{HS}^2 = \sum_{i=1}^\outputvarcard \|\sum_{c=1}^{\outputvarcard} \lossmatrix(i, c) v_c\|_{\hilbertspace}^2$ and from the triangular inequality $\|\sum_{c=1}^{\outputvarcard} \lossmatrix(i, c) v_c\|_{\hilbertspace} \leq \sum_{c=1}^{\outputvarcard} |\lossmatrix(i, c)|  \|v_c\|_{\hilbertspace} \leq \Lmax \qbound$ thus giving $\|  \lossmatrix V \|_{HS} \leq \sqrt{\outputvarcard} \Lmax \qbound$. 

Analogously, we now bound the Hilbert-Schmidt norm of the stochastic gradient~$\gv_{\inputvarv, \outputvarv}(\parammatrix)$.
\begin{align*}
\|\gv_{\inputvarv, \outputvarv}(\parammatrix)\|_{HS}
&\leq
\frac{1}{\outputvarcard}\| \scorematrix^\transpose \scorematrix \parammatrix \featuremap(\inputvarv) + \scorematrix^\transpose \lossmatrix(:, \outputvarv)) \|_2 \|\featuremap(\inputvarv)\|_{\hilbertspace}
\\
&\leq
\frac{1}{\outputvarcard}(\| \scorematrix^\transpose \scorematrix \parammatrix \featuremap(\inputvarv)\|_2 + \|\scorematrix^\transpose \lossmatrix(:, \outputvarv)) \|_2) \|\featuremap(\inputvarv)\|_{\hilbertspace}
\\
&\leq
\frac{1}{\outputvarcard}(\| \scorematrix^\transpose \scorematrix\|_2 \|\parammatrix\|_{HS} \|\featuremap(\inputvarv)\|_{\hilbertspace} + \|\scorematrix\|_2 \|\lossmatrix(:, \outputvarv)) \|_2) \|\featuremap(\inputvarv)\|_{\hilbertspace}
\\
&\leq
\frac{1}{\outputvarcard}\sigmamax^2(\scorematrix) D \rkhsbound^2 + \frac{1}{\outputvarcard}\sigmamax(\scorematrix) \sqrt{\outputvarcard}  \Lmax \rkhsbound =: M
\end{align*}
where $\rkhsbound$ is an upper bound on $\|\featuremap(\inputvarv)\|_{\hilbertspace}$ and $\sigmamax(\scorematrix)$ is a maximal singular value of~$\scorematrix$.
Here the first inequality follows from the fact that the rank of~$\gv_{\inputvarv, \outputvarv}(\parammatrix)$ equals 1 and from submultiplicativity of the spectral norm.
We also use the inequality~$\|\parammatrix \featuremap(\inputvarv)\|_{2} \leq \|\parammatrix\|_{HS} \|\featuremap(\inputvarv)\|_{\hilbertspace}$, which follows from the properties of the Hilbert-Schmidt norm.

The bound of Theorem~\ref{th:rkhsSgdConvergence} contains the quantity $DM$ and the step size of ASGD depends on $\frac{D}{M}$, so, to be practical, both quantities cannot be exponential (for numerical stability; but the important quantity is the number of iterations from Theorem~\ref{th:calibrationSGD:kernels}).
We have
\begin{align*}
DM
& =
\condnum^2(\scorematrix) \rkhsbound^2 \scoresubspacedim \Lmax^2 \qbound^2 + \condnum(\scorematrix) \rkhsbound \sqrt{\scoresubspacedim} \Lmax^2 \qbound
=
\Lmax^2 \xi(\condnum(\scorematrix) \sqrt{\scoresubspacedim} \rkhsbound  \qbound), \quad \xi(z) = z^2 + z,
\\
\frac{M}{D}
&=\frac{\sigmamax^2(\scorematrix)}{\outputvarcard} \rkhsbound^2 + \frac{\sigmamax(\scorematrix)\sigmamin(\scorematrix)}{\outputvarcard} \frac{\rkhsbound}{\qbound\sqrt{\scoresubspacedim}}
\end{align*}
where $\condnum(\scorematrix) = \frac{\sigmamax}{\sigmamin}$ is the condition number of $\scorematrix$.
Note that the quantity~$DM$ is invariant to the scaling of the matrix~$\scorematrix$.
The quantity~$\frac{D}{M}$ scales proportionally to the square of the scale of~$\scorematrix$ and thus rescaling~$\scorematrix$ can always bring it to~$\bigO(1)$. For the rest of the analysis, we consider $\rkhsbound$ and $\qbound$ to be well-behaved constants and thus focus on the dependence of the quantity~$DM$ on $\scorematrix$ and $\lossmatrix$. 

\subsection{Constants for specific losses}
\label{sec:tranferAndSgd:constants}
We now estimate the product~$DM$ from~\eqref{eq:sgdConstant} for the 0-1, block 0-1 and Hamming losses.
For the definition of the losses and the corresponding matrices~$\scorematrix$, we refer to Section~\ref{sec:bounds:interpretations}.

\textbf{0-1 loss.}
For the 0-1 loss~$\lossmatrix_{01}$ and $\scorematrix = \id_\outputvarcard$, we have $\Lmax=1$, $\scoresubspacedim=\outputvarcard$, $\sigmamin=\sigmamax=1$, thus $DM = \bigO(\outputvarcard)$ is very large leading to very slow convergence of ASGD.

\textbf{Block 0-1 loss.}
For the block 0-1 loss~$\lossmatrix_{01,\numblocks}$ and matrix~$\scorematrix_{01,\numblocks}$, we have $\Lmax=1$, $\scoresubspacedim=\numblocks$, $\sigmamin=\sigmamax=\sqrt{\blocksize}$, thus $DM = \bigO(\numblocks)$.

\textbf{Hamming loss.} For the Hamming loss, we have $\Lmax=1$, $\scoresubspacedim=\log_2 \outputvarcard + 1$, $\condnum(\scorematrix_{\hamming,\hamminglen}) \leq \log_2{\outputvarcard}+2$ (see the derivation in Section~\ref{sec:hammingLossProps}). Finally, we have $DM = \bigO(\log_2^3 \outputvarcard)$.


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


\section{Properties of the basis of the Hamming loss}
\label{sec:hammingLossProps}
As defined in~\eqref{eq:hammingLoss}, the matrix~$\lossmatrix_{\hamming,\hamminglen} \in \R^{\outputvarcard \times \outputvarcard}$ is the matrix of the Hamming loss between tuples of $\hamminglen$~binary variables, and the number of labels equals $\outputvarcard = 2^\hamminglen$.
Also recall that
$
\scorematrix_{\hamming,\hamminglen} := [ \frac12 \one_{2^\hamminglen}, \hv^{(1)}, \dots, \hv^{(\hamminglen)} ],
$
$
(\hv^{(\hammingindex)})_{\hat{\outputvarv}} := [\hat{\outputvar}_\hammingindex = 1],
$
$\hammingindex = 1,\dots,\hamminglen$.
We have $
\scoresubset_{\hamming,\hamminglen}
=
\colspace(\scorematrix_{\hamming,\hamminglen})
=
\colspace(\lossmatrix_{\hamming,\hamminglen})
$
 and $\rank(\lossmatrix_{\hamming,\hamminglen}) \!=\! \rank(\scorematrix_{\hamming,\hamminglen}) \!=\! \hamminglen\!+\!1$.

We now explicitly compute $\max_{i\neq j}\|\proj_{\scoresubset_{\hamming,\hamminglen}} \Delta_{ij}\|_2^2$.
We shortcut~$\scorematrix_{\hamming,\hamminglen}$ by~$\scorematrix$ and compute
\begin{equation}
\label{eq:hammingLoss:gramMatrix}
\scorematrix^\transpose \scorematrix = 2^{\hamminglen-2} \begin{bmatrix}
1 & 1 & \cdots  & 1 \\
1 & 2 & 1 & \cdots \\
1 & 1 & 2 & \cdots \\
\cdots & \cdots & \cdots & 1\\
1 & \cdots& 1  & 2
\end{bmatrix}.
\end{equation}
We can compute the inverse matrix explicitly as well:
\begin{equation}
\label{eq:hammingLoss:gramMatrixInv}
(\scorematrix^\transpose \scorematrix)^{-1} = 2^{2-\hamminglen} \begin{bmatrix}
1+\hamminglen & -1 & \cdots  & -1 \\
-1 & 1 & 0 & \cdots \\
-1 & 0 & 1 & \cdots \\
\cdots & \cdots & \cdots & 0\\
-1 & \cdots & 0  & 1
\end{bmatrix}.
\end{equation}
The vector $\scorematrix^\transpose \Delta_{ij}$ equals the difference of the two rows of~$\scorematrix$, i.e., $[0, c_1, \dots, c_\hamminglen]^\transpose \in \R^{\hamminglen+1}$ with each~$c_\hammingindex \in \{-1, 0, +1\}$. We explicitly compute the square norm~$\|\proj_{\scoresubset_{\hamming,\hamminglen}} \Delta_{ij}\|_2^2$: 
\[
\|\proj_{\scoresubset_{\hamming,\hamminglen}} \Delta_{ij}\|_2^2
=
\Delta_{ij}^\transpose \scorematrix (\scorematrix^\transpose \scorematrix)^{-1} \scorematrix^\transpose \Delta_{ij}
=
[0, c_1, \dots, c_\hamminglen] (\scorematrix^\transpose \scorematrix)^{-1} [0, c_1, \dots, c_\hamminglen]^\transpose
= 2^{2-\hamminglen} \sum_{t=1}^\hamminglen c_t^2,
\]
where the last equality follows from the identity submatrix of~\eqref{eq:hammingLoss:gramMatrixInv} and from the zero in the first position of the vector~$\scorematrix^\transpose \Delta_{ij}$.
The quantity $\|\proj_{\scoresubset_{\hamming,\hamminglen}} \Delta_{ij}\|_2^2$ is maximized when none of~$c_t$ equals zero, which is achievable, e.g., when the label~$i$ corresponds to all zeros and the label~$j$ to all ones.
We now have $\max_{i\neq j}\|\proj_{\scoresubset_{\hamming,\hamminglen}} \Delta_{ij}\|_2^2 = \frac{4\hamminglen}{2^\hamminglen}$.

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

We now compute the smallest and largest eigenvalues of the Gram matrix~\eqref{eq:hammingLoss:gramMatrix} for $\scorematrix_{\hamming,\hamminglen}$.
Ignoring the scaling factor~$2^{\hamminglen-2}$, we see by Gaussian elimination that the determinant and thus the product of all eigenvalues equals~$1$.
If we subtract~$\id_{\hamminglen+1}$ the matrix becomes of rank~$2$, meaning that $\hamminglen-1$ eigenvalues equal~$1$.
The trace, i.e., the sum of the eigenvalues of~\eqref{eq:hammingLoss:gramMatrix}, without the scaling factor~$2^{\hamminglen-2}$ equals~$2\hamminglen+1$.
Summing up, we have~$\lambdamin \lambdamax = 1$ and $\lambdamin + \lambdamax = \hamminglen + 2$.
We can now compute~$\lambdamin = \frac12(\hamminglen+2 - \sqrt{\hamminglen^2 + 4\hamminglen}) \in [\frac{1}{\hamminglen+2}, \frac{1}{\hamminglen}]$ and~$\lambdamax = \frac12(\hamminglen+2 + \sqrt{\hamminglen^2 + 4\hamminglen}) \in [\hamminglen+1, \hamminglen+2]$.
By putting back the multiplicative factor, we get 
$
\sigmamin = \sqrt{\lambdamin} \geq \frac{\sqrt{\outputvarcard}}{2\sqrt{\log_2{\outputvarcard}+2}}
$
and
$
\sigmamax = \sqrt{\lambdamax} \leq \frac{\sqrt{\outputvarcard}}{2}\sqrt{\log_2{\outputvarcard}+2}
$,
and thus the condition number is~$\condnum \leq \log_2{\outputvarcard}+2$.

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%




%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%


\end{document} 


%
%
%
%
%
%
%
%
%
%
\documentclass[11pt]{article}
\usepackage[dvips]{graphicx}
\usepackage{amssymb,amsmath,color}
\usepackage{url}

\usepackage[
    pdfstartview={FitH},
    bookmarks=true,
    bookmarksnumbered=true,
    bookmarksopen=true,
    bookmarksopenlevel=\maxdimen,
    pdfborder={0 0 0},
    colorlinks=true,
    linkcolor = blue,
    citecolor=blue,
    urlcolor=blue,
    filecolor=blue,
    pdfauthor={Simon Lacoste-Julien, Mark Schmidt, Francis Bach},
    pdftitle={A simpler approach to obtaining an O(1/t) convergence rate for the projected stochastic subgradient method},
    pdfdisplaydoctitle=true
]{hyperref}

\oddsidemargin .25in    %
\evensidemargin .25in \marginparwidth 0.07 true in
%
%
%
\topmargin -0.5in \addtolength{\headsep}{0.25in}
\textheight 8.5 true in       %
\textwidth 6.0 true in        %
\widowpenalty=10000 \clubpenalty=10000

%
\parindent 0pt
\topsep 4pt plus 1pt minus 2pt
\partopsep 1pt plus 0.5pt minus 0.5pt
\itemsep 2pt plus 1pt minus 0.5pt
\parsep 2pt plus 1pt minus 0.5pt
\parskip .5pc

\newcommand{\note}[1]{{\textbf{\color{red}#1}}}


\title{A simpler approach to obtaining an $O(1/t)$ convergence rate for the projected stochastic subgradient method}

\author{ \textbf{Simon Lacoste-Julien, Mark Schmidt, and Francis Bach}\\ 
INRIA - Sierra project-team\\
D\'epartement d'Informatique de l'Ecole Normale Sup\'erieure \\
Paris, France 
} 
 
\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}


\newcommand{\BEAS}{\begin{eqnarray*}}
\newcommand{\EEAS}{\end{eqnarray*}}
\newcommand{\BEA}{\begin{eqnarray}}
\newcommand{\EEA}{\end{eqnarray}}
\newcommand{\BEQ}{\begin{equation}}
\newcommand{\EEQ}{\end{equation}}
\newcommand{\BIT}{\begin{itemize}}
\newcommand{\EIT}{\end{itemize}}
\newcommand{\BNUM}{\begin{enumerate}}
\newcommand{\ENUM}{\end{enumerate}}
\newcommand{\BA}{\begin{array}}
\newcommand{\EA}{\end{array}}
\newcommand{\diag}{\mathop{\rm diag}}
\newcommand{\Diag}{\mathop{\rm Diag}}

\newcommand{\Cov}{{\mathop { \rm cov{}}}}
\newcommand{\cov}{{\mathop {\rm cov{}}}}
\newcommand{\corr}{{\mathop { \rm corr{}}}}
\newcommand{\Var}{\mathop{ \rm var{}}}
\newcommand{\argmin}{\mathop{\rm argmin}}
\newcommand{\argmax}{\mathop{\rm argmax}}
\newcommand{\var}{\mathop{ \rm var}}
\newcommand{\Tr}{\mathop{ \rm tr}}
\newcommand{\tr}{\mathop{ \rm tr}}
\newcommand{\sign}{\mathop{ \rm sign}}
\newcommand{\idm}{I}
\newcommand{\rb}{\mathbb{R}}
\newcommand{\BlackBox}{\rule{1.5ex}{1.5ex}}  %
\newcommand{\lova}{Lov\'asz }

\newenvironment{proof}{\par\noindent{\bf Proof\ }}{\hfill\BlackBox\\[2mm]}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}

\renewcommand{\labelitemiii}{$-$}

\newcommand{\mysec}[1]{Section~\ref{sec:#1}}
\newcommand{\eq}[1]{Eq.~(\ref{eq:#1})}
\newcommand{\myfig}[1]{Figure~\ref{fig:#1}}


\def \supp{ { \rm Supp }}
\def \card{ { \rm Card }}
\def \E { \mathbb{E} }

\begin{document}
 \maketitle
 
 \begin{abstract}
 In this note, we present a new averaging technique for the projected stochastic subgradient method. By using a weighted average with a weight of $t+1$ for each iterate $w_t$ at iteration $t$, we obtain the convergence rate of $O(1/t)$ with both an easy proof and an easy implementation. The new scheme is compared empirically to existing techniques, with similar performance behavior.
 \end{abstract}
 
 
 \section{Introduction}
 We consider a strongly convex function $f$ defined on a convex set $K$. We denote  by $\mu$ its strong convexity constant. Following~\cite{shalev2007pegasos,nemirovski2009robust,hazan2011beyond,rakhlin2012making}, we consider a stochastic approximation scenario where only unbiased estimates of subgradients of $f$ are available, with the projected stochastic subgradient method.
 
 More precisely, we assume that we have an increasing sequence of $\sigma$-fields $(\mathcal{F}_t)_{t \geqslant 0}$, such that 
 $w_0 \in K$ is $\mathcal{F}_0$-measurable and such that for all $t \geqslant 1$,
 \begin{equation} \label{eq:update}
 w_{t} = \Pi_K \big( w_{t-1} - \gamma_t g_t \big),
 \end{equation}
 where 
 
 \BIT
 \item[(a)]  $\Pi_K$ is the orthogonal projection on $K$, 
 \item[(b)] $ \E ( g_t | \mathcal{F}_{t-1} ) $ is almost surely a subgradient of $f$ at $w_{t-1}$ (which we denote $f'(w_{t-1})$), 
 \item[(c)]  $\E (\| g_t \|^2 ) \leqslant B^2 $ (finite variance condition).
 \EIT
 We denote by $w^\ast$ the unique minimizer of $f$ on $K$. 
 
 \section{Motivating example} \label{sec:example}
 Our main motivating example is the support vector machine (SVM) and its structured prediction extensions~\cite{shalev2010pegasos, tsochantaridis2006large,ratliff2007subgradient}, where the pairs $(x_t,y_t)$ for $t \geqslant 1$ are independent and identically distributed and $f(w) = \E \ell(y,w^\top x) + \frac{\mu}{2} \| w\|^2$, where $\ell(y,u)$ is a Lipschitz-continuous convex loss function (with respect to the second variable) and $K$ is the whole space (unconstrained setup). We then have 
$g_t = \ell'(y_t,w_{t-1}^\top x_t ) x_t + \mu w_{t-1}$, where $\ell'(y,u)$ denotes any subgradient with respect to the second variable.

If we make the additional assumption that $\E \| x\|^2$ is finite, then this setup satisfies the assumptions above with $B^2 = 4 L^2_{\ell} \E \| x\|^2$, where $L_{\ell}$ is the Lipschitz constant for $\ell$. We show this bound in Appendix~\ref{sec:SVMbound}.

Alternatively, we can consider $K$ to be a compact convex subset. This is used in particular in a projected version of the stochastic subgradient method for SVM in~\cite{shalev2007pegasos}. In this case, we can take $B^2 = (L_{\ell} \sqrt{\E \| x\|^2} + \mu \max_{w \in K} \| w\|)^2 $.
 
 \section{Convergence analysis}
 Following standard proof techniques~\cite{shalev2007pegasos,nemirovski2009robust}, we have:
 \BEAS
 \| w_t - w^\ast \|^2 & \leqslant & \| w_{t-1} - \gamma_t g_t - w^\ast \|^2 
 \mbox{ because orthogonal projections contract distances,}\\
  & = &  \| w_{t-1}- w^\ast \|^2 + \gamma_t^2 \| g_t \|^2 - 2 \gamma_t ( w_{t-1}- w^\ast)^\top g_t \\
 \E ( \| w_t - w^\ast \|^2  | \mathcal{F}_{t-1} ) 
 & \leqslant &  \| w_{t-1}- w^\ast \|^2 + \gamma_t^2 \E (\| g_t \|^2 | \mathcal{F}_{t-1} ) - 2 \gamma_t ( w_{t-1}- w^\ast)^\top f'(w_{t-1}) \\
 & \leqslant &  \| w_{t-1}- w^\ast \|^2 + \gamma_t^2 \E (\| g_t \|^2 | \mathcal{F}_{t-1} ) - 2 \gamma_t 
 \big[
 f(w_{t-1})  - f(w^\ast) + \frac{\mu}{2} \| w_{t-1}- w^\ast \|^2
 \big]
.
 \EEAS
 
 The last inequality is obtained from the $\mu$-strong convexity of $f$.
 Thus, by re-arranging the function values on the LHS and taking expectations on both sides, we get:
 \BEA
 2 \gamma_t \big[ \E f(w_{t-1}) - f(w^\ast)  \big]
 & \leqslant &  \gamma_t^2 \E \| g_t \|^2 + ( 1 - \mu \gamma_t ) \E \| w_{t-1}- w^\ast \|^2
 -\E \| w_t - w^\ast \|^2 \nonumber
\\
 \E f(w_{t-1}) - f(w^\ast)   
 & \leqslant &  \frac{\gamma_t B^2}{2} + \frac{ \gamma_t^{-1} - \mu  }{2} \E \| w_{t-1}- w^\ast \|^2
 - \frac{ \gamma_t^{-1}   }{2} \E \| w_t - w^\ast \|^2. \label{eq:ineq}
 \EEA
 
 \subsection{Classical analysis}
 
 With $\displaystyle \gamma_t = \frac{1}{\mu t}$, then inequality~\eqref{eq:ineq} becomes 
 $$
  \E f(w_{t-1}) - f(w^\ast)   
  \leqslant    \frac{  B^2}{2 \mu t} + \frac{\mu(t-1) }{2} \E \| w_{t-1}- w^\ast \|^2
 - \frac{ \mu t   }{2} \E \| w_t - w^\ast \|^2,
 $$
 and by summing from $t=1$ to $t=T$, we obtain:
 \BEAS
 \E f \bigg(
  \frac{1}{T} \sum_{t=1}^T w_{t-1}
 \bigg) - f(w^\ast) & \leqslant & 
 \frac{1}{T} \sum_{t=1}^T 
  \E f(w_{t-1}) - f(w^\ast)   \\
  &
  \leqslant &  \frac{B^2}{2 \mu T } \sum_{t=1}^T \frac{1}{t} + \frac{\mu}{2T} \left[0 -  
   T \E \| w_T- w^\ast \|^2 \right] \leqslant \frac{B^2}{2 \mu T } ( 1 + \log T).
 \EEAS
The first line used the convexity of $f$; the second line is obtained from a telescoping sum. We also obtain $\displaystyle \E \| w_T- w^\ast \|^2 \leqslant \frac{B^2}{\mu^2 T } ( 1 + \log T)$.

 \subsection{New analysis} \label{sec:new_analysis}

 With $\displaystyle \gamma_t = \frac{2}{\mu(t+1)}$ and multiplying inequality~\eqref{eq:ineq} by $t$, we obtain:
 
 \BEAS
 t \big[ \E f(w_{t-1}) - f(w^\ast)   \big]
 & \leqslant &  \frac{t B^2}{
\mu(t+1)} + 
\frac{\mu}{4} \bigg[ t (t-1)
  \E \| w_{t-1}- w^\ast \|^2
 - t(t+1) \E \| w_t - w^\ast \|^2
 \bigg]
\\
 & \leqslant &  \frac{  B^2}{
\mu } + 
\frac{\mu}{4} \bigg[ t (t-1)
  \E \| w_{t-1}- w^\ast \|^2
 - t(t+1) \E \| w_t - w^\ast \|^2
 \bigg].
 \EEAS
 
 By summing from $t = 1$ to $t=T$ these $t$-weighted inequalities, we obtain a similar telescoping sum, but this time the term with $B^2$ stays constant across the sum:
 
 \BEA
\sum_{t=1}^T  t \big[ \E f(w_{t-1}) - f(w^\ast)   \big]
   & \leqslant &  \frac{  T B^2}{
\mu } + 
\frac{\mu}{4} \bigg[ 0 
 - T(T+1) \E \| w_T - w^\ast \|^2
 \bigg]. \label{eq:telescope}
 \EEA
 
 Thus
 $$
 \E f \bigg( \frac{2}{T(T+1)} \sum_{t=0}^{T-1} (t+1) w_t \bigg) - f(w^\ast) 
 + \frac{\mu }{2} \E \| w_T - w^\ast \|^2
 \leqslant \frac{  2 B^2}{ \mu( T + 1 )}
 $$
 which implies 
 $$ \E f \bigg( \frac{2}{T(T+1)} \sum_{t=0}^{T-1} (t+1) w_t \bigg)   - f(w^\ast)  
 \leqslant
 \frac{  2 B^2}{ \mu( T + 1 )}$$
 and 
 $$
 \E \| w_T - w^\ast \|^2 \leqslant \frac{  4 B^2}{ \mu^2( T + 1 )}.
 $$
 
So by using the weighted average $\bar{w}_T \doteq \frac{2}{(T+1)(T+2)}\sum_{t=0}^T (t+1) w_t$ instead of a uniform average, we get a $O(\frac{1}{T})$ rate instead of $O(\frac{\log T}{T})$. Note that these averaging schemes are efficiently implemented in an online fashion as:
\begin{equation} \label{eq:wavg}
\bar{w}_t = (1-\rho_t) \bar{w}_{t-1}  + \rho_t w_t .
\end{equation}
For the proposed weighted averaging scheme, $\rho_t = 2/(t+2)$ (compare with $\rho_t = 1/(t+1)$ for the uniform averaging scheme).
 
 \section{Experiments}
 

\begin{figure}
\centering
\mbox{%
\includegraphics[width=.33\textwidth]{WSG_hinge_quantum.pdf} \hspace*{-.1cm}
\includegraphics[width=.33\textwidth]{WSG_hinge_protein.pdf} \hspace*{-.1cm}
\includegraphics[width=.33\textwidth]{WSG_hinge_sido.pdf} \hspace*{-.1cm}}
\mbox{%
\includegraphics[width=.33\textwidth]{WSG_hinge_rcv1.pdf} \hspace*{-.1cm}
\includegraphics[width=.33\textwidth]{WSG_hinge_covertype.pdf} \hspace*{-.1cm}
\includegraphics[width=.33\textwidth]{WSG_hinge_news.pdf} \hspace*{-.1cm}}
\caption{Comparison of optimization strategies for support vector machine objective. Top from to right: \emph{quantum}, \emph{protein}, and \emph{sido} data sets. Bottom from left to right: \emph{rcv1}, \emph{covertype}, and \emph{news} data sets. This figure is best viewed in colour.}
\label{fig:svm}
\end{figure} 
 
To test the empirical performance of the averaging scheme, we performed a series of experiments using the support vector machine optimization problem
\[
\min_w \; \frac{\lambda}{2}\|w\|^2 + \frac{1}{n}\sum_{i=1}^n \max\{0,1-y_iw^\top x_i\},
\]
where $x_i$ is in an Euclidean space and $y_i \in \{-1,1\}$.

We performed experiments on a set of freely available benchmark binary classification data sets.  The 
\emph{quantum} ($n=50 000$, $p=78$) and 
\emph{protein}   ($n=145 751$, $p=74$) data sets  were obtained from the KDD Cup 2004 website,\footnote{\small\url{http://osmot.cs.cornell.edu/kddcup}} the \emph{sido} data set ($n=12678$, $p=4932$) was obtained from the Causality Workbench website,\footnote{\small\url{http://www.causality.inf.ethz.ch/home.php}}
 while the \emph{rcv1}  ($n= 20 242$, $p=47 236$), \emph{covertype}   ($n=581 012 $, $p=54$), and \emph{news} ($n=19 996$, $p= 1 355 191$) data sets were obtained from the LIBSVM data website.\footnote{\small\url{http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets}} 
We added a (regularized) bias term to all data sets, and for dense features we standardized so that they would have a mean of zero and a variance of one. We set the regularization parameter $\lambda$ to $1/n$, although we found that the relative performance of the methods was not particularly sensitive to this choice. We didn't use any projection ($K$ is the whole space). Our experiments compared the following averaging strategies:
\BIT
\item[--] {\bf 0}:  No averaging.
\item[--] {\bf 1}: Averaging all iterates with uniform weight.
\item[--] {\bf 0.5}: Averaging the second half of the iterates with uniform weight, as proposed in~\cite{rakhlin2012making}.
\item[--] {\bf D}: Averaging all iterates since the last iteration that was a power of 2 with uniform weight (the `doubling trick'), also proposed in~\cite{rakhlin2012making}.
\item[--] {\bf W}: Averaging all iterates with a weight of $t+1$, as discussed in this note.
\item[--] {\bf W$^2$}: Averaging all iterates with a weight of $(t+1)^2$, which puts even further emphasis on recent iterations.
\EIT
We plot the performance of these different averaging strategies in Figure~\ref{fig:svm}, which shows the objective function against the number of effective passes through the data (the number of iterations divided by $n$). This figure uses a step size of $1/\mu t$ for all methods as we found this gave better performance than a step size of $2/\mu(t+1)$, although we include the performance of {\bf W} with the latter step-size for comparison.
In Figure~\ref{fig:svm}, we observe the following trends:
\BIT
\item[--] {\bf 0}: Not averaging at all is typically among the worst strategies. However, this proved to be the best strategy on the \emph{sido} data set. This may be because the method is still far from the solution after $50$ passes through the data.
\item[--] {\bf 1}: Uniform averaging of all iterates is always the worst strategy.
\item[--] {\bf 0.5}: Uniform averaging of the second half of the iterates is typically among the best strategies, provided we are in fact in the second half of the iterates.
\item[--] {\bf D}: The doubling trick typically gave among the best performance across the methods.
\item[--] {\bf W}: The proposed weighting typically performed between the doubling trick and not averaging.
\item[--] {\bf W$^2$}: Weighting the iterates by $(t+1)^2$ always outperformed weighting them by $t+1$.
\EIT

 
 \section{Discussion} 
 \BIT
 \item[--] We note that the averaging of linear approximations of $f$ (rather than the iterates) by $t+1$ is also used in the optimization strategy of Nesterov~\cite{nesterov2005smooth}, which achieves an optimal $O(1/t^2)$ convergence rate for optimizing (deterministic) objectives with Lipschitz-continuous gradients (see step 3 for their Equation 3.11). 
%
 \item[--] There are previous approaches to removing the $\log t$ term~\cite{hazan2011beyond,rakhlin2012making}, but the one presented in this note is arguably somewhat simpler to implement and analyze. Rakhlin et al. propose in~\cite{rakhlin2012making} the `1/2-suffix averaging' scheme (and the `doubling trick' that we used in the experiments). Their proof technique requires separately bounding $\E \| w_t-w^\ast\|^2$ and then controlling the sum of inequalities in~\eqref{eq:ineq} by using that only the last half of the iterations is averaged (the `1/2-suffix'). Hazan and Kale propose in~\cite{hazan2011beyond} the epoch-GD scheme, which uses a similar averaging schedule as in the `doubling trick' of~\cite{rakhlin2012making}, but using a fixed step-size within each geometrically sized `epoch' of averaging, as well as using the previous average as the initialization for an epoch.
 \item[--] We note that all the schemes presented in the experiments can have their convergence rate proven. Schemes \textbf{0} and \textbf{1} have $O((\log t)/ t)$ rate whereas the schemes \textbf{0.5}, \textbf{D}, \textbf{W} and \textbf{W$^2$} have $O(1/t)$ rate. We can show the $O(1/t)$ rate for general weighted averaging schemes (with weight $t^k$ for iterate $t$ for some fixed $k \ge 1$) as well as step-sizes of the form $\gamma_t = c/(t+b)$ for $c > 1/2$ and $b\geq 0$. The proof becomes longer though as the nice telescoping sum in~\eqref{eq:telescope} doesn't cancel out in these cases. One has to use instead a bound on $\E \| w_t-w^\ast\|^2$ such as in Lemma 1 in~\cite{rakhlin2012making} to control the non-canceling terms. The overall rate is still $O(1/t)$, but with different constants depending on $c$ and $k$. %
 \item[--] At the same time that we first posted this note, Shamir and Zhang independently proposed a similar weighted average scheme in~\cite{shamir2012sgd} which they call `polynomial-decay averaging'. They consider a running average scheme as in~\eqref{eq:wavg}, but with the more general $\rho_t = \frac{1+\eta}{t+1+\eta}$, where the integer $\eta \geq 0$ parameterizes the different schemes.\footnote{We note that the index $t$ is shifted by one between this note and their paper as their initial point is $w_1$ whereas ours is $w_0$. We also note that they use the misnomer `gradient descent' for their algorithm despite using subgradients which don't necessarily yield a descent direction.} $\eta = 0$ yields the standard uniform averaging scheme, whereas $\eta = 1$ yields the simple weighted average analyzed in Section~\ref{sec:new_analysis}. The general $\eta$ gives a weight of $O(t^\eta)$ for each iterate, similar to what was mentioned in the previous paragraph, but with a different exact formula. They provide in~\cite{shamir2012sgd} a proof of a rate of $O(1/t)$ for $\eta \geq 2$. The proof that we give in Section~\ref{sec:new_analysis} can be seen as complementary and is especially much simpler (as well as giving a tighter constant). We also note that the rate of $O((\log t)/ t)$ for the last iterate $w_t$ (scheme \textbf{0} above) is proven for the first time in~\cite{shamir2012sgd}.
 \item[--] While this paper focuses on the non-smooth case, it is still interesting to relate results to the smooth case (see, e.g.,~\cite{bach2011non} and references therein), where in the strongly convex case, averaging with longer step sizes---i.e., of the form $t^{-\alpha}$ with $\alpha \in (1/2,1)$---leads to better and more robust rates. Can larger step sizes improve results for the non-smooth case?
 \EIT
 
 \appendix
 
 \section{Finite variance bound for SVM} \label{sec:SVMbound}
 We derive here the finite variance bound $\E \| g_t \|^2  \leq  4 L^2_{\ell} \E \| x\|^2 = B^2$ for the general SVM-like objective considered in Section~\ref{sec:example} and update rule~\eqref{eq:update}. To see this, we consider the more general case of $f(w) = \E h(z,w) + \frac{\mu}{2} \| w\|^2$, where $h(z,w)$ is convex in $w$ for each $z$ (for SVM, $z=(x,y)$ and $h(z,w) = \ell(y,w^\top x)$). We make a Lipschitz-like (in expectation) assumption on $h$ that $\E \| h'(z,w) \|^2 \leq L^2$, where $h'(z,w)$ denotes any subgradient with respect to the second variable (note that $L^2 = L^2_{\ell} \E \| x\|^2$ for SVM). With $g_t = h'(z_t,w_{t-1})+\mu w_{t-1}$, Leibniz rule yields $\E (g_t | w_{t-1}) = f'(w_{t-1})$, as required by our setup (see~(1.3) in~\cite{nemirovski2009robust} for some regularity conditions for this to be true). Given this definition of $g_t$, we use the Minkowski inequality on the norm function\footnote{Minkowski inequality says that $\sqrt{\E (X+Y)^2} \leq \sqrt{\E X^2} + \sqrt{\E Y^2}$ for scalar random variables $X$ and $Y$. If we have $a=b+c$ for some random vectors $a,b,c$, by the triangle inequality, we have $\|a\|\leq\|b\|+\|c\|$ and so $\sqrt{\E \|a\|^2} \leq \sqrt{\E (\|b\|+\|c\|)^2} \leq \sqrt{\E \|b\|^2} + \sqrt{\E \|c\|^2}$ by using Minkowski on the norm of the random vectors.} to get:
 $$
 	\sqrt{\E \| g_t \|^2} \leq \sqrt{\E \| h'(z_t,w_{t-1}) \|^2} + \mu \sqrt{\E \| w_{t-1} \|^2}
 	\leq L + \mu \sqrt{\E \| w_{t-1} \|^2}.
 $$
We can then obtain the required bound of $(2L)^2$ on $\E \| g_t \|^2$ by showing that $\sqrt{\E \| w_{t-1} \|^2} \leq L/\mu$. This can easily be proven by induction, with the assumption that $\gamma_t \leq 1/\mu$ and either $\gamma_1 = 1/\mu$ or $\sqrt{\E \|w_0 \|^2} \leq L/\mu$ (these assumptions are satisfied by the step sizes considered in this note). To see this, we use the subgradient update~\eqref{eq:update} applied to this form of $f(w)$:
 \BEAS
w_t & = & (1-\mu \gamma_t) w_{t-1} - \gamma_t h'(z_t, w_{t-1}). \\
 \EEAS
Applying Minkowski inequality again, we get
 \BEAS
\sqrt{\E \| w_t \|^2} & \leq &(1-\mu \gamma_t) \sqrt{\E \| w_{t-1} \|^2}+ \gamma_t \sqrt{\E \| h'(z_t,w_{t-1} \|^2} \\
& \leq & (1-\mu \gamma_t) \sqrt{\E \| w_{t-1} \|^2} + \mu \gamma_t \frac{L}{\mu}.
 \EEAS
The first line above used the assumption that $\gamma_t \leq 1/\mu$ to ensure that $(1-\mu \gamma_t)$ is non-negative. The assumption $\gamma_1 = 1/\mu$ or $\sqrt{\E \|w_0 \|^2} \leq L/\mu$ then yields the base case of $t=1$. Plugging in the induction hypothesis then yields:
 \BEAS
\sqrt{\E \| w_t \|^2} & \leq &(1-\mu \gamma_t) \frac{L}{\mu} + \mu \gamma_t \frac{L}{\mu} = \frac{L}{\mu},
 \EEAS
which completes the proof.
  \bibliography{no_pain}
\bibliographystyle{unsrt}
 
 
   \end{document}\documentclass{article} %
\pdfoutput=1 %
\usepackage{nips15submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{amsthm,amsmath}
\usepackage{chngcntr}

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{comm}{Comment}
\newcommand{\Prob}{{{\bf P}}}
\renewcommand{\Re}{{{\mathbb R}}}
\renewcommand{\L}{{\mathcal L}}

\DeclareMathOperator*{\argmin}{argmin} 
\DeclareMathOperator*{\argmax}{argmax} 

%

%
\usepackage{times}
%
\usepackage{graphicx} %
%
\usepackage{subfigure} 

%
%

%
\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{amssymb, amsmath}
\usepackage{amsthm}
\usepackage{booktabs}  %
\usepackage{color}
\usepackage{centernot}
%
\usepackage{mathtools}
\usepackage{multirow}
\usepackage{sidecap}
\usepackage{tabularx}

\input{defs}
\newcommand{\comment}[1]{{\color{red} [#1]}}

\graphicspath{{images/}{figures/}}   %

\title{Variance Reduced Stochastic Gradient Descent \\with Neighbors}


\author{
Thomas Hofmann\\
Department of Computer Science\\
ETH Zurich, Switzerland
\And
Aurelien Lucchi\\
Department of Computer Science\\
ETH Zurich, Switzerland
\And
Simon Lacoste-Julien\\
INRIA - Sierra Project-Team\\
\'{E}cole Normale Sup\'{e}rieure, Paris, France 
\And
Brian McWilliams\\
Department of Computer Science\\
ETH Zurich, Switzerland
}

%
%
%
%
%
%
%

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\nipsfinalcopy %



\begin{document}


\maketitle

\input{nsaga_abstract}
\input{nsaga_introduction}
\input{nsaga_memorization}
\input{nsaga_sharing}
\input{nsaga_results}
\input{nsaga_conclusion}

\bibliographystyle{abbrv}
\bibliography{nsaga}

\input{nsaga_appendix}

\end{document}
%

\begin{abstract} 

Stochastic Gradient Descent (SGD) is a workhorse in machine learning, yet its slow convergence can be a computational bottleneck. Variance reduction techniques such as SAG, SVRG and SAGA have been proposed to overcome this weakness, achieving linear convergence. However, these methods are either based on computations of full gradients at pivot points, or on keeping per data point corrections in memory. Therefore speed-ups relative to SGD may need a minimal number of epochs in order to materialize. This paper investigates algorithms that can exploit neighborhood structure in the training data to share and re-use information about past stochastic gradients across data points, which offers advantages in the transient optimization phase.  As a side-product we provide a unified convergence analysis for a family of variance reduction algorithms, which we call memorization algorithms. We provide experimental results supporting our theory. 
\end{abstract} 
%

\newpage


\appendix

\section{Appendix}
\label{sec:appendix}

\setcounter{lemma}{0}
\setcounter{theorem}{0}
\setcounter{corollary}{0}

\begin{lemma}
For the iterate sequence of any algorithm that evolves solutions according to Eq.~\eqref{eq:sgd-corrected}, the following holds for a single update step, in expectation over the choice of $i$, with \\$\triangle := \| w - w^*\|^2  - \E \| w^+ - w^*\|^2$, then: 
\begin{align}
\triangle \ge  &\quad \gamma \mu \| w- w^*\|^2  - 2 \gamma^2  \E \| \alpha_i - f_i'(w^*)\|^2  + \left( 2\gamma - 4 \gamma^2 L  \right) f^\delta(w) \,.
\nonumber %
\end{align}
\vspace*{-6mm}
\begin{proof} 
Starting from Eq.~\eqref{eq:recurrence} we have
\begin{align*}
\triangle
= &
2 \gamma \langle f'(w), w-w^*\rangle - \gamma^2 \E \| g_i(w) \|^2 
%
\\ \stackrel{\eqref{eq:naive-strong-convexity}}{\ge} &  
\gamma \mu \| w-w^*\|^2 + 2 \gamma f^\delta (w)  - \gamma^2 \E \| g_i(w) \|^2 \\
%
\stackrel{\eqref{eq:beta-split}}{\ge} &  
\gamma \mu  \| w-w^*\|^2  +  2 \gamma f^\delta (w) 
%
- 2 \gamma^2 \E \| f'_i(w) - f'_i(w^*) \|^2 
-2 \gamma^2  \E \| \bar \alpha_i - f_i'(w^* )\|^2 \\
\stackrel{\eqref{eq:expected-smoothness},\eqref{eq:alpha-simplification}}{\ge} &  
\gamma \mu \| w- w^*\|^2  + 
2 \gamma ( 1 - 2\gamma L  )  f^\delta(w)
  - 2 \gamma^2 \E \| \alpha_i - f_i'(w^*)\|^2  \nonumber
\,.\end{align*}
\end{proof}
\end{lemma}

\begin{lemma} 
For a uniform $q$-memorization algorithm, it holds that 
\begin{align*}
\E \bar H^+ = \left( \frac{n-q}{n} \right) \bar H + \frac{2Lq}{n} \, f^\delta(w).
\end{align*}
\begin{proof} From the uniformity property $(*)$ in Definition \ref{def:memorization}, it follows that 
\begin{align}
n \E H^+ \! = \sum_{i=1}^n \E H^+_i 
\stackrel{(*)}= \! \sum_{i=1}^n \left( \left(1-\frac{q}{n} \right) H_i +  2L\left( \frac{q}{n} \right)h_i(w) \right)
= (n-q) \bar H + \frac {2L q}n \sum_{i=1}^n h_i(w).
\nonumber
\end{align}
Exploiting the fact that $\frac 1n \sum_{i=1}^n h_i(w) = f(w)-f(w^*) + 0= f^\delta(w)$  completes the proof. 
\end{proof} 
\label{lemma:hrecurrence2-appendix}
\end{lemma}

\begin{lemma}
\label{lemma:lyapunov-appendix}
Fix $c \in (0;1]$ and $\sigma \in [0;1]$ arbitrarily. For any uniform $q$-memorization algorithm with sufficiently small step size $\gamma$ such that
\begin{align*}
\gamma \le \frac{1}{L} \min \left\{\frac{K\sigma }{2K + 4c\sigma}, \frac{1-\sigma}{2} \right\}, \quad 
\text{and} \quad K:=  \frac{4qL}{n \mu},
\end{align*}
we have that
\begin{align}
\E \L_\sigma(w^+,H^+)  \le (1-\rho) \L_\sigma(w,H),\quad \text{with} \quad \rho := c \mu \gamma.
\end{align}
Note that $\gamma < \frac 1{2L} \max_{\sigma \in [0,1]} \min\{ \sigma ,  1-\sigma\} = \frac{1}{4L}$ (in the $c \to 0$ limit).
\begin{proof}
From Lemma \ref{lemma:w-recurrence}, we can see that we will have $\rho \leq \gamma \mu$ based on the $ \| w - w^*\|^2$ part of $\L_\sigma$. Hence, we can write the rate as $\rho = c \mu \gamma$, where  $0 < c \leq 1$. 

Let us now apply both, Lemma \ref{lemma:w-recurrence} and Lemma \ref{lemma:hrecurrence2}, to quantify the progress guaranteed to be made in one iteration of the algorithm in expectation, combining the changes to the iterate $w \to w^+$ as well as those to the memory $\alpha \to \alpha^+$ into $\L_\sigma$. Set $\triangle_\sigma := \L_\sigma(w,H) - \E \L_\sigma^+(w,H)$, then 
\begin{align}
\triangle_\sigma  = &  \| w- w^*\|^2 - \E \| w^+ - w^*\|^2 + S \sigma \,  \left( \bar H - \E \bar H^+ \right) \\
\ge &  \gamma \mu \| w- w^*\|^2  - 2 \gamma^2  \E \| \alpha_i - f_i'(w^*)\|^2  + 2\gamma \left( 1 - 2\gamma L  \right) f^\delta(w) \nonumber \\
& + S \sigma \left( \bar H -  \left( \frac{n-q}{n} \right) \bar H - \frac{2Lq}{n} \, f^\delta(w) ) \right).
\nonumber
\end{align}
As we argued after Eq.~\eqref{eq:recurrence-h1}, the definition of $H_i$ combined with property~\eqref{eq:alpha-bound} ensure the crucial bound $\E\| \alpha_i - f_i'(w^*) \|^2 \le \bar H$. Including it and gathering terms in the same ``units", we get: 
\begin{align}
\triangle_\sigma \ge &  \; \gamma \mu \| w- w^*\|^2 + 
\left[ S \sigma \left( \frac qn \right) - 2 \gamma^2  \right] \bar H 
 \label{eq:recbrackets2}
+ 2 \left[ 
   -S \sigma  L \left( \frac qn \right) 
  + \gamma \left( 1 -2 \gamma L \right) 
\right] f^\delta(w) 
\end{align}
We can further simplify the term in the second rectangular brackets with the definition of $S$ (in hindsight motivating its definition):
\begin{align}
- 2S \sigma  L \left( \frac qn \right)  + 2\gamma \left( 1 - 2\gamma L\right)
= 2\gamma \left[ -\sigma + \left( 1 - 2 \gamma L \right) \right] 
\end{align}
We require this term to be non-negative, so that we can safely drop it. This leads an upper bound requirement on the step size:
\begin{align}
- \sigma + 1 - 2\gamma L \ge 0 \iff 
\gamma \leq \frac{1- \sigma}{2 L}.
\end{align}
The term in the first rectangular brackets in Eq.~\eqref{eq:recbrackets2} needs to be $\ge \rho S \sigma$ in order to recover $\rho \L_\sigma = \rho \left( \| w- w^*\|^2 + S \sigma \bar H \right)$. Inserting the definition of $S$, $\rho$ and dividing by $\gamma$ yields 
\begin{align}
\frac {\sigma}{L} - 2 \gamma \ge \frac{\rho S \sigma}{\gamma}
 = c \gamma \mu \frac{n \sigma}{L q} = \frac{4c  \sigma \gamma}{K}
%
& \iff \gamma \le  \frac 1L \frac{K \sigma}{2 K + 4 c \sigma}
\end{align}
We can summarize the derivation in the claimed combined inequality.
\end{proof} 
\end{lemma}

\begin{theorem}
\label{theorem:main-appendix}
Consider  a uniform $q$-memorization algorithm. For any step size $\gamma = \frac a {4L}$, with $a<1$ the algorithm converges at a geometric rate of at least $(1-\rho(\gamma))$ with 
\begin{align*}
\rho(\gamma) = \frac{q}{n} \cdot \frac{1 - a}{1-a/2} 
= \frac{\mu}{4L} \cdot \frac{K (1 - a)}{1-a/2}, \;\;  \text{if} \;\gamma \geq \gamma^*(K), \;\; 
\text{otherwise} \;\; \rho(\gamma) = \mu \gamma   \;\; 
\end{align*}
where 
\begin{align*}
\gamma^*(K) :=  \frac {a^*(K)} {4 L}, \quad 
a^*(K) := \frac{2K}{1+K + \sqrt{1+K^2}},
%
\quad K:=  \frac{4qL}{n \mu}
\end{align*}
\begin{proof} 
Consider a fixed $\gamma < \frac 1{4L}$. There are potentially (infinitely) many choices of $(c,\sigma)$ that fulfill the condition in Eq.~\eqref{eq:gamma-admissible}. Among those, the largest rate is obtained by maximizing $c \le 1$ as $\rho(\gamma) = c \mu \gamma$. Note that for any $\gamma$ that does not achieve Eq.~\eqref{eq:gamma-admissible} with equality for both terms, one can find a larger $\gamma$ with the same choice of $c$ by either increasing (slack in the first inequality) or decreasing (slack in the second inequality) $\sigma$. We thus focus on step sizes that are maximal for some choice of $(c,\sigma)$. Equality with the second bound directly gives us
\begin{align} \label{eq:sigma-star}
\frac 1 L \frac{1-\sigma}{2} \stackrel != \gamma \; \Longrightarrow \; \sigma^*  = 1 - 2L \gamma.
\end{align} We plug this into the first bound and again equal $\gamma$, which yields an optimality condition for $c$
\begin{align}
& L \gamma  \stackrel != \frac{K \sigma^*}{2K + 4c \sigma^* } 
 %
 \iff   c^*  %
= \frac K{4\gamma L} \left[ 1 - \frac{2 \gamma L}{\sigma^*} \right]
\Longrightarrow c^* = \frac K{4\gamma L}  \frac{1 - 4 L \gamma}{1 - 2L \gamma}
\label{eq:c-optimal}
\end{align}
and thus
\begin{align}
& \rho = c \mu \gamma = \frac{\mu K}{4L} \frac{1 - 4 L \gamma}{1 - 2L \gamma}
= \frac{q}{n}  \frac{1 - 4 L \gamma}{1 - 2L \gamma}
\end{align}
It remains to check what the admissible range of $\gamma$ is that achieves the bound in Eq.~\eqref{eq:gamma-admissible} as we required. The latter is determined by the constraints $c \in (0;1]$.  From Eq.~\eqref{eq:c-optimal} we can read off for $c^*>0$, 
\begin{align}
1 - 4 L \gamma > 0  \iff \gamma < \frac {1}{4L} \,.
\end{align}
At the other extreme of $c=1$ we can solve the resulting quadratic equation in $\gamma$ 
\begin{align}
\gamma =  \frac{q}{n \mu}  \frac{1 - 4 L \gamma}{1 - 2L \gamma} = 
\frac{K}{4L} \frac{1 - 4 L \gamma}{1 - 2L \gamma} 
\end{align}
to get $\gamma= \gamma^*(K)$ as claimed in Eq.~\eqref{eq:gammalk} (excluding the second root which yields $\gamma > \frac{1}{4L}$). Moreover, for $\gamma < \gamma^*(K)$  we choose $c=1$ to maximize the rate and have $\rho = \mu \gamma$.
\end{proof}
\end{theorem}

%
%
%
%
%
%

\begin{corollary}
\label{corollary:opt-rho-appendix}
In Theorem \ref{theorem:main}, $\rho$ is maximized for $\gamma = \gamma^*(K)$. We can write $\rho^*(K) = \rho(\gamma^*)$ as 
\begin{align*}
\rho^*(K) = 
\frac \mu {4L} a^*(K) %
= \frac{q}{n}\frac{a^*(K)}{K} = \frac qn \left[ \frac{2}{1+K + \sqrt{1 + K^2} } \right]
\end{align*}
In the big data regime $\rho^* = \frac qn (1-\frac 12 K + O(K^3))$, whereas in the ill-conditioned case $\rho^* = \frac \mu{4L} (1 - \frac {1}{2}K^{-1} + O(K^{-3}))$.
\begin{proof}
Plugging in the definitions of $\gamma^*(K)$ and $K$ and performing some symbolic simplifications yields the result. 
\end{proof}
\end{corollary}



\begin{corollary}
\label{corollary:universal-appendix}
Choosing $\gamma = \frac{2 -\sqrt{2} }{4L}$, leads to $\rho(\gamma) \ge (2 - \sqrt{2}) \rho^* > \frac 12 \rho^*$.
\begin{proof} 
Write $\gamma = \frac{a}{4L}$, then if $\gamma \geq \gamma^*(K)$: $\frac{\rho}{\rho^*} \geq \frac{1 - a}{1- a/2}$, otherwise: $ \frac{\rho}{\rho^*} = \frac{\gamma}{\gamma^*} \ge  a$, with equality when $K = \infty$. Setting both equal yields $a =  2 - \sqrt{2} \approx 0.5858$. 
\end{proof} 
\end{corollary}

\begin{corollary}
Choosing $\gamma = \frac{a}{4L}$ with $ a<1$ yields $\rho = \min\{ \frac {1-a}{1 - \frac 12 a} \frac qn, \frac{a}{4} \frac \mu L \}$. In particular, we have for the choice $\gamma = \frac{1}{5L}$ that $\rho = \min\{ \frac {1}{3} \frac qn, \frac 15 \frac \mu L \}$.
\begin{proof} 
If $\gamma \ge \gamma^*(R)$ then $\rho = \frac{1-a}{1 -a/2}\frac qn = \frac{1}{3} \frac{q}{n}$; otherwise, $\rho = \mu \gamma = \mu \frac{a}{4L} = \frac{1}{5} \frac{\mu}{L}$.
\end{proof} 
\end{corollary}

\begin{theorem}
\label{theorem:approximate-appendix}
Consider a uniform $q$-memorization algorithm with $\alpha$-updates that are on average $\epsilon$-accurate (i.e. $\E\| \alpha_i - \beta_i \|^2 \le \epsilon$). For any step size $\gamma \le \tilde{\gamma}(K)$, where $\tilde{\gamma}$ is given in Eq.~\eqref{eq:atilde} in Corollary~\ref{corollary:patch} below (note that $\tilde{\gamma}(K) \geq \frac{2}{3} \gamma^*(K)$ and $\tilde{\gamma}(K) \to \gamma^*(K)$ as $K \to 0$), we get 
\begin{align}
\Efull \L(w^t,H^t) \le  (1-\mu \gamma)^t  \L_0+ \frac{4 \gamma \epsilon}{\mu}, \quad \text{ with } \L_0 := \|w^0 - w^*\|^2 + s(\gamma) \E \|f_i(w^*)\|^2 ,
\end{align}
where $\Efull$ denote the (unconditional) expectation over histories (in contrast to $\E$ which is conditional), and $ s(\gamma) := \frac{4 \gamma}{K \mu} (1-2 L \gamma)$.
\begin{proof} Following the same line of argument as in Lemma \ref{lemma:lyapunov} and Theorem \ref{theorem:main} with the modifications summarized in Corollary \ref{corollary:patch}
\begin{align*}
\E \L(w^+,H^+) \le  (1-\gamma\mu)  \L(w,H) + 4 \gamma^2 \epsilon
\end{align*}
and unrolling the recurrence over $t$ \vspace*{-5mm}
\begin{align*} 
    \Efull \L(w^t,H^t) \le  (1-\gamma\mu)^t  \L(w^0,H^0) + \overbrace{\left[ \sum_{s=0}^{t-1} (1- \gamma\mu)^s\right]}^{\leq 1/(\gamma \mu)} 4 \gamma^2 \epsilon
\end{align*}
using $\frac{1}{1-x} = \sum_{s=0}^\infty x^s$ applied with $x=(1-\rho)$ (see \cite{schmidt2014convergence} for its use for constant step size SGD). According to Eq.~\eqref{eq:lyapunov_fct}, $\L(w^0, H^0) = \|w-w^*\|^2 + S \sigma \bar{H}^0$, where $S = \frac{\gamma n}{L q} = \frac{4 \gamma}{K \mu}$. As the algorithm initializes $\alpha_i^0$ to $0$, we have $\bar{H}^0 = \E \|f'_i(w^*)\|^2$. Finally, the proof of Corollary~\ref{corollary:patch} follows the proof of Theorem~\ref{theorem:main-appendix} and also gives $\sigma^* = 1-2 L \gamma$ as in Eq.~\eqref{eq:sigma-star}. Substituting in $\L(w^0, H^0)$ gives $\L_0$.
\end{proof}
\end{theorem} 

\begin{corollary}
With $\gamma = \min\{\mu , \tilde{\gamma}(K)\}$ we have 
\begin{align*}
\frac {4 \gamma \epsilon }{\mu}  \le 4 \epsilon, \qquad 
\text{with a rate} \quad \rho = \min\{\mu^2, \mu \tilde{\gamma}\} \, .
\end{align*}
\begin{proof}
By definition, $\gamma \leq \mu$, thus $\gamma/ \mu \leq1$, yielding the first claim. By definition, we also have $\gamma \leq \tilde{\gamma}(K)$, thus from Theorem~\ref{theorem:main} adapted to Corollary~\ref{corollary:patch}, we know that $\rho = \mu \gamma$, which concludes the proof. (Note that  with $\gamma> \tilde{\gamma}(K)$ we will increase the error, while decreasing $\rho(\gamma) \le \tilde{\rho} = \mu \tilde{\gamma}$. This is why this choice is not sensible according to our theory.)
\end{proof} 
\end{corollary} 

\begin{corollary}[Patch-ups]
\label{corollary:patch}
Using Eq.\eqref{eq:alpha-tilde-bound} instead of Eq.~\eqref{eq:alpha-bound}, but then setting $\epsilon_i = 0$ yields the same results as before with the following changes: \\[3mm]
(a) Lemma \ref{lemma:lyapunov}: the bound becomes $\gamma \le \frac 1L \min \{ \frac 14 \frac{K \sigma}{K + c\sigma}, \frac {1-\sigma}{2} \} <  \frac{1}{6L}$. \\
(b) Theorem  \ref{theorem:main}: still using $\gamma = \frac{a}{4L}$, we require $a < \frac{2}{3}$ and we get a similar (slightly smaller) expression for $\rho$ as well as for the optimal step size $\tilde{\gamma}(K) := \frac{\tilde{a}(K)}{4L}$ replacing $\gamma^*$ in the theorem:
\begin{align} \label{eq:atilde}
\rho = \frac qn \frac{1-\frac 32 a}{1-\frac 12 a} \quad \text{and} \quad \tilde{a}(K) = \frac{2K}{1 + \frac3 2 K + \sqrt{1 + K + \left(\frac{3}{2}K \right)^2}} \geq \frac{2}{3} a^*(K) \, .
\end{align}
(c) The optimal asymptotic rate is  still $\tilde{\rho} \stackrel {n \to \infty} \longrightarrow  \frac qn$.
\begin{proof}
Redoing all proofs with an additional factor of $2$ on the RHS of  Eq.~\eqref{eq:alpha-bound}. One can also readily verify that the ratio $\frac{\tilde{a}(K)}{a^*(K)}$ (with $a^*(K)$ defined in Eq.~\eqref{eq:gammalk}) is a decreasing function of $K$, with value $1$ for $K=0$, and limiting value $\frac{2}{3}$ for $K \to \infty$.
\end{proof}
\end{corollary} 


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


\subsection{Implementation details}

\paragraph{Construction of the neighborhoods required by q-SAGA and ${\cal N}$-SAGA:} 
For each datapoint~$i$, we want to define a neighborhood ${\cal N}_i$ (defined as the set of children of $i$ in a directed graph) such that for $j \in {\cal N}_i$,
$\| \alpha_j - \beta_j \|^2 \le \epsilon$. The approximation bounds in Section~\ref{sect:sharing} show that this distance is a function of $w$, which is not known {\it a priori}. In order to address this issue, we used the distance between data points $\delta_{ij} := \| x_i - x_j \|$ as a surrogate. The construction of the neighborhoods then amounts to constructing a directed graph on $n$ nodes by setting the $q$ nearest points to $j$ as its \emph{parents}.\footnote{This can be naively implemented by computing all pairwise distances $\delta_{ij}$ between data points ($O(n^2)$), but more efficient data structures using hashing~\cite{andoni2018LSH} or randomized partition trees~\cite{dasgupat2015RPT} can be used.} This ensures that $| \{i: j \in {\cal N}_i\}|=q$ $(\forall j)$, i.e. every $j$ has exactly $q$ parents.
Note that this simple construction can yield asymmetric neighborhoods (i.e. $j \in {\cal N}_i \centernot\implies i \in {\cal N}_j$); ${\cal N}_i$ is the set of children of $i$, and does not have to be of size $q$. 
One could also construct a symmetric neighborhood by defining $j$ to be a child of $i$ if their distance is less than $\sqrt{\epsilon}$ (which is a symmetric relationship), where $\epsilon$ is a constant chosen such that $q \approx 20$. In practice, we did not find this construction to yield better performance (in addition to violating the uniform $q$-memorization property). Note also that the above constructions ensure that $i \in {\cal N}_i$.

\paragraph{Growing $n$ heuristic:} For all the $q$-memorization algorithms, we used the same initialization heuristic proposed in~\cite{schmidt2013minimizing,defazio2014} for which during the first pass, datapoints are introduced one by-one, with averages computed in terms of the number datapoints processed so far (i.e. the normalization for $\bar{\alpha}$ is the number of different points seen so far instead of $n$).

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\section{Introduction}

We consider a general problem that is pervasive in machine learning, namely optimization of an empirical or regularized convex risk function. Given a convex loss $l$ and a $\mu$-strongly convex regularizer $\Omega$, one aims at finding a parameter vector $w$ which minimizes the (empirical) expectation:
\begin{align}
w^* = \argmin_w f(w), \quad f(w) = \frac 1n \sum_{i=1}^n f_i(w), \quad f_i(w) := l(w, (x_i,y_i)) + \Omega(w)\,.
\label{eq:problem}
\end{align}
We assume throughout that each $f_i$ has $L$-Lipschitz-continuous gradients. Steepest descent can find the minimizer $w^*$, but requires repeated computations of full gradients $f'(w)$, which becomes prohibitive for massive data sets. Stochastic gradient descent (SGD) is a popular alternative, in particular in the context of large-scale learning~\cite{bottou2010, shalev2011}. SGD updates only involve $f'_i(w)$ for an index $i$ chosen uniformly at random, providing an unbiased gradient estimate, since $\E f'_i(w) = f'(w)$. 

It is a surprising recent finding \cite{shalev2013stochastic,johnson2013,schmidt2013minimizing,konevcny2013} that the finite sum structure of $f$ allows for significantly faster convergence in expectation. Instead of the standard $O(1/t)$ rate of SGD for strongly-convex functions, it is possible to obtain linear convergence with geometric rates. While SGD requires asymptotically vanishing learning rates, often chosen to be $O(1/t)$ \cite{robbins1951}, these more recent methods introduce  corrections that ensure convergence for constant learning rates. 

Based on the work mentioned above, the contributions of our  paper are as follows: First, we define a family of variance reducing SGD algorithms, called memorization algorithms, which includes  SAGA and SVRG as special cases, and develop a unifying  analysis technique for it. Second, we show geometric rates for all step sizes $\gamma < \frac 1{4L}$, including a universal ($\mu$-independent) step size choice, providing the first $\mu$-adaptive convergence proof for SVRG. Third, based on the above analysis, we present new insights into the trade-offs between freshness and biasedness of the corrections computed from previous stochastic gradients. Fourth, we propose a new class of algorithms that resolves this trade-off by computing corrections based on stochastic gradients at neighboring points. We experimentally show its benefits  in the regime of  learning with  a small number of epochs. 
%

\section{Memorization Algorithms}
\label{sect:memorization}

\subsection{Algorithms} 

\paragraph*{Variance Reduced SGD}

Given an optimization problem as in \eqref{eq:problem}, we investigate a class of stochastic gradient descent algorithms that generates an iterate sequence $w^t$ ($t \ge 0$) with updates taking the  form:
\begin{align}
w^+ = w - \gamma g_i(w),  \quad g_i(w)=  f'_i(w) - \bar \alpha_i \quad \text{with} \quad  \bar{\alpha}_i := \alpha_i - \bar{\alpha},
\label{eq:sgd-corrected}
\end{align}
where $\bar{\alpha} := \frac 1n \sum_{j=1}^n \alpha_j$.
Here $w$ is the current and $w^+$ the new parameter vector, $\gamma$ is the  step size,  and $i$ is an index selected uniformly at random. $\bar\alpha_i$ are variance correction terms such that  $\E \bar\alpha_i = 0$, which guarantees unbiasedness $\E g_i(w) =f'(w)$. The aim is to define updates of asymptotically vanishing variance, i.e.~$g_i(w) \to 0$ as $w \to w^*$, which  requires  $\bar\alpha_i \to f'_i(w^*)$. This implies that corrections need to be designed in a way to exactly cancel out the stochasticity of $f'_i(w^*)$ at the optimum. How the \emph{memory} $\alpha_j$ is updated distinguishes the different algorithms that we consider. 

\paragraph*{SAGA}

The SAGA algorithm \cite{defazio2014} maintains variance corrections $\alpha_i$  by memorizing stochastic gradients. The update rule is $\alpha^+_i  = f'_i(w)$ for the selected $i$, and $\alpha_j^+= \alpha_j$, for $j \neq i$. Note that these corrections will be used the next time the same index $i$ gets sampled.  Setting $\bar\alpha_i := \alpha_i - \bar \alpha$ guarantees unbiasedness. Obviously, $\bar \alpha$ can be updated incrementally. SAGA reuses the stochastic gradient $f_i'(w)$ computed at step $t$ to update $w$ as well as $\bar \alpha_i$.

\paragraph*{$q$-SAGA}
We  also consider $q$-SAGA, a method that updates $q \ge 1$ randomly chosen $\alpha_j$ variables at each iteration. This is a convenient reference point to investigate the advantages of ``fresher" corrections. Note that in SAGA the corrections will be on average $n$ iterations ``old". In $q$-SAGA this can be controlled to be $n/q$ at the expense of additional gradient computations.

\paragraph*{SVRG} 

We reformulate a variant of SVRG \cite{johnson2013} in our framework using a randomization argument similar to (but simpler than) the one suggested in \cite{konevcny2013}. Fix $q>0$ and draw in each  iteration $r \sim \text{Uniform}[0;1)$. If $ r< q/n$, a complete update, $\alpha_j^+  =  f_j'(w)$ ($\forall j$) is performed, otherwise they are left unchanged. While $q$-SAGA updates exactly $q$ variables in each iteration, SVRG occasionally updates all $\alpha$ variables by triggering an additional sweep through the data. There is an option to not maintain $\alpha$ variables explicitly and to save on space  by storing only $\bar \alpha = f'(w)$ and $w$.

\paragraph*{Uniform Memorization Algorithms} 

Motivated by SAGA and SVRG, we define a class of algorithms, which we call \textit{uniform memorization algorithms}. 
%
\begin{definition}
\label{def:memorization} 
A uniform $q$-memorization algorithm evolves iterates $w$ according to Eq.~\eqref{eq:sgd-corrected} and selects in each iteration a random index set $J$ of memory locations to update according to
\begin{align}
\alpha_j^+ := 
\begin{cases} 
f'_j(w) & \text{if $j \in J$} \\
\alpha_j & \text{otherwise,}
\end{cases}
\label{eq:memorization}
\end{align}
such that any $j$ has the \emph{same} probability of $q/n$ of being updated, i.e. $\forall j$, $\sum_{J \ni j} \Prob\{ J \} = \frac {q}{n}$.
\end{definition}
Note that $q$-SAGA  and the above  SVRG are special cases. For $q$-SAGA: $\Prob\{J\}=1/\binom n q$ if $|J|=q$ $\Prob\{J\}=0$ otherwise. For SVRG: $\Prob\{ \emptyset\} = 1-q/n$, $\Prob\{ [1:n]\}=q/n$, $\Prob\{J\}=0$, otherwise. 

\paragraph{${\mathcal N}$-SAGA}
Because we need it in Section \ref{sect:sharing}, we will also define an algorithm, which we call ${\cal N}$-SAGA, which makes use of a neighborhood system ${\cal N}_i \subseteq \{1,\dots,n\}$ and which selects neighborhoods uniformly, i.e.~$\Prob\{{\cal N}_i\}= \frac 1n$.  Note that Definition \ref{def:memorization} requires $| \{i: j \in {\cal N}_i\}|=q$ $(\forall j)$. 

Finally, note that for generalized linear models where $f_i$ depends on $x_i$ only through $\langle w, x_i \rangle$, we get $f_i'(w) = \xi'_i(w) x_i$, i.e.~the update direction is determined by $x_i$, whereas the effective step length depends on the derivative of a scalar function $\xi_i(w)$. As used in~\cite{schmidt2013minimizing}, this leads to significant memory savings as one only needs to store the scalars $\xi'_i(w)$ as $x_i$ is always \textit{given} when performing an update. 

%
\subsection{Analysis}
%
\paragraph*{Recurrence of Iterates}

The evolution equation \eqref{eq:sgd-corrected} in expectation implies the recurrence (by crucially using the unbiasedness condition $\E g_i(w) =f'(w)$):
\begin{align}
\E \| w^+\! -\! w^* \|^2 & %
= \| w - w^* \|^2 - 2 \gamma \langle f'(w), w-w^*\rangle + \gamma^2 \E \| g_i(w) \|^2 \,.
\label{eq:recurrence}
\end{align}
Here and in the rest of this paper, expectations are always taken only with respect to $i$ (conditioned on the past).   We utilize a number of bounds (see \cite{defazio2014}), which exploit strong convexity of $f$ (wherever $\mu$ appears) as well as Lipschitz continuity of the $f_i$-gradients (wherever $L$ appears):
\begin{align}
 \langle f'(w), w-w^* \rangle  & \ge  f(w) - f(w^*)+ \tfrac \mu 2  \| w-w^*\|^2\,,
\label{eq:naive-strong-convexity} \\
\E \|g_i(w)\|^2 & \le 2  \E \| f'_i(w) - f'_i(w^*) \|^2 + 2 \E \| \bar \alpha_i - f_i'(w^* )\|^2 \,,
\label{eq:beta-split} 
\\
%
%
\| f'_i(w) - f_i'(w^*) \|^2 & \le 2L h_i(w), \quad h_i(w)  := f_i(w) - f_i(w^*)-  \langle w-w^*, f_i'(w^*) \rangle
\label{eq:smoothness-bound}
\,, \\
\! \E \| f_i'(w) \!-\! f_i'(w^*)\|^2 & \le 2L f^\delta(w), \quad f^\delta(w) := f(w) - f(w^*) \,, 
\label{eq:expected-smoothness}
\\
\E \| \bar \alpha_i - f_i'(w^*)\|^2 & = \E \| \alpha_i - f_i'(w^*) \|^2 - \| \bar \alpha\|^2 \le \E \| \alpha_i - f_i'(w^*) \|^2 .
\label{eq:alpha-simplification}
\end{align}
%
%
Eq.~\eqref{eq:beta-split} can be generalized~\cite{defazio2014} using $\| x \pm y\|^2 \le (1+\beta) \| x\|^2 +  (1+\beta^{-1}) \|y\|^2$ with $\beta>0$. However for the sake of simplicity, we sacrifice tightness and choose $\beta=1$.
%
Applying all of the above yields:
\begin{lemma} 
\label{lemma:w-recurrence}
For the iterate sequence of any algorithm that evolves solutions according to Eq.~\eqref{eq:sgd-corrected}, the following holds for a single update step, in expectation over the choice of $i$:
\begin{equation*}
\| w - w^*\|^2  - \E \| w^+ - w^*\|^2 \ge \,\, \gamma \mu \| w- w^*\|^2  - 2 \gamma^2 \E \| \alpha_i - f_i'(w^*)\|^2  + \left( 2\gamma - 4\gamma^2 L \right) f^\delta(w) \,.
\end{equation*}
\end{lemma}
%
All proofs are deferred to the Appendix. 

\paragraph*{Ideal and Approximate Variance Correction}
 
Note that in the ideal case of $\alpha_i = f'_i(w^*)$, we would immediately get a condition for a contraction by choosing $\gamma = \frac{1}{2L}$, yielding a  rate of $1- \rho$ with $\rho = \gamma \mu = \frac{\mu}{2L}$, which is half the inverse of the condition number $\kappa:=L/\mu$.

%
%

How can we further bound  $\E \| \alpha_i - f_i'(w^*) \|^2$ in the case of ``non-ideal" variance-reducing SGD? A key insight is that for  memorization algorithms, we can apply the smoothness bound in Eq.~\eqref{eq:smoothness-bound}
\begin{align}
\| \alpha_i-f'_i(w^*)\|^2 
= \| f'_i(w^{\tau_i}) -f'_i(w^*)\|^2 
\le 2L  h_i(w^{\tau_i}), \quad \text{(where $w^{\tau_i}$ is old $w$)} \,.
\label{eq:alpha-bound}
\end{align}
Note that if we only had approximations $\beta_i$ in the sense that $\| \beta_i - \alpha_i \|^2 \le \epsilon_i$  (see Section \ref{sect:sharing}), then we can use $\|x-y\| \leq 2 \|x\| + 2 \|y\|$ to get the somewhat worse bound:
\begin{align}
\| \beta_i-f'_i(w^*)\|^2  \le 2\| \alpha_i-f'_i(w^*)\|^2  + 2 \| \beta_i - \alpha_i\|^2
\le 4L  h_i(w^{\tau_i})  + 2 \epsilon_i.
\label{eq:alpha-tilde-bound}
\end{align}


\paragraph*{Lyapunov Function}

Ideally, we would like to show that for a suitable choice of $\gamma$, each iteration results in a contraction  $\E \| w^+ - w^*\|^2 \le (1-\rho) \| w - w^*\|^2$, where $0 < \rho \leq 1$. However, the main challenge arises from the fact that the quantities $\alpha_i$ represent stochastic gradients from previous iterations. This requires a somewhat more complex proof technique. Adapting the Lyapunov function method from \cite{defazio2014}, we  define upper bounds $H_i \geq \| \alpha_i - f'_i(w^*)\|^2$ such that $H_i \to 0$ as $w \to w^*$. We start with $\alpha^0_i\!=\!0$ and (conceptually) initialize $H_i = \|f'_i(w^*)\|^2$, and then update $H_i$ in sync with $\alpha_i$,
%
\begin{align}
H_i^+ := 
\begin{cases}
2L \, h_i(w) & \text{if $\alpha_i$ is updated} \\
H_i & \text{otherwise}
\end{cases} 
\label{eq:recurrence-h1}
\end{align} 
so that we always maintain valid bounds $\| \alpha_i - f_i'(w^*) \|^2 \le H_i$ and $\E\| \alpha_i - f_i'(w^*) \|^2 \le \bar H$ with $\bar H := \frac 1n \sum_{i=1}^n H_i$. The $H_i$ are quantities showing up in the analysis, but need  \textit{not} be computed. We now define a $\sigma$-parameterized family of Lyapunov functions\footnote{This is a simplified version of the one appearing in~\cite{defazio2014}, as we assume $f'(w^*)=0$ (unconstrained regime).}
\begin{align} \label{eq:lyapunov_fct}
\L_\sigma(w,H) := \| w- w^*\|^2 + S \sigma \,  \bar H, 
\quad \text{with} \; \; S:= \left( \frac{\gamma n}{L q} \right) \quad \text{and}\quad  0\leq  \sigma \leq 1\,.
\end{align}
In expectation under a random update, the Lyapunov function $\L_\sigma$ changes as $ \E \L_\sigma(w^+,H^+)  = \E \| w^+ - w^*\|^2  + S \sigma\, \E \bar H^+$. 
%
We can readily apply Lemma \ref{lemma:w-recurrence} to bound the first part. The second part is due to \eqref{eq:recurrence-h1}, which mirrors the update of the $\alpha$ variables.  
By crucially using the property that any $\alpha_j$ has the same probability of being updated in~\eqref{eq:memorization}, we
get the following result: 
\begin{lemma} 
For a uniform $q$-memorization algorithm, it holds that 
\begin{align}
\E \bar H^+ = \left( \frac{n-q}{n} \right) \bar H + \frac{2Lq}{n} \, f^\delta(w) .
\label{eq:h-recurrence}
\end{align}
\label{lemma:hrecurrence2}
\end{lemma}
\vspace{-3.5mm}
Note that in expectation the shrinkage does not depend on the location of previous iterates $w^\tau$ and the new increment is  proportional to the sub-optimality of the current iterate $w$.  Technically, this is how the possibly complicated dependency on previous iterates is dealt with in an effective manner. 

\paragraph*{Convergence Analysis}


We first state our main Lemma about Lyapunov function contractions:
\begin{lemma}
\label{lemma:lyapunov}
Fix $c \in (0;1]$ and $\sigma \in [0;1]$ arbitrarily. For any uniform $q$-memorization algorithm with sufficiently small step size $\gamma$ such that
\begin{align}
\gamma \le \frac{1}{2L} \min \left\{\frac{K\sigma }{K + 2c\sigma}, 1-\sigma \right\}, \quad 
\text{and} \quad K:=  \frac{4qL}{n \mu},
\label{eq:gamma-admissible}
\end{align}
we have that 
\begin{align}
\E \L_\sigma(w^+,H^+)  \le (1-\rho) \L_\sigma(w,H),\quad \text{with} \quad \rho := c \mu \gamma.
\end{align}
Note that $\gamma < \frac 1{2L} \max_{\sigma \in [0,1]} \min\{ \sigma ,  1-\sigma\} = \frac{1}{4L}$ (in the $c \to 0$ limit).
\end{lemma}
%
By maximizing the bounds in Lemma \ref{lemma:lyapunov} over the choices of $c$ and $\sigma$, we obtain our main result that provides guaranteed geometric rates for all step sizes up to $\frac 1 {4L}$. 
\begin{theorem}
\label{theorem:main}
Consider  a uniform $q$-memorization algorithm. For any step size $\gamma = \frac a {4L}$ with $a<1$, the algorithm converges at a geometric rate of at least $(1-\rho(\gamma))$ with 
\begin{align}
\rho(\gamma) = \frac{q}{n} \cdot \frac{1 - a}{1-a/2} 
= \frac{\mu}{4L} \cdot \frac{K (1 - a)}{1-a/2}, \;\;  \text{if} \;\gamma \geq \gamma^*(K), \;\; 
\text{otherwise} \;\; \rho(\gamma) = \mu \gamma   \;\; 
\end{align}
where 
\begin{align}
\gamma^*(K) :=  \frac {a^*(K)} {4 L}, \quad 
a^*(K) := \frac{2K}{1+K + \sqrt{1+K^2}},
%
\quad K:=  \frac{4qL}{n \mu} = \frac{4q}{n} \kappa \, .
\label{eq:gammalk}
\end{align}
\end{theorem}
We would like to provide more insights into this result. 
\begin{corollary}
\label{corollary:opt-rho}
In Theorem \ref{theorem:main}, $\rho$ is maximized for $\gamma = \gamma^*(K)$. We can write $\rho^*(K) = \rho(\gamma^*)$ as 
\begin{align}
\rho^*(K) = 
\frac \mu {4L} a^*(K) %
= \frac{q}{n}\frac{a^*(K)}{K} = \frac qn \left[ \frac{2}{1+K + \sqrt{1 + K^2} } \right]
\end{align}
In the big data regime $\rho^* = \frac qn (1-\frac 12 K + O(K^3))$, whereas in the ill-conditioned case $\rho^* = \frac \mu{4L} (1 - \frac {1}{2}K^{-1} + O(K^{-3}))$.
\end{corollary}
The guaranteed rate is bounded by $\frac{\mu}{4L}$ in the regime where the condition number dominates $n$  (large $K$)  and by $\frac qn$ in the opposite regime of large data (small $K$). Note that if $K \le 1$, we have $\rho^* = \zeta  \frac qn$ with  $\zeta \in [2/(2 + \sqrt{2}); 1]  \approx [0.585;1]$. So for $q \le n \frac{\mu}{4L}$, it pays off to increase freshness as it affects the rate proportionally.  In the ill-conditioned regime ($\kappa > n$), the influence of $q$ vanishes. 

Note that for $\gamma \ge \gamma^*(K)$, $ \gamma \to \frac1{4L}$ the rate decreases monotonically, yet the decrease is only minor. With the exception of a small neighborhood around $\frac{1}{4L}$, the entire range of $\gamma \in [\gamma^*; \frac{1}{4L})$ results in very similar rates. Underestimating $\gamma^*$  however leads to a (significant) slow-down by a factor $\gamma/\gamma^*$.

As the optimal choice of $\gamma$ depends on $K$, i.e.~$\mu$, we would prefer step sizes that are $\mu$-independent, thus giving rates that adapt to the local curvature  (see \cite{schmidt2013minimizing}). It turns out that by choosing a step size that maximizes $\min_K \rho(\gamma)/\rho^*(K)$, we obtain a $K$-agnostic step size with rate off by at most $1/2$:
\begin{corollary}
\label{corollary:universal}
Choosing $\gamma = \frac{2 -\sqrt{2} }{4L}$, leads to $\rho(\gamma) \ge (2 - \sqrt{2}) \rho^*(K) > \frac 12 \rho^*(K)$ for all $K$.
\end{corollary}
To gain more insights into the trade-offs for these fixed large universal step sizes, the following corollary details the range of rates obtained:
\begin{corollary}
Choosing $\gamma = \frac{a}{4L}$ with $ a<1$ yields $\rho = \min\{ \frac {1-a}{1 - \frac 12 a} \frac qn, \frac{a}{4} \frac \mu L \}$. In particular, we have for the choice $\gamma = \frac{1}{5L}$ that $\rho = \min\{ \frac {1}{3} \frac qn, \frac 15 \frac \mu L \}$ (roughly matching the rate given in~\cite{defazio2014} for $q=1$).
\label{corollary:universal2}
%
%
%
\end{corollary}



%
%
\begin{figure*}
  \begin{center}
    \begin{tabular}{@{}c@{\hspace{2mm}}c@{\hspace{2mm}}c@{}}
      (a) {\it Cov} &
      (b) {\it Ijcnn1} &
      (c) {\it Year} \\    
      \includegraphics[width=0.32\linewidth]{figures/config_cov_log_q20_log_l1e-1_0.pdf} &
      \includegraphics[width=0.32\linewidth]{figures/config_ijcn_log_q20_log_l1e-1_0.pdf} &
      \includegraphics[width=0.32\linewidth]{config_year_q20_log_l1e-1_0.pdf} \\
      & {\scriptsize $\mu=10^{-1}$, gradient evaluation} & \vspace{2mm} \\ 
      \includegraphics[width=0.32\linewidth]{figures/config_cov_log_q20_log_l1e-3_0.pdf} &
      \includegraphics[width=0.32\linewidth]{figures/config_ijcn_log_q20_log_l1e-3_0.pdf} &
      \includegraphics[width=0.32\linewidth]{config_year_q20_log_l1e-3_0.pdf} \\
      & {\scriptsize $\mu=10^{-3}$, gradient evaluation} & \vspace{2mm} \\
      \includegraphics[width=0.32\linewidth]{figures/config_cov_log_q20_log_l1e-1_1.pdf} &
      \includegraphics[width=0.32\linewidth]{figures/config_ijcn_log_q20_log_l1e-1_1.pdf} &
      \includegraphics[width=0.32\linewidth]{config_year_q20_log_l1e-1_1.pdf} \\
      & {\scriptsize $\mu=10^{-1}$, datapoint evaluation} & \vspace{2mm} \\
      \includegraphics[width=0.32\linewidth]{figures/config_cov_log_q20_log_l1e-3_1.pdf} &
      \includegraphics[width=0.32\linewidth]{figures/config_ijcn_log_q20_log_l1e-3_1.pdf} &
      \includegraphics[width=0.32\linewidth]{config_year_q20_log_l1e-3_1.pdf} \\
      & {\scriptsize $\mu=10^{-3}$, datapoint evaluation} & \vspace{2mm} \\
    \end{tabular}
  \vspace{2mm}
  \caption{Comparison of $\epsilon {\cal N}$-SAGA, $q$-SAGA, SAGA and SGD (with decreasing and constant step size) on three datasets. The top two rows show the suboptimality as a function of the number of gradient evaluations for two different values of $\mu=10^{-1}, 10^{-3}$. The bottom two rows show the suboptimality as a function of the number of datapoint evaluations (i.e. number of stochastic updates) for two different values of $\mu=10^{-1}, 10^{-3}$.}
  \label{fig:results}
\end{center}
\end{figure*}
%

\section{Experimental Results}
\label{section:results}

\paragraph{Algorithms} We present experimental results on the performance of the different variants of memorization algorithms for variance reduced SGD as discussed in this paper. SAGA has been uniformly superior to SVRG in our experiments, so we compare SAGA and $\epsilon {\cal N}$-SAGA (from Eq.~\eqref{eq:en-saga}), alongside with SGD as a straw man and $q$-SAGA as a point of reference for speed-ups. We have chosen $q=20$ for $q$-SAGA and $\epsilon {\cal N}$-SAGA. The same setting was used across all data sets and experiments. 


\paragraph{Data Sets}

As special cases for the choice of the loss function and regularizer in Eq.~\eqref{eq:problem}, we consider two commonly occurring problems in machine learning, namely least-square regression and $\ell_2$-regularized logistic regression.
%
We apply least-square regression on the million song year regression from the UCI repository. This dataset contains $n = 515,345$ data points, each described by $d=90$ input features.
%
We apply logistic regression on the {\it cov} and {\it ijcnn1} datasets obtained from the {\it libsvm} website
\footnote{\url{http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets}}. The
         {\it cov} dataset contains $n = 581,012$ data points, each
         described by $d = 54$ input features. The {\it ijcnn1}
         dataset contains $n = 49,990$ data points, each described by
         $d = 22$ input features.
%
We added an $\ell_2$-regularizer $\Omega(w) = \mu \|w\|_2^2$ to ensure
the objective is strongly convex.

\paragraph{Experimental Protocol}

We have run the algorithms in question in an i.i.d.~sampling setting and averaged the results over 5 runs. Figure \ref{fig:results} shows the  evolution of the suboptimality $f^\delta$ of the objective as a function of two different metrics: (1) in terms of the number of update steps performed (``datapoint evaluation"), and (2) in terms of the number of gradient computations (``gradient evaluation"). Note that SGD and SAGA compute one stochastic gradient per update step unlike $q$-SAGA, which is included here not as a practically relevant algorithm, but as an indication of potential improvements that could be achieved by fresher corrections. A step size $\gamma=\frac {q}{\mu n}$ was used everywhere, except for ``plain SGD''. Note that as $K \ll 1$ in all cases, this is close to the optimal value suggested by our analysis; moreover, using a step size of $\sim \frac{1}{L}$ for SAGA as suggested in previous work~\cite{schmidt2013minimizing} did not appear to give better results. For plain SGD, we used a schedule of the form $\gamma_t = \gamma_0/t$ with constants optimized coarsely via cross-validation. The $x$-axis is expressed in units of $n$ (suggestively called "epochs"). 

\vspace{-1.3mm}
\paragraph{SAGA vs.~SGD cst}  As we can see, if we run SGD with the same constant step size as SAGA, it takes several epochs until SAGA really shows a significant gain. The constant step-size variant of SGD is faster in the early stages until it converges to a neighborhood of the optimum, where individual runs start showing a very noisy behavior. 

\vspace{-1.3mm}
\paragraph{SAGA vs.~$q$-SAGA} 
$q$-SAGA outperforms plain SAGA quite consistently when counting stochastic update steps. This establishes optimistic reference curves of what we can expect to achieve with $\epsilon {\cal N}$-SAGA. The actual  speed-up is somewhat data set dependent.  

\vspace{-1.3mm}
\paragraph{$\epsilon{\cal N}$-SAGA vs.~SAGA and $q$-SAGA} $\epsilon {\cal N}$-SAGA with sufficiently small $\epsilon$ can realize much of the possible freshness gains of $q$-SAGA and performs very similar for a few (2-10) epochs, where it traces nicely between the SAGA and $q$-SAGA curves. We see solid speed-ups on all three datasets for both $\mu=0.1$ and $\mu=0.001$. 

\vspace{-1.3mm}
\paragraph{Asymptotics} It should be clearly stated that running $\epsilon{\cal N}$-SAGA at a fixed $\epsilon$ for longer will not result in good  asymptotics on the empirical risk. This is because, as theory predicts, $\epsilon{\cal N}$-SAGA can not drive the suboptimality to zero, but rather levels-off at a point determined by $\epsilon$. In our experiments, the cross-over point with SAGA was typically after $5-15$ epochs. Note that the gains in the first epochs can be significant, though. In practice, one will either define a desired accuracy level and choose $\epsilon$ accordingly or one will switch to SAGA for accurate convergence. 
\vspace{-1mm}
%

\section{Sharing Gradient Memory}
\label{sect:sharing}

\subsection{$\epsilon$-Approximation Analysis} 

As we have seen, fresher gradient memory, i.e.~a larger choice for $q$, affects the guaranteed convergence rate as $\rho \sim q/n$. However, as long as one step of a $q$-memorization algorithm is as expensive as $q$ steps of a $1$-memorization algorithm, this insight does not lead to practical improvements \textit{per se}. Yet, it raises the question, whether we can accelerate these methods, in particular ${\cal N}$-SAGA, by approximating gradients stored in the $\alpha_i$ variables. Note that we are always using the correct stochastic gradients in the \textit{current} update and by assuring $\sum_{i} \bar\alpha_i=0$, we will not introduce any bias in the update direction. Rather, we lose the guarantee of asymptotically vanishing variance at $w^*$. However, as we will show, it is possible to retain geometric rates up to a $\delta$-ball around $w^*$. 

We will focus on SAGA-style updates for concreteness and investigate an algorithm that mirrors ${\cal N}$-SAGA with the only difference that it maintains approximations $\beta_i$ to the true $\alpha_i$ variables. We aim to guarantee  $\E\| \alpha_i - \beta_i \|^2 \le \epsilon$ and will use Eq.~\eqref{eq:alpha-tilde-bound} to modify the right-hand-side of Lemma \ref{lemma:w-recurrence}. We see that approximation errors $\epsilon_i$ are multiplied with $\gamma^2$, which implies that we should aim for small learning rates, ideally without compromising the ${\cal N}$-SAGA rate. From Theorem \ref{theorem:main} and Corollary \ref{corollary:opt-rho} we can see that we can choose $\gamma \lesssim  q/{\mu n}$ for $n$ sufficiently large, which indicates that there is hope to dampen the effects of the approximations. We now make this argument more precise.
%
%
%
%
\begin{theorem}
\label{theorem:approximate}
Consider a uniform $q$-memorization algorithm with $\alpha$-updates that are on average $\epsilon$-accurate (i.e. $\E\| \alpha_i - \beta_i \|^2 \le \epsilon$). For any step size $\gamma \le \tilde{\gamma}(K)$, where $\tilde{\gamma}$ is given by Corollary \ref{corollary:patch} in the appendix (note that $\tilde{\gamma}(K) \geq \frac{2}{3} \gamma^*(K)$ and $\tilde{\gamma}(K) \to \gamma^*(K)$ as $K \to 0$), we get 
\begin{align}
\Efull \L(w^t,H^t) \le  (1-\mu \gamma)^t  \L_0+ \frac{4 \gamma \epsilon}{\mu}, \quad \text{ with } \L_0 := \|w^0 - w^*\|^2 + s(\gamma) \E \|f_i(w^*)\|^2 ,
\end{align}
where $\Efull$ denote the (unconditional) expectation over histories (in contrast to $\E$ which is conditional), and $ s(\gamma) := \frac{4 \gamma}{K \mu} (1-2 L \gamma)$.
%
%
%
%
%
%
%
%
%
%
\end{theorem} 
\begin{corollary}
With $\gamma = \min\{\mu , \tilde{\gamma}(K)\}$ we have 
\begin{align}
\frac {4 \gamma \epsilon }{\mu}  \le 4 \epsilon, \qquad 
\text{with a rate} \quad \rho = \min\{\mu^2, \mu \tilde{\gamma}\} \, .
\end{align}
\end{corollary} 
\vspace{-1.5mm}
In the relevant case of $\mu \sim 1/\sqrt {n}$, we thus converge towards some $\sqrt{\epsilon}$-ball around $w^*$ at a similar rate as for the exact method. For $\mu \sim n^{-1}$, we have to reduce the step size significantly to compensate the extra variance and to still converge to an $\sqrt{\epsilon}$-ball, resulting in the slower rate $\rho \sim n^{-2}$, instead of $\rho \sim n^{-1}$.

We also note that the geometric convergence of SGD with a constant step size to a neighborhood of the solution (also proven in~\cite{schmidt2014convergence}) can arise as a special case in our analysis. By setting $\alpha_i = 0$ in Lemma~\ref{lemma:w-recurrence}, we can take $\epsilon = \E \|f'_i(w^*)\|^2$ for SGD. An approximate $q$-memorization algorithm can thus be interpreted as making $\epsilon$ an algorithmic parameter, rather than a fixed value as in SGD.

\subsection{Algorithms} 

\paragraph*{Sharing Gradient Memory} 

We now discuss our proposal of using neighborhoods for sharing gradient information between close-by data points. Thereby we avoid an increase in gradient computations relative to $q$- or ${\cal N}$-SAGA at the expense of suffering an approximation bias. This leads to a new tradeoff between freshness  and approximation quality, which can be resolved in non-trivial ways, depending on the desired final optimization accuracy. 

We distinguish two types of quantities. First, the gradient memory $\alpha_i$ as defined by the reference algorithm ${\cal N}$-SAGA. Second, the shared gradient memory state $\beta_i$, which is used in a modified update rule in Eq.~\eqref{eq:sgd-corrected}, i.e.~$w^+ = w - \gamma ( f'_i(w) - \beta_i + \bar \beta )$. Assume that we select an index $i$ for the weight update, then we generalize Eq.~\eqref{eq:memorization} as follows
\begin{align}
\beta^+_j := \begin{cases} 
f'_i(w) & \text{if $j \in {\cal N}_i$} \\
\beta_j & \text{otherwise}
\end{cases}, \qquad \bar \beta := \frac 1n \sum_{i=1}^n \beta_i, \quad \bar \beta_i := \beta_i - \bar \beta \,.
\label{eq:shared-memorization}
\end{align}

%

In the important case of generalized linear models, where  one has 
%
$f'_i(w) = \xi'_i(w) x_i$, we can  modify the relevant case in Eq.~\eqref{eq:shared-memorization} by $\beta_j^+ := \xi'_i(w) x_j$. This has the advantages of using the correct direction, while reducing storage requirements. 

\paragraph*{Approximation Bounds}

For our analysis, we need to control the error $\| \alpha_i - \beta_i \|^2 \le \epsilon_i$. This obviously requires problem-specific investigations. 

Let us first look at the case of ridge regression. $f_i(w) := \frac 12 (\langle x_i, w \rangle - y_i)^2 + \frac \lambda 2 \| w\|^2$ and thus $f'_i(w) = \xi'_i(w) x_i + \lambda w$ with $\xi'_i(w) := \langle x_i, w \rangle - y_i$. Considering $j \in {\cal N}_i$ being updated, we have
\begin{align}
\| \alpha_j^+ - \beta_j^+\| = | \xi'_j(w) - \xi'_i(w) | \| x_j\|
\le \left( \delta_{ij}  \|w\| +|y_j - y_i| \right) \| x_j\| =: \epsilon_{ij}(w)
\label{eq:bound-quadratic}
\end{align}
where $\delta_{ij} := \| x_i - x_j \|$. Note that this can be pre-computed with the exception of the norm $\| w\|$ that we only know at the time of an update. 

Similarly, for regularized logistic regression with $y \in \{-1,1\}$, we have $\xi'_i(w) = y_i/(1 + e^{y_i \langle x_i, w\rangle})$. With the requirement on neighbors that $y_i = y_j$ we get 
\begin{align}
\| \alpha^+_j - \beta^+_j\| \le \frac{e^{\delta_{ij} \| w\|}-1}{1+ e^{-\langle x_i, w \rangle}} \|x_j\|
 =: \epsilon_{ij}(w)
\label{eq:bound-logistic}
\end{align}
Again, we can pre-compute $\delta_{ij}$ and $\| x_j\|$. In addition to $\xi'_i(w)$ we can also store $\langle x_i, w \rangle$. %

\paragraph*{$\epsilon {\cal N}$-SAGA}

We can use these bounds in two ways. First, assuming that the iterates stay within a norm-ball (e.g.~$L_2$-ball), we can derive upper bounds
\begin{align}
\epsilon_j(r) \ge  \max\{ \epsilon_{ij}(w): j \in {\cal N}_i, \; \|w\| \le r\}, \qquad \epsilon(r) = \frac 1n \sum_{j} \epsilon_j(r)\,. 
\end{align}
Obviously, the more compact the neighborhoods are, the smaller $\epsilon(r)$. This is most useful for the analysis. 
%
 Second, we can specify a target accuracy $\epsilon$ and then prune neighborhoods dynamically. This approach is more practically relevant as it allows us to directly control $\epsilon$. However, a dynamically varying neighborhood violates Definition \ref{def:memorization}. We fix this in a sound manner by modifying the memory updates as follows:
 \begin{align}
\beta^+_j := \begin{cases} 
f'_i(w) & \text{if $j \in  {\cal N}_i$ and $\epsilon_{ij}(w) \le \epsilon$} \\
f'_j(w) & \text{if $j \in  {\cal N}_i$ and $\epsilon_{ij}(w) > \epsilon$} \\
\beta_j & \text{otherwise}
\end{cases}
\label{eq:en-saga}
 \end{align}
This allows us to interpolate between sharing more aggressively (saving computation) and performing more computations in an exact manner. In the limit of $\epsilon \to 0$, we  recover ${\cal N}$-SAGA, as $\epsilon \to \epsilon^{\max}$ we recover the first variant mentioned. 
%
%
 
\paragraph*{Computing Neighborhoods}

Note that the pairwise Euclidean distances show up in the bounds in Eq.~\eqref{eq:bound-quadratic} and \eqref{eq:bound-logistic}. In the classification case we also require $y_i = y_j$, whereas in the ridge regression case, we also want $| y_i - y_j|$ to be small. Thus modulo filtering, this suggests the use of Euclidean distances as the metric for defining neighborhoods. Standard approximation techniques for finding near(est) neighbors can be used. This comes with a computational overhead, yet the additional costs will amortize over multiple runs or multiple data analysis tasks.

%


\section{Optimization}
\label{sec:optim}

Optimizing problem~\eqref{eq:jointstateactionprob} poses several challenges that need to be addressed.
First, we propose a relaxation of the integer constraints and the distortion function (Section~\ref{sec:relax}).
Second, we optimize this relaxation using Frank-Wolfe with a new dynamic program able to handle our tracklet constraints (Section~\ref{subsec:frank-wolfe}).
Finally, we introduce a new rounding technique to obtain an integer candidate solution to our problem (Section~\ref{sec:rounding}).

%
%
%
%

\subsection{Relaxation}
\label{sec:relax}
Problem~\eqref{eq:jointstateactionprob} is NP-hard in general~\cite{loiola07qap} due to its specific integer constraints.
%
Inspired by the approach of~\cite{Bojanowski14weakly} that was successful to approximate combinatorial optimization problems, we propose to use the tightest convex relaxation of the feasible subset of binary matrices by taking its convex hull.
As our variables now can take values in $[0,1]$, we also have to propose a consistent extension for the different cost functions to handle fractional values as input.
For the cost functions $f$ and $g$, we can directly take their expression on the relaxed set as they are already expressed as (convex) quadratic functions.
Similarly, for the joint cost function $d$ in~\eqref{eq:dist}, we use its natural bilinear relaxation:


\vspace{-3mm}
\begin{align}
d(Z_n,Y_n) = \sum_{i=1}^{M_n} \sum_{t=1}^{T_n}  \Big( &(Y_n)_{i1} Z_{nt} [t_{ni}-t]_+ \,\, + \notag \\[-3mm]
&(Y_n)_{i2} Z_{nt} [t-t_{ni}]_+ \,\, \Big),
\label{eq:relaxeddist}
\end{align}
where $t_{ni}$ denotes the video time of tracklet $i$ in clip $n$.
This relaxation is equal to the function~\eqref{eq:dist} on the integer points.
However, it is not jointly convex in~$Y$ and~$Z$, thus we have to design an appropriate optimization technique to obtain good (relaxed) candidate solutions, as described next.

\subsection{Joint optimization using Frank-Wolfe}
\label{subsec:frank-wolfe}
When dealing with a constrained optimization problem for which it is easy to solve linear programs but difficult to project on the feasible set, the Frank-Wolfe algorithm is an excellent choice~\cite{Jaggi2013,Lacoste15GlobalLinearFW}.
It is exactly the case for our relaxed problem, where the linear program over the convex hull of feasible integer matrices can be solved efficiently via dynamic
programming.
%
%
Moreover, \cite{lacoste16nonconvexFW} recently showed that the Frank-Wolfe algorithm with line-search converges to a stationary point for non-convex objectives at a rate of $O(1/\sqrt{k})$.
We thus use this algorithm for the joint optimization of~\eqref{eq:jointstateactionprob}. As the objective is quadratic, we can perform exact line-search analytically, which speeds up convergence in practice.
Finally, in order to get a good initialization for both variables $Z$ and $Y$, we first optimize separately $f(Z)$ and $g(Y)$ (without the non-convex $d(Z,Y)$), which are both convex functions.

\noindent\textbf{Dynamic program for the tracklets.}
In order to apply the Frank-Wolfe algorithm, we need to solve a linear program (LP) over our set of constraints. 
Previous work has explored ``\textit{exact one}" ordering constraints for time localization problems~\cite{Bojanowski14weakly}.
%
Differently here, we have to deal with the spatial (non overlap) constraint  \emph{and} finding ``\textit{at least one}" candidate tracklet per state.
To deal with these two requirements, we propose a novel dynamic programming approach.
First, the ``at least one" constraint is encoded by having a memory variable which indicates whether state~1 or state~2 have already been visited. This variable is used to propose valid state decisions for consecutive tracklets.
Second, the challenging ``non-overlap" tracklet constraint is included by constructing valid left-to-right paths in a cost matrix while carefully considering the possible authorized transitions.  
%
We provide details of the formulation in Appendix~\ref{app:dp}.
In addition, we show in section~\ref{sec:exp_res} that these new constraints are \emph{key} for the success of the method. %

%
%
%
%
%


%
%
%

%
%
%
%
%
%
%


\subsection{Joint rounding method}
\label{sec:rounding}
Once we obtain a candidate solution of the relaxed problem, we have to round it to an integer solution in order to make predictions.
Previous works~\cite{Alayrac15Unsupervised,Bojanowski15weakly} have observed that using the learned $W^*$ classifier for rounding gave better results than other possible alternatives. We extend this approach to our joint setup by proposing the following new rounding procedure. 
We optimize problem~\eqref{eq:jointstateactionprob} but fix the values of $W$ in the discriminative clustering costs. 
Specifically, we minimize the following cost function over the integer points $Z\in \mathcal{Z}$ and $Y \in \mathcal{Y}$:
\begin{align}
\label{eq:jcrrounding}
\!\!\!\!\!\frac{1}{2T} \|Z - X_{v} W^*_{v}\|_F^2 +  \frac{1}{2M} \|Y - X_{s} W^*_{s}\|_F^2 +  d(Z, Y), \!\!
\end{align}
where $W^*_{v}$ and $W^*_{s}$ are the classifier weights obtained at the end of the relaxed optimization.
Because $y^2 = y$ when $y$ is binary, \eqref{eq:jcrrounding} is actually a \emph{linear} objective over the binary matrix~$Y_n$ for~$Z_n$ fixed.
Thus we can optimize~\eqref{eq:jcrrounding} \emph{exactly} by solving a dynamic program on $Y_n$ for each of the $T_n$ possibilities of $Z_n$, yielding $O(M_n T_n)$ time complexity per clip (see Appendix~\ref{app:jcr} for details).



\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage{subfloat}
\usepackage{titling}
\usepackage{iccv}
\usepackage{soul}
\usepackage{array}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
%
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs,siunitx}
\usepackage{adjustbox}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{subcaption}
\usepackage{footmisc}

\soulregister\cite7
\soulregister\ref7



%
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\DeclareRobustCommand{\JB}[1]{{\sethlcolor{cyan}\hl{JB: #1}}}
%
\hypersetup{
  plainpages=false,
  %
  colorlinks=true,   			%
  linkcolor=blue,    			%
  anchorcolor=blue,  			%
  citecolor=blue,    			%
  filecolor=blue,    			%
  pagecolor=blue,    			%
  urlcolor=blue,     			%
  pdfview=FitH,                 %
  pdfstartview=FitH,            %
  pdfpagelayout=SinglePage      %
}

%
%
%
%
%
\DeclareMathOperator*{\argmin}{arg\,min}
\iccvfinalcopy

\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

%
\ificcvfinal\pagestyle{empty}\fi


%
\begin{document}

\title{Joint Discovery of Object States and Manipulation Actions}
\author{
Jean-Baptiste Alayrac\thanks{D\'{e}partement d’informatique de l'ENS, \'{E}cole normale sup\'{e}rieure, CNRS, PSL Research University, 75005
	Paris, France.} \ \thanks{INRIA}
\and
Josef Sivic\footnotemark[1] \ \footnotemark[2] \ \thanks{Czech Institute of Informatics, Robotics and Cybernetics at the
Czech Technical University in Prague.}
\and
Ivan Laptev\footnotemark[1] \ \footnotemark[2]
\and
Simon Lacoste-Julien\thanks{Department of CS \& OR (DIRO), Universit\'{e} de Montr\'{e}al, Montr\'{e}al.}
}
\maketitle
\begin{abstract}
Many human activities involve object manipulations aiming to modify the object state.
Examples of common state changes include full/empty bottle, open/closed door, and attached/detached car wheel.
In this work, we seek to automatically discover the states of objects and the associated manipulation actions.
Given a set of videos for a particular task, we propose a joint model that learns to identify object states and to localize state-modifying actions.
Our model is formulated as a discriminative clustering cost with constraints.
We assume a consistent temporal order for the changes in object states and manipulation actions, 
and introduce new optimization techniques to learn model parameters without additional supervision.
We demonstrate successful discovery of seven manipulation actions and corresponding object states on a new dataset of videos depicting real-life object manipulations.
We show that our joint formulation results in an improvement of object state discovery by action recognition and vice versa.
%
%
%
%
%
%
%
%
%
\end{abstract}
\input{arxiv_intro.tex}
\input{arxiv_model.tex}
\input{optimization.tex}
\input{arxiv_experiment.tex}
\input{arxiv_conclusion.tex}

\footnotesize{
	\paragraph{Acknowledgments}
	This research was supported in part by a Google Research Award, ERC grants Activia (no. 307574) and LEAP (no. 336845), the CIFAR Learning in Machines \& Brains program and ESIF, OP Research, development and education Project IMPACT No. CZ.02.1.01/0.0/0.0/15 003/0000468.
}


{\small
\bibliographystyle{ieee}
\bibliography{biblio}
}


%
\clearpage
\appendix
\normalsize
\section*{Outline of Appendix}
%
This Appendix gives additional details and quantitative results for our method.
The organization is as follows.
In Appendix~\ref{app:svm}, we provide additional experimental details about the SVM training used to retrieve the clips with subtitles, described in Section~\ref{sec:exp_weak} of the main paper, along with a visualization of results.
In Appendix~\ref{app:dataset}, we give additional statistics and details about the dataset that was briefly introduced in Section~\ref{sec:dataset} of the main paper.
In Appendix~\ref{app:dp}, we give additional details about the dynamic program that we use to solve the linear program over the track constraints defined in Section~\ref{sec:statemodel} of the main paper as needed for the Frank-Wolfe optimization algorithm.
In Appendix~\ref{app:jcr}, we detail how we implement the \emph{new} joint rounding method~\eqref{eq:jcrrounding} that was introduced in Section~\ref{sec:joint} of the main paper.
In Appendix~\ref{app:supervised}, we give additional details about the supervised baselines results given in Section~\ref{sec:exp_res}.
Finally, in Appendix~\ref{app:failcases}, we comment on the common failure cases of the method.




%

\input{suppmat_svm.tex}
\input{suppmat_fig1.tex}
\input{suppmat_dataset.tex}
%
\input{suppmat_DP.tex}

%
\input{suppmat_roundingjcr.tex}

\input{suppmat_supervised.tex}

\input{suppmat_failurecases.tex}




\end{document}
\documentclass{sig-alternate}
\pdfoutput=1 %

\usepackage{amsmath,amsmath, bm, color,algorithm,array,multirow}
\usepackage[noend]{algorithmic}
%
%
\usepackage[
    pdfstartview={FitH},
    bookmarks=true,
    bookmarksnumbered=true,
    bookmarksopen=true,
    bookmarksopenlevel=\maxdimen,
    pdfborder={0 0 0},
    colorlinks=true,
    linkcolor = blue,
    citecolor=blue,
    urlcolor=blue,
    filecolor=blue,
    pdfauthor={Simon Lacoste-Julien, Konstantina Palla, Alex Davies, Gjergji Kasneci, Thore Graepel, Zoubin Ghahramani},
    pdftitle={SiGMa: Simple Greedy Matching for Aligning Large Knowledge Bases},
    pdfdisplaydoctitle=true
]{hyperref}
\usepackage{graphicx}

\toappear{}
%
%
%
%
%
%
%
%

\newcommand{\todo}[1]{\textcolor{red}{[#1]}}
%
\newcommand{\previousversion}[1]{}
\newcommand{\cut}[1]{}
\newcommand{\note}[1]{}
\newcommand{\D}{\displaystyle}
\renewcommand{\tt}[1]{\texttt{#1}}
\newcommand{\ts}[1]{\textsf{#1}}

\newcommand{\KB}{K\!B}

%
%

%
\newcommand{\specialcell}[3][c]{%
  \begin{tabular}[#1]{@{}#2@{}}#3\end{tabular}}


%
%
%
%
%
%
%
%



%
%
\renewcommand{\topfraction}{0.85}
\renewcommand{\textfraction}{0.1}
\renewcommand{\floatpagefraction}{0.75}

\addtolength{\textfloatsep}{-0.2\textfloatsep}

\begin{document}

%
\title{SiGMa: Simple Greedy Matching\\ for Aligning Large Knowledge Bases}

\numberofauthors{3} %

%
%
%

\author{
%
\alignauthor
Simon Lacoste-Julien\\
       \affaddr{INRIA - SIERRA project-team}\\
       \affaddr{Ecole Normale Superieure}
       \affaddr{Paris, France}\\
%
\alignauthor
Konstantina Palla\\
       \affaddr{University of Cambridge}\\
       \affaddr{Cambridge, UK}\\
%
\alignauthor Alex Davies\\
       \affaddr{University of Cambridge}\\
       \affaddr{Cambridge, UK}\\
\and  %
%
\alignauthor Gjergji Kasneci\\
       \affaddr{Microsoft Research}\\
       \affaddr{Cambridge, UK}\\
%
\alignauthor Thore Graepel\\
       \affaddr{Microsoft Research}\\
       \affaddr{Cambridge, UK}\\
%
\alignauthor Zoubin Ghahramani\\
       \affaddr{University of Cambridge}\\
       \affaddr{Cambridge, UK}\\
}


\maketitle
\begin{abstract}
The Internet has enabled the creation of a growing number of large-scale knowledge bases in a variety of domains containing complementary information. Tools for automatically aligning these knowledge bases would make it possible to unify many sources of structured knowledge and answer complex queries. However, the efficient alignment of \emph{large-scale} knowledge bases still poses a considerable challenge.  Here, we present \textsf{Simple Greedy Matching} (\textsf{SiGMa}), a simple algorithm for aligning knowledge bases with millions of entities and facts. \textsf{SiGMa} is an iterative propagation algorithm which leverages both the structural information from the relationship graph as well as flexible similarity measures between entity properties in a greedy local search, thus making it scalable. Despite its greedy nature, our experiments indicate that \textsf{SiGMa} can efficiently match some of the world's largest knowledge bases with high precision. We provide additional experiments on benchmark datasets which demonstrate that \ts{SiGMa} can outperform state-of-the-art approaches both in accuracy and efficiency.
\end{abstract}

%
%
%
%
%
%
%
%
%
%
%

\section{Introduction}
In the last decade, a growing number of large-scale knowledge bases have been created online. Examples of domains include
music, movies, publications and biological data\footnote{Such as \href{http://musicbrainz.org/}{MusicBrainz}, \href{http://www.imdb.com/}{IMDb}, \href{http://www.informatik.uni-trier.de/~ley/db}{DBLP} and \href{http://www.uniprot.org/}{UnitProt}.}. As these knowledge bases sometimes contain both overlapping and complementary information, there has been growing interest in attempting to merge them by \emph{aligning} their common elements. This alignment could have important uses for information retrieval and question answering. For example, one could be interested in finding a scientist with expertise on certain related protein functions -- information which could be obtained by aligning a biological database with a publication one. Unfortunately, this task is challenging to automate as different knowledge bases generally use different terms to represent their entities, and the space of possible matchings grows exponentially with the number of entities.

A significant amount of research has been done in this area -- particularly under the umbrella term of \emph{ontology matching} \cite{choi06survey, kalfoglou03om-state-of-the-art, euzenat07om-book}. An ontology is a formal collection of world knowledge and can take different structured representations. In this paper, we will use the term \emph{knowledge base} to emphasize that we assume very little structure about the ontology (to be specified in Section~\ref{sec:problem}). Despite the large body of literature in this area, most of the work on ontology matching has been demonstrated only on fairly small datasets of the order of a few hundred entities. In particular, Shvaiko and Euzenat~\cite{shvaiko08challenges} identified \emph{large-scale evaluation} as one of the ten challenges for the field of ontology matching.

In this paper, we consider the problem of aligning the \emph{instances} in \emph{large} knowledge bases, of the order of millions of entities and facts, where \emph{aligning} means automatically identifying corresponding entities and interlinking them. Our starting point was the challenging task of aligning the movie database \textsf{IMDb} to the Wikipedia-based \textsf{YAGO}~\cite{suchanek2007WWW}, as another step towards the Semantic Web vision of interlinking different sources of knowledge which is exemplified by the Linking Open Data Initiative\footnote{\href{http://linkeddata.org/}{http://linkeddata.org/}}~\cite{lee08WWW}. Initial attempts to
match \textsf{IMDb} entities to \textsf{YAGO} entities by naively exploiting string and neighborhood information failed, and so we designed \textsf{SiGMa} (\textsf{Simple Greedy Matching}), a scalable greedy iterative algorithm which is able to exploit previous matching decisions as well as the relationship graph information between entities.

The design decisions behind \textsf{SiGMa} were both to be able to take advantage of the combinatorial structure of the matching problem (by contrast with database record linkage approaches which make more independent decisions) as well as to focus on a simple approach which could be scalable. \textsf{SiGMa} works in two stages: it first starts with a small seed matching assumed to be of good quality. Then the algorithm incrementally augments the matching by using \emph{both} structural information and properties of entities such as their string representation to define a modular score function. Some key aspects of the algorithm are that (1) it uses the current matching to obtain structural information, thereby harnessing information from previous decisions; (2) it proposes candidate matches in a \emph{local} manner, from the structural information; and (3) it makes greedy decisions, enabling a scalable implementation. A surprising result is that we obtained accurate large-scale matchings in our experiments despite the greediness of the algorithm.

%

\paragraph*{Contributions} The contributions of the present work are the following:
\begin{enumerate}
\item We present \textsf{SiGMa}, a knowledge base alignment algorithm which can handle millions of entities. The algorithm is easily extensible with tailored scoring functions to incorporate domain knowledge. It also provides a natural tradeoff between precision and recall, as well as between computation and recall.
  \item In the context of testing the algorithm, we constructed two large-scale partially labeled knowledge base
    alignment datasets with hundreds of thousands of ground truth mappings. We expect these to be a useful resource for the research community to develop and evaluate new knowledge base alignment algorithms.
  \item We provide a detailed experimental comparison illustrating how \textsf{SiGMa} improves over the state-of-the-art. \textsf{SiGMa} is able to align knowledge bases with millions of entities with over 95\% precision in less than two hours (a 50x speed-up over~\cite{suchanek12PARIS}). On standard benchmark datasets, \textsf{SiGMa} obtains solutions with higher F-measure than the best previously published results.
\end{enumerate}

The remainder of the paper is organized as follows. Section~\ref{sec:problem} presents the knowledge base alignment problem with a real-world example as motivation for our assumptions. We describe the algorithm \textsf{SiGMa} in Section~\ref{sec:algorithm}. We evaluate it on benchmark and on real-world datasets in Section~\ref{sec:experiments}, and situate it in the context of related work in Section~\ref{sec:related}.



\section{Aligning Large-Scale Knowledge Bases} \label{sec:problem}

\subsection{Motivating example: \textsf{YAGO} and \textsf{IMDb}}
Consider merging the information in the following two knowledge bases:
\begin{enumerate}
    \item \textsf{YAGO}, a large semantic knowledge base derived from English Wikipedia~\cite{suchanek2007WWW}, WordNet~\cite{wordnet} and GeoNames.\footnote{\href{http://www.geonames.org/}{http://www.geonames.org/}}
 \item \textsf{IMDb}, a large popular online database that stores information about movies.\footnote{\href{http://www.imdb.com/}{http://www.imdb.com/}}
\end{enumerate}
The information in \textsf{YAGO} is available as a long list of triples (called \emph{facts}) that we formalize as:
\begin{equation}
\label{eq:triplet}
\langle e, r, e'\rangle,
\end{equation}
which means that the directed relationship $r$ holds from entity $e$ to entity $e'$, such as $\langle \textrm{John\_Travolta}, \textrm{ActedIn}, \textrm{Grease}\rangle$. The information from \textsf{IMDb} was originally available as several files which we merged into a similar list of triples.  We call these two databases \emph{knowledge bases} to emphasize that we are not assuming a richer representation, such as RDFS \cite{RDFs}, which would distinguish between classes and instances for example. In the language of ontology matching, our setup is the less studied \emph{instance matching} problem, as point\-ed out by Castano et al.~\cite{castano08instanceMatching}, for which the goal is to match concrete instantiations of concepts such as specific actors and specific movies rather than the general actor or movie class.
%
\previousversion{
%
%
%
}
%
%
\textsf{YAGO} comes with an RDFS representation, but not \textsf{IMDb}; therefore we will focus on methods that do not assume or require a class structure or rich hierarchy in order to find a one-to-one matching of instances between \textsf{YAGO} and \textsf{IMDb}.

We note that in the full generality of the ontology matching problem, both the schema and the instances of one ontology are to be related with the ones of the other ontology. Moreover, in addition to the \texttt{isSameAs} (or ``$\equiv$'') relationship that we consider, these matching relationships could be \texttt{isMoreGeneralThan} (``$\supseteq$''), \tt{isLessGeneral\-Than} (``$\subseteq$'') or even \texttt{hasPartialOverlap}. In our example, because the number of relations in the knowledge bases is relatively small (108 in \textsf{YAGO} and 10 in \textsf{IMDb}), we could align the relations manually, discovering six equivalent ones as listed in Table~\ref{tab:relations}. As we will see in our experiments, focussing uniquely on the \texttt{isSameAs} type of relationship between instances of the two knowledge bases is sufficient in the \textsf{YAGO}-\textsf{IMDb} setup to cover most cases. %
The exceptions are rare enough for \textsf{SiGMa} to obtain useful results while making the simplifying assumption that the alignment between the instances is \emph{injective} (1-1).
\begin{table}
\begin{center}
\begin{tabular}{cc}
\hline
\textsf{YAGO} & \textsf{IMDb} \\ \hline
actedIn  & actedIn  \\
directed & directed \\
produced & produced \\
created & composed \\
hasLabel$^*$ & hasLabel$^*$ \\
wasCreatedOnDate$^*$ & hasProductionYear$^*$ \\
\hline
\end{tabular}
\end{center}
\caption{Manually matched relations between \textsf{YAGO} and \textsf{IMDb}. \textnormal{The starred pairs are actually pairs of \emph{properties}, as defined in the text.}\label{tab:relations}}
\end{table}

\paragraph*{Relationships vs. properties} Given our assumption that the alignment is 1-1, it is important to  distinguish between two types of objects which could be present in the list of triples: \emph{entities} vs. \emph{literals}. By our definition, the \emph{entities} will be the only objects that we will try to align -- they will be objects like specific actors or specific movies which have a clear identity. The \emph{literals}, on the other hand, will correspond to a value related to an entity through a special kind of relationship that we will call \emph{property}. The defining characteristic of literals is that it would not make sense to try to align them between the two knowledge bases in a 1-1 fashion. For example, in the \textsf{YAGO} triple $\langle \tt{m1}$, $\tt{wasCreatedOnDate}$, $\tt{1999-12-11}\rangle$, the object \tt{1999-12-11} could be interpreted as a literal representing the value for the property \tt{wasCreated\-On\-Date} for the entity \tt{m1}. The corresponding property in our version of \textsf{IMDb} is \tt{has\-Production\-Year} which has values only at the year granularity (\tt{1999}). The 1-1 restriction would prevent us to align both \tt{1999-12-11} and \tt{1999-12-10} to \tt{1999}. On the other hand, we can use these literals to define a similarity score between entities from the two knowledge bases (for example in this case, whether the year matches, or how close the dates are to each other). We will thus have two types of triples: entity-relationship-entity and entity-property-literal. We assume that the distinction between relationships and properties (which depends on the domain and the user's goals) is easy to make; for example, in the \textsf{Freebase} dataset that we also used in our experiments, the entities would have unique identifiers but not the literals. Figure~\ref{fig:example} provides a concrete example of information presents in the two knowledge bases that we will keep re-using in this paper.

We are now in a position to state more precisely the problem that we address.

\textbf{Definition:} A \emph{knowledge base} $\KB$ is a tuple \\
$(\mathcal{E},\mathcal{L},\mathcal{R},\mathcal{P},\mathcal{F}_R,\mathcal{F}_P)$ where $\mathcal{E}$, $\mathcal{L}$, $\mathcal{R}$ and $\mathcal{P}$ are sets of entities, literals, relationships and properties respectively; $\mathcal{F}_R \subseteq \mathcal{E}\times\mathcal{R}\times\mathcal{E}$ is a set of relationship-facts whereas $\mathcal{F}_P \subseteq \mathcal{E}\times\mathcal{P}\times\mathcal{L}$ is a set of property-facts (both can be represented as a simple list of triples). To simplify the notation, we assume that all inverse relations are also present in $\mathcal{F}_R$ -- that is, if $\langle e,r,e' \rangle$ is in $\mathcal{F}_R$, we also have $\langle e',r^{-1}, e \rangle$ in $\mathcal{F}_R$, effectively doubling the number of possible relations in the $\KB$.\footnote{This allows us to look at only one standard direction of facts and cover all possibilities -- see for example how it is used in the definition of \tt{compatible-neigbhors} in \eqref{eq:compatible}.}

\textbf{Problem: one-to-one alignment of instances between two knowledge bases.}  Given two knowledge bases $\KB_1$ and $\KB_2$ as well as a partial mapping between their corresponding relationships and properties, we want to output a 1-1 partial mapping $m$ from $\mathcal{E}_1$ to $\mathcal{E}_2$ which represents the semantically equivalent entities in the two knowledge bases (by partial mapping, we mean that the domain of $m$ does not have to be the whole of $\mathcal{E}_1$). %

\subsection{Possible approaches} \label{ssec:approaches}
Standard approaches for the ontology matching problem, such as RiMOM~\cite{li09RiMOM}, could be used to align small knowledge bases. However, they do not scale to millions of entities as needed for our task given that they usually consider all pairs of entities, suffering from a quadratic scaling cost. On the other hand, the related problem of identifying duplicate entities known as \emph{record linkage} or \emph{duplicate detection} in the database field, and \emph{co-reference resolution} in the natural langue processing field, do have scalable solutions~\cite{arasu09deduplication,gracia09largeScaleSenses}, though these do not exploit the 1-1 matching combinatorial structure present in our task, which reduces their accuracy. More specifically, they usually make independent decisions for different entities using some kind of similarity function, rather than exploiting the competition between different assignments for entities.
%
A notable exception is the work on \emph{collective} entity resolution by Bhattacharya and Getoor~\cite{getoor07relational}, solved using a greedy agglomerative clustering algorithm. The algorithm \ts{SiGMa} that we present in Section~\ref{sec:algorithm} can actually be seen as an efficient specialization of their work to the task of knowledge base alignment.

Another approach to alignment arises from the word alignment problem in natural language processing~\cite{och03comparison}, which has been formulated as a maximum weighted bipartite matching problem~\cite{taskar05matching} (thus exploiting the 1-1 matching structure). It also has been formulated as a quadratic assignment problem in~\cite{lacoste06qap}, which encourages neighbor entities in one graph to align to neighbor entities in the other graph, thus enabling alignment decisions to depend on each other --- see the caption of Figure~\ref{fig:example} for an example of this in our setup. The quadratic assignment formulation~\cite{lawler63qap}, which can be solved as an integer linear program, is NP-hard in general though, and these approaches were only used to align at most one hundred entities. In the algorithm \ts{SiGMa} that we propose, we are interested in exploiting both the 1-1 matching constraint, as well as building on previous decisions, like these word alignment approaches, but in a scalable manner which would handle millions of entities. \ts{SiGMa} does this by greedily optimizing the quadratic assignment objective, as we will describe in Section~\ref{ssec:greedy}. Finally, Suchanek et al.~\cite{suchanek12PARIS} recently proposed an ontology matching approach called \ts{PARIS} that they have succeeded to apply on the alignment of \ts{YAGO} to \ts{IMDb} as well, though the scalability of their approach is not as clear, as we will explain in Section~\ref{sec:related}. We will provide a detailed comparison with \ts{PARIS} in the experiments section.

\subsection{Design choices and assumptions}
Our main design choices result from our need for a fast algorithm for knowledge base alignment which scales to millions of entities. %
To this end we made the following assumptions:

\textbf{1-1 matching and uniqueness.} We assume that the true alignment between the two $\KB$s is a partial function which is mainly 1-1. %
If there are duplicate entities inside a $\KB$, \textsf{SiGMa} will only align one of the duplicates to the corresponding entity in the other $\KB$.

\textbf{Aligned relationships.} We assume that we are given a partial alignment between relationships and between properties of the $\KB$s.

%
%
%
%

%
%
%
%
\begin{figure}
	\begin{center}
		\includegraphics[width=\columnwidth]{figures/graph_example.pdf}
	\end{center}
    \vspace*{-6mm}
	\caption{\textbf{Example of neighborhood to match in \ts{YAGO} and \ts{IMDb}.} \textnormal{Even though entities $i$ and $j$ have no words in common, the fact that several of their respective neighbors are matched together is a strong signal that $i$ and $j$ should be matched together. This is a real example from the dataset used in the experiments and \ts{SiGMa} was able to correctly match all these pairs ($i$ and $j$ are actually the same movie despite their different stored titles in each $\KB$).}} \label{fig:example}
\end{figure}

\section{The SiGMa Algorithm} \label{sec:algorithm}
\subsection{Greedy optimization of a quadratic assignment objective} \label{ssec:greedy}
%
The \textsf{SiGMa} algorithm can be seen as the greedy optimization of an objective function which globally scores the suitability of a particular matching $m$ for a pair of given $\KB$s. This objective function will use two sources of information useful to choose matches: a similarity function between pairs of entities defined from their properties; and a graph neighborhood contribution making use of neighbor pairs being matched (see Figure~\ref{fig:example} for a motivation). Let us encode the matching $m : \mathcal{E}_1 \rightarrow \mathcal{E}_2 $ by a matrix $y$ with entries indexed by the entities in each $\KB$, with $y_{ij} = 1$ if $m(i) = j$, meaning that $i \in \mathcal{E}_1$ is matched to $j \in \mathcal{E}_2$, and $y_{ij} = 0$ otherwise. The space of possible 1-1 partial mappings is thus represented by the set of binary matrices: $\mathcal{M} \doteq \{y \in \{0,1\}^{\mathcal{E}_1 \times \mathcal{E}_2} : \sum_{l} y_{il} \leq 1 \; \forall i \in \mathcal{E}_1$ and $\sum_{k} y_{kj} \leq 1 \; \forall{j} \in \mathcal{E}_2\}$. We define the following quadratic objective function which globally scores the suitability of a matching $y$:
\begin{equation} \label{eq:obj}
    \begin{split}
    \texttt{obj}(y) & \doteq \sum_{(i,j) \in \mathcal{E}_1  \times \mathcal{E}_2} y_{ij} \left[ (1-\alpha) s_{ij} + \alpha g_{ij}(y) \right], \\
    & \textrm{where} \qquad g_{ij}(y) \doteq \sum_{(k,l) \in \mathcal{N}_{ij}} y_{kl} \, w_{ij,kl}.
    \end{split}
\end{equation}
The objective contains linear coefficients $s_{ij}$ which encode a similarity between entity $i$ and $j$, as well as quadratic coefficients $w_{ij,kl}$ which control the algorithm's tendency to match $i$ with $j$ given that $k$ was matched to $l$\footnote{In the rest of this paper, we will use the convention that $i$ and $k$ are always entities in $\KB_1$; whereas $j$ and $l$ are in $\KB_2$. $e$ could be in either $\KB$.}. $\mathcal{N}_{ij}$ is a local neighborhood around $(i,j)$ that we define later and which will depend on the graph information from the $\KB$s -- $g_{ij}(y)$ is basically counting (in a weighted fashion) the number of matched pairs $(k,l)$ which are in the neighborhood of $i$ and $j$. $\alpha \in [0,1]$ is a tradeoff parameter between the linear and quadratic contributions. Our approach is motivated by the maximization problem:
%
%
%
%
\begin{equation} \label{eq:opt}
\begin{aligned}
\max_y &\quad \texttt{obj}(y) \\
\textrm{s.t.} &\quad y \in \mathcal{M}, \quad \lVert y \rVert_1 \leq R,
\end{aligned}
\end{equation}
where the norm $\lVert y \rVert_1 \doteq \sum_{ij} y_{ij}$ represents the number of elements matched
%
and $R$ is an unknown upper-bound which represents the size of the best partial mapping which can be made from $\KB_1$ to $\KB_2$. We note that if the coefficients are all positive (as will be the case in our formulation -- we are only encoding similarities and not repulsions between entities), then the maximizer $y^*$ will have $\lVert y^* \rVert_1 = R$. Problem~\eqref{eq:opt} is thus related to one of the variations of the quadratic assignment problems, a well-known NP-complete problem in operational research~\cite{lawler63qap}\footnote{See Appendix~\ref{ap:qap} for the traditional description of the quadratic assignment problem and its relationship to our problem.}.
Even though one could approximate the solution to the combinatorial optimization~\eqref{eq:opt} using a linear program relaxation (see Lacoste-Julien et al.~\cite{lacoste06qap}), the number of variables is quadratic in the number of entities, and so is obviously not scalable. Our approach is instead to \emph{greedily optimize}~\eqref{eq:opt} by adding the match element $y_{ij}=1$ at each iteration which increases the objective the most and selected amongst a small set of possibilities. In other words, the high-level operational definition of the \textsf{SiGMa} algorithm is as follows:
\begin{enumerate}
    \item Start with an initial good quality partial match $y_0$.
    \item At each iteration $t$, augment the previous matching with a new matched pair by setting $y_{ij}=1$ for the $(i,j)$ which maximally increases $\texttt{obj}$, chosen amongst a small set $\mathcal{S}_t$ of reasonable candidates which preserve the feasibility of the new matching.
    \item Stop when the bound $\lVert y \rVert_1 = R$ is reached (and never undo previous decisions).
\end{enumerate}

Having outlined the general framework, in the remainder of this section we will describe methods for choosing the similarity coefficients $s_{ij}$ and $w_{ij,kl}$ so that they guide the algorithm towards good matchings (Section~\ref{sec:score}), the choice of neighbors, $\mathcal{N}_{ij}$, the choice of a candidate set $\mathcal{S}_t$, and the stopping criterion, $R$. These choices influence both the speed and accuracy of the algorithm.

%
%
%
%
%
%

%

\textbf{Compatible-neighbors.} $\mathcal{N}_{ij}$ should be chosen so as to respect the graph structure defined by the $\KB$ facts. Its contribution in the objective crucially encodes the fact that a neighbor $k$ of $i$ being matched to a `compatible' neighbor $l$ of $j$ should encourage $i$ to be matched to $j$ --- see the caption of Figure~\ref{fig:example} for an example. Here, compatibility means that they are related by the same relationship (they have the same color in Figure~\ref{fig:example}). Formally, we define:
\begin{multline} \label{eq:compatible}
    \mathcal{N}_{ij} = \textrm{\tt{compatible-neighbors}}(i,j) \doteq \\
    \textrm{\parbox{0.8\columnwidth}{\{ $(k,l)$ : $\langle i,r,k \rangle$ is in $\mathcal{F}_{R1}$ and $\langle j,s,l \rangle$ is in $\mathcal{F}_{R2}$ and relationship $r$ is matched to $s$\}.}}
\end{multline}
Note that a property of this neighborhood is that $(k,l) \in \mathcal{N}_{ij}$ iff $(i,j) \in \mathcal{N}_{kl}$, as we have that the relationship $r$ is matched to $s$ iff $r^{-1}$ is matched to $s^{-1}$ as well. This means that the increase in the objective obtained by adding $(i,j)$ to the current matching $y$ defines the following \emph{context dependent similarity score function} which is used to pick the next matched pair in the step 2 of the algorithm:
    \begin{multline} \label{eq:score}
       \texttt{score}(i,j; y)  =  (1-\alpha) s_{ij} + \alpha \, \delta g_{ij}(y)   \\
        \textrm{where }  \delta g_{ij}(y)  \doteq \sum_{(k,l) \in \mathcal{N}_{ij}} y_{kl} \, (w_{ij,kl} + w_{kl,ij}).
    \end{multline}

\textbf{Information propagation on the graph.} The \tt{compati\-ble-neighbors} concept that we just defined is one of the most crucial characteristics of \ts{SiGMa}. It allows the information of a new matched pair to propagate amongst its neighbors. It also defines a powerful heuristic to suggest new candidate pairs to include in a small set $\mathcal{S}_t$ of matches to choose from: after matching $i$ to $j$, \ts{SiGMa} adds all the pairs $(k,l)$ from \tt{compatible-neighbors}$(i,j)$ as new candidates. This yields a fire propagation analogy for the algorithm: starting from an initial matching (fire) -- it starts to match their neighbors, letting the fire propagate through the graph. If the graph in each $\KB$ is well-connected in a similar fashion, it can visit most nodes this way. This heuristic enables \ts{SiGMa} to avoid the potential quadratic number of pairs to consider by only focussing its attention on the neighborhoods of current matches.

\textbf{Stopping criterion.} \ts{SiGMa} terminates when the variation in the objective value, \texttt{score}$(i,j; y)$, of the latest added match $(i,j)$ falls below a threshold (or the queue becomes empty). The threshold in effect controls the precision / recall tradeoff of the algorithm. By ensuring that the $s_{ij}$ and $g_{ij}(y)$ terms are normalized between 0 and 1, we can standardize the scale of the threshold for different score functions. In our experiments, a threshold of 0.25 is observed to correlate well with a point at which the F-measure stops increasing and the precision is significantly decreasing.

\subsection{Algorithm and implementation} \label{ssec:implementation}
We present the pseudo-code for \ts{SiGMa} in Table~\ref{tab:alg}. We now elaborate on the algorithm design as well as its implementation aspects. We note that the \texttt{score} defined in~\eqref{eq:score} to greedily select the next matched pair is composed of a static term $s_{ij}$, which does not depend on the evolving matching $y$, and a dynamic term $\delta g_{ij} (y)$, which depends on $y$, though only through the local neighborhood $\mathcal{N}_{ij}$. We call the $\delta g_{ij}$ component of the score function the graph contribution -- its local dependence means that it can be updated efficiently after a new match has been added. We explain in more details the choice of similarity measures for these components in Section~\ref{sec:score}.

%
{\addtolength{\textfloatsep}{-0.2\textfloatsep}
\begin{table}
\fbox{\parbox[t]{\columnwidth}{
\begin{algorithmic}[1]
    \STATE Initialize matching $m=m_0$.
    \STATE Initialize priority queue $\mathcal{S}$ of suggested candidate pairs as $\mathcal{S}_0 \cup \left(\bigcup_{(i,j) \in m} \mathcal{N}_{ij} \right)$ -- the \tt{compatible-neigbhors} of pairs in $m$, with $\texttt{score}(i,j; m)$ as their key.
    \WHILE{priority queue $\mathcal{S}$ is not empty}
        \STATE Extract $\langle \textrm{\tt{score}},i,j\rangle$ from queue $\mathcal{S}$
        %
        \STATE \textbf{if} $\tt{score} \leq \tt{threshold}$ \textbf{then stop}
        \IF{$i$ or $j$ is already matched to some entity} \STATE skip them and \textbf{continue} loop
        \ELSE \STATE Set $m(i)=j$. \\
            \COMMENT{We update candidate lists and scores:}
            \FOR{$(k,l)$ in $\mathcal{N}_{ij}$ and not already matched}
                \STATE Add $\langle \textrm{\tt{score}}(k,l; m),k,l\rangle$ to queue $\mathcal{S}$.
            \ENDFOR
        \ENDIF
    \ENDWHILE
\end{algorithmic}}}
\caption{\textsf{SiGMa} algorithm.} \label{tab:alg}
\end{table}
}

\paragraph*{Initial match structure $m_0$} The algorithm can take any initial matching seed assumed of good quality. In our current implementation, this is done by looking for entities with the same string representation (with minimal standardization such as removing capitalization and punctuation) with an \emph{unambiguous 1-1 match} -- that is, we do not include an exact matched pair when more than two entities have this same string representation, thereby increasing precision. %

\paragraph*{Increasing score function with local dependence} The score function has a component $s_{ij}$ which is static (fixed at the beginning of the algorithm) from the properties of entities such as their string representation, and a component $\delta g_{ij}(y)$ which is dynamic, looking at how many neighbors are correctly matched. The dynamic part can actually only increase when new neighbors are matched, and only the scores of neighbors can change when a new pair is matched.

\paragraph*{Optional static list of candidates $\mathcal{S}_0$} Optionally, we can initialize $\mathcal{S}$ with a static list $\mathcal{S}_0$ which only needs to be scored once as any score update will come from neighbors already covered by step 11 of the algorithm. $\mathcal{S}_0$ has the purpose to increase the possible exploration of the graph when another strong source of information (which is not from the graph) can be used. In our implementation, we use an inverted index built on words to efficiently suggest entities which have at least two words in common in their string representation as potential candidates.\footnote{To keep the number of suggestions manageable, we exclude a list of stop words built automatically from the 1,000 most frequent words of each $\KB$.} %

\paragraph*{Data-structures} We use a binary heap for the priority queue implementation---insertions will thus be $O(\log n)$ where $n$ is the size of the queue. Because the score function can only increase as we add new matches, we do not need to keep track of stale nodes in the priority queue in order to update their scores, yielding a significant speed-up.
%

%

\subsection{Score functions} \label{sec:score}
An important factor for any matching algorithm is the similarity function between pairs of elements to match. Designing good similarity functions has been the focus of much of the literature on record linkage, entity resolution, etc., and because \textsf{SiGMa} uses the score function in a modular fashion, \textsf{SiGMa} is free to use most of them for the term $s_{ij}$ as long as they can be computed efficiently. We provide in this section our implementation choices (which were motivated by simplicity), but we note that the algorithm can easily handle more powerful similarity measures. The generic score function used by \textsf{SiGMa} was given in~\eqref{eq:score}. In the current implementation, the static part $s_{ij}$ is defined through the \emph{properties} of entities only. The graph part $\delta g_{ij}(y)$ depends on the \emph{relationships} between entities (as this is what determines the graph), as well as the previous matching $y$. We also make sure that $s_{ij}$ and $g_{ij}$ stay normalized so that the score of different pairs are on the same scale.

\subsubsection{Static  similarity measure}
The static property similarity measure is further decomposed in two parts: we single out a contribution coming from the string representation property of entities (as it is such a strong signal for our datasets), and we consider the other properties together in a second term: %
\begin{equation} \label{eq:static}
s_{ij} = (1-\beta) \textrm{\tt{string}}(i,j) + \beta \textrm{\tt{prop}}(i,j),
\end{equation}
where $\beta \in [0,1]$ is a tradeoff coefficient between the two contributions set to 0.25 during the experiments.
%

%
\paragraph*{String similarity measure} For the string similarity measure, we primarily consider the number of words which two strings have in common, albeit weighted by their information content. In order to handle the varying lengths of strings, we use the Jaccard similarity coefficient between the sets of words, a metric often used in information retrieval and other data mining fields~\cite{hamers89Jaccard,getoor07relational}. The Jaccard similarity between set $A$ and $B$ is defined as $\textrm{Jaccard}(A,B) \doteq |A \cap B| / |A \cup B|$, which is a number between 0 and 1 and so is normalized as required. We also add a smoothing term in the denominator in order to favor longer strings with many words in common over very short strings. Finally, we use a \emph{weighted} Jaccard measure in order to capture the information that some words are more informative than others. In analogy to a commonly used feature in information retrieval, we use the IDF (inverse-document-frequency) weight for each word. The weight for word $v$ in $\KB_o$ is $w^o_v \doteq \log_{10} \frac{|\mathcal{E}_o|}{|E^o_v|}$, where $E^o_v \doteq \{e \in \mathcal{E}_o :$ $e$ has word $v$ in its string representation$\}$. Combining these elements, we get the following string similarity measure:
{\addtolength\abovedisplayskip{-4mm} %
\begin{equation} \label{eq:string}
\textrm{\tt{string}}(i,j) = \frac{\D \sum_{v \in \left(\mathcal{W}_i \cap  \mathcal{W}_j\right)} (w^1_v + w^2_v)}
    {\D \texttt{smoothing } + \sum_{v \in \mathcal{W}_i} w^1_v + \sum_{v' \in \mathcal{W}_j} w^2_{v'}},
\end{equation}
}where $\mathcal{W}_e$ is the set of words in the string representation of entity $e$ and \texttt{smoothing} is the scalar smoothing constant (we try different values in the experiments). Using unit weights and removing the smoothing term would recover the standard Jaccard coefficient between the two sets. As it operates on set of words, this measure is robust to word re-ordering, a frequently observed variation between strings representing the same entity in different knowledge bases. On the other hand, this measure is not robust to small typos or small changes of spelling of words. This problem could be addressed by using more involved string similarity measures such as \emph{approximate string matching}~\cite{chulman97wordMatching,stoilos05stringMetric}, which handles both word corruption as well as word reordering, though our current implementation only uses~\eqref{eq:string} for simplicity. We will explore the effect of different scoring functions in our experiments in Section~\ref{sec:parameters}.

\paragraph*{Property similarity measure} We recall that we assume that the user provided a partial matching between properties of both databases. This enables us to use them in a property similarity measure. In order to elegantly handle missing values of properties, varying number of property values present, etc., we also use a smoothed weighted Jaccard similarity measure between the sets of properties. The detailed formulation is given in Appendix~\ref{ap:property} for completeness, but we note that it can make use of a similarity measure between literals such a normalized distance on numbers (for dates, years etc.) or a string-edit distance on strings.

\subsubsection{Dynamic graph similarity measure}
We now introduce the part of the score function which enables \textsf{SiGMa} to build on previous decisions and exploit the relationship graph information. We need to determine $w_{ij,kl}$, the weight of the contribution of a neighboring matched pair $(k,l)$ for the score of the candidate pair $(i,j)$. The general idea of the graph score function is to count the number of compatible neighbors which are currently matched together for a pair of candidates (this is the $g_{ij}(y)$ contribution in~\eqref{eq:obj}). Going back at the example in Figure~\ref{fig:example}, there were three compatible matched pairs shown in the neighborhood of $i$ and $j$. We would like to normalize this count by dividing by the number of possible neighbors, and we would possibly want to weight each neighbor differently. We again use a smoothed weighted Jaccard measure to summarize this information, averaging the contribution from each $\KB$. This can be obtained by defining $w_{ij,kl}$ = $\gamma_{i} w_{ik} + \gamma_{j} w_{jl}$, where $\gamma_i$ and $\gamma_j$ are normalization factors specific to $i$ and $j$ in each database and $w_{ik}$ is the weight of the contribution of $k$ to $i$ in $\KB_1$ (and similarly for $w_{kl}$ in $\KB_2$). The graph contribution thus becomes:
\begin{equation} \label{eq:g_ij}
    g_{ij}(y) = \sum_{(k,l) \in \mathcal{N}_{ij}} y_{kl}  (\gamma_i w_{ik} + \gamma_j w_{jl}).
\end{equation}
So let $\mathcal{N}_{i}$ be the set of neighbors of entity $i$ in $\KB_1$, i.e. $\mathcal{N}_{i} \doteq \{ k : \exists r \textrm{ s.t. } (i,r,k) \in \mathcal{F}_{R1} \}$ (and similarly for $\mathcal{N}_j$). Then, remembering that $\sum_{k} y_{kl} \leq 1$ for a valid partial matching $y \in \mathcal{M}$, the following normalizations $\gamma_i$ and $\gamma_j$ will yield the average of two smoothed weighted Jaccard measures for $g_{ij}(y)$:
{\addtolength\abovedisplayskip{-3mm} %
\begin{equation} \label{eq:gamma_i}
    \gamma_i \doteq \frac{1}{2} \left(1 + \sum_{k \in \mathcal{N}_{i}} w_{ik} \right)^{-1} \gamma_j \doteq \frac{1}{2} \left(1 + \sum_{l \in \mathcal{N}_{j}} w_{jl} \right)^{-1}
\end{equation}
}We thus have $g_{ij}(y) \leq 1$ for $y \in \mathcal{M}$, keeping the contribution of each possible matched pair $(i,j)$ on the same scale in \texttt{obj} in~\eqref{eq:obj}.

The graph part of the score in~\eqref{eq:score} then takes the form:
\begin{equation} \label{eq:graphscore}
     \delta g_{ij}(y)  = \sum_{(k,l) \in \mathcal{N}_{ij}} y_{kl} \, ( \gamma_i w_{ik} + \gamma_j w_{jl} + \gamma_k w_{ki} + \gamma_l w_{lj}).
\end{equation}
The summation over the first two terms yields $g_{ij}(y)$ and so is bounded by $1$, but the summation over the last two terms could be greater than 1 in the case that $(i,j)$ is filling a `hole' in the graph (thus increasing the contribution of many neighbors $(k,l)$ in \texttt{obj} in~\eqref{eq:obj}). For example, suppose that $i$ has $n$ neighbors with degree 1 (i.e. they only have $i$ as neighbor); and the same thing for $j$, and that they are all matched pairwise --- Figure~\ref{fig:example} is an example of this with $n=3$ if we suppose that no other neighbors are present in the $\KB$. Suppose moreover that we use unit weights for $w_{ik}$ and $w_{jl}$. Then the normalization is $\gamma_k = 1/4$ for each $k \in \mathcal{N}_i$ (as they have degree 1); and similarly for $\gamma_l$. The contribution of the sum over the last two terms in~\eqref{eq:graphscore} is thus $n/2$ (whereas in this case $g_{ij}(y) = n/(n+1) \leq 1$).

\textbf{Neighbor weight $w_{ik}$.}
We finally need to specify the weight $w_{ik}$, which determines the strength of the contribution of the neighbor $k$ being correctly matched to the score of a suggested pair containing $i$. In our experiments, we consider both the constant weight $w_{ik} = 1$ and a weight $w_{ik}$ that varies inversely with the number of neighbors entity $k$ has where the relationship is of the same type as the one with entity $i$. The motivation for the latter is explained in Appendix~\ref{ap:weights}.

%
%
%
%
%
%
%
%
%

\section{Experiments} \label{sec:experiments}

\subsection{Setup} \label{ssec:setup}
We made a prototype implementation of \textsf{SiGMa} in Python\footnote{The code and datasets will be made available at \href{http://mlg.eng.cam.ac.uk/slacoste/sigma}{http://mlg.eng.cam.ac.uk/slacoste/sigma}.} and compared its performance on benchmark datasets as well as on large-scale knowledge bases. All experiments were run on a cluster node Hexacore Intel Xeon E5650 2.66GHz with 46GB of RAM running Linux. Each knowledge base is represented as two text files containing a list of triples of relationships-facts and property-facts. The input to \textsf{SiGMa} is a pair of such $\KB$s as well as a partial mapping between the relationships and properties of each $\KB$ which is used in the computation of the score in~\eqref{eq:score}, and the definition of \texttt{compatible-neighbors}~\eqref{eq:compatible}. The output of \textsf{SiGMa} is a list of matched pairs $(e_1,e_2)$ with their score information and the iteration number at which they were added to the solution. We evaluate the final alignment (after reaching the stopping threshold) by comparing it to ground truth using the standard metrics of precision, recall and F-measure on the number of \emph{entities} correctly matched.\footnote{Recall is defined in our setup as the number of correctly matched entities in $\KB_1$ divided by the number of entities with ground truth information in $\KB_1$. We note that recall is upper bounded by precision because our alignment is a 1-1 function.} The benchmark datasets are available together with corresponding ground truth data; for the large-scale knowledge bases, we built their ground truth using web url information as described in Section~\ref{sec:datasets}.

We found reasonable values for the parameters of \textsf{SiGMa} by
exploring its performance on the \ts{YAGO} to \ts{IMDb} pair (the
methodology is described in Section~\ref{sec:parameters}), and then
kept them fixed for all the other experimental comparisons
(Section~\ref{sec:exp1} and~\ref{sec:exp2}). This reflects the situation where one would like to apply \textsf{SiGMa} to a new dataset without ground truth or to minimize parameter adaptation. The standard parameters that we used in these experiments are given in Appendix~\ref{ap:params}.

\subsection{Datasets} \label{sec:datasets}

Our experiments were done both on several large-scale datasets and on
some standard benchmark datasets from the ontology alignment evaluation
initiative (OAEI) (Table~\ref{tab:all_stats}). We describe these datasets below.


\paragraph*{Large-scale datasets}
As mentioned throughout this paper so far, we used the dataset pair
\textsf{YAGO}-\textsf{IMDb}  as the main motivating example for
developing and testing \ts{SiGMa}. We also test \ts{SiGMa} on the pair
\ts{Freebase}-\textsf{IMDb}, for which we could obtain a sizable
ground truth. We describe here their construction. Both \textsf{YAGO}
and \ts{Freebase} are available as lists of triples from their respective
websites.\footnote{\textsf{YAGO2 core} was downloaded from:
  \href{http://www.mpi-inf.mpg.de/yago-naga/yago/downloads.html}{\small{http://www.mpi-inf.mpg.de/yago-naga/yago/downloads.html}}
  and \textsf{Freebase} from:
  \href{http://wiki.freebase.com/wiki/Data\_dumps}{\small{http://wiki.freebase.com/wiki/Data\_dumps}}.}
\textsf{IMDb}, on the other hand, is given as a list of text
files.\footnote{\href{http://www.imdb.com/interfaces\#plain}{http://www.imdb.com/interfaces\#plain}} There are
different files for different categories, e.g.: actors, producers,
etc. We use these categories to construct a list of triples containing
facts about movies and people. Because \ts{SiGMa} ignores
relationships and properties that are not matched between the $\KB$s,
we could reduce the size of \textsf{YAGO} and \ts{Freebase} by keeping
only those facts which had a 1-1 mapping with \ts{IMDb} as presented in Table~\ref{tab:largeKB}, and the entities appearing in these facts. To facilitate the comparison of \ts{SiGMa} with \ts{PARIS}, the authors of~\ts{PARIS} kindly provided us their own version of \ts{IMDb} that we will refer from now on as \textsf{IMDb\_PARIS} --- this version has actually a richer structure in terms of properties. We also kept in \textsf{YAGO} the relationships and properties which were aligned with those of \textsf{IMDb\_PARIS} (Table~\ref{tab:largeKB}). Table~\ref{tab:all_stats} presents the number of unique entities and relationship-facts included in the relevant reduced datasets. We constructed the ground truth for \textsf{YAGO}-\textsf{IMDb} by scraping the relevant Wikipedia pages of entities to extract their link to the corresponding \textsf{IMDb} page, which often appears in the `external links' section. We then obtained the entity name by scraping the corresponding \textsf{IMDb} page and matched it to our constructed database by using string matching (and some manual cleaning). We obtained 54K ground truth pairs this way. We used a similar process for \ts{Freebase}-\textsf{IMDb} by accessing the \textsf{IMDb} urls which were actually stored in the database. This yielded 293K pairs, probably one of the largest knowledge bases alignment ground truth sets to date.
\begin{table}
\begin{center}
\resizebox{1.0\columnwidth}{!}{
\begin{tabular}{@{}cccc@{}}
\hline
\textbf{YAGO} & \textbf{IMDb\_PARIS} & \textbf{IMDb}  &\textbf{Freebase}\\ \hline
 \multicolumn{4}{c}{\tt{Relations}} \\ \hline
actedIn & actedIn   & actedIn & actedIn\\
directed & directorOf  & directed & directed\\
produced & producerOf  & produced & produced\\
created & writerOf & composed & \\
wasBornIn & bornIn & &\\
diedIn & deceasedIn & &\\
capitalOf & locatedIn & &\\ \hline
 \multicolumn{4}{c}{\tt{Properties}} \\ \hline
hasLabel & hasLabel   & hasLabel & hasLabel\\
wasCreatedOnDate  &    & hasProductionYear & initialReleaseDate\\
wasBornOnDate & bornOn   &  & \\
diedOnDate & deceasedOn   &  & \\
hasGivenName  & firstName   &  & \\
hasFamilyName  & lastName   &  & \\
hasGender & gender   &  & \\
hasHeight  & hasHeight  &  & \\ \hline
\end{tabular}
}
\end{center}
\caption{Manually aligned movie related relationships and properties in large-scale $\KB$s.} \label{tab:largeKB}
\end{table}

\paragraph*{Benchmark datasets}

We also tested \ts{SiGMa} on three benchmark dataset pairs provided by
the ontology alignment evaluation initiative (OAEI), which allowed us
to compare the performance of \ts{SiGMa} to some previously published methods~\cite{li09RiMOM,hu11objectCoref}.
 From the OAEI 2009
 edition,\footnote{\href{http://oaei.ontologymatching.org/2009/instances/}{http://oaei.ontologymatching.org/2009/instances/}}
 we use the \textsf{Rexa}-\textsf{DBLP} instance matching benchmark
 from the domain of scientific publications.\footnote{We note that the
   smaller \ts{eprints} dataset also present in the benchmark was not
   suitable for 1-1 matchings as its ground truth had a large number
   of many-to-one matches.} \ts{Rexa} contains publications and
 authors as entities extracted from the search results of the \ts{Rexa} search
 server. \textsf{DBLP} is a version of the DBLP dataset
 listing publications from the computer science domain. The pair has
 one matched relationship, \texttt{author}, as well several matched
 properties such as \tt{year}, \tt{volume}, \tt{journal name},
 \tt{pages}, etc. Our goal was to align publications and authors. The other two datasets come from the Person-Restaurants (PR) task from the OAEI 2010 edition,\footnote{\href{http://oaei.ontologymatching.org/2010/im/index.html}{http://oaei.ontologymatching.org/2010/im/index.html}} containing data about people and restaurants. In particular, there are \textsf{person11}-\textsf{person12} pairs where the second entity is a copy of the first with one property field corrupted, and \textsf{restaurant1}-\textsf{restaurants2} pairs coming from two different online databases that were manually aligned. All datasets were downloaded from the corresponding OAEI webpages, with dataset sizes given in Table~\ref{tab:all_stats}.
%
\begin{table}
\centering
\resizebox{0.5\columnwidth}{!}{
\begin{tabular}{ccc}
\hline
\textbf{Dataset} & \textbf{\#facts} & \textbf{\#entities}\\ \hline
\textsf{YAGO}  & 442K   & 1.4M \\
\textsf{IMDb\_PARIS} & 20.9M  & 4.8M\\
\textsf{IMDb} & 9.3M  & 3.1M \\
\textsf{Freebase} & 1.5M & 474K  \\ \hline
\textsf{DBLP}  &  2.5M & 1.6M\\
\textsf{Rexa} & 12.6K & 14.7K\\
%
%
%
\textsf{person11} & 500 & 1000 \\
\textsf{person12} & 500 & 1000 \\
\textsf{restaurant1} & 113 & 339 \\
\textsf{restaurant2} & 752 & 2256 \\ \hline
\end{tabular}
}
\caption{Datasets statistics}\label{tab:all_stats}
\end{table}

\subsection{Exp. 1: Large-scale alignment} \label{sec:exp1}
In this experiment, we test the performance of \ts{SiGMa} on the three
pairs of large-scale $\KB$s and compare it with
\ts{PARIS}~\cite{suchanek12PARIS}, which is described in more details
in the related work Section~\ref{sec:related}. We also compare
\ts{SiGMa} and \ts{PARIS} with the simple baseline of doing the unambiguous exact string matching step described in Section~\ref{ssec:implementation} which is used to obtain an initial match $m_0$ (called \ts{Exact-string}). Table~\ref{tab:res_large} presents the results.
\begin{table}
\centering
\resizebox{\columnwidth}{!}{\begin{tabular}{l lrrrr c r}
\hline
\textbf{Dataset} & \textbf{System} & \textbf{Prec} & \textbf{Rec} & \textbf{F }& \textbf{GT size} & \textbf{\# pred.} & \textbf{Time} \\ \hline
\multirow{2}{*}{\ts{Freebase}-\ts{IMdb}} & \ts{SiGMa} & 99 & 95 & 97 & \multirow{2}{*}{255k} & 366k & 90 min\\
 & \ts{Exact-string} & 99 & 70 & 82 &   & 244k & 1 min\\ \hline
\multirow{2}{*}{\ts{YAGO}-\ts{IMDb}} & \ts{SiGMa} & 98 & 93 & 95 &  \multirow{2}{*}{54k} & 188k & 50 min\\
     & \ts{Exact-string}& 99 & 57 & 72 &   & 162k & 1 min \\ \hline
\multirow{3}{*}{\specialcell{l}{\ts{YAGO}-\ts{IMDb\_PARIS} \\ (new ground truth) } } & \ts{SiGMa} & 98 & 96 & 97 &  & 237k & 70 min\\
 & PARIS & 97 & 96 & 97 & 57k & 702k & 3100 min\\
         & \ts{Exact-string} & 99 & 56 & 72 &   & 202k & 1 min \\ \hline
\multirow{3}{*}{\specialcell{l}{\ts{YAGO}-\ts{IMDb\_PARIS} \\ (ground truth from~\protect\cite{suchanek12PARIS}) }} & \ts{SiGMa} & 98 & 84 & 91 &  & 237k & 70 min\\
 & PARIS & 94 & 90 & 92 & 11k & 702k &  3100 min\\
 & \ts{Exact-string} & 99 & 61 & 75 &  & 202k &  1 min \\ \hline
\end{tabular}
}
\caption{Exp. 1: Results (precision, recall, F-measure) on large-scale datasets for \ts{SiGMa} in comparison to a simple exact-matching phase on strings as well as \ts{PARIS}~\protect\cite{suchanek12PARIS}. \textnormal{The `GT Size' column gives the number entities with ground truth information. Time is total running time, including loading the dataset (quoted from~\protect\cite{suchanek12PARIS} for \ts{PARIS}).}}\label{tab:res_large}
\end{table}
Despite its simple greedy nature which never goes back to correct a mistake, \ts{SiGMa} obtains an impressive F-measure above 90\% for all datasets, significantly improving over the \ts{Exact-string} baseline. We tried running \ts{PARIS}~\cite{suchanek12PARIS} on a smaller subset of \ts{YAGO}-\ts{IMDb}, using the code available from its author's website. It did not complete its first iteration after a week of computation and so we halted it (we did not have the SSD drive which seems crucial to reasonable running times). The results for \ts{PARIS} in Table~\ref{tab:res_large} are thus computed using the prediction files provided to us by its authors on the \ts{YAGO}-\ts{IMDb\_PARIS} dataset. In order to better relate the \ts{YAGO}-\ts{IMDb\_PARIS} results with the \ts{YAGO}-\ts{IMDb} ones, we also constructed a larger ground truth reference on \ts{YAGO}-\ts{IMDb\_PARIS} by using the same process as described in Section~\ref{sec:datasets}. On both ground truth evaluations, \ts{SiGMa} obtains a similar F-measure as \ts{PARIS}, but in 50x less time. On the other hand, we note that \ts{PARIS} is solving the more general problem of instances and schema alignment, and was not provided any manual alignment between relationships. The large difference of recall between \ts{PARIS} and \ts{SiGMa} on the ground truth from~\cite{suchanek12PARIS} can be explained by the fact that more than a third of its entities had no neighbor; whereas the process used to construct the new larger ground truth included only entities participating in movie facts and thus having at least one neighbor. The recall of \ts{SiGMa} actually increases for entities with increasing number of neighbors (going from 68\% for entities in the ground truth from~\cite{suchanek12PARIS} with 0 neighbor to 97\% for entities with 5+ neighbors).

About 2\% of the predicted matched pairs from \ts{SiGMa} on \ts{YAGO}-\ts{IMDb} have no word in common and thus zero string similarity -- difficult pairs to match without any graph information. Examples of these pairs came from spelling variations of names, movie titles in different languages, foreign characters in names which are not handled uniformly or multiple titles for movies (such as the `Blood In, Blood Out' example of Figure~\ref{fig:example}).

\textbf{Error analysis.} Examining the few errors made by \ts{SiGMa}, we observed the following types of matching errors: 1) errors in the ground truth (either coming from the scraping scheme used; or from Wikipedia (\ts{YAGO}) which had incorrect information); 2) having multiple very similar entities (e.g. mistaking the `making of' of the movie vs. the movie itself); 3) pair of entities which shared exactly the same neighbors (e.g. two different movies with exactly the same actors) but without other discriminating information. Finally, we note that going through the predictions of \ts{SiGMa} that had a low property score revealed a significant number of errors in the databases (e.g. wildly inconsistent birth dates for people), indicating that \ts{SiGMa} could be used to highlight data inconsistencies between databases.
%
%

\subsection{Exp. 2: Benchmark comparisons} \label{sec:exp2}
In this experiment, we test the performance of \ts{SiGMa} on the three benchmark datasets and compare them with the best published results so far that we are aware of: \ts{PARIS}~\cite{suchanek12PARIS} for the Person-Restaurants datasets (which compared favorably over ObjectCoref~\cite{hu11objectCoref}); and \ts{RiMoM}~\cite{li09RiMOM} for \ts{Rexa-DBPL}. Table~\ref{tab:res_bench} presents the results. We also include the results for \ts{Exact-string} as a simple baseline as well as \ts{SiGMa-linear}, which is the \ts{SiGMa} algorithm without using the graph information at all\footnote{\ts{SiGMa-linear} is not using the graph score component ($\alpha$ is set to 0) and is only using the inverted index $\mathcal{S}_0$ to suggest candidates -- not the neighbors in $\mathcal{N}_{ij}$.}, to give an idea of how important the graph information is in these cases.

Interestingly, \ts{SiGMa} significantly improved the previous results without needing any parameter tweaking. The Person-Restaurants datasets did not have a rich relationship structure to exploit: each entity (a person or a restaurant) was linked to exactly one another in a 1-1 bipartite fashion (their address). This is perhaps why \ts{SiGMa-linear} is surprisingly able to \emph{perfectly} match both the Person and Restaurants datasets. Analyzing the errors made by \ts{SiGMa}, we noticed that they were due to a violation of the assumption that each entity is unique in each $\KB$: the same address is represented as different entities in \ts{Restaurant2}, and \ts{SiGMa} greedily matched the one which was not linked to another restaurant in \ts{Restaurant2}, thus reducing the graph score for the correct match. \ts{SiGMa-linear} couldn't suffer from this problem, and thus obtained a perfect matching.
%
%

The \ts{Rexa-DBLP} dataset has a more interesting relationship structure which is not just 1-1: papers have multiple authors and authors have written multiple papers, enabling the fire propagation algorithm to explore more possibilities. However, it appears that a purely string based algorithm can already do quite well on this dataset --- \ts{Exact-string} obtains a 89\% F-measure, already significantly improving the previously best published results (\ts{RiMOM} at 76\% F-measure). \ts{SiGMa-linear} improves this to 91\%, and finally using the graph structure helps to improve this to 94\%. This benchmark which has a medium size also highlights the nice scalability of \ts{SiGMa}: despite using the interpreted language Python, our implementation runs in less than 10 minutes on this dataset, which can be compared to \ts{RiMOM} taking 36 hours on a 8-core server in 2009.
%
\begin{table}
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{llrrrr}
\hline
\textbf{Dataset} & \textbf{System} & \textbf{Prec} & \textbf{Rec} & \textbf{F }& \textbf{GT size}\\ \hline
Person & \ts{SiGMa} & 100 & 100 & 100 & \multirow{2}{*}{500} \\
 & \ts{PARIS} & 100 & 100 & 100 & \\ \hline
Restaurant & \ts{SiGMa}-\ts{linear} & 100 & 100 & 100 & \multirow{4}{*}{89} \\ %
 & \ts{SiGMa} & 98 & 96 & 97 &  \\
 & \ts{PARIS} & 95 & 88 & 91 & \\
 & \ts{Exact-string} & 100 & 75 & 86 & \\ \hline
\ts{Rexa}-\ts{DBLP} & \ts{SiGMa} & 97 & 90 & 94 & \multirow{4}{*}{1464} \\
 & \ts{SiGMa-linear} & 96 & 86 & 91 & \\
 & \ts{Exact-string} & 98 & 81 & 89 & \\
 & \ts{RiMOM} & 80 & 72 & 76 & \\ \hline
%
%
%
%
\end{tabular}
}
\caption{Exp. 2: Results on the benchmark datasets for \ts{SiGMa}, compared with \ts{PARIS}~\protect\cite{suchanek12PARIS} and \ts{RiMOM}~\protect\cite{li09RiMOM}. \ts{SiGMa-linear} and \ts{Exact-string} are also included on the interesting datasets as further comparison points.}\label{tab:res_bench}
\end{table}


\subsection{Parameter experiments} \label{sec:parameters}
In this section, we explore the role of different configurations for \ts{SiGMa} on the \ts{YAGO}-\ts{IMDb} pair, as well as determine which parameters to use for the other experiments. We recall that \ts{SiGMa} with the final parameters (described in Appendix~\ref{ap:params}) yields a 95\% F-measure on this dataset (second section of Table~\ref{tab:res_large}). Experiments 5 and 6 which explore the optimal weighting schemes as well as the correct stopping threshold are described for completeness in Appendix~\ref{ap:param_exp}.

\subsubsection{Exp. 3: Score components}
In this experiment, we explore the importance of each part of the score function by running \ts{SiGMa} with some parts turned off (which can be done by setting the $\alpha$ and $\beta$ tradeoffs to 0 or 1). The resulting precision / recall curves are plotted in Figure~\ref{fig:test_scores}a. We can observe that turning off the static part of the score (string and property) has the biggest effect, decreasing the maximum F-measure from 95\% to about 80\% (to be contrasted with the 72\% F-measure for \ts{Exact-string} as shown in Table~\ref{tab:res_large}). By comparing \ts{SiGMa} with \textsf{SiGMa-linear}, we see that including the graph information moves the F-measure from a bit below 85\% to over 95\%, a significant gain, indicating that the graph structure is more important on this dataset than the OAEI benchmark datasets.
\begin{figure}
	\begin{center}
    \vspace*{-7mm}
    \includegraphics[width=\columnwidth]{figures/test_scores_newyago.pdf}
    \vspace*{-8mm}
    \end{center}
	\caption{\textbf{Exp. 3: Precision/Recall curves for \ts{SiGMa} on \ts{YAGO}-\ts{IMDb} with different scoring configurations.} \textnormal{The filled circles indicate the maximum F-measure position on each curve, with the corresponding diamond giving the F-measure value at this recall point.} \label{fig:test_scores} }
\end{figure}

\subsubsection{Exp. 4: Matching seed}
In this experiment, we tested how important the size of the matching seed $m_0$ is for the performance of \ts{SiGMa}. We report the following notable results. We ran \ts{SiGMa} with no exact seed matching at all: we initialized it with a random exact match pair and let it explore the graph greedily (with the inverted index still making suggestions). This obtained an even better score than the standard setup: 99\% of precision, 94\% recall and 96\% F-measure, demonstrating that \emph{a good initial seed is actually not needed for this setup}. If we do not use the inverted index but initialize \ts{SiGMa} with the top 5\% of the exact match sorted by their score in the context of the whole exact match, the performance drops a little, but \ts{SiGMa} is still able to explore a large part of the graph: it obtains 99\% / 87\% / 92\% of precision/recall/F-measure, illustrating the power of the graph information for this dataset.

%
%
%
%
%
%
%
%
%
%
%

\section{Related work} \label{sec:related}
We contrast here \ts{SiGMa} with the work already mentioned in Section~\ref{ssec:approaches} and provide further links. In the ontology matching literature, the only approach which was applied to datasets of the size that we considered in this paper is the recently proposed \ts{PARIS}~\cite{suchanek12PARIS}, which solves the more general problem of matching instances, relationships and classes. The \ts{PARIS} framework defines a normalized score between pairs of instances to match representing how likely they should be matched,\footnote{The authors call these `marginal probabilities' as they were motivated from probabilistic arguments, but these do not sum to one.} and which depends on the matching scores of their compatible neighbors. The final scores are obtained by first initializing (and fixing) the scores on pairs of literals, and then propagating the updates through the relationship graph using a fixed point iteration, yielding an analogous fire propagation of information as \ts{SiGMa}, though it works with soft [0-1]-valued assignment whereas \ts{SiGMa} works with hard \{0,1\}-valued ones. The authors handle the scalability issue of maintaining scores for all pairs by using a sparse representation with various pruning heuristics (in particular, keeping only the maximal assignment for each entity at each step, thus making the same 1-1 assumption that we did).  An advantage of \ts{PARIS} over \ts{SiGMa} is that it is able to include property values in its neighborhood graph (it uses soft-assignments between them) whereas \ts{SiGMa} only uses relationships given that a 1-1 matching of property values is not appropriate. We conjecture that this could explain the higher recall that \ts{PARIS} obtained on entities which had no relationship neighbors on the \ts{YAGO}-\ts{PARIS\_IMDB} dataset. On the other hand, \ts{PARIS} was limited to use a 0-1 similarity measure between property values for the large-scale experiments in~\cite{suchanek12PARIS}, as it is unclear how one could apply the same sparsity optimization in a scalable fashion with more involved similarity measures (such as the IDF one that \ts{SiGMa} is using). The use of a 0-1 similarity measure on strings could explain the lower performance of \ts{PARIS} on the Restaurants dataset in comparison to \ts{SiGMa}. We stress that \ts{SiGMa} is able in contrast to use sophisticated similarity measures in a scalable fashion, and had a 50x speed improvement over \ts{PARIS} on the large-scale datasets.

%
The \ts{SiGMa} algorithm is related to the collective entity resolution approach of Bhattacharya and Getoor~\cite{getoor07relational}, which proposed a greedy agglomerative clustering algorithm to cluster entities based on previous decisions. Their approach could handle constraints on the clustering, including a $1-1$ matching constraint in theory, though it was not implemented. A scalable solution for collective entity resolution was proposed recently in~\cite{restogi11largeEM}, by treating the sophisticated machine learning approaches to entity resolution as black boxes (see references therein), but running them on small neighborhoods and combining their output using a message-passing scheme. They do not consider exploiting a $1-1$ matching constraint though, as most entity resolution or record linkage work.

The idea to propagate information on a relationship graph has been used in several other approaches for ontology matching~\cite{hu05GMO,mao07network}, though none were scalable for the size of knowledge bases that we considered. An analogous `fire propagation' algorithm has been used to align social network graphs in~\cite{narayanan11deanonymization}, though with a very different objective function (they define weights in each graphs and want to align edges which has similar weights). The heuristic of propagating information on a relationship graph is related to a well-known heuristic for solving Constraint Satisfactions Problems known as constraint propagation~\cite{bessiere2006constraint}. Ehrig and Staab~\cite{ehrig04QOM} mentioned several heuristics to reduce the number of candidates to consider in ontology alignment, including a similar one to \texttt{compatible-neighbors}, though they tested their approach only on a few hundred instances. Finally, we mention that Peralta~\cite{peralta07movieMatching} aligned the movie database MovieLens to IMDb through a combination of steps of manual cleaning with some automation. \ts{SiGMa} could be considered as an alternative which does not require manual intervention apart specifying the score function to use.

%
%

%

%
%
%
%
%
%
%
%

\section{Conclusion}
We have presented \ts{SiGMa}, a simple and scalable algorithm for the alignment of large-scale knowledge bases. Despite making greedy decisions and never backtracking to correct decisions, \ts{SiGMa} obtained a higher F-measure than the previously best published results on the OAEI benchmark datasets, and matched the performance of the more involved algorithm~\ts{PARIS} while being 50x faster on large-scale knowledge bases of millions of entities. Our experiments indicate that \ts{SiGMa} can obtain good performance over a range of datasets with the same parameter setting. On the other hand, \ts{SiGMa} is easily extensible to more powerful scoring functions between entities, as long as they can be efficiently computed.

Some apparent limitations of \ts{SiGMa} are a) that it cannot correct previous mistakes and b) cannot handle alignments other than 1-1. Addressing these in a scalable fashion which preserves high accuracy are open questions for future work. We note though that the non-corrective nature of the algorithm didn't seem to be an issue in our experiments. Moreover, pre-processing each knowledge base with a de-duplication method can help make the 1-1 assumption more reasonable, which is a powerful feature to exploit in an alignment algorithm. Another interesting direction for future work would be to use machine learning methods to learn the parameters of more powerful scoring function. In particular, the `learning to rank' model seems suitable to learn a score function which would rank the correctly labeled matched pairs above the other ones. The current level of performance of \ts{SiGMa} already makes it suitable though as a powerful generic alignment tool for knowledge bases and hence takes us closer to the vision of Linked Open Data and the Semantic Web.

\textbf{Acknowledgments:} We thank Fabian Suchanek and Pierre Senellart for sharing their code and answering our questions about \ts{PARIS}. We thank Guillaume Obozinski for helpful discussions. This research was supported by a grant from Microsoft Research Ltd. and a Research in Paris fellowship.


%
%
%
%
%
%
%
%
%
%
%
%
%

%

%
%
%

%

\bibliographystyle{abbrv}
\small{\bibliography{sigma}}

\clearpage

\begin{appendix}

\section{Property similarity measure} \label{ap:property}
We describe here the property similarity measure used in our implementation. We use a smoothed weighted Jaccard similarity measure between the sets of properties defined as follows. Suppose that $e_1$ has properties $p_1, p_2, \ldots, p_{n_1}$ with respective literal values $v_1, v_2, \ldots, v_{n_1}$, and that $e_2$ has properties $q_1, q_2, \ldots, q_{n_2}$ with respective literal values $l_1, l_2, \ldots, l_{n_2}$. In analogy to the string similarity measure, we will also associate IDF weights to the possible property values $w^o_{p,v} \doteq \log_{10} \frac{N^o_p}{|E^o_{p,v}|}$ where $E^o_{p,v} \doteq \{e \in \mathcal{E}_o :$ $e$ has literal $v$  for property $p\}$ and $N^o_p$ is the total number of entities in knowledge base $o$ which have a value for property $p$. We then define the following property similarity measure:
\begin{equation} \label{eq:property}
\mathtt{prop}(i,j) = \frac{\D \sum_{(a,b) \in M_{12} } (w^1_{p_a,v_a} + w^2_{q_b,l_b}) \,\mathrm{Sim}_{p_a,q_b}(v_a,l_b)}
    {\D 2 + \sum_{a=1}^{n_1} w^1_{p_a,v_a} + \sum_{b=1}^{n_2} w^2_{q_b,l_b}}.
\end{equation}
where $M_{12}$ represents the property alignment: $M_{12} \doteq \{(a,b) : p_a \textrm{ is matched to } q_b\}$. $\mathrm{Sim}_{p_a,q_b}(v_a,l_b)$ is a $[0,1]$-valued similarity measure between literals; it could be a normalized distance on numbers (for dates, years, etc.), a string-edit distance on strings, etc. %

\section{Graph neighbor weight} \label{ap:weights}
We recall that the the graph weight $w_{ik}$ determines the strength of the contribution of the neighbor $k$ being correctly matched to the score of a suggested pair containing $i$. In our experiments, we consider both the constant weight $w_{ik} = 1$ and a weight $w_{ik}$ that varies inversely with the number of neighbors entity $k$ has where the relationship is of the same type as the one with entity $i$. To motivate the latter, we go back again to our running example of Figure~\ref{fig:example}, but switching the role of $i$ and $k$ as we need to look at the neighbors of $k$ -- this is illustrated in Figure~\ref{fig:graph_weight} and explained in its caption. In case there are multiple different relationships linking the same pair $i$ to $k$, we take the maximum of the weights over these (i.e. we pick the most informative information to weight it). So formally, we have:
 \begin{equation} \label{eq:w_ik}
    w_{ik} \doteq \max_{r \textrm{ s.t. } (i,r,k) \in \mathcal{F}_R} | \{i' : (i',r,k) \in \mathcal{F}_R | ^{-1}  .
\end{equation}

We also point out that the normalization of $g_{ij}(y)$ in~\eqref{eq:g_ij} is made over each $\KB$ independently, in contrast with the \texttt{string} and \texttt{prop} similarity measures~\eqref{eq:string} and~\eqref{eq:property} which are normalized in both $\KB$ jointly. The motivation for this is that the neighborhood size in \ts{YAGO} and \ts{IMDb} are overly asymmetric (there is much more information about each movie in \ts{IMDb}). The separate normalization means that as long as most of a neighborhood in \emph{one} $\KB$ is correctly aligned, the graph score will be high. The information about strings and properties is more symmetric in the $\KB$ pairs that we consider, so a joint normalization seems reasonable in this case.

\section{Quadratic assignment problem} \label{ap:qap}
The quadratic assignment problem is traditionally defined as finding a bijection between $R$ facilities and $R$ locations which \emph{minimizes} the expected cost of transport between the facilities. Given that facilities $i$ and $k$ are assigned to locations $j$ and $l$ respectively, the cost of transport between facility $i$ and $k$ is $w_{ij,kl} = n_{ik} c_{jl}$, where $n_{ik}$ is the expected number of units to ship between facilities $i$ and $k$, and $c_{jl}$ is the expected cost of shipment between locations $j$ and $l$ (depending on their distance). In its more general form~\cite{lawler63qap}, the coefficients can be negative, and so there is no major difference between minimizing and maximizing, and we see that our optimization problem~\eqref{eq:opt} is a special case of this.

\begin{figure}
	\begin{center}
		\includegraphics[width=0.5\columnwidth]{figures/graph_weight.pdf}
	\end{center}
    \vspace*{-10pt}
	\caption{ Graph weight illustration. \normalfont{The contribution of the movie match $y_{kl} = 1$ should be weighted more for the candidate match pairing the only director $i$ of $k$ with a director of movie $l$ ($w_{ik} = 1$) as compared to the candidate match pairing one of the many actors $i'$ of $k$ with an actor of the movie $l$ ($w_{i'k} = 1/2$ for two actors in movie $k$). This weighting scheme can also be thought of ensuring that the contribution of the match $(k,l)$ spreads uniformly amongst all its neighbors with one unit of influence per relationship type in each $\KB$ separately.}} \label{fig:graph_weight}
\end{figure}

\section{Parameters used for SiGMa} \label{ap:params}
We use $\alpha=1/3$ as the graph score tradeoff\footnote{This value of $\alpha$ has the nice theoretical justification that it gives twice much more weight to the linear term than the quadratic term, a standard weighting scheme given that the derivative of the quadratic yields the extra factor of two to compensate.}
in~\eqref{eq:score} and $\beta=0.25$ as the property score tradeoff in~\eqref{eq:static}. We set the string score \texttt{smoothing} term in~\eqref{eq:string} as the sum of the maximum possible word weights in each $\KB$ ($\log |\mathcal{E}_o|$). We use 0.25 as the score threshold for the stopping criterion (step 6 in the algorithm), and stop considering suggestions from the inverted index on strings when their score is below 0.75. We use as initial matching the unambiguous exact string comparison test as described in Section~\ref{sec:algorithm}. We use uniform weights $w_{ik} = 1$ for the matched neighbors contribution in the graph score~\eqref{eq:graphscore}.  We use a \texttt{Sim} measure on property values as used in~\eqref{eq:property} which depends on the type of property literals: for dates and numbers, we simply use $0$-$1$ similarity (1 when they are equal) with some processing --- e.g. for dates, we only consider the year; for secondary strings (i.e. strings for other properties than the main string representation of an entity), we use a weighted Jaccard measure on words as defined in~\eqref{eq:string} but with the IDF weights derived from the strings appearing in this property only.

\section{Additional parameter \\experiments} \label{ap:param_exp}
We provide here the additional parameter experiments which were skipped from the main text for brevity.

\subsection{Exp. 5: Weighting schemes, smoothing and tradeoffs}
In this experiment,  we explored the effect of the weighting scheme for the three different score components (string, property and graph) by trying two options per component, with precision / recall curves given in Figure~\ref{fig:test_weights}. For string and property components, we compared uniform weights vs. IDF weights. For the graph component, we compare uniform weights (which surprisingly got the best result) with the inverse number of neighbors weight proposed in~\eqref{eq:w_ik}. Overall, the effect for these variations was much smaller than the one for the score component experiment, with the biggest decrease of less than 1\% F-measure obtained by using uniform string weights instead of the IDF-scores. We also varied the 3 smoothing parameters (one for each score component) as well as the 2 tradeoff parameters linearly around their chosen values: the performance does not change much for changes of the order of 0.1-0.2 for the tradeoff, and 1.5 for the smoothing parameters (stay with 1\% range of F-measure).

\subsection{Exp. 6: Stopping threshold choice}
In this experiment, we studied whether the score information correlated with changes in the precision / recall information, in order to determine a possible stopping threshold. We overlay in Figure~\ref{fig:detailed_run} the precision / recall at each iteration of the algorithm (blue / red) with the score (in green) of the matched pair chosen at this iteration (as given by~\eqref{eq:score}). The vertical black dashed lines correspond to the iteration at which the score threshold of 0.35 and 0.25 are reached, respectively, which correlated with a drop of precision for the current predictions (black line with diamonds) and a leveling of the F-measure (curved dashed black line), respectively. We note that this correlation was also observed on all the other datasets, indicating that this threshold is robust to dataset variations.

\begin{figure}[!t]
	\begin{center}
    \includegraphics[width=\columnwidth]{figures/pr_rec_weights_newyago.pdf}
    \end{center}
	\caption{\textbf{Exp. 5: Precision/Recall curves for \ts{SiGMa} on \ts{YAGO}-\ts{IMDb} with different weighting configurations.} \textnormal{The filled circles indicate the maximum F-measure position on each curve, with the corresponding diamond giving the F-measure value at this recall point. Each curve is one of the 8 possibilities of having the weight `off' (set to unity) or `on', for the graph / property / string part of the score function. The legend indicates the difference between the reference setup (graph off / property on / string on) and the given curve.} \label{fig:test_weights} }
\end{figure}

\newpage

\begin{figure}[!t]
	\begin{center}
		\includegraphics[width=\columnwidth]{figures/detailed_run_newyago.pdf}
	\end{center}
	\caption{\textbf{Exp. 6: Precision/recall and score evolution for \ts{SiGMa} on the \ts{YAGO}-\ts{IMDb} dataset as a function of iterations (predictions).} \textnormal{The magenta line indicates the proportion out of the last 1k predictions for which we had ground truth information; the black line with diamonds indicate the precision for these 1k predictions. The score of the matching pair chosen at each iteration is shown in green; notice how the precision starts to drop when the score goes below 0.35 (first vertical black dashed line) and the F-measure starts to level when the score goes below 0.25 (second vertical dashed line). We note that the periodic increase of the score is explained by the fact that if compatible neighbors are matched, the graph score part~\eqref{eq:graphscore} of their neighbors can increase sufficiently to exceed the previous maximum score in the priority queue.
}} \label{fig:detailed_run}
\end{figure}

\mbox{   } %

\end{appendix}

\end{document} \appendix
\input{lemmas.tex}
\input{fixed_eps.tex}
\input{adaptive_eps.tex}
\input{trw_properties.tex}
\input{experiments_appendix.tex}
%
%
\input{approx_map.tex}
%
\bibliographystylesup{abbrvnat}
\small{
\bibliographysup{ref}
}
\section{Dataset of manipulated objects}
\label{app:dataset}

Table~\ref{tab:datasetstatistics} provides statistics for the dataset introduced in Section~\ref{sec:dataset} of the main paper.
For each object class we indicate associated action classes and the number of video clips for each action.
We also provide the list of states and the number of object tracklets with state annotations.
In total, we have around 20,000 annotated tracks which we use for the quantitative evaluation of state discovery.

\begin{table}[h!]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{@{}llll@{}}
\toprule
Objects    & \textbf{Actions} (\#clips)                     & \textit{States}                & \#Tracklets \\ \midrule
wheel      & \{\textbf{remove} (47),  \textbf{put} (46)\} & \textit{\{attached, detached\}} & 5447        \\
coffee cup & \{\textbf{fill} (57)\}                         & \textit{\{full, empty\}}        &  1819           \\
flower pot & \{\textbf{put plant} (27)\}                    & \textit{\{full, empty\}}        & 2463        \\
fridge     & \{\textbf{open} (234), \textbf{close} (191)\}   & \textit{\{open, closed\}}      & 7968        \\
oyster     & \{\textbf{open} (28)\}                              & \textit{\{open, closed\}}       & 1802        \\ \bottomrule
\end{tabular}
}
\caption{Statistics of our new dataset of manipulated objects \label{tab:datasetstatistics}}
\end{table}
  \begin{figure*}[t!]
  \begin{subfigure}{.44\textwidth}
    \raggedleft
    \includegraphics[width=\linewidth]{figs/DPgraph.pdf}
    \caption{Non-overlap data structure for tracklets}
    \label{fig:DPsub1}
  \end{subfigure}
  \hspace*{\fill}
  \begin{subfigure}{0.53\textwidth}
    \raggedright
    \includegraphics[width=\linewidth]{figs/costmatrixpathDP.pdf}
    \caption{Cost matrix $\tilde{C}_n$ for the dynamic program}
    \label{fig:DPsub2}
  \end{subfigure}
  \caption{\small
In \textbf{(a)}, we provide an illustration of a possible situation for the tracklets.
 $y_0$ and $y_f$ are two fictitious tracklets that encode the beginning and end of the video.
 Each tracklet is indexed based on its beginning time.
 The time overlap between tracklets is shown by the grey color.
 We specify for each tracklet its possible successors by the dotted red arrows (see main text).
 Finally an admissible labeling is illustrated by yellow tags where $y_1$ and $y_5$ have both been assigned to state~$1$ and $y_6$ to state~$2$.
In \textbf{(b)}, we give an illustration of our approach to solve~\eqref{eq:linearprogram} with a dynamic program.
We display the modified cost matrix $\tilde{C}_n$ (see main text).
A valid path has to go from the green dot ($y_0$) to the red dot ($y_f$).
The light yellow entries show part of the $C_n$ matrix that are inserted in $\tilde{C}_n$, whereas white entries encode the rows of $0$s that are inserted to impose the \textbf{at least one} ordering constraint.
The red arrows specify an example optimal path inside the matrix.
The red entries display the tracklets that have been assigned to state~$1$ ($y_1$ and $y_5$) or state $2$ ($y_6$) (equivalent to putting ones in the appropriate corresponding entries in $Y_n$).
Finally, the grey arrows display the possible valid transitions that can be made for the entries \emph{along the red path}, for clarity. We see for example that from $(2,y_1)$, there are $6$ possible transitions: two column choices from the two red arrows from $y_1$ in~(a) encoding the non-overlap constraint; and three row choices encoding the valid transition from ``state 1'' (corresponding to the choice ``state 1'', $0$ or ``state 2'' for the next tracklet) encoding the ``at least one'' ordering constraint.}
  \label{fig:DP}
\end{figure*}
  

\section{Dynamic program for the tracklets}
\label{app:dp}
The track constraints defined in Section~\ref{sec:statemodel} introduce new challenges compared to the previous related work~\cite{Alayrac15Unsupervised,Bojanowski14weakly,Joulin14efficient}.
Recall that there are three main components in the constraints.
First, we assume that only one object is manipulated at a given time. 
Thus \emph{at most one} tracklet can be assigned to a state at a given time.
This constraint is referred to as the \textbf{non-overlap constraint}.
Second, we have the \textbf{ordering constraint} that imposes that \emph{state~1} always happens \emph{before} \emph{state~2}.
The last constraint imposes that we have \textbf{at least one} tracklet labeled as \emph{state~1} and \emph{at least one} tracklet labeled as \emph{state~2}.
We need to be able to minimize linear functions over this set of constraints in order to use the Frank-Wolfe algorithm.
More precisely, as the constraints decompose over the different clips, we can solve independently for each clip $n$ the following linear problem:
\begin{align}
\label{eq:linearprogram}
\underset{\substack{Y_n\in\{0,1\}^{M_n\times 2}}}{\text{minimize}}  & \phantom{aa:}\text{Tr}(C_n^TY_n) \\
\text{ s.t. }   & \underbrace{Y_n \in \mathcal{Y}_n}_{\substack{\text{non-overlap + ordering}\\\text{+ at least one}}},   \nonumber
\end{align}
where $C_n\in\mathbb{R}^{M_n\times 2}$ is a cost matrix that typically comes from the computation of the gradient of the cost function at the current iterate.
In order to solve this problem, we use a dynamic program approach that we explain next.
Recall that we are given $M_n$ tracklets $(y_i)_{i=1}^{M_n}$ and our goal is to output the $Y_n$ matrix that assigns to each of these tracklets either state~$1$, state~$2$ or no state at all while respecting the constraints.
The whole method is illustrated in Figure~\ref{fig:DP} with a toy example.

\textbf{Non-overlap data structure for the tracklets.}
We first pre-process the tracklets to build an auxiliary data-structure that is used to enforce the non-overlap constraint between the tracklets, as illustrated in Figure~\ref{fig:DPsub1}.
First, we sort and index each tracklet by their beginning time, and add two fictitious tracklets: $y_0$ as the starting tracklet and $y_f$ as the ending tracklet.
These two tracklets are used to start and terminate the dynamic program.
If all the tracklets were sequentially ordered without any overlap in time, then we could simply make a decision for each of them sequentially as was done in previous work on action localization for example (one decision per time step)~\cite{Bojanowski14weakly}.
To enforce the non-overlap constraint, we force the decision process to choose \emph{only one} possible successor among the group of overlapping valid immediate successors of a tracklet. For each tracklet $y_i$, we thus define its (smallest) set of ``\emph{valid successors}'' as the earliest tracklet $y_j$\footnote{Earliest means the smallest $j$.} after $y_i$ that is also non-overlapping with $y_i$, as well as any other tracklet $y_l$ for $l > j$ that is overlapping with $y_j$ (thus giving the earliest valid group of overlapping tracklets). The valid successors are illustrated by red dotted arrows in Figure~\ref{fig:DPsub1}. For example, the valid successors of~$y_1$ are $y_3$ (the earliest one that is non-overlapping) as well as $y_4$ (which overlaps with $y_3$ thus forming an overlapping group). Skipping a tracklet in this decision process means that we assign it to zero (which trivially always satisfies the non-overlapping constraint); whereas once we choose a tracklet to potentially assign it to state 1 or 2, we cannot visit any overlapping tracklet by construction of the valid successors, thus maintaining the non-overlap constraint.

\textbf{Dynamic program.}
The dynamic programming approach is used when we can solve a large problem by solving a sequence of inclusive subproblems that are linked by a simple recursive formula and that use overlapping solutions (which can be stored in a table for efficiency).
In terms of implementation, \cite{Bojanowski14weakly} encoded their dynamic program as finding an optimal path inside a cost matrix.
This approach is particularly suited when the update cost rule depends \emph{only} on the arrival entry in the cost matrix as opposed to be transition dependent.
As we will show below, we can encode the solution to our problem in a way that satisfies this property.
We therefore use the framework of~\cite{Bojanowski14weakly} by casting our problem as a search for an optimal path inside a cost matrix $\tilde{C}_n$ illustrated in Figure~\ref{fig:DPsub2}, and where the valid transitions encode the possible constraints.

One main difference with~\cite{Bojanowski14weakly} is that we have to deal with the challenging \textbf{at least one} constraint in the context of ordered labels.
To do so, we can filter further the set of valid decisions by using ``memory states'' that encode in which of the following three situations we are: \textbf{(i)} that state~$1$ has not yet been visited, \textbf{(ii)} that state~$1$ has already been visited, but state~$2$ has not yet been visited (and thus that we can either come back to state~$1$ or go to state~$2$) and \textbf{(iii)} that both states have been visited.
These memory states can be encoded by interleaving complete rows of $0$s in between columns of $C_n$ stored as rows, to obtain the $5 \times M_n$ matrix~$\tilde{C}_n$.
These new rows encode the three different memory states previously described when making a prediction of $0$ for a specific tracklet, and we enforce the correct memory semantic by only allowing a path to move to the same row or the row immediately below, except for state~$1$ which can also move directly to state~$2$ (two rows below), and the middle ``between state 1/2'' row, where one can go up one row additionally to state~$1$. Finally, the valid transitions between columns (tracklets) are given by the \emph{valid successors} data structure as given in Figure~\ref{fig:DPsub1} to encode the non-overlap constraints. Combining these two constraints (at least one ordering and non-overlap), we illustrate with grey arrows in Figure~\ref{fig:DPsub2} the possible transitions from the states along the path in red. To describe the dynamic program recursion below, we need to go the opposite direction from the successors, and thus we say that $y_j$ is a \emph{predecessor} of $y_i$ if and only if $y_i$ is a successor of $y_j$.

To perform the dynamic program, we maintain a matrix $D_n$ of the same size as $\tilde{C}_n$ where
$D_n(k,i)$ contains the minimal valid path cost of going from $(1,y_0)$ to $(k,y_i)$ inside the cost matrix $\tilde{C}_n$.
To define the cost update recursion to compute $D_n(k,i)$, let $P(k,i)$ be the set of tuples $(l,j)$ for which it is possible to go from $(l,j)$ to $(k,y_i)$ according to the rules described above.  
The update rule is then as follows:
\begin{align}
\label{eq:updateruleDP}
D_n(k,i) = \min_{(l,j) \in P(k,i)} D_n(l,j)+\tilde{C}_n(k,y_i).
\end{align}
As we see here, the added cost depends \emph{only} on the arrival entry $\tilde{C}_n(k,y_i)$.
We can therefore use the approach of~\cite{Bojanowski14weakly} and only consider entry costs rather than edge costs.
Thanks to our indexing property (tracklets are sorted by the beginning time), we can update the dynamic program matrix by filling each column of $D_n$ one after the other.
Once this update is finished, we back-track to get the best path by starting from the ending track (predecessors of $y_f$) at the last row (to be sure that both states have been visited) that has the lowest score in the $D_n$ matrix.
The total complexity of this algorithm is of order $\mathcal{O}(M_n)$.

\section{Failure cases}
\label{app:failcases}

\begin{figure}[t!]
	\centering
	\includegraphics[width=\linewidth]{figs/failmode.pdf}
	\caption{
		Typical failure cases for ``\textit{removing car wheel}'' (top) and
		`̀`\textit{fill coffee cup}'' (middle, bottom) actions. 
		Yellow indicates correct predictions; red indicates mistakes. Top: the removed wheel is incorrectly localized (right). 
		Middle: the ``empty cup'' is incorrectly
		localized (left). 
		Bottom: In this case, both object tracklets are annotated as ``\textit{ambiguous}'' in the ground truth as they occur during
		the pouring action and hence the predictions, while they appear
		reasonable, are deemed incorrect.
	}
	\vspace{-2mm} %
	\label{fig:failcases}
\end{figure}	

We observed two main types of failures, illustrated in Figure~\ref{fig:failcases}. 
The first one occurs when a false positive object detection consistently satisfies the hypothesis of our model in multiple videos (the top two rows in Figure~\ref{fig:failcases}).
The second typical failure mode is due to ambiguous labels (bottom row in Figure~\ref{fig:failcases}). 
This highlights the difficulty in
annotating ground truth for long actions such as ``\textit{pouring coffee}".\definecolor{color0}{rgb}{0.252663,0.332837,0.783665}
\definecolor{color1}{rgb}{0.275827,0.366717,0.812553}
\definecolor{color2}{rgb}{0.299441,0.400248,0.839842}
\definecolor{color3}{rgb}{0.323718,0.433158,0.864722}
\definecolor{color4}{rgb}{0.348323,0.465711,0.888346}
\definecolor{color5}{rgb}{0.373552,0.497499,0.909467}
\definecolor{color6}{rgb}{0.399231,0.528528,0.928459}
\definecolor{color7}{rgb}{0.425199,0.559058,0.946061}
\definecolor{color8}{rgb}{0.451739,0.588181,0.960201}
\definecolor{color9}{rgb}{0.478462,0.616564,0.972721}
\definecolor{color10}{rgb}{0.505423,0.643995,0.983157}
\definecolor{color11}{rgb}{0.532568,0.669801,0.990393}
\definecolor{color12}{rgb}{0.559747,0.694768,0.996075}
\definecolor{color13}{rgb}{0.586921,0.718121,0.998874}
\definecolor{color14}{rgb}{0.613933,0.739923,0.999142}
\definecolor{color15}{rgb}{0.640828,0.760752,0.997846}
\definecolor{color16}{rgb}{0.667253,0.779176,0.992959}
\definecolor{color17}{rgb}{0.693321,0.796314,0.986308}
\definecolor{color18}{rgb}{0.718985,0.811993,0.977656}
\definecolor{color19}{rgb}{0.743754,0.825125,0.965798}
\definecolor{color20}{rgb}{0.768034,0.837035,0.952488}
\definecolor{color21}{rgb}{0.791392,0.846750,0.936641}
\definecolor{color22}{rgb}{0.813693,0.854282,0.918480}
\definecolor{color23}{rgb}{0.835345,0.860514,0.898970}
\definecolor{color24}{rgb}{0.855378,0.863778,0.876587}
\definecolor{color25}{rgb}{0.875557,0.860242,0.851430}
\definecolor{color26}{rgb}{0.895882,0.849906,0.823499}
\definecolor{color27}{rgb}{0.912765,0.836682,0.794512}
\definecolor{color28}{rgb}{0.928116,0.822197,0.765141}
\definecolor{color29}{rgb}{0.940879,0.805596,0.735167}
\definecolor{color30}{rgb}{0.950956,0.786875,0.704761}
\definecolor{color31}{rgb}{0.959518,0.766973,0.674145}
\definecolor{color32}{rgb}{0.964835,0.744614,0.643239}
\definecolor{color33}{rgb}{0.968203,0.720844,0.612293}
\definecolor{color34}{rgb}{0.969851,0.695830,0.581312}
\definecolor{color35}{rgb}{0.968105,0.668475,0.550486}
\definecolor{color36}{rgb}{0.964911,0.640159,0.519806}
\definecolor{color37}{rgb}{0.959385,0.610306,0.489382}
\definecolor{color38}{rgb}{0.951254,0.578799,0.459408}
\definecolor{color39}{rgb}{0.941728,0.546413,0.429707}
\definecolor{color40}{rgb}{0.929357,0.512254,0.400673}
\definecolor{color41}{rgb}{0.915157,0.476927,0.372179}
\definecolor{color42}{rgb}{0.899534,0.440692,0.344107}
\definecolor{color43}{rgb}{0.880896,0.402331,0.317115}
\definecolor{color44}{rgb}{0.861054,0.362916,0.290628}
\definecolor{color45}{rgb}{0.839365,0.321856,0.264924}
\definecolor{color46}{rgb}{0.815508,0.277781,0.240294}
\definecolor{color47}{rgb}{0.790562,0.231397,0.216242}
\definecolor{color48}{rgb}{0.763520,0.178667,0.193396}
\definecolor{color49}{rgb}{0.735077,0.104460,0.171492}

\setlength{\fboxsep}{0mm}
\setlength{\tabcolsep}{2pt}
\renewcommand{\arraystretch}{1}

\begin{table*}[ht!]{
\begin{tabular}{p{\textwidth}}
	\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} we }}\hspace{-0.10cm}
	\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} 've }}\hspace{-0.10cm}
	\colorbox{color14}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} also }}\hspace{-0.10cm}
	\colorbox{color15}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} talked }}\hspace{-0.10cm}
	\colorbox{color15}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} the }}\hspace{-0.10cm}
	\colorbox{color16}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} wheel }}\hspace{-0.10cm}
	\colorbox{color16}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} on }}\hspace{-0.10cm}
	\colorbox{color15}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} the }}\hspace{-0.10cm}
	\colorbox{color14}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} opposite }}\hspace{-0.10cm}
	\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} side }}\hspace{-0.10cm}
	\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} of }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} the }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} flat }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} tire }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} so }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} we }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} 're }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} ready }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} to }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} start }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} changing }}\hspace{-0.10cm}
	\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} it }}\hspace{-0.10cm}
	\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} the }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} first }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} thing }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} we }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} 'll }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} do }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} is }}\hspace{-0.10cm}
	\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} jack }}\hspace{-0.10cm}
	\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} the }}\hspace{-0.10cm}
	\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} vehicle }}\hspace{-0.10cm}
	\colorbox{color8}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} up }}\hspace{-0.10cm}
	\colorbox{color8}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} you }}\hspace{-0.10cm}
	\colorbox{color7}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} have }}\hspace{-0.10cm}
	\colorbox{color7}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} to }}\hspace{-0.10cm}
	\colorbox{color7}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} loosen }}\hspace{-0.10cm}
	\colorbox{color7}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} the }}\hspace{-0.10cm}
	\colorbox{color7}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} jack }}\hspace{-0.10cm}
	\colorbox{color8}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} a }}\hspace{-0.10cm}
	\colorbox{color8}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} little }}\hspace{-0.10cm}
	\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} bit }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} in }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} order }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} to }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} get }}\hspace{-0.10cm}
	\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} the }}\hspace{-0.10cm}
	\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} handle }}\hspace{-0.10cm}
	\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} off }}\hspace{-0.10cm}
	\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} this }}\hspace{-0.10cm}
	\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} particular }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} model }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} has }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} a }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} lug }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} wrench }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} built }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} right }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} in }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} before }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} we }}\hspace{-0.10cm}
	\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} jack }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} they }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} would }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} want }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} to }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} loosen }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} the }}\hspace{-0.10cm}
	\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} lug }}\hspace{-0.10cm}
	\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} nuts }}\hspace{-0.10cm}
	\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} because }}\hspace{-0.10cm}
	\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} once }}\hspace{-0.10cm}
	\colorbox{color14}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} it }}\hspace{-0.10cm}
	\colorbox{color14}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} 's }}\hspace{-0.10cm}
	\colorbox{color14}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} jacked }}\hspace{-0.10cm}
	\colorbox{color14}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} up }}\hspace{-0.10cm}
	\colorbox{color14}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} it }}\hspace{-0.10cm}
	\colorbox{color14}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} 's }}\hspace{-0.10cm}
	\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} going }}\hspace{-0.10cm}
	\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} to }}\hspace{-0.10cm}
	\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} be }}\hspace{-0.10cm}
	\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} very }}\hspace{-0.10cm}
	\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} difficult }}\hspace{-0.10cm}
	\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} to }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} get }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} those }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} lug }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} nuts }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} off }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} this }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} particular }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} model }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} has }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} a }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} hubcap }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} which }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} we }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} 'll }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} just }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} loosen }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} those }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} up }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} does }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} n't }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} take }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} much }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} because }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} they }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} 're }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} made }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} out }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} of }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} plastic }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} that }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} the }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} hubcap }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} out }}\hspace{-0.10cm}
	\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} of }}\hspace{-0.10cm}
	\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} the }}\hspace{-0.10cm}
	\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} way }}\hspace{-0.10cm}
	\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} i }}\hspace{-0.10cm}
	\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} have }}\hspace{-0.10cm}
	\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} access }}\hspace{-0.10cm}
	\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} to }}\hspace{-0.10cm}
	\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} the }}\hspace{-0.10cm}
	\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} lug }}\hspace{-0.10cm}
	\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} nuts }}\hspace{-0.10cm}
	\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} loosen }}\hspace{-0.10cm}
	\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} the }}\hspace{-0.10cm}
	\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} lug }}\hspace{-0.10cm}
	\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} nuts }}\hspace{-0.10cm}
	\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} position }}\hspace{-0.10cm}
	\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} yourself }}\hspace{-0.10cm}
	\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} firmly }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} pressing }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} counterclockwise }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} to }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} loosen }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} the }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} lug }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} nuts }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} not }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} to }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} lose }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} just }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} enough }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} to }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} crack }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} them }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} is }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} a }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} little }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} stubborn }}\hspace{-0.10cm}
	\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} position }}\hspace{-0.10cm}
	\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} yourself }}\hspace{-0.10cm}
	\colorbox{color8}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} over }}\hspace{-0.10cm}
	\colorbox{color8}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} using }}\hspace{-0.10cm}
	\colorbox{color7}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} your }}\hspace{-0.10cm}
	\colorbox{color7}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} knee }}\hspace{-0.10cm}
	\colorbox{color7}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} or }}\hspace{-0.10cm}
	\colorbox{color7}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} your }}\hspace{-0.10cm}
	\colorbox{color7}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} foot }}\hspace{-0.10cm}
	\colorbox{color7}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} you }}\hspace{-0.10cm}
	\colorbox{color8}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} can }}\hspace{-0.10cm}
	\colorbox{color8}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} gain }}\hspace{-0.10cm}
	\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} leverage }}\hspace{-0.10cm}
	\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} now }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} we }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} 're }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} ready }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} to }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} jack }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} it }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} up }}\hspace{-0.10cm}
	\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} this }}\hspace{-0.10cm}
	\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} is }}\hspace{-0.10cm}
	\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} the }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} common }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} scissors }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} jack }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} the }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} screen }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} moves }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} in }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} and }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} out }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} allowing }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} the }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} mac }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} mechanism }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} to }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} move }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} up }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} and }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} down }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} and }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} lift }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} your }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} car }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} so }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} as }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} you }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} turn }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} it }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} to }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} the }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} right }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} it }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} will }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} go }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} up }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} as }}\hspace{-0.10cm}
	\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} you }}\hspace{-0.10cm}
	\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} loosen }}\hspace{-0.10cm}
	\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} it }}\hspace{-0.10cm}
	\colorbox{color8}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} the }}\hspace{-0.10cm}
	\colorbox{color8}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} jack }}\hspace{-0.10cm}
	\colorbox{color8}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} will }}\hspace{-0.10cm}
	\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} collapse }}\hspace{-0.10cm}
	\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} allowing }}\hspace{-0.10cm}
	\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} the }}\hspace{-0.10cm}
	\colorbox{color8}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} car }}\hspace{-0.10cm}
	\colorbox{color8}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} to }}\hspace{-0.10cm}
	\colorbox{color8}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} come }}\hspace{-0.10cm}
	\colorbox{color8}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} down }}\hspace{-0.10cm}
	\colorbox{color8}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} now }}\hspace{-0.10cm}
	\colorbox{color8}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} will }}\hspace{-0.10cm}
	\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} position }}\hspace{-0.10cm}
	\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} the }}\hspace{-0.10cm}
	\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} jack }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} you }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} want }}\hspace{-0.10cm}
	\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} to }}\hspace{-0.10cm}
	\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} be }}\hspace{-0.10cm}
	\colorbox{color14}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} sure }}\hspace{-0.10cm}
	\colorbox{color14}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} to }}\hspace{-0.10cm}
	\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} get }}\hspace{-0.10cm}
	\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} the }}\hspace{-0.10cm}
	\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} jack }}\hspace{-0.10cm}
	\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} on }}\hspace{-0.10cm}
	\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} a }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} good }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} part }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} of }}\hspace{-0.10cm}
	\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} the }}\hspace{-0.10cm}
	\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} frame }}\hspace{-0.10cm}
	\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} your }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} owners }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} manual }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} is }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} a }}\hspace{-0.10cm}
	\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} good }}\hspace{-0.10cm}
	\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} place }}\hspace{-0.10cm}
	\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} to }}\hspace{-0.10cm}
	\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} find }}\hspace{-0.10cm}
	\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} where }}\hspace{-0.10cm}
	\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} did }}\hspace{-0.10cm}
	\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} properly }}\hspace{-0.10cm}
	\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} jack }}\hspace{-0.10cm}
	\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} the }}\hspace{-0.10cm}
	\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} vehicle }}\hspace{-0.10cm}
	\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} you }}\hspace{-0.10cm}
	\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} can }}\hspace{-0.10cm}
	\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} raise }}\hspace{-0.10cm}
	\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} it }}\hspace{-0.10cm}
	\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} up }}\hspace{-0.10cm}
	\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} by }}\hspace{-0.10cm}
	\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} hand }}\hspace{-0.10cm}
	\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} until }}\hspace{-0.10cm}
	\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} it }}\hspace{-0.10cm}
	\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} contacts }}\hspace{-0.10cm}
	\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} the }}\hspace{-0.10cm}
	\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} frame }}\hspace{-0.10cm}
	\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} it }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} is }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} in }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} good }}\hspace{-0.10cm}
	\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} position }}\hspace{-0.10cm}
	\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} we }}\hspace{-0.10cm}
	\colorbox{color8}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} use }}\hspace{-0.10cm}
	\colorbox{color8}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} the }}\hspace{-0.10cm}
	\colorbox{color8}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} jack }}\hspace{-0.10cm}
	\colorbox{color8}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} handle }}\hspace{-0.10cm}
	\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} to }}\hspace{-0.10cm}
	\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} raise }}\hspace{-0.10cm}
	\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} the }}\hspace{-0.10cm}
	\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} vehicle }}\hspace{-0.10cm}
	\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} insert }}\hspace{-0.10cm}
	\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} the }}\hspace{-0.10cm}
	\colorbox{color8}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} end }}\hspace{-0.10cm}
	\colorbox{color8}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} of }}\hspace{-0.10cm}
	\colorbox{color7}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} the }}\hspace{-0.10cm}
	\colorbox{color7}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} jack }}\hspace{-0.10cm}
	\colorbox{color7}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} handle }}\hspace{-0.10cm}
	\colorbox{color6}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} into }}\hspace{-0.10cm}
	\colorbox{color6}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} the }}\hspace{-0.10cm}
	\colorbox{color6}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} jack }}\hspace{-0.10cm}
	\colorbox{color7}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} using }}\hspace{-0.10cm}
	\colorbox{color8}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} it }}\hspace{-0.10cm}
	\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} as }}\hspace{-0.10cm}
	\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} leverage }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} it }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} will }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} help }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} make }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} the }}\hspace{-0.10cm}
	\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} car }}\hspace{-0.10cm}
	\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} go }}\hspace{-0.10cm}
	\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} up }}\hspace{-0.10cm}
	\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} easier }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} remember }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} turning }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} clockwise }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} to }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} go }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} up }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} and }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} counterclockwise }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} to }}\hspace{-0.10cm}
	\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} go }}\hspace{-0.10cm}
	\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} down }}\hspace{-0.10cm}
	\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} carefully }}\hspace{-0.10cm}
	\colorbox{color8}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} jack }}\hspace{-0.10cm}
	\colorbox{color8}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} the }}\hspace{-0.10cm}
	\colorbox{color8}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} car }}\hspace{-0.10cm}
	\colorbox{color8}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} up }}\hspace{-0.10cm}
	\colorbox{color8}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} never }}\hspace{-0.10cm}
	\colorbox{color8}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} stick }}\hspace{-0.10cm}
	\colorbox{color8}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} your }}\hspace{-0.10cm}
	\colorbox{color8}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} hands }}\hspace{-0.10cm}
	\colorbox{color8}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} or }}\hspace{-0.10cm}
	\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} your }}\hspace{-0.10cm}
	\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} legs }}\hspace{-0.10cm}
	\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} under }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} the }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} vehicle }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} as }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} the }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} car }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} can }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} fall }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} and }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} cause }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} damage }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} checking }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} can }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} take }}\hspace{-0.10cm}
	\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} a }}\hspace{-0.10cm}
	\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} while }}\hspace{-0.10cm}
	\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} so }}\hspace{-0.10cm}
	\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} be }}\hspace{-0.10cm}
	\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} patient }}\hspace{-0.10cm}
	\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} and }}\hspace{-0.10cm}
	\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} cautious }}\hspace{-0.10cm}
	\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} as }}\hspace{-0.10cm}
	\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} you }}\hspace{-0.10cm}
	\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} jack }}\hspace{-0.10cm}
	\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} ok }}\hspace{-0.10cm}
	\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} now }}\hspace{-0.10cm}
	\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} that }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} the }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} car }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} is }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} jacked }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} up }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} now }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} we }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} 'll }}\hspace{-0.10cm}
	\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} take }}\hspace{-0.10cm}
	\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} the }}\hspace{-0.10cm}
	\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} lug }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} nuts }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} loose }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} remember }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} we }}\hspace{-0.10cm}
	\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} already }}\hspace{-0.10cm}
	\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} pre }}\hspace{-0.10cm}
	\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} loosen }}\hspace{-0.10cm}
	\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} them }}\hspace{-0.10cm}
	\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} when }}\hspace{-0.10cm}
	\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} the }}\hspace{-0.10cm}
	\colorbox{color14}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} car }}\hspace{-0.10cm}
	\colorbox{color14}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} was }}\hspace{-0.10cm}
	\colorbox{color15}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} on }}\hspace{-0.10cm}
	\colorbox{color15}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} the }}\hspace{-0.10cm}
	\colorbox{color14}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} ground }}\hspace{-0.10cm}
	\colorbox{color14}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} making }}\hspace{-0.10cm}
	\colorbox{color14}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} it }}\hspace{-0.10cm}
	\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} easy }}\hspace{-0.10cm}
	\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} to }}\hspace{-0.10cm}
	\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} notice }}\hspace{-0.10cm}
	\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} will }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} come }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} right }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} off }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} now }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} trying }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} to }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} do }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} that }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} while }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} the }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} car }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} was }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} jacked }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} up }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} would }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} be }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} very }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} difficult }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} remember }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} to }}\hspace{-0.10cm}
	\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} keep }}\hspace{-0.10cm}
	\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} your }}\hspace{-0.10cm}
	\colorbox{color8}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} lug }}\hspace{-0.10cm}
	\colorbox{color8}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} nuts }}\hspace{-0.10cm}
	\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} in }}\hspace{-0.10cm}
	\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} close }}\hspace{-0.10cm}
	\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} hand }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} you }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} do }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} n't }}\hspace{-0.10cm}
	\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} want }}\hspace{-0.10cm}
	\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} to }}\hspace{-0.10cm}
	\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} roll }}\hspace{-0.10cm}
	\colorbox{color14}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} away }}\hspace{-0.10cm}
	\colorbox{color14}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} in }}\hspace{-0.10cm}
	\colorbox{color14}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} the }}\hspace{-0.10cm}
	\colorbox{color14}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} grass }}\hspace{-0.10cm}
	\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} because }}\hspace{-0.10cm}
	\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} it }}\hspace{-0.10cm}
	\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} 'll }}\hspace{-0.10cm}
	\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} be }}\hspace{-0.10cm}
	\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} hard }}\hspace{-0.10cm}
	\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} to }}\hspace{-0.10cm}
	\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} find }}\hspace{-0.10cm}
	\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} and }}\hspace{-0.10cm}
	\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} that }}\hspace{-0.10cm}
	\colorbox{color14}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} 's }}\hspace{-0.10cm}
	\colorbox{color14}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} what }}\hspace{-0.10cm}
	\colorbox{color15}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} holds }}\hspace{-0.10cm}
	\colorbox{color15}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} your }}\hspace{-0.10cm}
	\colorbox{color16}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} tire }}\hspace{-0.10cm}
	\colorbox{color16}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} on }}\hspace{-0.10cm}
	\colorbox{color15}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} again }}\hspace{-0.10cm}
	\colorbox{color15}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} we }}\hspace{-0.10cm}
	\colorbox{color14}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} 're }}\hspace{-0.10cm}
	\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} turning }}\hspace{-0.10cm}
	\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} them }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} counterclockwise }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} to }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} get }}\hspace{-0.10cm}
	\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} them }}\hspace{-0.10cm}
	\colorbox{color8}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} loose }}\hspace{-0.10cm}
	\colorbox{color7}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} or }}\hspace{-0.10cm}
	\colorbox{color7}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} to }}\hspace{-0.10cm}
	\colorbox{color6}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} the }}\hspace{-0.10cm}
	\colorbox{color6}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} left }}\hspace{-0.10cm}
	\colorbox{color5}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} we }}\hspace{-0.10cm}
	\colorbox{color4}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} 'll }}\hspace{-0.10cm}
	\colorbox{color4}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} remove }}\hspace{-0.10cm}
	\colorbox{color4}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} the }}\hspace{-0.10cm}
	\colorbox{color4}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} flat }}\hspace{-0.10cm}
	\colorbox{color5}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} tire }}\hspace{-0.10cm}
	\colorbox{color6}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} set }}\hspace{-0.10cm}
	\colorbox{color7}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} it }}\hspace{-0.10cm}
	\colorbox{color7}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} over }}\hspace{-0.10cm}
	\colorbox{color8}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} here }}\hspace{-0.10cm}
	\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} out }}\hspace{-0.10cm}
	\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} of }}\hspace{-0.10cm}
	\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} the }}\hspace{-0.10cm}
	\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} way }}\hspace{-0.10cm}
	\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} now }}\hspace{-0.10cm}
	\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} there }}\hspace{-0.10cm}
	\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} was }}\hspace{-0.10cm}
	\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} heavy }}\hspace{-0.10cm}
	\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} traffic }}\hspace{-0.10cm}
	\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} area }}\hspace{-0.10cm}
	\colorbox{color14}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} you }}\hspace{-0.10cm}
	\colorbox{color15}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} would }}\hspace{-0.10cm}
	\colorbox{color15}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} n't }}\hspace{-0.10cm}
	\colorbox{color16}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} want }}\hspace{-0.10cm}
	\colorbox{color17}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} to }}\hspace{-0.10cm}
	\colorbox{color18}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} leave }}\hspace{-0.10cm}
	\colorbox{color19}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} it }}\hspace{-0.10cm}
	\colorbox{color19}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} in }}\hspace{-0.10cm}
	\colorbox{color18}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} the }}\hspace{-0.10cm}
	\colorbox{color17}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} street }}\hspace{-0.10cm}
	\colorbox{color16}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} you }}\hspace{-0.10cm}
	\colorbox{color16}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} might }}\hspace{-0.10cm}
	\colorbox{color15}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} want }}\hspace{-0.10cm}
	\colorbox{color15}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} to }}\hspace{-0.10cm}
	\colorbox{color14}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} put }}\hspace{-0.10cm}
	\colorbox{color15}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} it }}\hspace{-0.10cm}
	\colorbox{color17}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} to }}\hspace{-0.10cm}
	\colorbox{color19}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} the }}\hspace{-0.10cm}
	\colorbox{color22}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} rear }}\hspace{-0.10cm}
	\colorbox{color25}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} of }}\hspace{-0.10cm}
	\colorbox{color28}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} the }}\hspace{-0.10cm}
	\colorbox{color31}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} car }}\hspace{-0.10cm}
	\colorbox{color34}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} grab }}\hspace{-0.10cm}
	\colorbox{color37}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} the }}\hspace{-0.10cm}
	\colorbox{color39}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} spare }}\hspace{-0.10cm}
	\colorbox{color38}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} tire }}\hspace{-0.10cm}
	\\[-0.2cm]
	\begin{center}
		{
			\fboxrule=6pt
			\fboxsep=0pt
			\fcolorbox{color49}{color49}{\includegraphics[width=0.180000\textwidth]{./figs/changing_tire_0043_005850.jpg}
				\includegraphics[width=0.180000\textwidth]{./figs/changing_tire_0043_005970.jpg}
				\includegraphics[width=0.180000\textwidth]{./figs/changing_tire_0043_006090.jpg}
				\includegraphics[width=0.180000\textwidth]{./figs/changing_tire_0043_006210.jpg}
				\includegraphics[width=0.180000\textwidth]{./figs/changing_tire_0043_006330.jpg}}
		}\end{center}
		\\
		\colorbox{color37}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} put }}\hspace{-0.10cm}
		\colorbox{color37}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} it }}\hspace{-0.10cm}
		\colorbox{color36}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} in }}\hspace{-0.10cm}
		\colorbox{color36}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} position }}\hspace{-0.10cm}
		\colorbox{color36}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} you }}\hspace{-0.10cm}
		\colorbox{color37}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} 're }}\hspace{-0.10cm}
		\colorbox{color39}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} going }}\hspace{-0.10cm}
		\colorbox{color40}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} to }}\hspace{-0.10cm}
		\colorbox{color42}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} center }}\hspace{-0.10cm}
		\colorbox{color46}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} the }}\hspace{-0.10cm}
		\colorbox{color48}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} spare }}\hspace{-0.10cm}
		\colorbox{color49}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} tire }}\hspace{-0.10cm}
		\colorbox{color48}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} on }}\hspace{-0.10cm}
		\colorbox{color46}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} the }}\hspace{-0.10cm}
		\colorbox{color43}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} wheel }}\hspace{-0.10cm}
		\colorbox{color39}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} studs }}\hspace{-0.10cm}
		\colorbox{color35}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} that }}\hspace{-0.10cm}
		\colorbox{color31}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} 's }}\hspace{-0.10cm}
		\colorbox{color27}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} what }}\hspace{-0.10cm}
		\colorbox{color22}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} the }}\hspace{-0.10cm}
		\colorbox{color19}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} lug }}\hspace{-0.10cm}
		\colorbox{color16}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} nuts }}\hspace{-0.10cm}
		\colorbox{color15}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} go }}\hspace{-0.10cm}
		\colorbox{color15}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} so }}\hspace{-0.10cm}
		\colorbox{color14}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} go }}\hspace{-0.10cm}
		\colorbox{color14}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} ahead }}\hspace{-0.10cm}
		\colorbox{color15}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} and }}\hspace{-0.10cm}
		\colorbox{color15}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} do }}\hspace{-0.10cm}
		\colorbox{color15}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} that }}\hspace{-0.10cm}
		\colorbox{color16}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} lining }}\hspace{-0.10cm}
		\colorbox{color16}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} it }}\hspace{-0.10cm}
		\colorbox{color16}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} up }}\hspace{-0.10cm}
		\colorbox{color17}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} you }}\hspace{-0.10cm}
		\colorbox{color17}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} see }}\hspace{-0.10cm}
		\colorbox{color17}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} it }}\hspace{-0.10cm}
		\colorbox{color17}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} lines }}\hspace{-0.10cm}
		\colorbox{color17}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} are }}\hspace{-0.10cm}
		\colorbox{color17}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} pretty }}\hspace{-0.10cm}
		\colorbox{color17}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} easy }}\hspace{-0.10cm}
		\colorbox{color17}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} then }}\hspace{-0.10cm}
		\colorbox{color17}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} install }}\hspace{-0.10cm}
		\colorbox{color16}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} your }}\hspace{-0.10cm}
		\colorbox{color15}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} lug }}\hspace{-0.10cm}
		\colorbox{color14}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} nuts }}\hspace{-0.10cm}
		\colorbox{color14}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} again }}\hspace{-0.10cm}
		\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} clockwise }}\hspace{-0.10cm}
		\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} is }}\hspace{-0.10cm}
		\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} tight }}\hspace{-0.10cm}
		\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} counterclockwise }}\hspace{-0.10cm}
		\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} is }}\hspace{-0.10cm}
		\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} loose }}\hspace{-0.10cm}
		\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} so }}\hspace{-0.10cm}
		\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} we }}\hspace{-0.10cm}
		\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} 'll }}\hspace{-0.10cm}
		\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} take }}\hspace{-0.10cm}
		\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} them }}\hspace{-0.10cm}
		\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} up }}\hspace{-0.10cm}
		\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} by }}\hspace{-0.10cm}
		\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} hand }}\hspace{-0.10cm}
		\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} as }}\hspace{-0.10cm}
		\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} far }}\hspace{-0.10cm}
		\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} as }}\hspace{-0.10cm}
		\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} they }}\hspace{-0.10cm}
		\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} 'll }}\hspace{-0.10cm}
		\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} go }}\hspace{-0.10cm}
		\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} once }}\hspace{-0.10cm}
		\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} the }}\hspace{-0.10cm}
		\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} tire }}\hspace{-0.10cm}
		\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} centered }}\hspace{-0.10cm}
		\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} lug }}\hspace{-0.10cm}
		\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} nuts }}\hspace{-0.10cm}
		\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} are }}\hspace{-0.10cm}
		\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} hand }}\hspace{-0.10cm}
		\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} tight }}\hspace{-0.10cm}
		\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} take }}\hspace{-0.10cm}
		\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} the }}\hspace{-0.10cm}
		\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} lug }}\hspace{-0.10cm}
		\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} wrench }}\hspace{-0.10cm}
		\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} again }}\hspace{-0.10cm}
		\colorbox{color8}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} turning }}\hspace{-0.10cm}
		\colorbox{color8}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} it }}\hspace{-0.10cm}
		\colorbox{color8}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} clockwise }}\hspace{-0.10cm}
		\colorbox{color8}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} to }}\hspace{-0.10cm}
		\colorbox{color7}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} tighten }}\hspace{-0.10cm}
		\colorbox{color8}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} the }}\hspace{-0.10cm}
		\colorbox{color8}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} lugs }}\hspace{-0.10cm}
		\colorbox{color8}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} to }}\hspace{-0.10cm}
		\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} their }}\hspace{-0.10cm}
		\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} firm }}\hspace{-0.10cm}
		\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} not }}\hspace{-0.10cm}
		\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} too }}\hspace{-0.10cm}
		\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} tight }}\hspace{-0.10cm}
		\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} because }}\hspace{-0.10cm}
		\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} remember }}\hspace{-0.10cm}
		\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} your }}\hspace{-0.10cm}
		\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} cars }}\hspace{-0.10cm}
		\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} up }}\hspace{-0.10cm}
		\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} on }}\hspace{-0.10cm}
		\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} a }}\hspace{-0.10cm}
		\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} jack }}\hspace{-0.10cm}
		\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} we }}\hspace{-0.10cm}
		\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} would }}\hspace{-0.10cm}
		\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} n't }}\hspace{-0.10cm}
		\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} want }}\hspace{-0.10cm}
		\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} it }}\hspace{-0.10cm}
		\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} falling }}\hspace{-0.10cm}
		\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} off }}\hspace{-0.10cm}
		\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} with }}\hspace{-0.10cm}
		\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} too }}\hspace{-0.10cm}
		\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} much }}\hspace{-0.10cm}
		\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} leverage }}\hspace{-0.10cm}
		\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} of }}\hspace{-0.10cm}
		\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} force }}\hspace{-0.10cm}
		\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} insert }}\hspace{-0.10cm}
		\colorbox{color8}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} the }}\hspace{-0.10cm}
		\colorbox{color8}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} jack }}\hspace{-0.10cm}
		\colorbox{color7}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} handle }}\hspace{-0.10cm}
		\colorbox{color7}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} back }}\hspace{-0.10cm}
		\colorbox{color7}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} into }}\hspace{-0.10cm}
		\colorbox{color7}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} the }}\hspace{-0.10cm}
		\colorbox{color7}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} jack }}\hspace{-0.10cm}
		\colorbox{color8}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} turning }}\hspace{-0.10cm}
		\colorbox{color8}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} it }}\hspace{-0.10cm}
		\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} counterclockwise }}\hspace{-0.10cm}
		\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} again }}\hspace{-0.10cm}
		\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} we }}\hspace{-0.10cm}
		\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} 're }}\hspace{-0.10cm}
		\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} going }}\hspace{-0.10cm}
		\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} to }}\hspace{-0.10cm}
		\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} lower }}\hspace{-0.10cm}
		\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} the }}\hspace{-0.10cm}
		\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} vehicle }}\hspace{-0.10cm}
		\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} again }}\hspace{-0.10cm}
		\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} this }}\hspace{-0.10cm}
		\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} will }}\hspace{-0.10cm}
		\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} take }}\hspace{-0.10cm}
		\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} some }}\hspace{-0.10cm}
		\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} time }}\hspace{-0.10cm}
		\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} but }}\hspace{-0.10cm}
		\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} going }}\hspace{-0.10cm}
		\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} down }}\hspace{-0.10cm}
		\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} a }}\hspace{-0.10cm}
		\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} lot }}\hspace{-0.10cm}
		\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} easier }}\hspace{-0.10cm}
		\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} than }}\hspace{-0.10cm}
		\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} going }}\hspace{-0.10cm}
		\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} up }}\hspace{-0.10cm}
		\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} it }}\hspace{-0.10cm}
		\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} seems }}\hspace{-0.10cm}
		\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} to }}\hspace{-0.10cm}
		\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} be }}\hspace{-0.10cm}
		\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} going }}\hspace{-0.10cm}
		\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} down }}\hspace{-0.10cm}
		\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} easy }}\hspace{-0.10cm}
		\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} you }}\hspace{-0.10cm}
		\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} can }}\hspace{-0.10cm}
		\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} just }}\hspace{-0.10cm}
		\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} do }}\hspace{-0.10cm}
		\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} it }}\hspace{-0.10cm}
		\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} like }}\hspace{-0.10cm}
		\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} this }}\hspace{-0.10cm}
		\colorbox{color14}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} by }}\hspace{-0.10cm}
		\colorbox{color14}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} doing }}\hspace{-0.10cm}
		\colorbox{color14}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} it }}\hspace{-0.10cm}
		\colorbox{color14}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} straight }}\hspace{-0.10cm}
		\colorbox{color14}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} instead }}\hspace{-0.10cm}
		\colorbox{color14}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} of }}\hspace{-0.10cm}
		\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} having }}\hspace{-0.10cm}
		\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} an }}\hspace{-0.10cm}
		\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} angle }}\hspace{-0.10cm}
		\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} but }}\hspace{-0.10cm}
		\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} if }}\hspace{-0.10cm}
		\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} it }}\hspace{-0.10cm}
		\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} 's }}\hspace{-0.10cm}
		\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} too }}\hspace{-0.10cm}
		\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} tough }}\hspace{-0.10cm}
		\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} you }}\hspace{-0.10cm}
		\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} can }}\hspace{-0.10cm}
		\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} angle }}\hspace{-0.10cm}
		\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} it }}\hspace{-0.10cm}
		\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} and }}\hspace{-0.10cm}
		\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} get }}\hspace{-0.10cm}
		\colorbox{color8}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} the }}\hspace{-0.10cm}
		\colorbox{color7}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} leverage }}\hspace{-0.10cm}
		\colorbox{color5}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} that }}\hspace{-0.10cm}
		\colorbox{color4}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} you }}\hspace{-0.10cm}
		\colorbox{color3}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} 'll }}\hspace{-0.10cm}
		\colorbox{color2}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} need }}\hspace{-0.10cm}
		\colorbox{color1}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} to }}\hspace{-0.10cm}
		\colorbox{color0}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} lower }}\hspace{-0.10cm}
		\colorbox{color0}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} the }}\hspace{-0.10cm}
		\colorbox{color0}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} jack }}\hspace{-0.10cm}
		\colorbox{color0}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} or }}\hspace{-0.10cm}
		\colorbox{color0}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} to }}\hspace{-0.10cm}
		\colorbox{color1}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} raise }}\hspace{-0.10cm}
		\colorbox{color1}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} the }}\hspace{-0.10cm}
		\colorbox{color2}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} jack }}\hspace{-0.10cm}
		\colorbox{color3}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} turn }}\hspace{-0.10cm}
		\colorbox{color4}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} the }}\hspace{-0.10cm}
		\colorbox{color5}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} jack }}\hspace{-0.10cm}
		\colorbox{color6}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} low }}\hspace{-0.10cm}
		\colorbox{color7}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} enough }}\hspace{-0.10cm}
		\colorbox{color8}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} you }}\hspace{-0.10cm}
		\colorbox{color9}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} can }}\hspace{-0.10cm}
		\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} do }}\hspace{-0.10cm}
		\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} it }}\hspace{-0.10cm}
		\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} by }}\hspace{-0.10cm}
		\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} hand }}\hspace{-0.10cm}
		\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} now }}\hspace{-0.10cm}
		\colorbox{color14}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} that }}\hspace{-0.10cm}
		\colorbox{color14}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} it }}\hspace{-0.10cm}
		\colorbox{color15}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} does }}\hspace{-0.10cm}
		\colorbox{color15}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} n't }}\hspace{-0.10cm}
		\colorbox{color15}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} have }}\hspace{-0.10cm}
		\colorbox{color15}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} contact }}\hspace{-0.10cm}
		\colorbox{color15}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} with }}\hspace{-0.10cm}
		\colorbox{color15}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} the }}\hspace{-0.10cm}
		\colorbox{color14}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} frame }}\hspace{-0.10cm}
		\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} and }}\hspace{-0.10cm}
		\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} we }}\hspace{-0.10cm}
		\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} can }}\hspace{-0.10cm}
		\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} remove }}\hspace{-0.10cm}
		\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} it }}\hspace{-0.10cm}
		\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} now }}\hspace{-0.10cm}
		\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} that }}\hspace{-0.10cm}
		\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} the }}\hspace{-0.10cm}
		\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} tires }}\hspace{-0.10cm}
		\colorbox{color14}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} on }}\hspace{-0.10cm}
		\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} the }}\hspace{-0.10cm}
		\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} ground }}\hspace{-0.10cm}
		\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} we }}\hspace{-0.10cm}
		\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} 'll }}\hspace{-0.10cm}
		\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} go }}\hspace{-0.10cm}
		\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} ahead }}\hspace{-0.10cm}
		\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} and }}\hspace{-0.10cm}
		\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} give }}\hspace{-0.10cm}
		\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} them }}\hspace{-0.10cm}
		\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} that }}\hspace{-0.10cm}
		\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} final }}\hspace{-0.10cm}
		\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} type }}\hspace{-0.10cm}
		\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} we }}\hspace{-0.10cm}
		\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} 're }}\hspace{-0.10cm}
		\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} going }}\hspace{-0.10cm}
		\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} to }}\hspace{-0.10cm}
		\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} start }}\hspace{-0.10cm}
		\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} down }}\hspace{-0.10cm}
		\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} here }}\hspace{-0.10cm}
		\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} then }}\hspace{-0.10cm}
		\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} move }}\hspace{-0.10cm}
		\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} to }}\hspace{-0.10cm}
		\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} the }}\hspace{-0.10cm}
		\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} top }}\hspace{-0.10cm}
		\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} one }}\hspace{-0.10cm}
		\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} then }}\hspace{-0.10cm}
		\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} back }}\hspace{-0.10cm}
		\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} down }}\hspace{-0.10cm}
		\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} over }}\hspace{-0.10cm}
		\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} and }}\hspace{-0.10cm}
		\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} back }}\hspace{-0.10cm}
		\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} again }}\hspace{-0.10cm}
		\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} a }}\hspace{-0.10cm}
		\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} little }}\hspace{-0.10cm}
		\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} star }}\hspace{-0.10cm}
		\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} pattern }}\hspace{-0.10cm}
		\colorbox{color14}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} making }}\hspace{-0.10cm}
		\colorbox{color15}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} sure }}\hspace{-0.10cm}
		\colorbox{color15}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} we }}\hspace{-0.10cm}
		\colorbox{color16}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} have }}\hspace{-0.10cm}
		\colorbox{color17}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} equal }}\hspace{-0.10cm}
		\colorbox{color17}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} torque }}\hspace{-0.10cm}
		\colorbox{color18}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} on }}\hspace{-0.10cm}
		\colorbox{color19}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} the }}\hspace{-0.10cm}
		\colorbox{color21}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} wheel }}\hspace{-0.10cm}
		\colorbox{color22}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} stud }}\hspace{-0.10cm}
		\colorbox{color23}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} ok }}\hspace{-0.10cm}
		\colorbox{color24}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} what }}\hspace{-0.10cm}
		\colorbox{color26}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} we }}\hspace{-0.10cm}
		\colorbox{color27}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} have }}\hspace{-0.10cm}
		\colorbox{color28}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} our }}\hspace{-0.10cm}
		\colorbox{color29}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} spare }}\hspace{-0.10cm}
		\colorbox{color29}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} tire }}\hspace{-0.10cm}
		\colorbox{color28}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} on }}\hspace{-0.10cm}
		\colorbox{color26}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} it }}\hspace{-0.10cm}
		\colorbox{color24}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} set }}\hspace{-0.10cm}
		\colorbox{color22}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} it }}\hspace{-0.10cm}
		\colorbox{color21}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} up }}\hspace{-0.10cm}
		\colorbox{color19}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} we }}\hspace{-0.10cm}
		\colorbox{color17}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} 're }}\hspace{-0.10cm}
		\colorbox{color15}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} ready }}\hspace{-0.10cm}
		\colorbox{color13}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} to }}\hspace{-0.10cm}
		\colorbox{color12}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} go }}\hspace{-0.10cm}
		\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} a }}\hspace{-0.10cm}
		\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} couple }}\hspace{-0.10cm}
		\colorbox{color11}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} things }}\hspace{-0.10cm}
		\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} to }}\hspace{-0.10cm}
		\colorbox{color10}{\mbox{\rule[-0.1cm]{0mm}{0.4cm} remember }}\hspace{-0.10cm}
		\\
	\end{tabular}
			\renewcommand{\tablename}{Figure}
			 \caption{\label{tab:svm_1} 
			Illustration of our text based clip retrieval approach for the action ``put wheel on a car''.
			We display the narration of the video that is obtained from Automatic Speech Recognition (ASR).
			The text is highlighted with different colors.
			These colors correspond to the score of the SVM that has been trained to detect sentences which refer to the action of interest.
			More precisely, for each word, we compute the average score over all the windows that contain it.
			Red indicates high score, blue indicates low score.
			The shown frames correspond to the top scoring part of the narration.
			We can see the coherence between what the person says (highlighted in red) and does in the video.
			Note the different challenges of the problem. %In this example, we can appreciate the different challenges of this problem.
			First, the input narration is very long.
			Second, the text directly comes from ASR, therefore it contains mistakes and does not have any punctuation.
			Third, several different expressions are similar and could refer to the action of interest.
			Despite all these challenges, our method is able to correctly retrieve the clip that contains the action of interest.}}
\end{table*}
\addtocounter{figure}{+1}
			
		\section{Joint cost rounding method}
\label{app:jcr}

Recall that we propose to use a convex relaxation approach in order to obtain a candidate solution of main problem~\eqref{eq:jointstateactionprob}.
Thus, we need to \emph{round} the relaxed solution afterward in order to get a valid integer solution.
We propose here a new rounding that is adapted to our joint problem.
We referred to this rounding as the \textbf{joint cost rounding} (see Section~\ref{sec:optim} of main paper).
This rounding is inspired by~\cite{Bojanowski15weakly,Alayrac15Unsupervised}.
They observe that using the learned $W^*$ classifier to round gives them better solutions, both in terms of objective value and performance.
We propose to use its natural extension for our joint model.
We first fix the classifiers for actions $W_a$ and for states $W_s$ to their relaxed solution $(W_a^*, W_s^*)$ and find, for each clip $n$, the couple $(Z_n,Y_n)$ that minimizes the joint cost~\eqref{eq:jcrrounding}.
To do so, we observe that we can enumerate all $T_n$ possibilities for $Z_n$, and solve for each of them the minimization of the joint cost with respect to $Y_n$.
The minimization with respect to $Y_n$ can be addressed as follows.
First, we observe that the distortion function~\eqref{eq:relaxeddist} is bilinear in $(Z_n,Y_n)$.
Let $Z_n$ be a $T_n \times 1$ vector, and let $\mathbf{1}_2$ be a vector of ones of length 2.
We can actually write: $d(Z_n,Y_n)=\text{Tr}(( B_n Z_n \mathbf{1}_2^\top)^\top Y_n)$ for some matrix $B_n\in\mathbb{R}^{M_n \times T_n}$.
Thus, when $Z_n$ is fixed, the joint term $d(Z_n,Y_n)$ is actually a simple \emph{linear} function of $Y_n$.
In addition, the quadratic term in $Y_n$ coming from~\eqref{eq:discr_state} is also \emph{linear} over the integer points (using the fact that $y^2=y$ for $y\in\{0,1\}$). 
Thus, when $Z_n$, $W_a^*$ and $W_s^*$ are fixed, the minimization over $Y_n$ is a linear program~\eqref{eq:linearprogram} that we solve using our dynamic program from the previous section.
The final algorithm is given in Algorithm~\ref{jcrAlgo}.
Its complexity is of order $\mathcal{O}(T_nM_n)$.

\definecolor{comment}{RGB}{86, 115, 154}
\begin{algorithm}[t!]
	\caption{Joint cost rounding for video $n$}
	\label{jcrAlgo}
	\begin{algorithmic}
		\State Get  $W_s^*$ and $W_a^*$ from the relaxed problem.
		\State Initialize $Z^*$, $Y^*$ and $\text{val}^*=+\infty$.
		\State {\color{comment} \footnotesize \# Loop over all possibilities for $Z_n$ (saliency)}
		\For{ $t$ in 1 : $T_n$}
			\State $Z$ $\gets$ \texttt{zeros}($T_n$, $1$) {\color{comment} \footnotesize \# Set the $t$-th entry of $Z$ to $1$}
			\State $Z_t$ $\gets$ $1$ 			
			\State {\color{comment} \footnotesize \# Definition of the cost matrix}
			\State $C_n$ $\gets$ $\frac{1}{2M}\left(\texttt{ones}(M_n, 2)-2X_sW_s\right)+\frac{\nu}{T} B_n Z \mathbf{1}_2^\top$ 
			\State {\color{comment} \footnotesize \# Dynamic program for the tracks}
			\State $Y_{\min}$ $\gets$ $\argmin_{Y \in \mathcal{Y}_n} \text{Tr}(C_n^TY)$ 
			\State {\color{comment} \footnotesize \# Cost computation}
			\State $\text{cost}_Z \gets \frac{1}{2T} \|Z - X_{a} W_{a}\|_F^2$
			\State $\text{cost}_Y \gets \frac{1}{2M} \|Y_{\min} - X_{s} W_{s}\|_F^2$
			\State $\text{cost}_{ZY} \gets \frac{\nu}{T}d(Z,Y_{\min})$
			\State {\color{comment} \footnotesize \# Update solution if better}
			\State $\text{val} \gets \text{cost}_Z + \text{cost}_Y + \text{cost}_{ZY}$
			\If{$\text{val} < \text{val}^*$} 
				\State {$Z^*$ $\gets$ $Z$} 
				\State {$Y^*$ $\gets$ $Y_{\min}$} 
				\State {$\text{val}^*$ $\gets$ $\text{val}$} 
			\EndIf
		\EndFor
		\State\Return $Z^*,Y^*$
	\end{algorithmic}
\end{algorithm}

\section{Supervised baselines for Action Localization}
\label{app:supervised}

\begin{table}[t!]
	\centering
	\resizebox{\linewidth}{!}{
		\begin{tabular}{@{}cccccccc|c@{}}
			\toprule
			\multirow{2}{*}{\textbf{Features}}   & put                       & remove                    & fill                    & open                       & fill                         & open                       & close                       & \multirow{2}{*}{\textbf{Average}} \\
			& \multicolumn{1}{l}{wheel} & \multicolumn{1}{l}{wheel} & \multicolumn{1}{l}{pot} & \multicolumn{1}{l}{oyster} & \multicolumn{1}{l}{coff.cup} & \multicolumn{1}{l}{fridge} & \multicolumn{1}{l|}{fridge} &                                   \\ \midrule
			\multicolumn{1}{l}{\textbf{(1)} CNN + HOF} & \textbf{0.65}                      & 0.68                      & 0.56                    & 0.11                       & 0.91                         & 0.54                       & 0.59                        & 0.58                              \\
			\multicolumn{1}{l}{\textbf{(2)} CNN + IDT}        & \textbf{0.65}                      & \textbf{0.72}                      & \textbf{0.56}                    & \textbf{0.21}                       & \textbf{0.93}                         & \textbf{0.6}                        & \textbf{0.62}                        & \textbf{0.61}                              \\ \bottomrule
		\end{tabular}
	}
	\caption{ \small \label{tab:supexp} Results of supervised baselines for action localization.}
\end{table}

We have run supervised baseline methods with state-of-the-art features. 
To be able to compare numbers with our experiment, we used a leave-one-out technique. 
For each action, we train a binary classifier with SVM on all videos except one. 
Similarly to our setting, we then select the top scoring time interval of the left alone test video. 
We repeat this process for all videos and report the metric used in our paper. 
For baseline \textbf{(1)}, we use the same features we are using in the main paper.
For baseline \textbf{(2)}, we complete our features with all channels of Improved Dense Trajectories (IDT)~\cite{Wang13action}.
Detailed results are given in Table~\ref{tab:supexp}.
We observe that we obtain results that are on par with our weakly supervised baselines (0.55 versus 0.58), therefore demonstrating the potential of using the information of object states for action localization.

\section{SVM training for clip retrieval}
\label{app:svm}

In Section~\ref{sec:exp_weak} we proposed an automatic method for retrieving video clips with manipulated objects.
This method makes use of narrations that come along with instructional videos. 
Narrations in the form of text are first obtained \emph{automatically} with Automatic Speech Recognition (ASR)\footnote{The ASR transcriptions are directly downloaded from YouTube.} and then processed as detailed below.

\textbf{Language dataset.}
For each manipulation action, we first find relevant positive and negative sentences on instruction websites such as Wikihow.
On average we obtain about 12 positive and 50 negative sentences per action.

\textbf{Language features.}
Off-the-shelf methods for text parsing typically fail in the absence of punctuation.
To process ASR output, which comes without punctuation, we propose to use the following simple but robust text representation. 
We represent every 10-word window of the narration by a TF-IDF vector based of uni-grams and bi-grams.
We use the same TF-IDF representation to encode text in our Language dataset on the level of sentences.

\textbf{SVM training.}
We train binary linear SVM classifiers to identify manipulation actions using the Language dataset for training and the regularization parameter $C=10$.
The obtained classifiers are then used to score every 10-word window of text narrations.
Video clips with the temporal correspondence to the top-scoring text narrations for each action are then retrieved. 
To deduce the temporal extent of the video clip given the top-scoring window, we trim 5 seconds before and 15 seconds after the corresponding timing.
This is to account for the fact that people are usually doing the action after speaking about it.
These are the clips we use for our evaluation in Section~\ref{sec:exp_weak} of the main paper.

\textbf{Illustration.} 
In Figure~\ref{tab:svm_1}, we provide an illustration of text based clip retrieval.
This visualization demonstrates the difficulty of the addressed problem (see caption for details).




\vspace{-2mm}
\section{Optimizing over Contractions of the Marginal Polytope}
\vspace{-3mm}

\textbf{Motivation}: 
We wish to (1) use the fewest possible MAP calls, and (2)
avoid regions near the boundary where the unbounded curvature of the function
slows down convergence.  
A viable option to address (1) is through the use of \emph{correction steps}, where after a Frank-Wolfe step, 
one optimizes
over the polytope defined by previously visited vertices of $\MARG$ (called the fully-corrective
Frank-Wolfe (FCFW) algorithm and proven to be linearly convergence for strongly convex objectives~\citep{lacoste2015MFW}). 
This does not require additional %
MAP calls. 
%
%
However, we found (see Sec.~\ref{sec:M_M_eps}) that when optimizing the TRW objective over $\MARG$, performing correction steps
can surprisingly \emph{hurt} performance.
%
%
%
%
This leaves us in a dilemma: correction steps 
%
enable decreasing the objective without additional MAP calls,
but they can also slow global progress since iterates after correction
sometimes lie close to the boundary of the polytope (where the FW directions become less informative). 
In a manner akin to barrier methods and to \citet{garber2013linearly}'s
local linear oracle, our proposed solution maintains the iterates within a contraction of the
polytope. This gives us most of the mileage obtained from performing the
correction steps \emph{without} suffering the consequences of venturing too close to the
boundary of the polytope. We prove a global convergence rate for the
iterates with respect to the true solution over the full polytope.
%

We describe convergent algorithms to optimize $\TRW$ for $\vmu \in \MARG$. 
%
The approach we adopt to deal with the issue of unbounded gradients at the boundary
is to perform Frank-Wolfe within a contraction of the marginal polytope
given by $\Meps$ for $\shrinkAmount \in [0,1]$, with either a fixed $\shrinkAmount$ or an adaptive~$\shrinkAmount$.
\begin{definition}[Contraction polytope]
	$\Meps := (1-\shrinkAmount) \MARG + \shrinkAmount\, \unif$,
        where $\unif\in \MARG$ is the vector representing the uniform
        distribution.
\end{definition}
\vspace{-1mm}
%
Marginal vectors that lie within $\Meps$ are bounded away from zero as all the components of $\unif$
are strictly positive. 
%
%
%
Denoting $\Vertices^{(\shrinkAmount)}$ as the
set of vertices of $\Meps$, $\Vertices$ as the set of vertices of $\MARG$ and $f(\vmu) := -\TRW$, the key insight
that enables our novel approach is that:
%
%
%
%
%
%
%
\small
\begin{equation*}
	\underbrace{\argmin_{\vv^{(\shrinkAmount)}\in \Vertices^{(\shrinkAmount)}}\innerProd{\nabla f}{\vv^{(\shrinkAmount)}}}_{\algComment{Linear Minimization over $\Meps$}} \equiv\argmin_{\vv \in \Vertices} \underbrace{\innerProd{\nabla f}{(1-\shrinkAmount)\vv + \shrinkAmount\unif}}_{\algComment{Definition of $\vv^{(\shrinkAmount)}$}} 
\equiv\underbrace{(1-\shrinkAmount) \argmin_{\vv \in \Vertices}\innerProd{\nabla f}{\vv} + \shrinkAmount\unif.}_{\algComment{Run MAP solver and shift vertex}} 
\end{equation*}
\normalsize
Therefore, to solve the FW subproblem~\eqref{eqn:lin_subproblem} over $\Meps$, we can run as usual a MAP solver  and simply shift the resulting vertex of $\MARG$ towards $\unif$ to obtain a vertex of $\Meps$. 
Our solution to optimize over restrictions of the polytope is more broadly applicable to the optimization problem defined below, with $f$ satisfying Prop.~\ref{prop:prop_bounded_grad} (satisfied by the TRW objective) in order to get convergence rates.
\begin{problem} 	
\label{prop:generic_fxn}
Solve $\min_{\x\in\DOM}f(\x)$ where $\DOM$ is a compact convex set and $f$ is convex and continuously differentiable on the relative interior of $\DOM$.
\end{problem}
%
%
%
%
%
%
%
%
\begin{properties}
\hyperref[sec:trw_bounded_lip]{(Controlled growth of Lipschitz constant over $\DOMeps$)}.
\label{prop:prop_bounded_grad}
We define $\DOMeps:= (1-\shrinkAmount)\DOM + \shrinkAmount\unif$ for a fixed $\unif$ in the relative interior of $\DOM$. We suppose that there exists a fixed $p \geq 0$ and $L$ such that for any $\shrinkAmount > 0$, $\nabla f(\x)$ has a bounded Lipschitz constant $L_{\shrinkAmount} \leq L\shrinkAmount^{-p}\,\,\,\forall \x\in\DOMeps$.
\end{properties}
%
%
%
%
%
%

\textbf{Fixed $\shrinkAmount$:}
The first algorithm fixes a value for $\shrinkAmount$ a-priori
and performs the optimization over $\DOMeps$. 
%
%
The following theorem bounds the 
sub-optimality of the iterates with respect to the optimum over $\DOM$. 

\begin{theorem}[Suboptimality bound for fixed-$\shrinkAmount$ algorithm]
	\label{thm:convergence_fixed_eps_main}
	Let $f$ satisfy the properties in Prob.~\ref{prop:generic_fxn} and Prop.~\ref{prop:prop_bounded_grad}, and suppose further that $f$ is finite on the boundary of $\DOM$. 
	Then the use of Frank-Wolfe for $\min_{\x\in\DOMeps} f(\x)$ realizes a sub-optimality over $\DOM$ bounded as:
	$$f(\xk)-f(\xopt)\leq
        \frac{2C_{\shrinkAmount}}{(k+2)}+\modContinuity \left( \shrinkAmount \diam(\DOM)\right),$$ 
        where $\xopt$ is the optimal
        solution in $\DOM$, $C_{\shrinkAmount} \leq 
        L_{\shrinkAmount} \diam_{||.||}(\DOMeps)^2 $, and $\modContinuity$
        is the modulus of continuity function of the (uniformly) continuous $f$ (in particular, $\modContinuity(\delta) \downarrow 0$ as $\delta \downarrow 0$).
\end{theorem}
%
The full proof is given in App.~\ref{sec:theory_fixed_eps}.
The first term of the bound
comes from the standard Frank-Wolfe convergence analysis of the
sub-optimality of $\xk$ relative to $\xopteps$, the optimum over $\DOMeps$, as in~\eqref{eqn:fw_theorem_convergence_original} and using Prop.~\ref{prop:prop_bounded_grad}.
The second term arises by bounding $f(\xopteps) - f(\x^*) \leq f(\tilde{\x}) - f(\x^*)$ with 
a cleverly chosen $\tilde{\x} \in \DOMeps$ (as $\xopteps$ is optimal in $\DOMeps$). We pick 
$\tilde{\x} := (1-\shrinkAmount) \x^* + \shrinkAmount \unif$ and note that $\|\tilde{\x} - \x^*\| \leq \shrinkAmount \diam(\DOM)$. As $f$ is continuous on a compact set, it is uniformly continuous 
and we thus have $f(\tilde{\x}) - f(\x^*) \leq \modContinuity (\shrinkAmount \diam(\DOM))$ with $\modContinuity$ its modulus of continuity function.

%
\textbf{Adaptive $\shrinkAmount$: }
The second variant to solve $\min_{\x\in\DOM} f(\x)$ iteratively perform FW steps over $\DOMeps$, but also decreases $\shrinkAmount$ adaptively.
The update schedule for $\shrinkAmount$
is given in Alg.~\ref{alg:adaptive_update} and is motivated by the convergence proof.
The idea is to ensure that the FW gap over $\DOMeps$ is always at least half the FW gap over $\DOM$,
relating the progress over $\DOMeps$ with the one over $\DOM$. 
It turns out that $\text{FW-gap-}\DOMeps = (1-\shrinkAmount) \text{FW-gap-}\DOM + \shrinkAmount\cdot\gunif$, where the ``uniform gap'' $\gunif$ quantifies the decrease of the function when contracting towards $\unif$.
%
When $\gunif$ is negative and large compared to 
the FW gap, we need to shrink $\shrinkAmount$ (see step~5 in~Alg.~\ref{alg:adaptive_update}) to
ensure that the $\shrinkAmount$-modified direction 
%
is a sufficient descent direction. 
%
%
%
\begin{algorithm}[t]
	\caption{Updates to $\shrinkAmount$ after a MAP call (Adaptive $\shrinkAmount$ variant)} 
	\label{alg:adaptive_update}
	\begin{algorithmic}[1]
	\STATE At iteration $k$. Assuming $\xk,\unif,\epsk{k-1},f$ are defined and $\sk$ has been computed
	\STATE Compute $\gk=\brangle{-\nabla f(\xk),\sk-\xk}$ \algComment{Compute FW gap}
	\STATE Compute $\gunif = \brangle{-\nabla f(\xk),\unif-\xk}$ \algComment{Compute ``uniform gap''}
		\IF{$\gunif<0$}
		\STATE Let $\tilde{\shrinkAmount} = \frac{\gk}{-4\gunif}$  \algComment{Compute new proposal for $\shrinkAmount$}
			\IF{$\tilde{\shrinkAmount}<\epsk{k-1}$}
				\STATE $\epsk{k} = \min\left(\tilde{\shrinkAmount},\frac{\epsk{k-1}}{2}\right)$ \algComment{Shrink by at least a factor of two if proposal is smaller}
			\ENDIF
		\ENDIF	\algComment{and set $\epsk{k} = \epsk{k-1}$ if it was not updated}
	\end{algorithmic}
\end{algorithm}
We can show that the algorithm converges to the global solution as follows:
\begin{theorem}[Global convergence for adaptive-$\shrinkAmount$ variant over $\DOM$]
	\label{thm:convergence_adaptive_eps_main}
	For a function $f$ satisfying the properties in Prob.~\ref{prop:generic_fxn} and Prop.~\ref{prop:prop_bounded_grad}, the sub-optimality of the iterates obtained by running the FW updates over $\DOMeps$ with $\shrinkAmount$ updated according to Alg.~\ref{alg:adaptive_update}
	is bounded as:
	%
	%
	%
	%
	%
	%
	%
	$$f(\xk)-f(\xopt)\leq O\left(k^{-\frac{1}{p+1}}\right).$$
\end{theorem}
A full proof with a precise rate and constants is given in App.~\ref{sec:theory_adaptive_eps}.
The sub-optimality $\hk := f(\xk)-f(\xopt)$ traverses three stages with an overall rate as above. 
%
%
The updates to $\shrinkAmount^{(k)}$ as in Alg.~\ref{alg:adaptive_update} 
enable us to (1) upper bound the duality gap over $\DOM$ as a function of the duality gap in $\DOMeps$ 
and (2) lower bound the value
of $\shrinkAmount^{(k)}$ as a function of $\hk$. Applying the standard Descent Lemma with the Lipschitz constant on the gradient of the form $L\shrinkAmount^{-p}$ (Prop.~\ref{prop:prop_bounded_grad}), and replacing $\shrinkAmount^{(k)}$ by its bound in $\hk$, we get the recurrence: $\hnext \leq \hk - C \hk^{p+2}$. Solving this gives us the desired bound.

%
\textbf{Application to the TRW Objective: }
$\min_{\vmu\in\MARG}-\TRW$ is akin to $\min_{\x\in\DOM}f(\x)$ and the
(strong) convexity of $-\TRW$ has been previously shown \citep{wainwright2005new,london_icml15}.
The gradient of the TRW objective is Lipschitz continuous over $\Meps$
since all marginals are strictly positive. Its growth for
Prop.~\ref{prop:prop_bounded_grad} can be bounded with $p=1$ as we
show in App.~\ref{sec:trw_bounded_lip}. This gives a rate of
convergence of $O(k^{-1/2})$ for the adaptive-$\shrinkAmount$ variant,
which interestingly is a typical rate for non-smooth convex optimization. The hidden constant is of the order~$O(\| \theta \| \cdot |V|)$. The modulus of continuity $\modContinuity$ for the TRW objective is close to linear (it is almost a Lipschitz function), and its constant is instead of the order $O(\| \theta \| + |V|)$.

%
%
%
%
%
%
%

%
%
%


\section{Properties of the TRW Objective \label{sec:trw_properties}}
In this section, we explicitly compute bounds for the constants appearing in the convergence statements for our fixed-$\shrinkAmount$ and adaptive-$\shrinkAmount$ algorithms for the optimization problem given by:
$$\min_{\vmu\in\MARG}-\TRW.$$ 
In particular, we compute the Lipschitz constant for its gradient over $\Meps$ (Property~\ref{prop:prop_bounded_grad}), we give a form for its modulus of continuity function $\omega(\cdot)$ (used in Theorem~\ref{thm:convergence_fixed_eps_main}), and we compute $B$, the upper bound on the negative uniform gap (as used in Lemma~\ref{prop:prop_bounded_gunif}).

\subsection{Property~\ref{prop:prop_bounded_grad} : Controlled Growth of Lipschitz Constant over $\Meps$} \label{sec:trw_bounded_lip}
%
%

We first motivate our choice of norm over $\MARG$. Recall that $\vmu$ can be decomposed into $|V| + |E|$ blocks, with one pseudo-marginal vector $\bmu_i \in \Delta_{\VAL_i}$ for each node $i \in V$, and one vector $\bmu_{ij} \in \Delta_{\VAL_i \VAL_j}$ per edge $\{i,j\} \in E$, where $\Delta_d$ is the probability simplex over $d$ values.
We let $c$ be the cliques in the graph (either nodes or edges). From its definition in~\eqref{eqn:trw_entropy_like_bethe}, $f(\vmu) := -\TRW$ decomposes as a separable sum of functions of each block only:
	\begin{equation} \label{eq:TRW_decompsed}
	f(\vmu) := -\TRW = -\sum_c \left(K_c H(\bmu_c) +\innerProd{\btheta_c}{\bmu_c}\right) =: \sum_c g_c(\bmu_c),
	\end{equation}
where $K_c$ is $(1- \sum_{j \in \nbrs{i}} \rho_{ij})$ if $c = i$ and $\rho_{ij}$ if $c = \{i,j\}$. The function $g_c$ also decomposes as a separable sum: 
\begin{equation} \label{eq:gc_function}
g_c(\bmu_c) := \sum_{x_c} K_c \mu_c(x_c) \log (\mu_c(x_c)) - \theta_c(x_c) \mu_c(x_c) =: \sum_{x_c} g_{c,x_c} (\mu_c(x_c)). 
\end{equation}
As $\MARG$ is included in a product of probability simplices, we will use the natural $\ell_\infty/\ell_1$ block-norm, i.e. $\| \vmu \|_{\infty,1} := \max_{c} \|\bmu_c \|_1$. The diameter of $\MARG$ in this norm is particularly small: $\diam_{\|\cdot \|_{\infty,1}}(\MARG) \leq 2$. The dual norm of the $\ell_\infty/\ell_1$ block-norm is the $\ell_1/\ell_\infty$ block-norm, which is what we will need to measure the Lipschitz constant of the gradient (because of the dual norm pairing requirement from the Descent Lemma~\ref{lem:descent_lemma}).


\begin{lemma}
	\label{lem:trw_bounded_lip_eps}
	Consider the $\ell_\infty/\ell_1$ norm on $\MARG$ and its dual norm $\ell_1/\ell_\infty$ to measure the gradient. Then
$\nabla \TRW$ is Lipschitz continuous over $\Meps$ with respect to these norms with Lipschitz constant 
$L_{\shrinkAmount} \leq \frac{L}{\shrinkAmount}$ with:
\begin{equation} \label{eq:TRW_L_constant} 
L \leq 4 |V| \,  \max_{ij \in E} \,\, (\VAL_i\VAL_j).
\end{equation}
\end{lemma}
\begin{proof}
	We first consider one scalar component of the separable $g_c(\bmu_c)$ function given in~\eqref{eq:gc_function} (i.e. for one $\mu_c(x_c)$ coordinate). Its derivative is $K_c (1 + \log (\mu_c(x_c)) - \theta_c(x_c)$ with second derivative $\frac{K_c}{ \mu_c(x_c)}$. If $\vmu \in \Meps$, then we have $\mu_c(x_c) \geq \shrinkAmount u_0(x_c) = \frac{\shrinkAmount}{n_c}$, where $n_c$ is the number of possible values that the assignment variable $x_c$ can take. Thus for $\vmu \in \Meps$, we have that the $x_c$-component of $g_c$ is Lipschitz continuous with constant $|K_c| n_c / \shrinkAmount$. We thus have:
	\begin{align*}
	\| \nabla g_c(\bmu_c) - \nabla g_c(\bmu'_c) \|_\infty &= \max_{x_c} \,\, | g'_{c, x_c} (\mu(x_c)) - g'_{c, x_c} (\mu'(x_c)) | \\
	&\leq \frac{|K_c| n_c}{\shrinkAmount} \|\bmu_c - \bmu_c' \|_\infty \leq \frac{|K_c| n_c}{\shrinkAmount} \|\bmu_c - \bmu_c' \|_1.
	\end{align*}
	Considering now the $\ell_1$-sum over blocks, we have:
	\begin{align*}
	\|\nabla f(\vmu) - \nabla f(\vmu') \|_{1,\infty} &= \sum_c \| \nabla g_c(\bmu_c) - \nabla g_c(\bmu'_c) \|_\infty  \\  
	&\leq \sum_c \frac{K_c n_c}{\shrinkAmount} \|\bmu_c - \bmu_c' \|_1 \leq \frac{1}{\shrinkAmount} \left(\sum_c K_c n_c \right) \| \vmu - \vmu' \|_{\infty, 1}.
	\end{align*}
	The Lipschitz constant is thus indeed $\frac{L}{\shrinkAmount}$ with $L := \sum_c |K_c| n_c$.
	Let us first consider the sum for $c \in V$; we have $K_i = 1 - \sum_{j \in \nbrs{i}} \rho_{ij}$. 
	Thus:
	\begin{align*}
	\sum_i |K_i| &\leq |V| + \sum_i \sum_{j \in \nbrs{i}} \rho_{ij} \\
				&= |V| + 2 \sum_{ij \in E} \rho_{ij} = |V| + 2 (|V| - 1) \leq 3 |V|. 	
	\end{align*}
	Here we used the fact that $\rho_{ij}$ came from the marginal probability of edges of spanning trees (and so with $|V|-1$ edges). Similarly, we have $\sum_{ij \in E} |K_{ij}| \leq |V|$. Combining these we get:
	\begin{align} \label{eq:Kc_bound}
	L = \sum_c |K_c| n_c \leq (\max_c n_c) \sum_c |K_c| \leq \max_{ij \in E} \VAL_i \VAL_j 4 |V|.
	\end{align}
\end{proof}

\begin{remark}
The important quantity in the convergence of Frank-Wolfe type algorithms is $\tilde{C} = L \diam(\MARG)^2$. We are free to take any dual norm pairs to compute this quantity, but some norms are better aligned with the problem than others. Our choice of norm in Lemma~\ref{lem:trw_bounded_lip_eps} gives $\tilde{C} \leq 16 |V| k^2$ where $k$ is the maximum number of possible values a random variable can take. It is interesting that $|E|$ does not appear in the constant. If instead we had used the $\ell_2/\ell_1$ block-norm on $\MARG$, we get that $\diam_{\ell_2/\ell_1}(\MARG)^2 = 4 (|V| + |E|)$, while the constant $L$ with dual norm $\ell_2/\ell_\infty$ would be instead $\max_c |K_c| n_c$ which is bigger than $\max_c n_c = k^2$, thus giving a worse bound. 
\end{remark}

\subsection{Modulus of Continuity Function} \label{sec:trw_weak_lip}
%
We begin by computing a modulus of continuity function for $-x \log x$ with an additive linear term.

\begin{lemma}
  \label{lem:entropy_lipschitz} Let $g(x) := -Kx\log x + \theta x$. Consider $x, x' \in [0,1]$ such that $|x-x'| \leq\sigma$, then:
  \begin{equation} \label{eqn:modulus_g}
  |g(x')-g(x)|\leq \sigma|\theta| + 2 \sigma |K| \max\{-\log(2\sigma),1\} =: \omega_g(\sigma).
  \end{equation}
\end{lemma}
\begin{proof}
	Without loss of generality assume $x'>x$, then we have two cases:

	\textbf{Case i.} 
	If $x>\sigma$, then we have that the Lipschitz constant of $g(x)$ is $L_{\sigma} = |\theta| + |K||(1+\log\sigma)|$ (obtained by taking 
	the supremum of its derivative). Therefore, we have that $|g(x')-g(x)|\leq L_{\sigma}\sigma$. Note that $L_\sigma\sigma\to 0$ when $\sigma\to 0$ even if
	$L_\sigma\to\infty$, since $L_\sigma$ grows logarithmically.

	\textbf{Case ii.}
	If $x\leq\sigma$, then $x'\leq x + \sigma \leq 2\sigma$. Therefore: 
	\begin{equation} \label{eqn:g_inequal}
	|g(x')-g(x)| \leq |K||x\log x-x'\log x'| + |\theta||x'-x|.
	\end{equation}
	Now, we have that $-x\log x$ is non-negative for $x\in [0,1]$. Furthermore, we have that 
	$-x\log x$ is increasing when $x<\exp(-1)$ and decreasing afterwards.
	First suppose that $2 \sigma \leq \exp(-1)$; then $-x'\log x' \geq -x\log x \geq 0$ which implies:
	$$
	|x\log x-x'\log x'| \leq -x' \log x' \leq - 2\sigma \log (2\sigma).
	$$
	In the case $2 \sigma > \exp(-1)$, then we have:
	$$ |x\log x-x'\log x'| \leq \max_{y \in [0,1]} \{-y \log y\} = \exp(-1) \leq 2 \sigma.
	$$
	Combining these two possibilities, we get:
	$$
	 |x\log x-x'\log x'| \leq 2\sigma \max\{- \log (2\sigma), 1\}.
	$$
	The inequality~\eqref{eqn:g_inequal} thus becomes:
	$$|g(x')-g(x)| \leq |K|2 \sigma  \max\{- \log (2\sigma), 1\} + |\theta|\sigma,$$
	which is what we wanted to prove.
\end{proof}
For small $\sigma$, the dominant term of the function $\omega_g(\sigma)$ in Lemma~\ref{lem:entropy_lipschitz} is of the 
form $C\cdot-\sigma\log\sigma$ for a constant~$C$. If we require that this be smaller than some small $\xi > 0$, then we can choose an approximate 
$\sigma$ by solving for $x$ in $-Ax\log x = \xi$ yielding $x = \exp( W_{-1} \frac{\xi}{A})$ where $W_{-1}$ is the negative 
branch of the Lambert W-function. This is almost linear and yields approximately $x = O(\xi)$ for small $\xi$. In fact, we have that $\omega_g(\sigma) \leq C' \sigma^\alpha$ for any $\alpha < 1$, and thus $g$ is ``almost'' Lipschitz continuous.

\begin{lemma}
  \label{lem:trw_obj_weak_lipschitz}
  The following function is a modulus of continuity function for the $\TRW$ objective over $\MARG$ with respect to the $\ell_\infty$ norm:
  \begin{equation}
   \omega(\sigma) := \sigma\| \theta \|_1 + 2 \sigma \tilde{K} \max\{-\log(2\sigma),1\},
   \end{equation}
   where $\tilde{K} := 4 |V| \max_{ij \in E} \VAL_i \VAL_j$.
   
   That is, for $\vmu, \vmu' \in \MARG$ with $\| \vmu' - \vmu \|_\infty \leq \sigma$, we have:
   $$ | \text{TRW}(\vmu;\vtheta,\vrho) - \text{TRW}(\vmu';\vtheta,\vrho) | \leq \omega(\sigma) .$$
\end{lemma}
\begin{proof}
%
%
$\TRW$ can be decomposed into 
functions of the form $-Kx\log x + \theta x$ (see~\eqref{eq:TRW_decompsed} and~\eqref{eq:gc_function}) and so we apply the Lemma~\ref{lem:entropy_lipschitz} element-wise. Let $c$ index the clique component in the marginal vector. 
\begin{align*}
	| \text{TRW}(\vmu;\vtheta,\vrho) - \text{TRW}(\vmu';\vtheta,\vrho) | &= 
	\sum_c \sum_{x_c} | g_{c,x_c}(\mu_c(x_c)) - g_{c,x_c}(\mu'_c(x_c))| \\
	%
	&\algComment{Using Lemma \ref{lem:entropy_lipschitz} and $\| \vmu' - \vmu \|_\infty \leq \sigma$}\\
	&\leq \sum_c \sum_{x_c} (|K_c|2 \sigma  \max\{- \log (2\sigma), 1\} + |\theta(x_c)|\sigma )\\
	&= 2\sigma  \max\{- \log (2\sigma), 1\} \sum_c |K_c|n_c +\|\theta\|_1\sigma,
\end{align*}
where we recall $n_c$ is the number of values that $x_c$ can take. By re-using the bound on $\sum_c |K_c| n_c$ from~\eqref{eq:Kc_bound}, we get the result.
\end{proof}


\subsection{Bounded Negative Uniform Gap} \label{sec:trw_bounded_unif}
%

%
%
%
%
%

\begin{lemma}[Bound for the negative uniform gap of TRW objective]
	\label{lem:bounded_gu0}
For the negative TRW objective $f(\vmu) := -\TRW$, the bound $B$ on the negative uniform gap as given in Lemma~\ref{prop:prop_bounded_gunif} for $\unif$ being the uniform distribution can be taken as:
\begin{equation} \label{eq:uniformBoundTRW}
	B =  2\sum_{c} \max_{x_c} |\theta_c(x_c)| =: 2 \|\vtheta \|_{1, \infty}
\end{equation}
\end{lemma}
\begin{proof}
	From Lemma~\ref{prop:prop_bounded_gunif}, we want to bound 
	$\| \nabla f(\unif) \|_* = \| \vtheta + \nabla_{\vmu} H(\unif;\vrho)) \|_*$. The clique entropy terms $H(\bmu_c)$ are maximized by the uniform distribution, and thus $\unif$ is a stationary point of the TRW entropy function with zero gradient. We can thus simply take $B = \|\vtheta\|_* \diam_{\| \cdot \|} (\MARG)$. By taking the $\ell_\infty / \ell_1$ norm on $\MARG$, we get a diameter of $2$, giving the given bound.
\end{proof}



\subsection{Summary}

%
%
We now give the details of suboptimality guarantees for our suggested algorithm to optimize $f(\vmu) := -\TRW$ over $\MARG$.
The (strong) convexity of the negative TRW objective is shown in \citep{wainwright2005new,london_icml15}. 
$\MARG$ is the convex hull of a finite number of vectors representing assignments to random variables and therefore a compact convex set. The entropy function is continously differentiable on the relative interior of the probability simplex, and thus the TRW objective has the same property on the relative interior of $\MARG$. Thus $-\TRW$ satisfies the properties laid out in Problem~\ref{prop:generic_fxn}.

%
%
%
%
%
%
%
%
%
%
%

\begin{lemma}[Suboptimality bound for optimizing $-\TRW$ with the fixed-$\shrinkAmount$ algorithm]
  \label{thm:convergence_fixed_eps_trw}
  For the optimization of $-\TRW$ over $\Meps$ with $\shrinkAmount\in(0,1]$, the suboptimality is bounded as: 
  
	\begin{equation} 
		\text{TRW}(\vmu^{*};\vtheta,\vrho) - \text{TRW}(\vmu^{(k)};\vtheta,\vrho) \leq \frac{2\mathcal{C}_{\shrinkAmount}}{(k+2)}+\modContinuity \left( 2\shrinkAmount \right), 
	\end{equation}
	 with $\vmu^*$ the optimizer of $\TRW$ in $\MARG$, 
	 where $C_{\shrinkAmount} \leq 16 \frac{|V|\max_{(ij)\in E}\VAL_i\VAL_j}{\shrinkAmount}$, 
	 and $\modContinuity(\sigma) = \sigma\|\vtheta\|_{1}+2\sigma \tilde{K}\max\{-\log (2\sigma),1\}$, where $\tilde{K} := 4 |V| \max_{ij \in E} \VAL_i \VAL_j$.
\end{lemma}
\begin{proof}
	Using $\diam_{\|\cdot \|_{\infty,1}}(\MARG) \leq 2$, and $L_\shrinkAmount$ from Lemma \ref{lem:trw_bounded_lip_eps},
	we can compute $C_{\shrinkAmount}\leq \diam(\MARG)^2L_{\shrinkAmount}$. 
	Lemma 
\ref{lem:trw_obj_weak_lipschitz} computes the modulus of continuity $\omega(\sigma)$.
The rate then follows directly from Theorem \ref{thm:convergence_fixed_eps}. 
\end{proof}



\begin{lemma}[Global convergence rate for optimizing $-\TRW$ with the adaptive-$\shrinkAmount$ algorithm]
	\label{thm:convergence_adaptive_eps_trw}
	Consider the optimization of $-\TRW$ over $\MARG$ with the optimum given by $\vmu^*$.
	The iterates~$\vmu^{(k)}$ obtained by running the Frank-Wolfe updates over $\Meps$ using line-search with $\shrinkAmount$ updated according to Algorithm~\ref{alg:adaptive_update} (or as summarized in a FCFW variant in Algorithm~\ref{alg:adaptive_eps}), have suboptimality~$h_k = \text{TRW}(\vmu^{*};\vtheta,\vrho) - \text{TRW}(\vmu^{(k)};\vtheta,\vrho)$ upper bounded as:
	\begin{enumerate}
		\item $\hk\leq \left(\frac{1}{2}\right)^{k}\h_{0} +\frac{\Cconst}{\shrinkAmount_0}$ for $k$ such that $\hk \geq \max\{ B\shrinkAmount_0, \frac{2\Cconst}{\shrinkAmount_0}\}$, \\
		\item $\hk\leq\frac{2\Cconst}{\shrinkAmount_0}\left[\frac{1}{\frac{1}{4} (k-k_0)+1}\right]$ for $k$ such that $\gunifBound\shrinkAmount_0\leq\hk\leq\frac{2\Cconst}{\shrinkAmount_0}$,\\
		\item $\hk\leq \left[\frac{\max(\Cconst,\gunifBound\shrinkAmount_0^{2})\gunifBound}{\frac{1}{4}(k-k_1)+1}\right]^{\frac{1}{2}} = O(k^{-\frac{1}{2}})$ for $k$ such that $\hk\leq\gunifBound\shrinkAmount_0$,\\
	\end{enumerate}
	where 

	\begin{itemize}

		\item $\shrinkAmount_0 = \epsk{\mathrm{init}} \leq \frac{1}{4}$ \\

		\item $\Cconst := 16|V|\max_{(ij)\in E}(\VAL_i\VAL_j) $\\
	
		\item $B = 16 \|\vtheta \|_{1, \infty}$
	
		\item $h_0$ is the initial suboptimality\\

		\item	$k_0$ and $k_1$ are the number of steps to reach stage~2 and~3 respectively which are bounded as: $k_0\leq \max(0,  \lceil\log_{\frac{1}{2}}\frac{\Cconst}{ h_0 \shrinkAmount_0}\rceil )$ 
		$k_0 \leq k_1\leq k_0 + \max\left(0,\lceil \frac{8\Cconst}{\gunifBound\shrinkAmount_0^{2}}\rceil -4\right)$\\
	\end{itemize}
\end{lemma}
\begin{proof}
	Using $\diam_{\|\cdot \|_{\infty,1}}(\MARG) \leq 2$,
	we bound $\Cconst\leq L\diam_{\|\cdot \|_{\infty,1}}(\MARG)^2$ with $L$ (from Property \ref{prop:prop_bounded_grad}) derived in Lemma \ref{lem:trw_bounded_lip_eps}. 
	We bound $-8g_u(\vmu^{(k)})$ (the upper bound on the negative uniform gap) using the value derived in Lemma \ref{lem:bounded_gu0}.
	The rate then follows directly from Theorem \ref{thm:convergence_adaptive_eps} using $p=1$ (see Lemma \ref{lem:trw_bounded_lip_eps} where $L_\shrinkAmount \leq \frac{L}{\delta}$).
\end{proof}
The dominant term in Lemma~\ref{thm:convergence_adaptive_eps_trw} is $\Cconst B \, k^{-\frac{1}{2}}$, with $\Cconst B  = O( \|\vtheta \|_{1, \infty} |V|)$.
We thus find that both bounds depend on norms of $\vtheta$. This is unsurprising since large potentials
drive the solution of the marginal inference problem away from the centre of $\MARG$, corresponding to regions of high entropy,
and towards the boundary of the polytope (lower entropy). Regions of low entropy correspond to smaller components
of the marginal vector, which in turn result in larger and poorly behaved gradients of $-\TRW$, which slows down the resulting optimization. 

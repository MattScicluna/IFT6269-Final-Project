%

\section{Memorization Algorithms}
\label{sect:memorization}

\subsection{Algorithms} 

\paragraph*{Variance Reduced SGD}

Given an optimization problem as in \eqref{eq:problem}, we investigate a class of stochastic gradient descent algorithms that generates an iterate sequence $w^t$ ($t \ge 0$) with updates taking the  form:
\begin{align}
w^+ = w - \gamma g_i(w),  \quad g_i(w)=  f'_i(w) - \bar \alpha_i \quad \text{with} \quad  \bar{\alpha}_i := \alpha_i - \bar{\alpha},
\label{eq:sgd-corrected}
\end{align}
where $\bar{\alpha} := \frac 1n \sum_{j=1}^n \alpha_j$.
Here $w$ is the current and $w^+$ the new parameter vector, $\gamma$ is the  step size,  and $i$ is an index selected uniformly at random. $\bar\alpha_i$ are variance correction terms such that  $\E \bar\alpha_i = 0$, which guarantees unbiasedness $\E g_i(w) =f'(w)$. The aim is to define updates of asymptotically vanishing variance, i.e.~$g_i(w) \to 0$ as $w \to w^*$, which  requires  $\bar\alpha_i \to f'_i(w^*)$. This implies that corrections need to be designed in a way to exactly cancel out the stochasticity of $f'_i(w^*)$ at the optimum. How the \emph{memory} $\alpha_j$ is updated distinguishes the different algorithms that we consider. 

\paragraph*{SAGA}

The SAGA algorithm \cite{defazio2014} maintains variance corrections $\alpha_i$  by memorizing stochastic gradients. The update rule is $\alpha^+_i  = f'_i(w)$ for the selected $i$, and $\alpha_j^+= \alpha_j$, for $j \neq i$. Note that these corrections will be used the next time the same index $i$ gets sampled.  Setting $\bar\alpha_i := \alpha_i - \bar \alpha$ guarantees unbiasedness. Obviously, $\bar \alpha$ can be updated incrementally. SAGA reuses the stochastic gradient $f_i'(w)$ computed at step $t$ to update $w$ as well as $\bar \alpha_i$.

\paragraph*{$q$-SAGA}
We  also consider $q$-SAGA, a method that updates $q \ge 1$ randomly chosen $\alpha_j$ variables at each iteration. This is a convenient reference point to investigate the advantages of ``fresher" corrections. Note that in SAGA the corrections will be on average $n$ iterations ``old". In $q$-SAGA this can be controlled to be $n/q$ at the expense of additional gradient computations.

\paragraph*{SVRG} 

We reformulate a variant of SVRG \cite{johnson2013} in our framework using a randomization argument similar to (but simpler than) the one suggested in \cite{konevcny2013}. Fix $q>0$ and draw in each  iteration $r \sim \text{Uniform}[0;1)$. If $ r< q/n$, a complete update, $\alpha_j^+  =  f_j'(w)$ ($\forall j$) is performed, otherwise they are left unchanged. While $q$-SAGA updates exactly $q$ variables in each iteration, SVRG occasionally updates all $\alpha$ variables by triggering an additional sweep through the data. There is an option to not maintain $\alpha$ variables explicitly and to save on space  by storing only $\bar \alpha = f'(w)$ and $w$.

\paragraph*{Uniform Memorization Algorithms} 

Motivated by SAGA and SVRG, we define a class of algorithms, which we call \textit{uniform memorization algorithms}. 
%
\begin{definition}
\label{def:memorization} 
A uniform $q$-memorization algorithm evolves iterates $w$ according to Eq.~\eqref{eq:sgd-corrected} and selects in each iteration a random index set $J$ of memory locations to update according to
\begin{align}
\alpha_j^+ := 
\begin{cases} 
f'_j(w) & \text{if $j \in J$} \\
\alpha_j & \text{otherwise,}
\end{cases}
\label{eq:memorization}
\end{align}
such that any $j$ has the \emph{same} probability of $q/n$ of being updated, i.e. $\forall j$, $\sum_{J \ni j} \Prob\{ J \} = \frac {q}{n}$.
\end{definition}
Note that $q$-SAGA  and the above  SVRG are special cases. For $q$-SAGA: $\Prob\{J\}=1/\binom n q$ if $|J|=q$ $\Prob\{J\}=0$ otherwise. For SVRG: $\Prob\{ \emptyset\} = 1-q/n$, $\Prob\{ [1:n]\}=q/n$, $\Prob\{J\}=0$, otherwise. 

\paragraph{${\mathcal N}$-SAGA}
Because we need it in Section \ref{sect:sharing}, we will also define an algorithm, which we call ${\cal N}$-SAGA, which makes use of a neighborhood system ${\cal N}_i \subseteq \{1,\dots,n\}$ and which selects neighborhoods uniformly, i.e.~$\Prob\{{\cal N}_i\}= \frac 1n$.  Note that Definition \ref{def:memorization} requires $| \{i: j \in {\cal N}_i\}|=q$ $(\forall j)$. 

Finally, note that for generalized linear models where $f_i$ depends on $x_i$ only through $\langle w, x_i \rangle$, we get $f_i'(w) = \xi'_i(w) x_i$, i.e.~the update direction is determined by $x_i$, whereas the effective step length depends on the derivative of a scalar function $\xi_i(w)$. As used in~\cite{schmidt2013minimizing}, this leads to significant memory savings as one only needs to store the scalars $\xi'_i(w)$ as $x_i$ is always \textit{given} when performing an update. 

%
\subsection{Analysis}
%
\paragraph*{Recurrence of Iterates}

The evolution equation \eqref{eq:sgd-corrected} in expectation implies the recurrence (by crucially using the unbiasedness condition $\E g_i(w) =f'(w)$):
\begin{align}
\E \| w^+\! -\! w^* \|^2 & %
= \| w - w^* \|^2 - 2 \gamma \langle f'(w), w-w^*\rangle + \gamma^2 \E \| g_i(w) \|^2 \,.
\label{eq:recurrence}
\end{align}
Here and in the rest of this paper, expectations are always taken only with respect to $i$ (conditioned on the past).   We utilize a number of bounds (see \cite{defazio2014}), which exploit strong convexity of $f$ (wherever $\mu$ appears) as well as Lipschitz continuity of the $f_i$-gradients (wherever $L$ appears):
\begin{align}
 \langle f'(w), w-w^* \rangle  & \ge  f(w) - f(w^*)+ \tfrac \mu 2  \| w-w^*\|^2\,,
\label{eq:naive-strong-convexity} \\
\E \|g_i(w)\|^2 & \le 2  \E \| f'_i(w) - f'_i(w^*) \|^2 + 2 \E \| \bar \alpha_i - f_i'(w^* )\|^2 \,,
\label{eq:beta-split} 
\\
%
%
\| f'_i(w) - f_i'(w^*) \|^2 & \le 2L h_i(w), \quad h_i(w)  := f_i(w) - f_i(w^*)-  \langle w-w^*, f_i'(w^*) \rangle
\label{eq:smoothness-bound}
\,, \\
\! \E \| f_i'(w) \!-\! f_i'(w^*)\|^2 & \le 2L f^\delta(w), \quad f^\delta(w) := f(w) - f(w^*) \,, 
\label{eq:expected-smoothness}
\\
\E \| \bar \alpha_i - f_i'(w^*)\|^2 & = \E \| \alpha_i - f_i'(w^*) \|^2 - \| \bar \alpha\|^2 \le \E \| \alpha_i - f_i'(w^*) \|^2 .
\label{eq:alpha-simplification}
\end{align}
%
%
Eq.~\eqref{eq:beta-split} can be generalized~\cite{defazio2014} using $\| x \pm y\|^2 \le (1+\beta) \| x\|^2 +  (1+\beta^{-1}) \|y\|^2$ with $\beta>0$. However for the sake of simplicity, we sacrifice tightness and choose $\beta=1$.
%
Applying all of the above yields:
\begin{lemma} 
\label{lemma:w-recurrence}
For the iterate sequence of any algorithm that evolves solutions according to Eq.~\eqref{eq:sgd-corrected}, the following holds for a single update step, in expectation over the choice of $i$:
\begin{equation*}
\| w - w^*\|^2  - \E \| w^+ - w^*\|^2 \ge \,\, \gamma \mu \| w- w^*\|^2  - 2 \gamma^2 \E \| \alpha_i - f_i'(w^*)\|^2  + \left( 2\gamma - 4\gamma^2 L \right) f^\delta(w) \,.
\end{equation*}
\end{lemma}
%
All proofs are deferred to the Appendix. 

\paragraph*{Ideal and Approximate Variance Correction}
 
Note that in the ideal case of $\alpha_i = f'_i(w^*)$, we would immediately get a condition for a contraction by choosing $\gamma = \frac{1}{2L}$, yielding a  rate of $1- \rho$ with $\rho = \gamma \mu = \frac{\mu}{2L}$, which is half the inverse of the condition number $\kappa:=L/\mu$.

%
%

How can we further bound  $\E \| \alpha_i - f_i'(w^*) \|^2$ in the case of ``non-ideal" variance-reducing SGD? A key insight is that for  memorization algorithms, we can apply the smoothness bound in Eq.~\eqref{eq:smoothness-bound}
\begin{align}
\| \alpha_i-f'_i(w^*)\|^2 
= \| f'_i(w^{\tau_i}) -f'_i(w^*)\|^2 
\le 2L  h_i(w^{\tau_i}), \quad \text{(where $w^{\tau_i}$ is old $w$)} \,.
\label{eq:alpha-bound}
\end{align}
Note that if we only had approximations $\beta_i$ in the sense that $\| \beta_i - \alpha_i \|^2 \le \epsilon_i$  (see Section \ref{sect:sharing}), then we can use $\|x-y\| \leq 2 \|x\| + 2 \|y\|$ to get the somewhat worse bound:
\begin{align}
\| \beta_i-f'_i(w^*)\|^2  \le 2\| \alpha_i-f'_i(w^*)\|^2  + 2 \| \beta_i - \alpha_i\|^2
\le 4L  h_i(w^{\tau_i})  + 2 \epsilon_i.
\label{eq:alpha-tilde-bound}
\end{align}


\paragraph*{Lyapunov Function}

Ideally, we would like to show that for a suitable choice of $\gamma$, each iteration results in a contraction  $\E \| w^+ - w^*\|^2 \le (1-\rho) \| w - w^*\|^2$, where $0 < \rho \leq 1$. However, the main challenge arises from the fact that the quantities $\alpha_i$ represent stochastic gradients from previous iterations. This requires a somewhat more complex proof technique. Adapting the Lyapunov function method from \cite{defazio2014}, we  define upper bounds $H_i \geq \| \alpha_i - f'_i(w^*)\|^2$ such that $H_i \to 0$ as $w \to w^*$. We start with $\alpha^0_i\!=\!0$ and (conceptually) initialize $H_i = \|f'_i(w^*)\|^2$, and then update $H_i$ in sync with $\alpha_i$,
%
\begin{align}
H_i^+ := 
\begin{cases}
2L \, h_i(w) & \text{if $\alpha_i$ is updated} \\
H_i & \text{otherwise}
\end{cases} 
\label{eq:recurrence-h1}
\end{align} 
so that we always maintain valid bounds $\| \alpha_i - f_i'(w^*) \|^2 \le H_i$ and $\E\| \alpha_i - f_i'(w^*) \|^2 \le \bar H$ with $\bar H := \frac 1n \sum_{i=1}^n H_i$. The $H_i$ are quantities showing up in the analysis, but need  \textit{not} be computed. We now define a $\sigma$-parameterized family of Lyapunov functions\footnote{This is a simplified version of the one appearing in~\cite{defazio2014}, as we assume $f'(w^*)=0$ (unconstrained regime).}
\begin{align} \label{eq:lyapunov_fct}
\L_\sigma(w,H) := \| w- w^*\|^2 + S \sigma \,  \bar H, 
\quad \text{with} \; \; S:= \left( \frac{\gamma n}{L q} \right) \quad \text{and}\quad  0\leq  \sigma \leq 1\,.
\end{align}
In expectation under a random update, the Lyapunov function $\L_\sigma$ changes as $ \E \L_\sigma(w^+,H^+)  = \E \| w^+ - w^*\|^2  + S \sigma\, \E \bar H^+$. 
%
We can readily apply Lemma \ref{lemma:w-recurrence} to bound the first part. The second part is due to \eqref{eq:recurrence-h1}, which mirrors the update of the $\alpha$ variables.  
By crucially using the property that any $\alpha_j$ has the same probability of being updated in~\eqref{eq:memorization}, we
get the following result: 
\begin{lemma} 
For a uniform $q$-memorization algorithm, it holds that 
\begin{align}
\E \bar H^+ = \left( \frac{n-q}{n} \right) \bar H + \frac{2Lq}{n} \, f^\delta(w) .
\label{eq:h-recurrence}
\end{align}
\label{lemma:hrecurrence2}
\end{lemma}
\vspace{-3.5mm}
Note that in expectation the shrinkage does not depend on the location of previous iterates $w^\tau$ and the new increment is  proportional to the sub-optimality of the current iterate $w$.  Technically, this is how the possibly complicated dependency on previous iterates is dealt with in an effective manner. 

\paragraph*{Convergence Analysis}


We first state our main Lemma about Lyapunov function contractions:
\begin{lemma}
\label{lemma:lyapunov}
Fix $c \in (0;1]$ and $\sigma \in [0;1]$ arbitrarily. For any uniform $q$-memorization algorithm with sufficiently small step size $\gamma$ such that
\begin{align}
\gamma \le \frac{1}{2L} \min \left\{\frac{K\sigma }{K + 2c\sigma}, 1-\sigma \right\}, \quad 
\text{and} \quad K:=  \frac{4qL}{n \mu},
\label{eq:gamma-admissible}
\end{align}
we have that 
\begin{align}
\E \L_\sigma(w^+,H^+)  \le (1-\rho) \L_\sigma(w,H),\quad \text{with} \quad \rho := c \mu \gamma.
\end{align}
Note that $\gamma < \frac 1{2L} \max_{\sigma \in [0,1]} \min\{ \sigma ,  1-\sigma\} = \frac{1}{4L}$ (in the $c \to 0$ limit).
\end{lemma}
%
By maximizing the bounds in Lemma \ref{lemma:lyapunov} over the choices of $c$ and $\sigma$, we obtain our main result that provides guaranteed geometric rates for all step sizes up to $\frac 1 {4L}$. 
\begin{theorem}
\label{theorem:main}
Consider  a uniform $q$-memorization algorithm. For any step size $\gamma = \frac a {4L}$ with $a<1$, the algorithm converges at a geometric rate of at least $(1-\rho(\gamma))$ with 
\begin{align}
\rho(\gamma) = \frac{q}{n} \cdot \frac{1 - a}{1-a/2} 
= \frac{\mu}{4L} \cdot \frac{K (1 - a)}{1-a/2}, \;\;  \text{if} \;\gamma \geq \gamma^*(K), \;\; 
\text{otherwise} \;\; \rho(\gamma) = \mu \gamma   \;\; 
\end{align}
where 
\begin{align}
\gamma^*(K) :=  \frac {a^*(K)} {4 L}, \quad 
a^*(K) := \frac{2K}{1+K + \sqrt{1+K^2}},
%
\quad K:=  \frac{4qL}{n \mu} = \frac{4q}{n} \kappa \, .
\label{eq:gammalk}
\end{align}
\end{theorem}
We would like to provide more insights into this result. 
\begin{corollary}
\label{corollary:opt-rho}
In Theorem \ref{theorem:main}, $\rho$ is maximized for $\gamma = \gamma^*(K)$. We can write $\rho^*(K) = \rho(\gamma^*)$ as 
\begin{align}
\rho^*(K) = 
\frac \mu {4L} a^*(K) %
= \frac{q}{n}\frac{a^*(K)}{K} = \frac qn \left[ \frac{2}{1+K + \sqrt{1 + K^2} } \right]
\end{align}
In the big data regime $\rho^* = \frac qn (1-\frac 12 K + O(K^3))$, whereas in the ill-conditioned case $\rho^* = \frac \mu{4L} (1 - \frac {1}{2}K^{-1} + O(K^{-3}))$.
\end{corollary}
The guaranteed rate is bounded by $\frac{\mu}{4L}$ in the regime where the condition number dominates $n$  (large $K$)  and by $\frac qn$ in the opposite regime of large data (small $K$). Note that if $K \le 1$, we have $\rho^* = \zeta  \frac qn$ with  $\zeta \in [2/(2 + \sqrt{2}); 1]  \approx [0.585;1]$. So for $q \le n \frac{\mu}{4L}$, it pays off to increase freshness as it affects the rate proportionally.  In the ill-conditioned regime ($\kappa > n$), the influence of $q$ vanishes. 

Note that for $\gamma \ge \gamma^*(K)$, $ \gamma \to \frac1{4L}$ the rate decreases monotonically, yet the decrease is only minor. With the exception of a small neighborhood around $\frac{1}{4L}$, the entire range of $\gamma \in [\gamma^*; \frac{1}{4L})$ results in very similar rates. Underestimating $\gamma^*$  however leads to a (significant) slow-down by a factor $\gamma/\gamma^*$.

As the optimal choice of $\gamma$ depends on $K$, i.e.~$\mu$, we would prefer step sizes that are $\mu$-independent, thus giving rates that adapt to the local curvature  (see \cite{schmidt2013minimizing}). It turns out that by choosing a step size that maximizes $\min_K \rho(\gamma)/\rho^*(K)$, we obtain a $K$-agnostic step size with rate off by at most $1/2$:
\begin{corollary}
\label{corollary:universal}
Choosing $\gamma = \frac{2 -\sqrt{2} }{4L}$, leads to $\rho(\gamma) \ge (2 - \sqrt{2}) \rho^*(K) > \frac 12 \rho^*(K)$ for all $K$.
\end{corollary}
To gain more insights into the trade-offs for these fixed large universal step sizes, the following corollary details the range of rates obtained:
\begin{corollary}
Choosing $\gamma = \frac{a}{4L}$ with $ a<1$ yields $\rho = \min\{ \frac {1-a}{1 - \frac 12 a} \frac qn, \frac{a}{4} \frac \mu L \}$. In particular, we have for the choice $\gamma = \frac{1}{5L}$ that $\rho = \min\{ \frac {1}{3} \frac qn, \frac 15 \frac \mu L \}$ (roughly matching the rate given in~\cite{defazio2014} for $q=1$).
\label{corollary:universal2}
%
%
%
\end{corollary}




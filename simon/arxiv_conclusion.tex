
\section{Conclusion and future work}
\vspace{-2mm}
We have described a joint model that relates object states and manipulation actions.
Given a set of input videos, our model both localizes the manipulation actions \emph{and} discovers the corresponding object states. 
We have demonstrated that our joint approach improves performance of both object state recognition and action recognition. 
More generally, our work provides evidence that actions should be modeled in the larger context of goals and effects.
Finally, our work opens up the possibility of Internet-scale learning of manipulation actions from narrated video sequences.
%

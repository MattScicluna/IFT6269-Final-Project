%

\section{Conclusion}


We have generalized variance reduced SGD methods under the name of memorization algorithms and presented a corresponding analysis, which commonly applies to all such methods. We have investigated in detail the range of safe step sizes with their corresponding geometric rates as guaranteed by our theory. This has delivered a number of new insights, for instance about the trade-offs between small ($\sim \frac 1n$) and large ($\sim \frac 1{4L})$ step sizes in different regimes as well as about the role of the freshness of stochastic gradients evaluated at past  iterates. 


We have also investigated and quantified the effect of additional errors in the variance correction terms on the convergence behavior. Dependent on how $\mu$ scales with $n$, we have shown that such errors can be tolerated, yet, for small $\mu$, may have a negative effect on the convergence rate as much smaller step sizes are needed to still guarantee convergence to a small region. 
%
We believe this result to be relevant for a number of approximation techniques in the context of variance reduced SGD. 

Motivated by these insights and results of our analysis, we have proposed ${\epsilon \cal N}$-SAGA, a modification of SAGA that exploits similarities between training data points by defining a neighborhood system. Approximate versions of  per-data point gradients are then computed by sharing information among neighbors. This opens-up the possibility of variance-reduction in a streaming data setting, where each data point is only seen once. We believe this to be a promising direction for future work. 

Empirically, we have been able to achieve consistent speed-ups for the initial phase of regularized risk minimization. This shows that approximate computations of variance correction terms constitutes a promising approach of trading-off computation with solution accuracy. 
%

\paragraph{Acknowledgments} We would like to thank Yannic Kilcher, Martin Jaggi, R\'{e}mi Leblond and the anonymous reviewers for helpful suggestions and corrections. 


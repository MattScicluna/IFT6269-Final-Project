\vspace{-2mm}
\section{Background}
\vspace{-3mm}

{\bf Markov Random Fields}:
MRFs are undirected probabilistic graphical models 
where the probability distribution factorizes over cliques in the graph.
We consider marginal inference on pairwise MRFs with $N$ random variables $X_1,X_2,\ldots,X_N$
where each variable takes discrete states $x_i\in \VAL_i$. Let $G=(V,E)$ be the Markov 
network with an undirected edge $\edges$ for every two 
variables $X_i$ and $X_j$ that are connected together. Let $\nbrs{i}$ refer to the 
set of neighbors of variable $X_i$. We organize the edge log-potentials $\theta_{ij}(x_i, x_j)$
for all possible values of $x_i \in \VAL_i$, $x_j \in \VAL_j$ in the vector $\btheta_{ij}$,
and similarly for the node log-potential vector $\btheta_i$. We regroup these in the overall
vector~$\vtheta$. We introduce a similar grouping for the marginal vector $\vmu$:
for example, $\mu_i(x_i)$ gives the coordinate of the marginal vector corresponding
to the assignment $x_i$ to variable $X_i$.

{\bf Tree Re-weighted Objective} \citep{wainwright2005new}:
%
%
Let $Z(\vtheta)$ be the partition function for the MRF and $\MARG$ be the set of all valid marginal vectors (the marginal polytope). The maximization of the TRW objective gives the following
upper bound on the log partition function:
\begingroup 
\vspace{-1mm}
\setlength\belowdisplayskip{-12pt} %
\begin{equation}
\begin{split}
\label{eqn:upperBoundPartition}
\log Z(\vtheta) &\leq \min_{\vrho \in \TREEPOL} \max_{\vmu \in \MARG}\quad \underbrace{\brangle{\vtheta,\vmu} + \trwent}_{\TRW},
\end{split}
\end{equation}
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
where the TRW entropy is:
\endgroup
\begin{equation}
	\label{eqn:trw_entropy_like_bethe}
	\trwent := \sum_{i\in V}(1-\hspace{-3mm}\sum_{j\in \nbrs{i}}\hspace{-2mm}\rho_{ij})H(\bmu_i) 
	+\hspace{-2mm} \sum_{(ij)\in
          E}\hspace{-1mm}\rho_{ij}H(\bmu_{ij}), \quad H(\bmu_i) :=   -\sum_{x_i} \mu_i(x_i)\log\mu_i(x_i).
\end{equation}
%
$\TREEPOL$ is the spanning tree polytope, the convex hull
of edge indicator vectors of all possible spanning trees of the graph. 
Elements of $\vrho\in \TREEPOL$ specify the probability of an
edge being present under a specific distribution over spanning trees.
$\MARG$ is difficult to optimize over, and most
TRW algorithms optimize over a relaxation called
the local consistency polytope $\LOCAL \supseteq \MARG$:
\vspace{-1mm}
\begin{equation*}
%
%
\resizebox{\hsize}{!}{$\LOCAL:=\left\{\vmu\geq \bm{0},\;\sum_{x_i} \mu_i(x_i) = 1 \;\forall i\in V,\;\sum_{x_i} \mu_{ij}(x_i,x_j) = \mu_j(x_j), \sum_{x_j} \mu_{ij}(x_i,x_j) = \mu_i(x_i)\;\;\forall \edges \right\}.$}
%
%
%
%
%
%
\end{equation*}
\vspace{-3mm}

%
%
%
%
%
%

%
The TRW objective $\TRW$ is a globally concave function of $\vmu$ over $\LOCAL$,
assuming that $\vrho$ is obtained from a valid distribution over spanning
trees of the graph (i.e. $\vrho \in \TREEPOL$). 
%

{\bf Frank-Wolfe (FW) Algorithm:} 
In recent years, the Frank-Wolfe (aka conditional gradient) algorithm has gained 
popularity in machine learning \citep{jaggi2013revisiting} for the 
optimization of convex functions over compact domains (denoted $\DOM$). The algorithm is used to 
solve 
$\min_{\x\in \DOM} f(\x)$	
by iteratively finding a good descent vertex by solving the linear subproblem:
%
\begin{equation}
\label{eqn:lin_subproblem}
\sk = \argmin_{\s\in\mathcal{D}}\brangle{\nabla f(\xk), \s} \qquad \text{(FW oracle)},
\end{equation}
%
and then taking a convex step towards this vertex: $\x^{(k+1)} = (1-\stepsize) \x^{(k)} + \stepsize \sk$ for
a suitably chosen step-size $\stepsize \in [0,1]$.
The algorithm remains within the feasible set (is projection free), is invariant to affine transformations of the domain, and can be implemented in a memory efficient manner. Moreover, the FW gap $g(\x^{(k)}) := \brangle{-\nabla f(\xk), \sk - \x^{(k)}}$ provides an upper bound on the suboptimality of the iterate $\x^{(k)}$.
%
%
%
The primal convergence of the Frank-Wolfe algorithm is given by 
Thm.~$1$ in \citet{jaggi2013revisiting}, restated here for convenience: for $k\geq1$, the iterates $\xk$ satisfy:
\vspace{-3mm}
\begin{dmath}
	\label{eqn:fw_theorem_convergence_original}
f(\xk) - f(\xopt) \leq \frac{2\Cf}{k+2} ,
\end{dmath}
\vspace{-3mm}
where $\Cf$ is called the ``curvature constant''. Under the assumption 
that $\nabla f$ is $L$-Lipschitz continuous\footnote{I.e. $\|\nabla{f}(\x)-\nabla{f}(\x')\|_* \leq L \|\x-\x'\|$ for $\x, \x' \in \DOM$. Notice that the dual norm $\| \cdot \|_*$ is needed here.} on $\DOM$, we can bound it as
$\Cf \leq L \diam_{||.||}(\mathcal{D})^2$.
%
%
%
%
%
%
%
%
%

\textbf{Marginal Inference with Frank-Wolfe:}
To optimize $\max_{\vmu\in\MARG} \TRW$ with Frank-Wolfe,
the linear subproblem~\eqref{eqn:lin_subproblem} becomes $\argmax_{\vmu\in\mathcal{M}}\brangle{\tilde{\btheta},\vmu}$, where the perturbed potentials $\tilde{\btheta}$ 
correspond to the gradient of $\TRW$ with respect to $\vmu$. %
Elements of $\tilde{\btheta}$ are of the 
form $\theta_c(x_c) + K_c (1+\log\mu_c(x_c))$, evaluated at the pseudomarginals'
current location in $\mathcal{M}$, where 
$K_c$ is the coefficient of the entropy for the node/edge
term in~\eqref{eqn:trw_entropy_like_bethe}.
The FW linear subproblem here is thus equivalent to performing MAP inference in a graphical model with
potentials $\tilde{\btheta}$ \citep{belangerWorkshop2013}, as
the vertices of the marginal polytope are in 1-1 
correspondence with valid joint assignments to the random variables of the MRF,
and the solution of a linear program is always achieved at a
vertex of the polytope. 
%
%
The TRW objective does not have a Lipschitz continuous gradient over $\MARG$, and so standard convergence proofs for Frank-Wolfe do not hold.
%

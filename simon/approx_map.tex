\section{Bounding $\log Z$ with Approximate MAP Solvers\label{sec:approx_map_logz}}

Suppose that we use an approximate MAP solver for line 7 of Algorithm \ref{alg:algInfadaptive}.
We show in this section that if the solver returns an {\em upper bound} on the value of the MAP
assignment (as do branch-and-cut solvers for integer linear programs), we can use this to get an upper bound on $\log Z$.
%
%
For notational consistency, we consider using Algorithm
\ref{alg:algInfadaptive} for $\min_{\x\in\DOM}f(\x)$, where
$f(\x) = -\TRW$ is convex, $\x=\vmu$, and $\DOM = \MARG$.

The property that the duality gap may be used as a certificate of optimality \citep{jaggi2013revisiting} gives us:
\begin{equation}
	\label{eqn:dual_gap_cert}
f(\bm{x}^*)\geq\fk-\gk\implies -f(\bm{x}^*)\leq -\fk + \gk.
\end{equation}
Adding the gap onto the TRW objective yields an upper bound on the optimum (which from Equation \ref{eqn:upperBoundPartition} is an
upper bound on $\log Z$), i.e. $\log Z\leq -f(\bm{x}^*)$. From our definition of the duality gap $\gk$ (line 8 in Algorithm \ref{alg:algInfadaptive}) and \eqref{eqn:dual_gap_cert}, we have:
\begin{align*}
\log Z \leq -f(\bm{x}^*)     &\leq -\fk + \innerProd{-\nabla \fk}{\sk-\xk} \\
&= -\fk + \underbrace{\innerProd{-\nabla \fk}{\sk}}_{\text{MAP call}} - \underbrace{\innerProd{-\nabla \fk}{\xk}}_{\text{Can be computed efficiently}},
\end{align*}
where $\sk = \argmin_{\vv\in\DOM}\innerProd{\nabla \fk}{\vv} =
\argmax_{\vv\in\DOM}\innerProd{-\nabla \fk}{\vv}$ (line~7 in Algorithm~\ref{alg:algInfadaptive}). Thus, if the approximate MAP solver returns
an upper bound $\kappa$ such that $\max_{\vv\in\DOM}\innerProd{-\nabla
  \fk}{\vv} \leq \kappa$, then we get the following upper bound on the
log-partition function:
\begin{equation}
\log Z \leq -\fk + \kappa - \innerProd{-\nabla \fk}{\xk}.
\end{equation}
%
%

For example, we could use a linear programming relaxation or a
message-passing algorithm based on dual decomposition such as
\citet{SontagEtAl_uai08} to obtain the upper bound $\kappa$.
There is a subtle but important point to note about this approach.
Despite the fact that we may use a relaxation
of $\MARG$ such as $\LOCAL$ or the cycle relaxation to compute the upper bound, we evaluate it at $\vmu^{(k)}$
that is {\em guaranteed} to be within $\MARG$. This should be
contrasted to instead optimizing over a relaxation such as $\LOCAL$ directly with Algorithm \ref{alg:algInfadaptive}. 
In the latter setting, the moment we move towards a fractional vertex (in line ~14)
we would immediately take $\vmu^{(k+1)}$ out of $\MARG$. Because
of this difference, we expect that this approach will typically result in
significantly tighter upper bounds on $\log Z$.

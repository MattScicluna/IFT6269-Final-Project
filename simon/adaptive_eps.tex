\section{Convergence with Adaptive-$\shrinkAmount$ \label{sec:theory_adaptive_eps}}

In this section, we show the convergence of the adaptive-$\shrinkAmount$ FW algorithm
to optimize a function $f$ satisfying the properties in Problem~\ref{prop:generic_fxn} and Property~\ref{prop:prop_bounded_grad} (Lipschitz gradient over $\DOMeps$ with bounded growth).

The adaptive update for $\shrinkAmount$ (given in Algorithm~\ref{alg:adaptive_update}) can
be used with the standard Frank-Wolfe optimization algorithm or also the fully corrective Frank-Wolfe (FCFW) variant. In FCFW, we ensure that every update makes more progress than a standard FW step with line-search, and thus we will show the convergence result in this section for standard FW (which also applies to FCFW). We describe the FCFW variant with approximate correction steps in Algorithm~\ref{alg:adaptive_eps}, as this is what we used in our experiments.

We first list a few definitions and lemmas that will be used for the main convergence convergence result given in Theorem~\ref{thm:convergence_adaptive_eps}.
We begin with the definitions of duality gaps that we use throughout this section. The Frank-Wolfe 
gap is our primary criterion for halting and measuring the progress of the optimization over $\DOM$.
The uniform gap is a measure of the decrease obtainable from moving towards the uniform distribution.

\begin{definition}
	\label{prop:g_k_properties_true}
	We define the following gaps:
	\begin{enumerate}
	\item The Frank-Wolfe (FW) gap is defined as:
	$\gk :=  \brangle{-\nabla f(\xk),\sk-\xk}$. 

	\item The uniform gap is defined as:
	$\gunif := \brangle{-\nabla f(\xk),\unif-\xk}$. 
	
	\item The FW gap over $\DOMeps$ is:
	$\gepsk{k} := \brangle{-\nabla f(\xk),\skeps-\xk}$. 
	\end{enumerate}
\end{definition}

The name for the uniform gap comes from the fact that the FW gap over $\DOMeps$ can be expressed as a convex combination of the FW gap over $\DOM$ and the uniform gap:
\begin{align}
\gepsk{k} &=  \brangle{-\nabla f(\xk), \,\, (1-\epsk{k}) \sk + \epsk{k} \unif-\xk} \nonumber \\
		  &=  (1-\shrinkAmount^{(k)})\gk + \shrinkAmount^{(k)} \gunif. \label{eq:gap_delta_decomposition}
\end{align}
The uniform gap represents the negative directional derivative of $f$ at $\xk$ in the direction $\unif - \xk$. When the uniform gap is negative (thus $f$ is increasing when moving towards $\unif$ from $\xk$), then the contraction is hurting progress, which explains the type of adaptive update for $\shrinkAmount$ given by Algorithm~\ref{alg:adaptive_update} where we consider shrinking $\shrinkAmount$ in this case. This enables us to crucially relate the FW gap over $\DOMeps$ with the one over $\DOM$, as given in the following lemma, using the assumption that $\shrinkAmount^{(\mathrm{init})} \leq \frac{1}{4}$.
\begin{lemma}[Gaps relationship]
	\label{prop:g_k_properties_adaptive}
	For iterates progressing as in Algorithm~\ref{alg:adaptive_eps} with adaptive update on $\shrinkAmount$ as given in Algorithm~\ref{alg:adaptive_update}, the gap over $\DOMeps$ and $\DOM$ are related as : $\gepsk{k}\geq\frac{\gk}{2}$.
\end{lemma}
\begin{proof}
	The duality gaps $\gk$ and $\gepsk{k}$ computed as defined in~\eqref{prop:g_k_properties_true} during Algorithm~\ref{alg:adaptive_eps} are related by equation~\eqref{eq:gap_delta_decomposition}.
	
We analyze two cases separately:

(1)	When $\gunif\geq 0$, for $\shrinkAmount^{(\mathrm{init})}\leq\frac{1}{4}$, we have $\gepsk{k} \geq \frac{3}{4}\gk$ as $\epsk{k} \leq \shrinkAmount^{(\mathrm{init})}$.

(2)	When $\gunif <0$, from the update rule in lines~5 to~7 in Algorithm~\ref{alg:adaptive_update}, we have
	$\epsk{k}\leq \frac{\gk}{-4\gunif}\implies \epsk{k}\gunif\geq-\frac{\gk}{4}$. Therefore, $\gepsk{k} \geq \frac{3}{4}\gk-\frac{\gk}{4}  = \frac{\gk}{2}$.

Therefore, the gap over $\DOMeps$ and $\DOM$ are related as : $\gepsk{k}\geq\frac{\gk}{2}$.
\end{proof}

Another property that we will use in the convergence proof is that $-\gunifNox$ is upper bounded for any convex function $f$:\footnote{Note that on the other hand, $\gunifNox(\x)$ might go to infinity as $\x$ gets close to the boundary of $\DOM$ as the gradient of $f$ is allowed to be unbounded. Fortunately, we only need an upper bound on $-\gunifNox$, not a lower bound.}
\begin{lemma}[Bounded negative uniform gap]
\label{prop:prop_bounded_gunif}
Let $f$ be a continuously differentiable convex function on the relative interior of $\DOM$. Then for any fixed $\unif$ in the relative interior of $\DOM$, $\exists \gunifBound$
s.t. 
\begin{equation} 
\forall \x\in\DOM, \;\; -\gunifNox(\x) = \innerProd{\nabla f(\x)}{\unif-\x} \leq \gunifBound.
\end{equation}
In particular, we can take the finite value:
\begin{equation} \label{eq:uniformBound}
	B := \| \nabla f(\unif) \|_* \diam_{\| \cdot \|} (\DOM)
\end{equation}
\end{lemma}
\begin{proof}
As $f$ is convex, its directional derivative is a monotone increasing function in any direction. Let $\unif$ and $\x$ be points in the relative interior of $\DOM$; then their gradient exists and we have by the monotonicity property:
\begin{align*}
\innerProd{\nabla f(\unif) - \nabla f(\x)}{\unif-\x} \geq 0 \\
\implies \innerProd{\nabla f(\unif)}{\unif-\x} \geq \innerProd{\nabla f(\x)}{\unif-\x} . 
\end{align*}
This inequality is valid for all $\x$ in the relative interior of $\DOM$, and can be extended to the boundary by taking limits (with potentially the RHS become minus infinity, but this is not a problem). Finally, by the definition of the dual norm (generalized Cauchy-Schwartz), we have $ \innerProd{\nabla f(\unif)}{\unif-\x} \leq \| \nabla f(\unif) \|_* \| \unif - \x \| \leq \| \nabla f(\unif) \|_*  \diam_{\| \cdot \|} (\DOM)$.
\end{proof}

Finally, we need a last property of Algorithm~\ref{alg:adaptive_eps} that allows us to bound the amount of perturbation $\epsk{k}$ of the polytope
at every iteration as a function of the sub-optimality over $\DOM$.  
\begin{lemma}[Lower bound on perturbation]
	\label{prop:eps_k_properties}
	Let $B$ be a bound such that $-8\gunifNox(\x)\leq \gunifBound$ for all $\x \in \DOM$ (given by Lemma~\ref{prop:prop_bounded_gunif}). Then at every stage of Algorithm~\ref{alg:adaptive_eps}, we have that:
	$$
		\epsk{\mathrm{init}} \geq \epsk{k}\geq\min\left\{\frac{\hk}{\gunifBound},\epsk{\mathrm{init}}\right\},
    $$
	where $\epsk{\mathrm{init}}$ is the initial value of $\shrinkAmount$ and $\hk := f(\xk)-f(\x^*)$ is the sub-optimality of the iterate.
\end{lemma}

\begin{proof}
	When defining $\epsk{k}$ in step 10 of Algorithm~\ref{alg:adaptive_eps}, we either preserve the value of $\epsk{k-1}$ or if we update it, then by the lines~6 and~7 of Algorithm~\ref{alg:adaptive_update}, we have $\epsk{k} \geq \frac{\tilde{\shrinkAmount}}{2} = \frac{1}{2} \frac{\gk}{-4\gunif} \geq  \frac{\gk}{\gunifBound}$ (by using $\gunif < 0$ in this case). Since $\gk\geq\hk$ (the FW gap always upper bounds the suboptimality), we conclude $\epsk{k} \geq \min\{\frac{h_k}{\gunifBound}, \epsk{k-1}\}$. 
	Unrolling this recurrence, we thus get:
	$$\epsk{k} \geq \min\left\{ \min_{0\leq l \leq k} \frac{\hkarg{l}}{\gunifBound}, \, \, \epsk{\mathrm{init}}\right\}
	=  \min\left\{ \frac{\hk}{\gunifBound}, \epsk{\mathrm{init}}\right\}. $$
	For the last equality, we used the fact that $h_k$ is non-increasing since Algorithm~\ref{alg:adaptive_eps} decreases the objective at every iteration (using the line-search in step~14).
\end{proof}

We now bound the generalization of a standard recurrence that will arise in the proof of convergence. This is a generalization of the technique used in~\citetsup{teo2007erm} (also used in the context of Frank-Wolfe in the proof of Theorem C.4 in \citet{lacoste2012block}). The basic idea is that one can bound a recurrence inequality by the solution to a differential equation. 
We provide a detailed proof of the 
bound for completeness here. 
\begin{lemma}[Recurrence inequality solution]
	\label{lem:differential_bound}
	Let $1< a \leq b$. Suppose that $\h_k$ is any non-negative sequence that satisfies the recurrence inequality: 
	$$\h_{k+1}\leq \h_k-\frac{1}{b C_0} (\h_k)^a \qquad \text{with initial condition} \quad\h_0^{a-1}\leq C_0.$$
	Then $\h_k$ is strictly decreasing (unless it equals zero) and can be bounded for $k \geq 0$ as: 
	$$\h_{k}\leq \left(\frac{C_0}{(\frac{a-1}{b})k+1}\right)^{\frac{1}{a-1}}$$
\end{lemma}

\begin{proof}
Taking the continuous time analog of the recurrence inequality, we consider
the differential equation:
$$\frac{\del h}{\del t} = \frac{-h^a}{bC_0} \qquad \text{with initial condition} \quad h(0)=C_0^{\frac{1}{a-1}}.$$
Solving it:
	\begin{equation*}
		\begin{split}
		&\frac{\del h}{\del t} = \frac{-h^a}{b C_0}\\
		\implies &\int \frac{\del h}{h^a} = \int \frac{-\del t}{b C_0}\\
		\implies &\left[\frac{-h^{1-a}}{a-1}\right]^{h(t)}_{h(0)} = -\frac{t-0}{b C_0}\\
		&\algComment{ Using the initial conditions:}\\
		\implies &\frac{-1}{h(t)^{a-1}} + \frac{1}{C_0} = \frac{-t(a-1)}{b C_0}\\
		\implies &\frac{1}{h(t)^{a-1}} = \left( (\frac{a-1}{b}) t + 1\right)\frac{1}{C_0}\\
		\implies &h(t) = \left(\frac{C_0}{ (\frac{a-1}{b}) t +1}\right)^{\frac{1}{a-1}}.
		\end{split}
	\end{equation*}
We now denote the solution to the differential equation as $\tilde{h}(t)$. Note that it is a strictly decreasing convex function 
(which could also be directly implied from the differential equation as: 
$\frac{\del^2 h}{\del t^2} = -a\underbrace{\frac{h^{a-1}}{b C_0}}_{>0} \underbrace{h'(t)}_{<0}>0$ ).
Our strategy will be to show by induction that if $\h_k\leq \tilde{h}(k)$, then $\h_{k+1}\leq \tilde{h}(k+1)$. This allows us to bound the recurrence by the solution to the differential equation. 

Assume that $\h_k\leq \tilde{h}(k)$. The base case is $\h_0\leq\tilde{h}(0)= C_0^{\frac{1}{a-1}}$, which is true by the initial condition on $\h_0$.

Consider the utility function $l(h) := h-\frac{h^a}{b C_0}$ which is maximized at $\bar{h} := \left(\frac{b C_0}{a}\right)^{\frac{1}{a-1}}$. This function can be verified to be strictly concave for $a > 1$ and therefore is increasing for $h\leq \bar{h}$. Note that the recurrence inequality can be written as $\h_{k+1} \leq l(\h_k)$.
Since $\tilde{h}$ is decreasing and that $\tilde{h}(0)) = C_0^{\frac{1}{a-1}} \leq \left(\frac{bC_0}{a}\right)^{\frac{1}{a-1}} = \bar{h}$ (the last inequality holds since $b\geq a$), we have $\tilde{h}(t) \leq \bar{h}$ for all $t \geq 0$, and so $\tilde{h}(t)$ is always in the monotone increasing region of $l$. 

From the induction hypothesis and the monotonicity of $l$, we thus get that $l(\h_k)\leq l(\tilde{h}(k))$. 

Now the convexity of $\tilde{h}(t)$ gives us $\tilde{h}(k+1)\geq \tilde{h}(k)+\tilde{h}'(k)=\tilde{h}(k)-\frac{\tilde{h}(k)^a}{b C_0} = l(\tilde{h}(k))$.
Combining these two facts with the recurrence inequality $\h_{k+1} \leq l(\h_k)$, we get:
$\h_{k+1}\leq l(\h_k) \leq l(\tilde{h}(k))\leq \tilde{h}(k+1)$, completing the induction step
and the main part of the proof.

Finally, whenever $\h_k > 0$, we have that $\h_{k+1} < \h_k$ from the recurrence inequality, and so 
$\h_k$ is strictly decreasing as claimed.
\end{proof}

Given these elements, we are now ready to state the main convergence result for Algorithm~\ref{alg:adaptive_eps}. The convergence rate goes through three stages with increasingly slower rate. The level of suboptimality $\hk$ determines the stage. We first give the high level intuition behind these stages. Recall that by Lemma~\ref{prop:eps_k_properties}, $\hk$ lower bounds the amount of perturbation $\epsk{k}$, and thus when $\hk$ is big, the function $f$ is well-behaved by Property~\ref{prop:prop_bounded_grad}. In the first stage, the suboptimality is bigger than some target constant (which implies that the FW gap is big), yielding a geometric rate of decrease of error (as is standard for FW with line-search in the first few steps). In the second stage, the suboptimality is in an intermediate regime: it is smaller than the target constant, but big enough compared to the initial $\shrinkAmount^{\mathrm{init}}$ so that $f$ is still well-behaved on $\DOM_{\epsk{k}}$. We get there the usual $O(1/k)$ rate as in standard FW. Finally, in the third stage, we get the slower $O(k^{-\frac{1}{p+1}})$ rate where the growth in $O(\shrinkAmount^{-p})$ of the Lipschitz constant of $f$ over $\DOMeps$ comes into play.
\begin{theorem}[Global convergence for adaptive-$\shrinkAmount$ variant over $\DOM$]
	\label{thm:convergence_adaptive_eps}
	Consider the optimization of $f$ satisfying the properties in Problem~\ref{prop:generic_fxn} and Property~\ref{prop:prop_bounded_grad}.  Let~$\Cconst := L \diam_{\|\cdot\|}(\DOM)^2$, where $L$ is from Property~\ref{prop:prop_bounded_grad}. Let~$B$ be the upper bound on the negative uniform gap:~$-8\gunifNox(\x)\leq \gunifBound$ for all $\x \in \DOM$, as used in Lemma~\ref{prop:eps_k_properties} (arising from Lemma~\ref{prop:prop_bounded_gunif}). Then the iterates~$\xk$ obtained by running the Frank-Wolfe updates over $\DOMeps$ with line-search with $\shrinkAmount$ updated according to Algorithm~\ref{alg:adaptive_update} (or as summarized in a FCFW variant in Algorithm~\ref{alg:adaptive_eps}), have suboptimality~$h_k$ upper bounded as:
	\begin{enumerate}
		\item $\hk\leq \left(\frac{1}{2}\right)^{k}\h_{0} +\frac{\Cconst}{\shrinkAmount_0^p}$ for $k$ such that $\hk \geq \max\{ B\shrinkAmount_0, \frac{2\Cconst}{\shrinkAmount_0^p}\}$, \\
		\item $\hk\leq\frac{2\Cconst}{\shrinkAmount_0^p}\left[\frac{1}{\frac{1}{4} (k-k_0)+1}\right]$ for $k$ such that $\gunifBound\shrinkAmount_0\leq\hk\leq\frac{2\Cconst}{\shrinkAmount_0^p}$,\\
		\item $\hk\leq \left[\frac{\max(\Cconst,\gunifBound\shrinkAmount_0^{p+1})\gunifBound^p}{\frac{p+1}{\max(8,p+2)}(k-k_1)+1}\right]^{\frac{1}{p+1}} = O(k^{-\frac{1}{p+1}})$ for $k$ such that $\hk\leq\gunifBound\shrinkAmount_0$,\\
	\end{enumerate}
	where $\shrinkAmount_0 = \epsk{\mathrm{init}}$, $h_0$ is the initial suboptimality, and $k_0$ and $k_1$ are the number of steps to reach stage~2 and~3 respectively which are bounded as: $k_0\leq \max(0,  \lceil\log_{\frac{1}{2}}\frac{\Cconst}{ h_0 \shrinkAmount_0^p}\rceil )$, 
	$k_0 \leq k_1\leq k_0 + \max\left(0,\lceil \frac{8\Cconst}{\gunifBound\shrinkAmount_0^{p+1}}\rceil -4\right)$.
	
\end{theorem}
\begin{proof}
	Let $\x_\stepsize := \xk + \stepsize \dd_k^\FW$ with $\dd_k^\FW$ defined in step~12 in Algorithm~\ref{alg:adaptive_eps}. Note that $\x_\stepsize \in \DOMeps$ with $\shrinkAmount = \epsk{k}$ for all $\stepsize \in [0,1]$. We apply the Descent Lemma~\ref{lem:descent_lemma} on this update to get:
	$$f(\x_\stepsize) \leq f(\xk) + \stepsize\brangle{\nabla f(\xk),\dd_k^\FW} + \stepsize^2\frac{L \|\dd_k^\FW\|^2}{2\left(\epsk{k}\right)^p} \qquad \forall \stepsize \in [0,1].$$
	We have $L \| \dd_k^\FW \|^2 \leq \Cconst$ by assumption and $\brangle{\nabla f(\xk),\dd_k^\FW} = -\geps$ by definition. Moreover, $\xnext$ is defined to make at least as much progress than the line-search result
	$\min_{\stepsize \in [0,1]} f(\x_\stepsize)$ (line~14 and~15), and so we have:
	%
	%
	\begin{align*}
		f(\xnext) &\leq f(\xk) - \stepsize\geps + \stepsize^2\frac{\Cconst}{2 \left(\epsk{k}\right)^p} \quad \forall \stepsize \hiderel{\in} [0,1] \\
		&\leq f(\xk) - \frac{\stepsize}{2}\gk + \stepsize^2\frac{\Cconst}{2 \left(\epsk{k}\right)^p} \quad \forall \stepsize \hiderel{\in} [0,1].
	\end{align*}
	For the final inequality, we used Lemma~\ref{prop:g_k_properties_adaptive} which relates the gap over $\DOMeps$ to the gap over $\DOM$.

	Subtracting $f(\xopt)$ from both sides and using $\gk\geq\hk$ by convexity, we get:
	$$\hnext\leq\hk-\frac{\stepsize\hk}{2} + \frac{\stepsize^2\Cconst}{2 \left(\epsk{k}\right)^p}.$$

	Now, using Lemma~\ref{prop:eps_k_properties}, we have that $\epsk{k}\geq\min(\frac{\hk}{\gunifBound},\epsk{\mathrm{init}})$:
	\begin{align}
		\label{eqn:master_eqn}
		\hnext &\leq\hk-\stepsize\frac{\hk}{2} + \frac{\stepsize^2}{2}\frac{\Cconst}{\left(\min(\frac{\hk}{\gunifBound},\epsk{\mathrm{init}})\right)^p} \quad \forall \stepsize \hiderel{\in} [0,1].
	\end{align}

	We refer to \eqref{eqn:master_eqn} as the master inequality. Since we no longer have a dependance on $\epsk{k}$, we refer to $\epsk{\mathrm{init}}$ as $\shrinkAmount_0$. 
	We now follow a similar form of analysis as in the proof of Theorem~C.4 in~\citet{lacoste2012block}.
	To solve this and bound the suboptimality, we consider three stages:
	\begin{enumerate}
		\item \textbf{Stage 1:} The $\min$ in the denominator is $\shrinkAmount_0$ and $h_k$ is big: $\hk\geq \max\{ B\shrinkAmount_0, \frac{2\Cconst}{\shrinkAmount_0^p}\}$.
		\item \textbf{Stage 2:} The $\min$ in the denominator is $\shrinkAmount_0$ and $h_k$ is small: $\gunifBound\shrinkAmount_0\leq\hk\leq\frac{2\Cconst}{\shrinkAmount_0^p}$.
		\item \textbf{Stage 3:} The $\min$ in the denominator is $\frac{\hk}{\gunifBound}$, i.e.: $\hk\leq\gunifBound\shrinkAmount_0$.
	\end{enumerate}

	Since $\hk$ is decreasing, once we leave a stage, we no longer re-enter it. 
	The overall strategy for each stage is as follows. For each recurrence that we get, we select
	a $\stepsize^*$ that realizes the tightest upper bound on it. 
%
%

	Since we are restricted that 
	$\stepsize^*\in[0,1]$, we have to consider when $\stepsize^*>1$ and $\stepsize^*\leq 1$. For the former,
	we bound the recurrence obtained by substituting $\stepsize=1$ into \eqref{eqn:master_eqn}. For the latter, 
	we substitute the form of $\stepsize^*$ into the recurrence and bound the result.

	\subsection*{Stage 1}
	We consider the case where $\hk\geq\gunifBound\shrinkAmount_0$. This yields: 
	\begin{align}
		\label{eqn:case_1}
		\hnext &\leq\hk - \frac{\stepsize\hk}{2} + \frac{\stepsize^2 \Cconst}{2 \left(\shrinkAmount_0\right)^p}	
	\end{align}
	The bound is minimized by setting $\stepsize^* = \frac{\hk\shrinkAmount_0^p}{2\Cconst}$. On the other hand, the bound is only valid for $\stepsize \in [0,1]$, and thus if $\stepsize^* > 1$, i.e. $\hk > \frac{2\Cconst}{\shrinkAmount_0^p}$ (stage 1), then $\stepsize = 1$ will yield the minimum feasible value for the bound. Unrolling the recursion~\eqref{eqn:case_1} for $\stepsize = 1$ during this stage (where $\h_l > \frac{2\Cconst}{\shrinkAmount_0^p}$ for $l < k$ as $\h_k$ is decreasing), we get:
	
	\begin{align}
		\hnext &\leq \frac{\hk}{2} + \frac{\Cconst}{2\shrinkAmount_0^p} \nonumber \\	
	&\leq \frac{1}{2}\left(\frac{\hprev}{2} + \frac{\Cconst}{2\shrinkAmount_0^p}\right) + \frac{\Cconst}{2\shrinkAmount_0^p} \nonumber \\
	&\leq \left(\frac{1}{2}\right)^{k+1} \h_0 + \frac{\Cconst}{2\shrinkAmount_0^p}\underbrace{\sum_{l=0}^{k}\left(\frac{1}{2}\right)^l}_{\leq\sum_{l=0}^{\infty}\left(\frac{1}{2}\right)^{l} = 2} \nonumber \\
	\text{thus} \quad \h_k &\leq \left(\frac{1}{2}\right)^{k} \h_0 + \frac{\Cconst}{\shrinkAmount_0^p} \label{eqn:hk_stage1},
	\end{align}
	giving the bound for the iterates in the first stage.

%
%
%
	
	We can compute an upper bound on the number of steps it takes to reach a suboptimality of $\frac{2\Cconst}{\shrinkAmount_0^p}$ by looking at the minimum $k$ which ensures that the bound in~\eqref{eqn:hk_stage1} becomes smaller than $\frac{2\Cconst}{\shrinkAmount_0^p}$, yielding
	$k_{\max} = \max(0,\lceil\log_{\frac{1}{2}}\frac{\Cconst}{\h_0\shrinkAmount_0^p}\rceil)$. Therefore, let $k_0\leq k_{\max}$ be the 
	first $k$ such that $\hk\leq\frac{2\Cconst}{\shrinkAmount_0^p}$.

	\subsection*{Stage 2}
	For this case analysis, we refer to $k$ as being the iterations \emph{after} $k_0$ steps have elapsed. I.e. if $k_{\mathrm{new}} := k-k_0$, then we refer to $k_{\mathrm{new}}$ as $k$ moving forward. 
	
	In stage~2, we suppose that $\gunifBound\shrinkAmount_0\leq\hk\leq\frac{2\Cconst}{\shrinkAmount_0^p}$. This means that $\stepsize^* = \frac{\hk\shrinkAmount_0^p}{2\Cconst}\leq 1$.
	
	Substituting $\stepsize=\stepsize^*$ into \eqref{eqn:case_1} yields: $\hnext\leq\hk -\hk^2\frac{\shrinkAmount_0^p}{8\Cconst}$.

	Using the result of Lemma~\ref{lem:differential_bound} with $a=2$, $b=4$ and $C_0 = \frac{2\Cconst}{\shrinkAmount_0^p}$, we get the bound:	
	$$\hk\leq\frac{\frac{2\Cconst}{\shrinkAmount_0^p}}{\frac{k-k_0}{4}+1}.$$
	
	It is worthwhile to point out at this juncture that the bound obtained for stage~$2$ is the same as the one for regular Frank-Wolfe, but with a factor of $4$ worse due to the factor of $\frac{1}{2}$ in front of the FW gap which appeared due to Lemma~\ref{prop:g_k_properties_adaptive}.

	\subsection*{Stage 3}
	Here, we suppose $\hk\leq\gunifBound\shrinkAmount_0$. We can compute a bound on the number of steps $k_1$ needed get to stage~3 by looking at the number of steps it takes for the bound in stage~2 to becomes less than $\gunifBound\shrinkAmount_0$:
	\begin{equation*}
		\begin{split}
		\frac{2\Cconst}{\shrinkAmount_0^p} \left[\frac{4}{k_1-k_0+4}\right] \leq \gunifBound\shrinkAmount_0\\
		\left[\frac{1}{k_1-k_0+4}\right] \leq \frac{\gunifBound\shrinkAmount_0^{p+1}}{8\Cconst}\\
		k_1 \geq k_0 + \lceil\frac{8\Cconst}{\gunifBound\shrinkAmount_0^{p+1}}\rceil - 4.
		\end{split}
	\end{equation*}

	As before, moving forward, our notation on $k$ represents the number of steps taken after $k_1$ steps. 
	
	Then, the master inequality~\eqref{eqn:master_eqn} becomes: 
		$$\hnext\leq \hk -\frac{\stepsize}{2}\hk + \frac{\stepsize^2\Cconst\gunifBound^p}{2\hk^p}.$$

	To simplify the rest of the analysis, we replace $\Cconst\gunifBound^p$ with $F := \max(\gunifBound\shrinkAmount_0^{p+1},\Cconst)\gunifBound^p$. We then get the bound:
	\begin{equation}
		\label{eqn:case_2_mod}
		\hnext\leq \hk-\frac{\stepsize}{2}\hk + \frac{\stepsize^2F}{2\hk},
	\end{equation}
	which is minimized by setting $\stepsize^*:=\frac{\hk^{p+1}}{2F}$.  
	Since $F\geq \gunifBound^{p+1}\shrinkAmount_0^{p+1}$ (by construction) and $\hk^{p+1}\leq (\gunifBound\shrinkAmount_0)^{p+1}$ (by the condition to be in stage~3),
	we necessarily have that $\stepsize^*\leq 1$. We chose the value of $F$ to avoid
	having to consider the possibility $\stepsize^* > 1$ as we did in the distinction between stage~1 and stage~2.
	
	Hence, substituting $\stepsize = \stepsize^*$ in~\eqref{eqn:case_2_mod}, we get:
	$$\hnext\leq\hk-\frac{\hk^{p+2}}{8F}.$$

	Using the result of Lemma~\ref{lem:differential_bound} with $a=p+2$, $b=\max(8,p+2)$ and $C_0 = F$, we get the bound:	
	$$\hk\leq\left[\frac{\max(\Cconst,\gunifBound \shrinkAmount_0^{p+1})\gunifBound^p}{\frac{p+1}{\max(8,p+2)}(k-k_1)+1}\right]^{\frac{1}{p+1}} = O(k^{-\frac{1}{p+1}}),$$
	concluding the proof.
\end{proof}

Interestingly, the obtained rate of $O(1/\sqrt{k})$ for $p=1$ (for the TRW objective e.g.) is the standard rate that one would get for the optimization of a general non-smooth convex function with the projected subgradient method (and it is even a lower bound for some class of first-order methods; see e.g. Section~3.2 in~\citetsup{nesterov2004lectures}). The fact that our function $f$ does not have Lipschitz continuous gradient on the whole domain brings us back to the realm of non-smooth optimization. It is an open question whether Algorithm~\ref{alg:adaptive_eps} has an optimal rate for the class of functions defined in the assumptions of Theorem~\ref{thm:convergence_adaptive_eps}.



\section{Properties of the TRW Objective \label{sec:trw_properties}}
In this section, we explicitly compute bounds for the constants appearing in the convergence statements for our fixed-$\shrinkAmount$ and adaptive-$\shrinkAmount$ algorithms for the optimization problem given by:
$$\min_{\vmu\in\MARG}-\TRW.$$ 
In particular, we compute the Lipschitz constant for its gradient over $\Meps$ (Property~\ref{prop:prop_bounded_grad}), we give a form for its modulus of continuity function $\omega(\cdot)$ (used in Theorem~\ref{thm:convergence_fixed_eps_main}), and we compute $B$, the upper bound on the negative uniform gap (as used in Lemma~\ref{prop:prop_bounded_gunif}).

\subsection{Property~\ref{prop:prop_bounded_grad} : Controlled Growth of Lipschitz Constant over $\Meps$} \label{sec:trw_bounded_lip}
%
%

We first motivate our choice of norm over $\MARG$. Recall that $\vmu$ can be decomposed into $|V| + |E|$ blocks, with one pseudo-marginal vector $\bmu_i \in \Delta_{\VAL_i}$ for each node $i \in V$, and one vector $\bmu_{ij} \in \Delta_{\VAL_i \VAL_j}$ per edge $\{i,j\} \in E$, where $\Delta_d$ is the probability simplex over $d$ values.
We let $c$ be the cliques in the graph (either nodes or edges). From its definition in~\eqref{eqn:trw_entropy_like_bethe}, $f(\vmu) := -\TRW$ decomposes as a separable sum of functions of each block only:
	\begin{equation} \label{eq:TRW_decompsed}
	f(\vmu) := -\TRW = -\sum_c \left(K_c H(\bmu_c) +\innerProd{\btheta_c}{\bmu_c}\right) =: \sum_c g_c(\bmu_c),
	\end{equation}
where $K_c$ is $(1- \sum_{j \in \nbrs{i}} \rho_{ij})$ if $c = i$ and $\rho_{ij}$ if $c = \{i,j\}$. The function $g_c$ also decomposes as a separable sum: 
\begin{equation} \label{eq:gc_function}
g_c(\bmu_c) := \sum_{x_c} K_c \mu_c(x_c) \log (\mu_c(x_c)) - \theta_c(x_c) \mu_c(x_c) =: \sum_{x_c} g_{c,x_c} (\mu_c(x_c)). 
\end{equation}
As $\MARG$ is included in a product of probability simplices, we will use the natural $\ell_\infty/\ell_1$ block-norm, i.e. $\| \vmu \|_{\infty,1} := \max_{c} \|\bmu_c \|_1$. The diameter of $\MARG$ in this norm is particularly small: $\diam_{\|\cdot \|_{\infty,1}}(\MARG) \leq 2$. The dual norm of the $\ell_\infty/\ell_1$ block-norm is the $\ell_1/\ell_\infty$ block-norm, which is what we will need to measure the Lipschitz constant of the gradient (because of the dual norm pairing requirement from the Descent Lemma~\ref{lem:descent_lemma}).


\begin{lemma}
	\label{lem:trw_bounded_lip_eps}
	Consider the $\ell_\infty/\ell_1$ norm on $\MARG$ and its dual norm $\ell_1/\ell_\infty$ to measure the gradient. Then
$\nabla \TRW$ is Lipschitz continuous over $\Meps$ with respect to these norms with Lipschitz constant 
$L_{\shrinkAmount} \leq \frac{L}{\shrinkAmount}$ with:
\begin{equation} \label{eq:TRW_L_constant} 
L \leq 4 |V| \,  \max_{ij \in E} \,\, (\VAL_i\VAL_j).
\end{equation}
\end{lemma}
\begin{proof}
	We first consider one scalar component of the separable $g_c(\bmu_c)$ function given in~\eqref{eq:gc_function} (i.e. for one $\mu_c(x_c)$ coordinate). Its derivative is $K_c (1 + \log (\mu_c(x_c)) - \theta_c(x_c)$ with second derivative $\frac{K_c}{ \mu_c(x_c)}$. If $\vmu \in \Meps$, then we have $\mu_c(x_c) \geq \shrinkAmount u_0(x_c) = \frac{\shrinkAmount}{n_c}$, where $n_c$ is the number of possible values that the assignment variable $x_c$ can take. Thus for $\vmu \in \Meps$, we have that the $x_c$-component of $g_c$ is Lipschitz continuous with constant $|K_c| n_c / \shrinkAmount$. We thus have:
	\begin{align*}
	\| \nabla g_c(\bmu_c) - \nabla g_c(\bmu'_c) \|_\infty &= \max_{x_c} \,\, | g'_{c, x_c} (\mu(x_c)) - g'_{c, x_c} (\mu'(x_c)) | \\
	&\leq \frac{|K_c| n_c}{\shrinkAmount} \|\bmu_c - \bmu_c' \|_\infty \leq \frac{|K_c| n_c}{\shrinkAmount} \|\bmu_c - \bmu_c' \|_1.
	\end{align*}
	Considering now the $\ell_1$-sum over blocks, we have:
	\begin{align*}
	\|\nabla f(\vmu) - \nabla f(\vmu') \|_{1,\infty} &= \sum_c \| \nabla g_c(\bmu_c) - \nabla g_c(\bmu'_c) \|_\infty  \\  
	&\leq \sum_c \frac{K_c n_c}{\shrinkAmount} \|\bmu_c - \bmu_c' \|_1 \leq \frac{1}{\shrinkAmount} \left(\sum_c K_c n_c \right) \| \vmu - \vmu' \|_{\infty, 1}.
	\end{align*}
	The Lipschitz constant is thus indeed $\frac{L}{\shrinkAmount}$ with $L := \sum_c |K_c| n_c$.
	Let us first consider the sum for $c \in V$; we have $K_i = 1 - \sum_{j \in \nbrs{i}} \rho_{ij}$. 
	Thus:
	\begin{align*}
	\sum_i |K_i| &\leq |V| + \sum_i \sum_{j \in \nbrs{i}} \rho_{ij} \\
				&= |V| + 2 \sum_{ij \in E} \rho_{ij} = |V| + 2 (|V| - 1) \leq 3 |V|. 	
	\end{align*}
	Here we used the fact that $\rho_{ij}$ came from the marginal probability of edges of spanning trees (and so with $|V|-1$ edges). Similarly, we have $\sum_{ij \in E} |K_{ij}| \leq |V|$. Combining these we get:
	\begin{align} \label{eq:Kc_bound}
	L = \sum_c |K_c| n_c \leq (\max_c n_c) \sum_c |K_c| \leq \max_{ij \in E} \VAL_i \VAL_j 4 |V|.
	\end{align}
\end{proof}

\begin{remark}
The important quantity in the convergence of Frank-Wolfe type algorithms is $\tilde{C} = L \diam(\MARG)^2$. We are free to take any dual norm pairs to compute this quantity, but some norms are better aligned with the problem than others. Our choice of norm in Lemma~\ref{lem:trw_bounded_lip_eps} gives $\tilde{C} \leq 16 |V| k^2$ where $k$ is the maximum number of possible values a random variable can take. It is interesting that $|E|$ does not appear in the constant. If instead we had used the $\ell_2/\ell_1$ block-norm on $\MARG$, we get that $\diam_{\ell_2/\ell_1}(\MARG)^2 = 4 (|V| + |E|)$, while the constant $L$ with dual norm $\ell_2/\ell_\infty$ would be instead $\max_c |K_c| n_c$ which is bigger than $\max_c n_c = k^2$, thus giving a worse bound. 
\end{remark}

\subsection{Modulus of Continuity Function} \label{sec:trw_weak_lip}
%
We begin by computing a modulus of continuity function for $-x \log x$ with an additive linear term.

\begin{lemma}
  \label{lem:entropy_lipschitz} Let $g(x) := -Kx\log x + \theta x$. Consider $x, x' \in [0,1]$ such that $|x-x'| \leq\sigma$, then:
  \begin{equation} \label{eqn:modulus_g}
  |g(x')-g(x)|\leq \sigma|\theta| + 2 \sigma |K| \max\{-\log(2\sigma),1\} =: \omega_g(\sigma).
  \end{equation}
\end{lemma}
\begin{proof}
	Without loss of generality assume $x'>x$, then we have two cases:

	\textbf{Case i.} 
	If $x>\sigma$, then we have that the Lipschitz constant of $g(x)$ is $L_{\sigma} = |\theta| + |K||(1+\log\sigma)|$ (obtained by taking 
	the supremum of its derivative). Therefore, we have that $|g(x')-g(x)|\leq L_{\sigma}\sigma$. Note that $L_\sigma\sigma\to 0$ when $\sigma\to 0$ even if
	$L_\sigma\to\infty$, since $L_\sigma$ grows logarithmically.

	\textbf{Case ii.}
	If $x\leq\sigma$, then $x'\leq x + \sigma \leq 2\sigma$. Therefore: 
	\begin{equation} \label{eqn:g_inequal}
	|g(x')-g(x)| \leq |K||x\log x-x'\log x'| + |\theta||x'-x|.
	\end{equation}
	Now, we have that $-x\log x$ is non-negative for $x\in [0,1]$. Furthermore, we have that 
	$-x\log x$ is increasing when $x<\exp(-1)$ and decreasing afterwards.
	First suppose that $2 \sigma \leq \exp(-1)$; then $-x'\log x' \geq -x\log x \geq 0$ which implies:
	$$
	|x\log x-x'\log x'| \leq -x' \log x' \leq - 2\sigma \log (2\sigma).
	$$
	In the case $2 \sigma > \exp(-1)$, then we have:
	$$ |x\log x-x'\log x'| \leq \max_{y \in [0,1]} \{-y \log y\} = \exp(-1) \leq 2 \sigma.
	$$
	Combining these two possibilities, we get:
	$$
	 |x\log x-x'\log x'| \leq 2\sigma \max\{- \log (2\sigma), 1\}.
	$$
	The inequality~\eqref{eqn:g_inequal} thus becomes:
	$$|g(x')-g(x)| \leq |K|2 \sigma  \max\{- \log (2\sigma), 1\} + |\theta|\sigma,$$
	which is what we wanted to prove.
\end{proof}
For small $\sigma$, the dominant term of the function $\omega_g(\sigma)$ in Lemma~\ref{lem:entropy_lipschitz} is of the 
form $C\cdot-\sigma\log\sigma$ for a constant~$C$. If we require that this be smaller than some small $\xi > 0$, then we can choose an approximate 
$\sigma$ by solving for $x$ in $-Ax\log x = \xi$ yielding $x = \exp( W_{-1} \frac{\xi}{A})$ where $W_{-1}$ is the negative 
branch of the Lambert W-function. This is almost linear and yields approximately $x = O(\xi)$ for small $\xi$. In fact, we have that $\omega_g(\sigma) \leq C' \sigma^\alpha$ for any $\alpha < 1$, and thus $g$ is ``almost'' Lipschitz continuous.

\begin{lemma}
  \label{lem:trw_obj_weak_lipschitz}
  The following function is a modulus of continuity function for the $\TRW$ objective over $\MARG$ with respect to the $\ell_\infty$ norm:
  \begin{equation}
   \omega(\sigma) := \sigma\| \theta \|_1 + 2 \sigma \tilde{K} \max\{-\log(2\sigma),1\},
   \end{equation}
   where $\tilde{K} := 4 |V| \max_{ij \in E} \VAL_i \VAL_j$.
   
   That is, for $\vmu, \vmu' \in \MARG$ with $\| \vmu' - \vmu \|_\infty \leq \sigma$, we have:
   $$ | \text{TRW}(\vmu;\vtheta,\vrho) - \text{TRW}(\vmu';\vtheta,\vrho) | \leq \omega(\sigma) .$$
\end{lemma}
\begin{proof}
%
%
$\TRW$ can be decomposed into 
functions of the form $-Kx\log x + \theta x$ (see~\eqref{eq:TRW_decompsed} and~\eqref{eq:gc_function}) and so we apply the Lemma~\ref{lem:entropy_lipschitz} element-wise. Let $c$ index the clique component in the marginal vector. 
\begin{align*}
	| \text{TRW}(\vmu;\vtheta,\vrho) - \text{TRW}(\vmu';\vtheta,\vrho) | &= 
	\sum_c \sum_{x_c} | g_{c,x_c}(\mu_c(x_c)) - g_{c,x_c}(\mu'_c(x_c))| \\
	%
	&\algComment{Using Lemma \ref{lem:entropy_lipschitz} and $\| \vmu' - \vmu \|_\infty \leq \sigma$}\\
	&\leq \sum_c \sum_{x_c} (|K_c|2 \sigma  \max\{- \log (2\sigma), 1\} + |\theta(x_c)|\sigma )\\
	&= 2\sigma  \max\{- \log (2\sigma), 1\} \sum_c |K_c|n_c +\|\theta\|_1\sigma,
\end{align*}
where we recall $n_c$ is the number of values that $x_c$ can take. By re-using the bound on $\sum_c |K_c| n_c$ from~\eqref{eq:Kc_bound}, we get the result.
\end{proof}


\subsection{Bounded Negative Uniform Gap} \label{sec:trw_bounded_unif}
%

%
%
%
%
%

\begin{lemma}[Bound for the negative uniform gap of TRW objective]
	\label{lem:bounded_gu0}
For the negative TRW objective $f(\vmu) := -\TRW$, the bound $B$ on the negative uniform gap as given in Lemma~\ref{prop:prop_bounded_gunif} for $\unif$ being the uniform distribution can be taken as:
\begin{equation} \label{eq:uniformBoundTRW}
	B =  2\sum_{c} \max_{x_c} |\theta_c(x_c)| =: 2 \|\vtheta \|_{1, \infty}
\end{equation}
\end{lemma}
\begin{proof}
	From Lemma~\ref{prop:prop_bounded_gunif}, we want to bound 
	$\| \nabla f(\unif) \|_* = \| \vtheta + \nabla_{\vmu} H(\unif;\vrho)) \|_*$. The clique entropy terms $H(\bmu_c)$ are maximized by the uniform distribution, and thus $\unif$ is a stationary point of the TRW entropy function with zero gradient. We can thus simply take $B = \|\vtheta\|_* \diam_{\| \cdot \|} (\MARG)$. By taking the $\ell_\infty / \ell_1$ norm on $\MARG$, we get a diameter of $2$, giving the given bound.
\end{proof}



\subsection{Summary}

%
%
We now give the details of suboptimality guarantees for our suggested algorithm to optimize $f(\vmu) := -\TRW$ over $\MARG$.
The (strong) convexity of the negative TRW objective is shown in \citep{wainwright2005new,london_icml15}. 
$\MARG$ is the convex hull of a finite number of vectors representing assignments to random variables and therefore a compact convex set. The entropy function is continously differentiable on the relative interior of the probability simplex, and thus the TRW objective has the same property on the relative interior of $\MARG$. Thus $-\TRW$ satisfies the properties laid out in Problem~\ref{prop:generic_fxn}.

%
%
%
%
%
%
%
%
%
%
%

\begin{lemma}[Suboptimality bound for optimizing $-\TRW$ with the fixed-$\shrinkAmount$ algorithm]
  \label{thm:convergence_fixed_eps_trw}
  For the optimization of $-\TRW$ over $\Meps$ with $\shrinkAmount\in(0,1]$, the suboptimality is bounded as: 
  
	\begin{equation} 
		\text{TRW}(\vmu^{*};\vtheta,\vrho) - \text{TRW}(\vmu^{(k)};\vtheta,\vrho) \leq \frac{2\mathcal{C}_{\shrinkAmount}}{(k+2)}+\modContinuity \left( 2\shrinkAmount \right), 
	\end{equation}
	 with $\vmu^*$ the optimizer of $\TRW$ in $\MARG$, 
	 where $C_{\shrinkAmount} \leq 16 \frac{|V|\max_{(ij)\in E}\VAL_i\VAL_j}{\shrinkAmount}$, 
	 and $\modContinuity(\sigma) = \sigma\|\vtheta\|_{1}+2\sigma \tilde{K}\max\{-\log (2\sigma),1\}$, where $\tilde{K} := 4 |V| \max_{ij \in E} \VAL_i \VAL_j$.
\end{lemma}
\begin{proof}
	Using $\diam_{\|\cdot \|_{\infty,1}}(\MARG) \leq 2$, and $L_\shrinkAmount$ from Lemma \ref{lem:trw_bounded_lip_eps},
	we can compute $C_{\shrinkAmount}\leq \diam(\MARG)^2L_{\shrinkAmount}$. 
	Lemma 
\ref{lem:trw_obj_weak_lipschitz} computes the modulus of continuity $\omega(\sigma)$.
The rate then follows directly from Theorem \ref{thm:convergence_fixed_eps}. 
\end{proof}



\begin{lemma}[Global convergence rate for optimizing $-\TRW$ with the adaptive-$\shrinkAmount$ algorithm]
	\label{thm:convergence_adaptive_eps_trw}
	Consider the optimization of $-\TRW$ over $\MARG$ with the optimum given by $\vmu^*$.
	The iterates~$\vmu^{(k)}$ obtained by running the Frank-Wolfe updates over $\Meps$ using line-search with $\shrinkAmount$ updated according to Algorithm~\ref{alg:adaptive_update} (or as summarized in a FCFW variant in Algorithm~\ref{alg:adaptive_eps}), have suboptimality~$h_k = \text{TRW}(\vmu^{*};\vtheta,\vrho) - \text{TRW}(\vmu^{(k)};\vtheta,\vrho)$ upper bounded as:
	\begin{enumerate}
		\item $\hk\leq \left(\frac{1}{2}\right)^{k}\h_{0} +\frac{\Cconst}{\shrinkAmount_0}$ for $k$ such that $\hk \geq \max\{ B\shrinkAmount_0, \frac{2\Cconst}{\shrinkAmount_0}\}$, \\
		\item $\hk\leq\frac{2\Cconst}{\shrinkAmount_0}\left[\frac{1}{\frac{1}{4} (k-k_0)+1}\right]$ for $k$ such that $\gunifBound\shrinkAmount_0\leq\hk\leq\frac{2\Cconst}{\shrinkAmount_0}$,\\
		\item $\hk\leq \left[\frac{\max(\Cconst,\gunifBound\shrinkAmount_0^{2})\gunifBound}{\frac{1}{4}(k-k_1)+1}\right]^{\frac{1}{2}} = O(k^{-\frac{1}{2}})$ for $k$ such that $\hk\leq\gunifBound\shrinkAmount_0$,\\
	\end{enumerate}
	where 

	\begin{itemize}

		\item $\shrinkAmount_0 = \epsk{\mathrm{init}} \leq \frac{1}{4}$ \\

		\item $\Cconst := 16|V|\max_{(ij)\in E}(\VAL_i\VAL_j) $\\
	
		\item $B = 16 \|\vtheta \|_{1, \infty}$
	
		\item $h_0$ is the initial suboptimality\\

		\item	$k_0$ and $k_1$ are the number of steps to reach stage~2 and~3 respectively which are bounded as: $k_0\leq \max(0,  \lceil\log_{\frac{1}{2}}\frac{\Cconst}{ h_0 \shrinkAmount_0}\rceil )$ 
		$k_0 \leq k_1\leq k_0 + \max\left(0,\lceil \frac{8\Cconst}{\gunifBound\shrinkAmount_0^{2}}\rceil -4\right)$\\
	\end{itemize}
\end{lemma}
\begin{proof}
	Using $\diam_{\|\cdot \|_{\infty,1}}(\MARG) \leq 2$,
	we bound $\Cconst\leq L\diam_{\|\cdot \|_{\infty,1}}(\MARG)^2$ with $L$ (from Property \ref{prop:prop_bounded_grad}) derived in Lemma \ref{lem:trw_bounded_lip_eps}. 
	We bound $-8g_u(\vmu^{(k)})$ (the upper bound on the negative uniform gap) using the value derived in Lemma \ref{lem:bounded_gu0}.
	The rate then follows directly from Theorem \ref{thm:convergence_adaptive_eps} using $p=1$ (see Lemma \ref{lem:trw_bounded_lip_eps} where $L_\shrinkAmount \leq \frac{L}{\delta}$).
\end{proof}
The dominant term in Lemma~\ref{thm:convergence_adaptive_eps_trw} is $\Cconst B \, k^{-\frac{1}{2}}$, with $\Cconst B  = O( \|\vtheta \|_{1, \infty} |V|)$.
We thus find that both bounds depend on norms of $\vtheta$. This is unsurprising since large potentials
drive the solution of the marginal inference problem away from the centre of $\MARG$, corresponding to regions of high entropy,
and towards the boundary of the polytope (lower entropy). Regions of low entropy correspond to smaller components
of the marginal vector, which in turn result in larger and poorly behaved gradients of $-\TRW$, which slows down the resulting optimization. 

%

\section{Introduction}

We consider a general problem that is pervasive in machine learning, namely optimization of an empirical or regularized convex risk function. Given a convex loss $l$ and a $\mu$-strongly convex regularizer $\Omega$, one aims at finding a parameter vector $w$ which minimizes the (empirical) expectation:
\begin{align}
w^* = \argmin_w f(w), \quad f(w) = \frac 1n \sum_{i=1}^n f_i(w), \quad f_i(w) := l(w, (x_i,y_i)) + \Omega(w)\,.
\label{eq:problem}
\end{align}
We assume throughout that each $f_i$ has $L$-Lipschitz-continuous gradients. Steepest descent can find the minimizer $w^*$, but requires repeated computations of full gradients $f'(w)$, which becomes prohibitive for massive data sets. Stochastic gradient descent (SGD) is a popular alternative, in particular in the context of large-scale learning~\cite{bottou2010, shalev2011}. SGD updates only involve $f'_i(w)$ for an index $i$ chosen uniformly at random, providing an unbiased gradient estimate, since $\E f'_i(w) = f'(w)$. 

It is a surprising recent finding \cite{shalev2013stochastic,johnson2013,schmidt2013minimizing,konevcny2013} that the finite sum structure of $f$ allows for significantly faster convergence in expectation. Instead of the standard $O(1/t)$ rate of SGD for strongly-convex functions, it is possible to obtain linear convergence with geometric rates. While SGD requires asymptotically vanishing learning rates, often chosen to be $O(1/t)$ \cite{robbins1951}, these more recent methods introduce  corrections that ensure convergence for constant learning rates. 

Based on the work mentioned above, the contributions of our  paper are as follows: First, we define a family of variance reducing SGD algorithms, called memorization algorithms, which includes  SAGA and SVRG as special cases, and develop a unifying  analysis technique for it. Second, we show geometric rates for all step sizes $\gamma < \frac 1{4L}$, including a universal ($\mu$-independent) step size choice, providing the first $\mu$-adaptive convergence proof for SVRG. Third, based on the above analysis, we present new insights into the trade-offs between freshness and biasedness of the corrections computed from previous stochastic gradients. Fourth, we propose a new class of algorithms that resolves this trade-off by computing corrections based on stochastic gradients at neighboring points. We experimentally show its benefits  in the regime of  learning with  a small number of epochs. 
